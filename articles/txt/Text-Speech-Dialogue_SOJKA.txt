Petr Sojka
Ivan Kopecˇek
Karel Pala
Aleš Horák (Eds.)
4
Text, Speech, 
8
2
2
1
 
I and Dialogue
A
N
L
23rd International Conference, TSD 2020
Brno, Czech Republic, September 8–11, 2020
Proceedings
 123ﬁ
Lecture Notes in Arti cial Intelligence 12284
Subseries of Lecture Notes in Computer Science
Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Yuzuru Tanaka
Hokkaido University, Sapporo, Japan
Wolfgang Wahlster
DFKI and Saarland University, Saarbrücken, Germany
Founding Editor
Jörg Siekmann
DFKI and Saarland University, Saarbrücken, GermanyMore information about this series at http://www.springer.com/series/1244č
Petr Sojka Ivan Kope ek
(cid:129) (cid:129)
š á
Karel Pala Ale Hor k (Eds.)
(cid:129)
Text, Speech,
and Dialogue
23rd International Conference, TSD 2020
–
Brno, Czech Republic, September 8 11, 2020
Proceedings
123Editors
Petr Sojka IvanKopeček
Facultyof Informatics Facultyof Informatics
MasarykUniversity MasarykUniversity
Brno, Czech Republic Brno, Czech Republic
Karel Pala AlešHorák
Facultyof Informatics Facultyof Informatics
MasarykUniversity MasarykUniversity
Brno, Czech Republic Brno, Czech Republic
ISSN 0302-9743 ISSN 1611-3349 (electronic)
Lecture Notesin Artiﬁcial Intelligence
ISBN 978-3-030-58322-4 ISBN978-3-030-58323-1 (eBook)
https://doi.org/10.1007/978-3-030-58323-1
LNCSSublibrary:SL7–ArtiﬁcialIntelligence
©SpringerNatureSwitzerlandAG2020
Thisworkissubjecttocopyright.AllrightsarereservedbythePublisher,whetherthewholeorpartofthe
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storageandretrieval,electronicadaptation,computersoftware,orbysimilarordissimilarmethodologynow
knownorhereafterdeveloped.
Theuseofgeneraldescriptivenames,registerednames,trademarks,servicemarks,etc.inthispublication
doesnotimply,evenintheabsenceofaspeciﬁcstatement,thatsuchnamesareexemptfromtherelevant
protectivelawsandregulationsandthereforefreeforgeneraluse.
Thepublisher,theauthorsandtheeditorsaresafetoassumethattheadviceandinformationinthisbookare
believedtobetrueandaccurateatthedateofpublication.Neitherthepublishernortheauthorsortheeditors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissionsthatmayhavebeenmade.Thepublisherremainsneutralwithregardtojurisdictionalclaimsin
publishedmapsandinstitutionalafﬁliations.
ThisSpringerimprintispublishedbytheregisteredcompanySpringerNatureSwitzerlandAG
Theregisteredcompanyaddressis:Gewerbestrasse11,6330Cham,SwitzerlandPreface
TheannualText,SpeechandDialogueConference(TSD),whichoriginatedin1998,is
continuing its third decade. During this time, thousands of authors from all over the
world have contributed to the proceedings. TSD constitutes a recognized platform for
the presentation and discussion of state-of-the-art technology and recent achievements
in the ﬁeld of natural language processing (NLP). It has become an interdisciplinary
forum, interweaving the themes of speech technology and language processing. The
conferenceattractsresearchersnotonlyfromCentralandEasternEuropebutalsofrom
otherpartsoftheworld.Indeed,oneofitsgoalshasalwaysbeentobringtogetherNLP
researcherswithdifferentinterestsfromdifferentpartsoftheworldandtopromotetheir
mutual cooperation.
One of the declared goals of the conference has always been, as its title suggests,
twofold: not only to deal with language processing and dialogue systems as such, but
also to stimulate dialogue between researchers in the two areas of NLP, i.e., between
textandspeechpeople.Inourview,TSD2020wasagainsuccessfulinthisrespect.We
hadthepleasuretowelcomethreeprominentinvitedspeakersthisyear:DianaMaynard
presented keynote titled “Combining Expert Knowledge with NLP for Specialised
Applications” with insight on how to combine expert human knowledge with auto-
mated NLP technologies, Joakim Nivre showed how to combine multilingual parsing
with deep learning techniques in “Multilingual Dependency Parsing from Universal
DependenciestoSesameStreet,”andPaoloRossoreportedon“MultimodalFakeNews
Detection with Textual, Visual and Semantic Information.”
This volume contains the proceedings of the 23rd TSD conference, held in Brno,
Czech Republic, in September 2020. In the review process, 54 papers were accepted
out of 110 submitted, each based on three reviews, with an acceptance rate of 49%.
Even though this year's organization was affected by the global epidemic of
COVID-19, the scientiﬁc quality of the contributions was at the highest level.
Wewouldliketothankalltheauthorsfortheeffortstheyputintotheirsubmissions,
and the members of the Program Committee and reviewers who did a wonderful job
selecting the best papers. We are also grateful to the invited speakers for their con-
tributions. Their talks provided insight into important current issues, applications, and
techniques related to the conference topics.
Special thanks are due to the members of Local Organizing Committee for their
tirelesseffortinorganizingtheconference.The pertiseofPetrSojkaresultedinthe
production of the volume that you are holding in your hands.vi Preface
We hopethat thereaders will beneﬁtfrom the results of thiseventanddisseminate
the ideas of the TSD conference all over the world. Enjoy the proceedings!
July 2020 Aleš Horák
Ivan Kopeček
Karel Pala
Petr SojkaOrganization
TSD 2020 was organized by the Faculty of Informatics, Masaryk University, in
cooperation with the Faculty of Applied Sciences, University of West Bohemia in
Plzeň. The conference webpage is located at http://www.tsdconference.org/tsd2020/.
Program Committee
Nöth, Elmar (General Chair), Germany Kotelnikov, Evgeny, Russia
Agerri, Rodrigo, Spain Král, Pavel, Czech Republic
Agirre, Eneko, Spain Kunzmann, Siegfried, Germany
Benko, Vladimir, Slovakia Ljubešić, Nikola, Croatia
Bhatia, Archna, USA Loukachevitch, Natalija, Russia
Černocký, Jan, Czech Republic Magnini, Bernardo, Italy
Dobrisek, Simon, Slovenia Marchenko, Oleksandr, Ukraine
Ekstein, Kamil, Czech Republic Matoušek, Václav, Czech Republic
Evgrafova, Karina, Russia Mihelić, France, Slovenia
Fedorov, Yevhen, Ukraine Mouček, Roman, Czech Republic
Ferrer, Carlos, Cuba Mykowiecka, Agnieszka, Poland
Fischer, Volker, Germany Ney, Hermann, Germany
Fiser, Darja, Slovenia Orozco-Arroyave,JuanRafael,Colombia
Galiotou, Eleni, Greece Pala, Karel, Czech Republic
Gambäck, Björn, Norway Pavesić, Nikola, Slovenia
Garabík, Radovan, Slovakia Piasecki, Maciej, Poland
Gelbukh, Alexander, Mexico Psutka, Josef, Czech Republic
Guthrie, Louise, UK Pustejovsky, James, USA
Haderlein, Tino, Germany Rigau, German, Spain
Hajič, Jan, Czech Republic Rothkrantz, Leon, The Netherlands
Hajičová, Eva, Czech Republic Rumshinsky, Anna, USA
Haralambous, Yannis, France Rusko, Milan, Slovakia
Hermansky, Hynek, USA Rychlý, Pavel, Czech Republic
Hlaváčová, Jaroslava, Czech Republic Sazhok, Mykola, Ukraine
Horák, Aleš, Czech Republic Scharenborg, Odette, The Netherlands
Hovy, Eduard, USA Skrelin, Pavel, Russia
Jouvet, Denis, France Smrž, Pavel, Czech Republic
Khokhlova, Maria, Russia Sojka, Petr, Czech Republic
Khusainov, Aidar, Russia Stemmer, Georg, Germany
Kocharov, Daniil, Russia Šikonja, Marko Robnik, Slovenia
Konopík, Miloslav, Czech Republic Stemmer, Georg, Germany
Kopeček, Ivan, Czech Republic Štruc, Vitomir, Slovenia
Kordoni, Valia, Germany Tadić, Marko, Croatiaviii Organization
Trmal, Jan, Czech Republic Wilks, Yorick, UK
Varadi, Tamas, Hungary Wołinski, Marcin, Poland
Vetulani, Zygmunt, Poland Wróblewska, Alina, Poland
Wawer, Aleksander, Poland Zakharov, Victor, Russia
Wiggers, Pascal, The Netherlands Žganec Gros, Jerneja, Slovenia
Additional Referees
Akhmetov, Iskander Nevěřilová, Zuzana
Ayetiran, Eniafe Festus Rossenbach, Nick
Baisa, Vít Škvorc, Tadej
Beňuš, Štefan Tihelka, Dan
Fedorov, Yevhen Ulčar, Matej
Marciniak, Małgorzata Vesnicer, Boštjan
Matoušek, Jiří Vieting, Peter
Medveď, Marek Zhou, Whei
Michel, Wilfried
Organizing Committee
AlešHorák(Co-chair),IvanKopeček,KarelPala(Co-chair),AdamRambousek(Web
System), Pavel Rychlý, Petr Sojka (Proceedings)
Sponsors and Support
The TSD conference is regularly supported by International Speech Communication
Association (ISCA). We would like to express our thanks to the Lexical Computing
Ltd., IBM Česká republika, spol. s r. o., and Amazon Alexa for their kind sponsoring
contribution to TSD 2020.Contents
Invited Papers
Combining Expert Knowledge with NLP for Specialised Applications. . . . . . 3
Diana Maynard and Adam Funk
Multilingual Dependency Parsing from Universal Dependencies
to Sesame Street. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Joakim Nivre
Multimodal Fake News Detection with Textual, Visual
and Semantic Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Anastasia Giachanou, Guobiao Zhang, and Paolo Rosso
Text
A Twitter Political Corpus of the 2019 10N Spanish Election. . . . . . . . . . . . 41
Javier Sánchez-Junquera, Simone Paolo Ponzetto, and Paolo Rosso
Mining Local Discourse Annotation for Features of Global
Discourse Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Lucie Poláková and Jiří Mírovský
Diversification of Serbian-French-English-Spanish Parallel Corpus
ParCoLab with Spoken Language Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Dušica Terzić, Saša Marjanović, Dejan Stosic, and Aleksandra Miletic
Quantitative Analysis of the Morphological Complexity
of Malayalam Language. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Kavya Manohar, A. R. Jayan, and Rajeev Rajan
Labeling Explicit Discourse Relations Using Pre-trained
Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Murathan Kurfalı
EPIE Dataset: A Corpus for Possible Idiomatic Expressions. . . . . . . . . . . . . 87
Prateek Saxena and Soma Paul
Experimenting with Different Machine Translation Models
in Medium-Resource Settings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Haukur Páll Jónsson, Haukur Barri Símonarson,
Vésteinn Snæbjarnarson, Steinþór Steingrímsson,
and Hrafn Loftssonx Contents
FinEst BERT and CroSloEngual BERT: Less Is More
in Multilingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Matej Ulčar and Marko Robnik-Šikonja
Employing Sentence Context in Czech Answer Selection. . . . . . . . . . . . . . . 112
Marek Medveď, Radoslav Sabol, and Aleš Horák
Grammatical Parallelism of Russian Prepositional Localization
and Temporal Constructions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
Victor Zakharov and Irina Azarova
Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces . . . . . . 135
Petra Barančíková and Ondřej Bojar
Next Step in Online Querying and Visualization of Word-Formation
Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
Jonáš Vidra and Zdeněk Žabokrtský
Evaluating a Multi-sense Definition Generation Model
for Multiple Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Arman Kabiri and Paul Cook
Combining Cross-lingual and Cross-task Supervision
for Zero-Shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Matúš Pikuliak and Marián Šimko
Reading Comprehension in Czech via Machine Translation
and Cross-Lingual Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Kateřina Macková and Milan Straka
Measuring Memorization Effect in Word-Level Neural Networks Probing . . . 180
Rudolf Rosa, Tomáš Musil, and David Mareček
Semi-supervised Induction of Morpheme Boundaries in Czech
Using a Word-Formation Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Jan Bodnár, Zdeněk Žabokrtský, and Magda Ševčíková
Interpreting Word Embeddings Using a Distribution Agnostic Approach
Employing Hellinger Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
Tamás Ficsor and Gábor Berend
Verb Focused Answering from CORD-19 . . . . . . . . . . . . . . . . . . . . . . . . . 206
Elizabeth Jasmi George
Adjusting BERT’s Pooling Layer for Large-Scale Multi-Label
Text Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
Jan Lehečka, Jan Švec, Pavel Ircing, and Luboš ŠmídlContents xi
Recognizing Preferred Grammatical Gender in Russian Anonymous
Online Confessions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
Anton Alekseev and Sergey Nikolenko
Attention to Emotions: Detecting Mental Disorders in Social Media . . . . . . . 231
Mario Ezra Aragón, A. Pastor López-Monroy, Luis C. González,
and Manuel Montes-y-Gómez
Cross-Lingual Transfer for Hindi Discourse Relation Identification . . . . . . . . 240
Anirudh Dahiya, Manish Shrivastava, and Dipti Misra Sharma
Authorship Verification with Personalized Language Models . . . . . . . . . . . . 248
Milton King and Paul Cook
A Semantic Grammar for Augmentative and Alternative
Communication Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Jayr Pereira, Natália Franco, and Robson Fidalgo
Assessing Unintended Memorization in Neural Discriminative
Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
Mossad Helali, Thomas Kleinbauer, and Dietrich Klakow
InvestigatingtheImpactofPre-trainedWordEmbeddingsonMemorization
in Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
AleenaThomas,DavidIfeoluwaAdelani,AliDavody,AdityaMogadala,
and Dietrich Klakow
Speech
Investigating the Corpus Independence of the
Bag-of-Audio-Words Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Mercedes Vetráb and Gábor Gosztolya
Developing Resources for Te Reo Māori Text To Speech
Synthesis System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
Jesin James, Isabella Shields, Rebekah Berriman,
Peter J. Keegan, and Catherine I. Watson
Acoustic Characteristics of VOT in Plosive Consonants Produced
by Parkinson’s Patients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
Patricia Argüello-Vélez, Tomas Arias-Vergara,
María Claudia González-Rátiva, Juan Rafael Orozco-Arroyave,
Elmar Nöth, and Maria Elke Schusterxii Contents
A Systematic Study of Open Source and Commercial
Text-to-Speech (TTS) Engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
Jordan Hosier, Jordan Kalfen, Nikhita Sharma, and Vijay K. Gurbani
Automatic Correction of i/y Spelling in Czech ASR Output. . . . . . . . . . . . . 321
Jan Švec, Jan Lehečka, Luboš Šmídl, and Pavel Ircing
Transfer Learning to Detect Parkinson’s Disease from Speech In Different
Languages Using Convolutional Neural Networks with Layer Freezing . . . . . 331
Cristian David Rios-Urrego, Juan Camilo Vásquez-Correa,
Juan Rafael Orozco-Arroyave, and Elmar Nöth
Speaker-Dependent BiLSTM-Based Phrasing . . . . . . . . . . . . . . . . . . . . . . . 340
Markéta Jůzová and Daniel Tihelka
Phonetic Attrition in Vowels’ Quality in L1 Speech of Late Czech-French
Bilinguals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
Marie Hévrová, Tomáš Bořil, and Barbara Köpke
Assessing the Dysarthria Level of Parkinson’s Disease Patients
with GMM-UBM Supervectors Using Phonological Posteriors
and Diadochokinetic Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
Gabriel F. Miller, Juan Camilo Vásquez-Correa, and Elmar Nöth
Voice-Activity and Overlapped Speech Detection Using x-Vectors . . . . . . . . 366
Jiří Málek and Jindřich Žďánský
Introduction of Semantic Model to Help Speech Recognition . . . . . . . . . . . . 377
Stephane Level, Irina Illina, and Dominique Fohr
Towards Automated Assessment of Stuttering and Stuttering Therapy . . . . . . 386
Sebastian P. Bayerl, Florian Hönig, Joëlle Reister,
and Korbinian Riedhammer
Synthesising Expressive Speech – Which Synthesiser for VOCAs?. . . . . . . . 397
Jan-Oliver Wülfing, Chi Tai Dang, and Elisabeth André
Perceived Length of Czech High Vowels in Relation to Formant
Frequencies Evaluated by Automatic Speech Recognition. . . . . . . . . . . . . . . 409
Tomáš Bořil and Jitka Veroňková
Inserting Punctuation to ASR Output in a Real-Time Production
Environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
Pavel Hlubík, Martin Španěl, Marek Boháč, and Lenka Weingartová
Very Fast Keyword Spotting System with Real Time Factor Below 0.01. . . . 426
Jan Nouza, Petr Červa, and Jindřich ŽďánskýContents xiii
On theEffectiveness ofNeural TextGeneration Based DataAugmentation
for Recognition of Morphologically Rich Speech . . . . . . . . . . . . . . . . . . . . 437
Balázs Tarján, György Szaszák, Tibor Fegyó, and Péter Mihajlik
Context-Aware XGBoost for Glottal Closure Instant Detection
in Speech Signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
Jindřich Matoušek and Michal Vraštil
LSTM-Based Speech Segmentation Trained on Different
Foreign Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
Zdeněk Hanzlíček and Jakub Vít
Complexity of the TDNN Acoustic Model with Respect
to the HMM Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
Josef V. Psutka, Jan Vaněk, and Aleš Pražák
Dialogue
Leyzer: A Dataset for Multilingual Virtual Assistants . . . . . . . . . . . . . . . . . 477
Marcin Sowański and Artur Janicki
Registering Historical Context for Question Answering in a Blocks World
Dialogue System. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
Benjamin Kane, Georgiy Platonov, and Lenhart Schubert
At Home with Alexa: A Tale of Two Conversational Agents . . . . . . . . . . . . 495
JenniferUreta,CelinaIrisBrito,JilyanBiancaDy,Kyle-AltheaSantos,
Winfred Villaluna, and Ethel Ong
ConversIAmo: Improving Italian Question Answering Exploiting IBM
Watson Services. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
Chiara Leoni, Ilaria Torre, and Gianni Vercelli
Modification of Pitch Parameters in Speech Coding
for Information Hiding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
Adrian Radej and Artur Janicki
ConfNet2Seq: Full Length Answer Generation from Spoken Questions . . . . . 524
Vaishali Pal, Manish Shrivastava, and Laurent Besacier
Graph Convolutional Networks for Student Answers Assessment . . . . . . . . . 532
Nisrine Ait Khayi and Vasile Rus
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541Invited PapersCombining Expert Knowledge with NLP
for Specialised Applications
B
Diana Maynard( ) and Adam Funk
Department of Computer Science, University of Sheﬃeld, Sheﬃeld, UK
d.maynard@sheffield.ac.uk
Abstract. Traditionally, there has been a disconnect between custom-
built applications used to solve real-world information extraction prob-
lems in industry, and automated learning-based approaches developed
inacademia.Despiteapproachessuchastransfer-basedlearning,adapt-
ing these to more customised solutions where the task and data may
be diﬀerent, and where training data may be largely unavailable, is still
hugely problematic, with the result that many systems still need to be
custom-built using expert hand-crafted knowledge, and do not scale. In
the legal domain, a traditional slow adopter of technology, black box
machinelearning-basedsystemsaretoountrustworthytobewidelyused.
In industrial settings, the ﬁne-grained highly specialised knowledge of
humanexpertsisstillcritical,anditisnotobvioushowtointegratethis
intoautomatedclassiﬁcationsystems.Inthispaper,weexaminetwocase
studies from recent work combining this expert human knowledge with
automated NLP technologies.
· ·
Keywords: Natural language processing Ontologies Information
extraction
1 Introduction
Although machine learning, and more recently deep learning-based approaches,
have shown enormous promise and success in Natural Language Processing
(NLP), and more generally in the ﬁeld of Artiﬁcial Intelligence (AI), there are
nevertheless a number of drawbacks when applied to many real-world appli-
cations in industrial settings. The medical and legal domains have been tradi-
tionally slow to adopt automated technologies, due partly to the critical eﬀect
of mistakes. On the other hand, driverless cars and autonomous robots are fast
becominganeverydayreality,despitethenumerousethicalconsiderations.When
ahumandriverhitsthebrakesinordertoavoidhittingachildwhorunsinfront
of a car, they make a moral decision to shift the riskfrom the child to their pas-
sengers. How should an autonomous car react in such a situation? One piece of
This work was partially supported by European Union under grant agreement
No.726992 KNOWMAK and No. 825091 RISIS; and by Innovate UK.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.3–10,2020.
https://doi.org/10.1007/978-3-030-58323-1_14 D. Maynard and A. Funk
research [3] showed that in surveys, people preferred an autonomous vehicle to
protect pedestrians even if it meant sacriﬁcing its passengers, as most human
drivers would do, but paradoxically, these people claimed that they would not
want to buy one if it were programmed to do so.
TherecentCOVID-19pandemichasdrivenawealthofinterestinautomated
AItechnologysuchascallsystems.Whilecallcentershavelongbeenaforerunner
in the use of such tools, the pandemic has accelerated their growth due to the
combination of a shortage of workers and an enormous increase in calls. IBM
witnesseda40%increaseinuseofWatsonAssistantbetweenFebruaryandApril
2020, and other technologies show a similar popularity rise.1
However,automatedcallsystemsonlydealwithpartoftheproblem,andare
still relatively simple. They are best at signposting users to sources of informa-
tion and mostly rely on posing pre-set questions with simple answers that can
be easily be processed (e.g. yes/no questions, or by spotting simple keywords).
Adapting these kinds of conversational agents to the speciﬁc demands of indi-
vidual businesses requires intensive labour and training materials, so is not a
project to be undertaken lightly or urgently.
Inthispaper,wefocusontwocasestudiesinwhichwehaveinvestigatedhow
expert human knowledge can be interlinked with the advantages of automated
technologies. These enable traditional manual tasks to be carried out faster and
more accurately by processing huge amounts of data, while still ensuring both
the consistency and ﬂexibility to deal with new data as it emerges. The ﬁrst of
theseisinthelegaldomain,wherewehavedevelopedtoolstoassistconsultants
to review collateral warranties - an expensive and time-consuming task which
nevertheless demands high precision and intricate levels of linguistic detail. The
second is in the wider ﬁeld of European scientiﬁc and technological knowledge
production and policy making, where tools are needed to assist policymakers in
understanding the nature of this enormous, highly complex and fast-changing
domain.
2 Legal IE
The reviewing of collateral warranties is an important legal and economic task
in the construction industry. These warranties are a type of contract by which
a member of the construction team (e.g. an architect) promises a third party
(e.g. the project funder) that they have properly discharged their contract. For
example, an architect of a new oﬃce development owes a duty of care to the
occupier of the development, concerning any design defects that might show up
later. Without a collateral warranty, the architect would typically not be liable.
Collateral warranties may include ‘step-in’ rights which allow the beneﬁciary to
step into the role of the main contractor. This can be important, for example to
banks providing funding for a project, enabling them to ensure that the project
is completed if that contractor becomes insolvent.
1 https://www.technologyreview.com/2020/05/14/1001716/ai-chatbots-take-call-
center-jobs-during-coronavirus-pandemic.Combining Expert Knowledge with NLP 5
Thereareanumberofstandardformsofcollateralwarranty,buttheirspeciﬁc
terms can be disputed, with clients often claiming that industry standard war-
rantiesfavoursubcontractorsanddesigners.Theremayalsobecomplexwording
or terminology in standard contracts which make them too risky because they
are outside the scope of the warranty giver’s insurance cover. Therefore, many
collateral warranties are bespoke. However, completing collateral warranties to
the satisfaction of all parties is incredibly diﬃcult, especially for large projects
with many consultants and sub-contractors, as well as multiple occupants, and
it is legally complex and onerous for lawyers to review them. A single manual
review typically takes 3 h, but is often not properly valued by clients, who see
it as a sideline to the main construction contract.
We have therefore been developing prototype software to assist lawyers in
reviewing collateral warranties. The legal industry typically does not make use
of automated software for these kind of tasks. Existing contract review software
islimited andbasedonmachine learning, which tendstobeinadequate because
it neither analyses collateral warranties to the level of detail required, nor does
it provide explanatory output. Furthermore, it is unclear how the highly spe-
cialised human expertise can be replicated in an automated approach. For this
reason, our system uses a rule-based approach which automates some of the
more straightforward parts of the review process and focuses on breaking the
documents down into relevant sections pertaining to each kind of problem the
human reviewer must address. It uses a traﬃc light system to check standard
protocols and to ﬂag possible problems that the lawyer should investigate, with
explanations as to the nature of the problem.
Fig.1. Sample annotations in a collateral warranty in the GATE GUI (Color ﬁgure
online)
ThewarrantyannotationtoolisbasedontheGATEarchitectureforNLP[4],
an open source toolkit which has been in development at the University of
Sheﬃeld for more than 20years. A rule-based approach is used to annotate dif-
ferent sections of the document and to recognise certain relevant entities (such
as copyright issues, the warranty beneﬁciary, the warranty giver, and so on).6 D. Maynard and A. Funk
Figure 1 shows an example of a mocked-up warranty annotated in the GATE
GUI.2 Two annotations are highlighted here, which concern the extent of the
warranty standard and the future warranty standard. In the bottom part of the
picture, we see that these have features red and yellow respectively. This indi-
cates that this part of the contract is something that a human reviewer needs
to check manually.
The human reviewer does not see the GATE GUI at all; we show it only
to explain the underlying technology. Instead, they use the reviewing interface
also developed in the project, which enables them to upload a document, select
someparameters,andrunGATEonitviaawebservice.Theycanthenviewthe
contractintheinterfaceandzoominondiﬀerentpartsofthedocumenttoseethe
suggestionsandhighlightsthatGATEhasmadeinaneasilyunderstandableway.
The yellow and red ﬂags (“translated” from the GATE features) indicate that
they need to review these parts, and the review cannot be marked as completed
until these are satisfactory. Figure2 shows the same mocked-up document now
inthereviewinginterface.Thereviewingprocesssemi-automaticallygeneratesa
ﬁnal report (for the lawyer’s client) based on the current human-written report,
with warnings about the risky passages in the document.
Fig.2. Sample annotations in a mock-up collateral warranty in the GATE GUI
3 Understanding Scientiﬁc Knowledge Production
in Europe
Understanding knowledge production and co-creation in key emerging areas of
European research is critical for policy makers wishing to analyse impact and
2 The warranty is not a real one, for legal reasons, but the annotation is genuine.Combining Expert Knowledge with NLP 7
make strategic decisions. Essentially, they need to know who is doing research
on what topic and in which country or region. The RISIS-KNOWMAK tool3 is
the result of a 3-year European project enabling the user to combine multiple
data sources (publications, patents, and European projects), connect the dots
by analysing knowledge production by topics and geography, and to pick from
diﬀerent kinds of visualisation options for the data they are interested in.
The tool generates aggregated indicators to characterise geographical spaces
(countries or regions) and actors (public research organisations and companies)
intermsofvariousdimensionsofknowledgeproduction.Foreachtopicorcombi-
nation of topics, the mapping of documents enables the generation of indicators
such as the number of publications, EU-FP projects, and patents in a speciﬁc
region, as well as various composite indicators combining dimensions, such as
the aggregated knowledge production share and intensity, and the publication
degree centrality.
Current methods for characterising and visualising the ﬁeld have limitations
concerning the changing nature of research, diﬀerences in language and topic
structure between policies and scientiﬁc topics, and coverage of a broad range
of scientiﬁc and political issues that have diﬀerent characteristics. The kind of
languageusedinpatentdescriptionsisverydiﬀerentfromthatusedinscientiﬁc
publications, and even the terminology can be very diﬀerent, so it is hard to
develop tools which can classify both kinds of document in the same way.
Inrecentyears,aprioriclassiﬁcationsystemsforscienceandtechnology,such
as the Field of Science Classiﬁcation (OECD, 2002) and IPC codes for patents
[6], have been increasingly replaced by data-driven approaches, relying on the
automated treatment of large corpora, such as word co-occurrences in academic
papers [2], clustering through co-citation analysis [9], and overlay maps to visu-
alise knowledge domains [7]. These approaches have obvious advantages, since
they are more ﬂexible to accommodate the changing structures of science, and
areabletodiscoverlatentstructuresofscienceratherthanimposeapre-deﬁned
structure over the data [8]. Yet, when the goal is to produce indicators for poli-
cymakers,purelydata-drivenmethodsalsodisplaylimitations.Ontheonehand,
suchmethodsprovideverydetailedviewsofspeciﬁcknowledgedomains,butare
less suited to large-scale mapping across the whole science and technology land-
scape.Ontheotherhand,lackingacommonontologyofscientiﬁcandtechnolog-
ical domains [5], such mappings are largely incommensurable across dimensions
of knowledge production. Perhaps even more importantly, data-driven methods
do not allow presumptions of categories used in the policy debate to be inte-
grated in the classiﬁcation process. These are largely implicit and subjective,
implying that there is no gold standard against which to assess the quality and
relevance of the indicators, but these are inherently debatable [1].
The RISIS-KNOWMAK classiﬁcation tool is a GATE-based web service
which classiﬁes each document according to the relevant topics it is concerned
with. This involves the novel use of ontologies and semantic technologies as
a means to bridge the linguistic and conceptual gap between policy questions
3 https://www.knowmak.eu/.8 D. Maynard and A. Funk
and (disparate) data sources. Our experience suggests that a proper interlink-
ing between intellectual tasks and the use of advanced techniques for language
processing is key for the success of this endeavour.
Our approach was based on two main elements: a) the design of an ontol-
ogyoftheKeyEnablingTechnologiesandSocietalGrandChallenges(KETand
SGC) knowledge domains to make explicit their content and to provide a com-
monstructureacrossdimensionsofknowledgeproduction;andb)theintegration
betweenNLPtechniques(toassociatedatasourceswiththeontologycategories)
andexpert-basedjudgement(tomakesensiblechoicesforthematchingprocess).
This drove a recursive process where the ontology development and data anno-
tation were successively reﬁned based on expert assessment of the generated
indicators.
Ontology development in our application involves three aspects: ﬁrst, the
designoftheontologystructure,consistingofasetofrelatedtopicsandsubtopics
intherelevantsubjectareas;second,populatingtheontologywithkeywords;and
third, classifying documents based on the weighted frequency of keywords. The
mapping process can be seen as a problem of multi-class classiﬁcation, with a
largenumberofclasses,andisachievedbyrelyingonsource-speciﬁcvocabularies
andmappingtechniquesthatalsoexploit(expert)knowledgeaboutthestructure
ofindividualdatasources.Thisisaniterativeprocess,basedonco-dependencies
between data, topics, and the representation system.
Our initial ontology derived from policy documents was manually enriched
andcustomised,basedontheoutcomeofthematchingprocessandexpertassess-
mentoftheresults.Eventually,theoriginalontologyclassesmayalsobeadapted
basedontheirdistinctivenessintermsofdataitems.Suchastagedapproach,dis-
tinguishing between core elements that are stabilised (the ontology classes) and
elements that are dynamic and can be revised (the assignment of data items to
classes),isdesirablefromadesignanduserperspective.Therefore,theapproach
is ﬂexible, for example to respond to changes in policy interests, and scalable
since new data sources can be integrated within the process whenever required.
All three steps require human intervention to deﬁne prior assumptions and to
evaluate outcomes, but they integrate automatic processing through advanced
NLPtechniques.Consequently,ifchangesaredeemednecessary,theprocesscan
easily be re-run and the data re-annotated within a reasonable period of time.
The ontology is freely available on the project web page4; we refer the inter-
ested reader also to the publications and documentation found there for full
detailsofthetechnology.Ourexperiencewiththisspecialisedontologyandclas-
siﬁcation shows that while NLP techniques are critical for linking ontologies
with large datasets, some key design choices on the ontology and its application
to data are of an intellectual nature and closely associated with speciﬁc user
needs. This suggests that the design of interactions between expert-based a pri-
ori knowledge and the use of advanced data techniques is a key requirement for
robust S&T ontologies.
4 https://gate.ac.uk/projects/knowmak/.Combining Expert Knowledge with NLP 9
Wehavealsoproducedanumberofcasestudiesofhowthetoolcouldbeused
for policy making. In the ﬁeld of genomics, we compared the technological and
scientiﬁcknowledgeproductioninEuropeintheperiod2010–2014.Technological
production is measured by patents, while scientiﬁc production is measured by
publications.Theseshowdiﬀerentgeographicaldistributions.Theformerismore
concentrated in space: in terms of volume, Paris is the biggest cluster for both
types. Within regions, production varies a lot: London is the biggest producer
of both types, while Eindhoven is key in terms of technological knowledge (both
for volume and intensity). These ﬁndings clearly reﬂect the diﬀerent structure
of public and private knowledge.
Fig.3. Specialisation indexes in biotechnology around Europe
Another example is based on the topic of Industrial Biotechnology (IB),
which oﬀers new tools, products and technologies based on the exploitation of
biologicalprocesses,organisms,cellsorcellularcomponents.Policymakersmight
liketoknow,forexample,whichEuropeancountriesare(more)specialisedinthis
ﬁeld, and whether there are diﬀerences in the extent of specialisation when con-
sideringscientiﬁcandtechnologicaldevelopment.Thetoolprovidesready-to-use
indicators to answer these questions. Figure3 indicates the country specialisa-
tionindexesinbiotechnology forthethreemeasuresofknowledge productionin
the period 2010–2014. Values greater/lower than 0 in the specialisation indexes
imply that a country is more/less specialised in IB compared with the average
European country. Amongst larger countries in terms of knowledge production,
Germany, France, Italy and the Netherlands exhibit no clear specialisation in10 D. Maynard and A. Funk
IB, with all indexes ranging at moderate levels from −0.09 to 0.07. The only
exception is the UK, which is more specialised in terms of EU-FP projects (spe-
cialisation higher than 0.1).
4 Conclusions
This paper has focused on two case studies based around tools we have devel-
opedforspecialisedapplications(inthelegalandscientometricsdomains)where
standard NLP tools based on machine learning are unlikely to be satisfactory
due to the kinds of knowledge and output required, and to other constraints
such as explainability (in the legal case) and ﬂexibility (in the scientometrics
case). While new advances in deep learning continue to transform the levels of
achievement of automated tools for a number of NLP classiﬁcation tasks, as
wellasinmachinetranslationandinspeechandimagerecognition,nevertheless
theyarenotsuitableforallNLPtasks,atleastasstand-alonetools.Rule-based
systems and the incorporation of human expert knowledge interweaved with
advanced learning may provide better approaches in some cases, as we have
demonstrated. Important future directions in the ﬁeld of NLP lie not only in
improving the explainability of machine learning tools, such as with the use of
adversarialexamples,andimprovedlinguisticknowledgeinneuralnetworks,but
also in investigating more deeply the ways in which expert knowledge can best
be integrated.
References
1. Barr´e, R.: Sense and nonsense of S&T productivity indicators. Sci. Public Policy
28(4), 259–266 (2001)
2. VandenBesselaar,P.,Heimeriks,G.:Mappingresearchtopicsusingword-reference
co-occurrences:amethodandanexploratorycasestudy.Scientometrics68(3),377–
393 (2006)
3. Bonnefon,J.F.,Shariﬀ,A.,Rahwan,I.:Thesocialdilemmaofautonomousvehicles.
Science 352(6293), 1573–1576 (2016)
4. Cunningham, H.: GATE, a general architecture for text engineering. Comput.
Humanit. 36(2), 223–254 (2002). https://doi.org/10.1023/A:1014348124664
5. Daraio,C.,etal.:Dataintegrationforresearchandinnovationpolicy:anontology-
based data management approach. Scientometrics 106(2), 857–871 (2016)
6. Debackere, K., Luwel, M.: Patent data for monitoring S&T portfolios. In: Moed,
H.F.,Gl¨anzel,W.,Schmoch,U.(eds.)HandbookofQuantitativeScienceandTech-
nologyResearch,pp.569–585.Springer,Dordrecht(2004).https://doi.org/10.1007/
1-4020-2755-9 27
7. Rafols,I.,Porter,A.L.,Leydesdorﬀ,L.:Scienceoverlaymaps:anewtoolforresearch
policyandlibrarymanagement.J.Am.Soc.Inform.Sci.Technol.61(9),1871–1887
(2010)
8. Shiﬀrin, R.M., B¨orner, K.: Mapping knowledge domains. PNAS 101, 5183–5185
(2004)
9. Sˇubelj, L., van Eck, N.J., Waltman, L.: Clustering scientiﬁc publications based on
citation relations: a systematic comparison of diﬀerent methods. PloS ONE 11(4),
e0154404 (2016)Multilingual Dependency Parsing from
Universal Dependencies to Sesame Street
B
Joakim Nivre( )
Department of Linguistics and Philology, Uppsala University, Uppsala, Sweden
joakim.nivre@lingfil.uu.se
Abstract. Research on dependency parsing has always had a strong
multilingual orientation, but the lack of standardized annotations for a
long time made it diﬃcult both to meaningfully compare results across
languages and to develop truly multilingual systems. The Universal
Dependencies project has during the last ﬁve years tried to overcome
thisobstaclebydevelopingcross-linguisticallyconsistentmorphosyntac-
ticannotationformanylanguages.Duringthesameperiod,dependency
parsing (like the rest of NLP) has been transformed by the adoption of
continuousvectorrepresentationsandneuralnetworktechniques.Inthis
paper,IwillintroducetheframeworkandresourcesofUniversalDepen-
dencies, and discuss advances in dependency parsing enabled by these
resources in combination with deep learning techniques, ranging from
traditionalwordandcharacterembeddingstodeepcontextualizedword
representations like ELMo and BERT.
· · ·
Keywords: Dependency parsing Multilingual UD Word
representations
1 Introduction
Dependency parsing is arguably the dominant approach to syntactic analysis in
NLPtoday,especiallyforlanguagesotherthanEnglish.Itsincreasingpopularity
overthelastoneandahalfdecadeisundoubtedlyduetoseveralfactors.Firstof
all, a dependency tree provides a simple and transparent encoding of predicate-
argument structure that has proven useful in many downstream applications,
such as information extraction [37], often acting as a crude proxy for a semantic
representation. Second, dependency parsing can be achieved with models that
are both simple and eﬃcient, sometimes with linear runtime guarantees [24,26],
which facilitates the implementation and deployment of parsers for large-scale
applications.Finally,dependency-basedsyntacticrepresentationsarecompatible
with many linguistic traditions around the world, and annotated corpora for a
widerangeoflanguages arethereforemorereadilyavailable thanforalternative
representations. This is clearly reﬂected in the research on dependency parsing
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.11–29,2020.
https://doi.org/10.1007/978-3-030-58323-1_212 J. Nivre
in recent years, which has had a strongly multilingual orientation starting from
the inﬂuential CoNLL shared tasks in 2006 and 2007 [2,29].1
The ﬁrst shared task on multilingual dependency parsing was held as part
of the CoNLL-X conference in 2006 [2], and the community owes a huge debt
to the organizing committee consisting of Sabine Buchholz, Amit Dubey, Yuval
Krymolowski and Erwin Marsi, who not only managed to collect dependency
treebanksfrom13diﬀerentlanguagesbutalsoconvertedthemtoasingleuniﬁed
data format, the CoNLL-X format, which has been a de facto standard for
dependency treebanks and dependency parsing ever since. In this way, they
enabled a new line of multilingual research which has been very fruitful for the
community. The ﬁrst shared task was followed by a second edition in 2007 [29],
this time involving 10 diﬀerent languages, and the data sets thus created for 19
diﬀerent languages2 are still used as benchmarks.
One of the most striking observations in the multilingual evaluation of these
shared tasks is the substantial variation in parsing accuracy across languages.
In 2006, the highest labeled attachment score (LAS) achieved for any language
was 91.7 for Japanese, while the lowest was a modest 65.7 for Turkish. In 2007,
the corresponding extreme points were English (89.6) and Greek (76.3). While
this variation clearly depends on multiple factors, including training set size
and type of text, the organizers of the 2007 shared task in their error analysis
found that one of the best predictors of accuracy level was language type [29].
HighaccuracywasmainlyachievedforlanguagessimilartoEnglish,withlimited
morphology and relatively ﬁxed word order, while lower accuracy was attained
forlanguageslikeGreek,withrichmorphologyandfreerwordorder.Thisledto
an increased interest in studying the challenges posed by diﬀerent languages for
syntactic parsing, in particular parsing of morphologically rich languages [41].3
However,eﬀortstodisentangletheinﬂuenceoflanguagetypologyonparsing
also encountered obstacles. In particular, while the CoNLL-X format imposes a
uniform representation of dependency trees, it does not in any way standardize
the content of the linguistic annotations. Thus, assumptions about what consti-
tutes a syntactic head or what categories should be used to classify syntactic
relations could (and did) vary almost without limit. This in turn meant that it
wasveryhardtodeterminewhetherdiﬀerencesinparsingaccuracybetweenlan-
guageswereduetodiﬀerencesinlanguagestructureordiﬀerencesinannotation
schemes(oracombinationofthetwo).Evidencethatdivergentannotationscan
be a confound came especially from studies that observed very diﬀerent results
for closely related languages, for example, Russian and Czech [28].
The desire to compare parsing results across languages in a more mean-
ingful way was one of several motivations behind the Universal Dependencies
1 Amultilingualperspectiveisalsoprevalentinthetheoreticaltraditionofdependency
grammar, starting with the seminal work of Tesni`ere [38], and in earlier rule-based
approaches to dependency parsing [34].
2 There was an overlap of 4 languages between the two shared tasks.
3 Thisresearchtrendwasnotlimitedtodependencyparsing,butalsoincludedground-
breaking work on constituency parsing.Multilingual Dependency Parsing 13
(UD) initiative [27,30,31], which aims to create cross-linguistically consistent
morphosyntactic annotation for as many languages as possible. Started as a
small-scale project in 2014, UD has grown into a large community eﬀort involv-
ing over 300 researchers around the world and has to date released over 150
treebanks in 90 languages. In addition to parsing research and benchmarking
for individual languages [42,44], the treebanks are widely used in research on
cross-lingual learning [7,9,40] as well as for linguistic research on word order
typology [8,16,32], to mention only a few applications.
In this paper, I will ﬁrst introduce the UD annotation framework and the
resources that have been made available through the project. I will then review
three recent studies that take a multilingual perspective on dependency parsing
and uses data from UD to cast light on cross-linguistic similarities and dif-
ferences. These studies explore diﬀerent ways of representing words in neural
dependency parsing, ranging from traditional word and character embeddings
to deep contextualized word representations like ELMo [33] and BERT [4].
2 Universal Dependencies
ThemaingoaloftheUDprojectistodevelopcross-linguisticallyconsistentmor-
phosyntactic annotation for as many languages as possible in order to facilitate
multilingual research within NLP and linguistics. Ideally, the annotation should
allowmeaningfullinguisticanalysisacrosslanguages,enableresearchonsyntac-
tic parsing in multilingual settings, support the development of NLP systems
for multiple languages, and facilitate resource-building for new languages. Since
cross-linguisticconsistencybynecessityimpliessomeabstractionoverlanguage-
speciﬁc details, UD may not be ideal for in-depth analysis of a single language
and should therefore be seen as a complement to language-speciﬁc annotation
schemes, rather than as a replacement.
The UD project started in 2014, and the ﬁrst version of the guidelines was
releasedthe same year together with an initial batch of ten treebanks [27,30]. A
second version of the guidelines was launched in 2016, and treebanks have been
released(roughly)everysixmonths,withthelatestrelease(v2.5)containing157
treebanksrepresenting90languages [31].Formorecompletedocumentation, we
refer to the UD website.4
2.1 Basic Principles of UD
The main idea underlying UD is to achieve cross-linguistically consistent anno-
tation by focusing on grammatical relations between words, especially content
words.ThisisillustratedinFig.1,whichshowstwoparallelsentencesinEnglish
and Finnish, two typologically rather diﬀerent languages. The two sentences
are similar in that they consist of a verbal predicate with a subject, an object
and a locative modiﬁer, but they diﬀer in how the grammatical relations are
4 https://universaldependencies.org.14 J. Nivre
morphosyntactically encoded. English relies on word order to distinguish the
subject and the object, which are both realized as bare noun phrases, while the
locative modiﬁer is introduced by a preposition. Finnish instead uses diﬀerent
morphological cases to distinguish all three relations. In addition, the English
nounphrasesuseanarticletoencodedeﬁniteness,acategorythatisnotovertly
marked at all in Finnish.
obl
obj case
det nsubj det det
the dog chased the cat from the room
DET NOUN VERB DET NOUN ADP DET NOUN
Case=Nom Case=Acc Case=Ela
NOUN VERB NOUN NOUN
koira jahtasi kissan huoneesta
nsubj obj
obl
Fig.1. Simpliﬁed UD annotation for equivalent sentences from English (top) and
Finnish (bottom)
The goal is to bring out the similarities without obscuring the diﬀerences.
Concretely, this is achieved by giving priority in the annotation to argument
and modiﬁer relations involving predicates and nominals – the three relations
thatarecommontotheEnglishandFinnishsentencesinFig.1.Ontopofthese
grammaticalrelations,wethencaptureconcreteaspectsofmorphosyntacticreal-
ization in two diﬀerent ways. On the one hand, we use part-of-speech tags and
morphological features to describewordsandtheir inﬂections, asexempliﬁed by
the nominal case-marking in Finnish.5 On the other hand, we use special syn-
tactic relations to link grammatical function words to their hosts, as shown by
the articles and the preposition in English.
Itisimportanttonotethatthenotionofwordthatisrelevanthereisthatof
a syntactic word, which does not always coincide with orthographical or phono-
logicalunits.Forinstance,cliticsoftenneedtobeseparatedfromtheirhostsand
treated as independent words even if they are not recognized as such in conven-
tional orthography, as in Spanishd´amelo = da me lo (lit. give me it), and many
contractions need to be split into several words, as in French au = `a le (lit. at
the). Conversely, compound words need a special treatment in languages where
their written form may contain boundary markers such as whitespace. In fact,
coming up with good criteria for determining word boundaries across languages
with diﬀerent writing systems and orthographic conventions has turned out to
be one of the main challenges of the UD enterprise.
5 The features displayed in Fig.1 are only a small subset of the features that would
appear in a complete annotation of the two sentences.Multilingual Dependency Parsing 15
2.2 Morphological Annotation
The morphological annotation of a (syntactic) word in the UD scheme consists
of three levels of representation:
1. A lemma representing the base form of the word.
2. A part-of-speech tag representing the grammatical category of the word.
3. A set of features representing lexical and grammatical properties associated
with the particular word form.
The lemma is the canonical form of the word, which is the form typically found
in dictionaries. The list of universal part-of-speech tags is a ﬁxed list containing
17 tags, shown in Table1. Languages are not required to use all tags, but the
listcannotbeextendedtocoverlanguage-speciﬁccategories.Instead,moreﬁne-
grained classiﬁcation of words can be achieved via the use of features, which
specify additional information about morphosyntactic properties. We provide
an inventory of features that are attested in multiple languages and need to be
encodedinauniformway,listedinTable1.Userscanextendthissetofuniversal
features and add language-speciﬁc features when necessary.
Table1.PoStags(left),morphologicalfeatures(center)andsyntacticrelations(right).
Syntactic Relations
Features Clausal
PoS Tags Inﬂectional Lexical Core Non-Core Nominal
ADJ Animacy Abbr nsubj advcl acl
ADP Aspect Foreign csubj advmod amod
ADV Case NumType ccomp aux appos
AUX Clusivity Poss iobj cop case
CCONJ Deﬁnite PronType obj discourse clf
DET Degree Reﬂex xcomp dislocated det
INTJ Evident Typo expl nmod
NOUN Gender mark nummod
NUM Mood obl
PART NounClass vocative
PRON Number Linking MWE Special
PROPN Person cc compound dep
PUNCT Polarity conj ﬁxed goeswith
SCONJ Polite list ﬂat orphan
SYM Tense parataxis punct
VERB VerbForm reparandum
X Voice root16 J. Nivre
2.3 Syntactic Annotation
Syntactic annotation in the UD scheme consists of typed dependency relations
between words,prioritizing predicate-argument andmodiﬁer relations that hold
directlybetweencontentwords,asopposedtobeingmediatedbyfunctionwords.
As stated previously, the rationale is that this makes more transparent what
grammatical relations are shared across languages, even when the languages
diﬀer in the way that they use word order, function words or morphological
inﬂection to encode these relations. UD provides a taxonomy of 37 universal
relation types to classify syntactic relations, as shown in Table1. The taxonomy
distinguishes between relations that occur at the clause level (linked to a pred-
icate) and those that occur in noun phrases (linked to a nominal head). At the
clause level, a distinction is made between core arguments (essentially subjects
andobjects)andallotherdependents[1,39].Itisimportanttonotethatnotall
relationsinthetaxonomyaresyntacticdependencyrelationsinthenarrowsense.
First, there are special relations for function words like determiners, classiﬁers,
adpositions, auxiliaries, copulas and subordinators, whose dependency status
is controversial. In addition, there are a number of special relations for link-
ing relations (including coordination), certain types of multiword expressions,
and special phenomena like ellipsis, disﬂuencies, punctuation and typographi-
cal errors. Many of these relations cannot plausibly be interpreted as syntactic
head-dependent relations, and should rather be thought of as technical devices
for encoding ﬂat structures in the form of a tree. The inventory of universal
relation types is ﬁxed, but subtypes can be added in individual languages to
capture additional distinctions that are useful.
2.4 UD Treebanks
UD release v2.56 [43] contains 157 treebanks representing 90 languages. Table2
speciﬁesforeachlanguagethenumberoftreebanksavailable,aswellasthetotal
number of annotated sentences and words in that language. It is worth noting
thattheamountofdatavariesconsiderablybetweenlanguages,fromSkoltSa´mi
with 36 sentences and 321 words, to German with over 200,000 sentences and
nearly4millionwords.Themajorityoftreebanksaresmallbutitshouldbekept
inmindthatmanyofthesetreebanksarenewinitiativesandcanbeexpectedto
growsubstantiallyinthefuture.ThelanguagesinUDv2.5represent20diﬀerent
language families (or equivalent). The selection is very heavily biased towards
Indo-European languages (48 out of 90), and towards a few branches of this
family – Germanic (10), Romance (8) and Slavic (13) – but it is worth noting
that the bias is (slowly) becoming less extreme over time.7
6 UD releases are numbered by letting the ﬁrst digit (2) refer to the version of the
guidelines and the second digit (5) to the number of releases under that version.
7 The proportion of Indo-European languages has gone from 60% in v2.1 to 53% in
v2.5.Multilingual Dependency Parsing 17
Table2.LanguagesinUDv2.5;numberoftreebanks(#),sentences(S)andwords(W).
Language #S W Language #S W Language #S W
Afrikaans 1 1,934 49,276 German 4 208,440 3,753,947 OldRussian 2 17,548 168,522
Akkadian 1 101 1,852 Gothic 1 5,401 55,336 Persian 1 5,997 152,920
Amharic 1 1,074 10,010 Greek 1 2,521 63,441 Polish 3 40,398 499,392
Ancient 2 30,999 416,988 Hebrew 1 6,216 161,417 Portuguese 3 22,443 570,543
Greek
Arabic 3 28,402 1,042,024 Hindi 2 17,647 375,533 Romanian 3 25,858 551,932
Armenian 1 2502 52630 HindiEnglish1 1,898 26,909 Russian 4 71,183 1,262,206
Assyrian 1 57 453 Hungarian 1 1,800 42,032 Sanskrit 1 230 1,843
Bambara 1 1,026 13,823 Indonesian 2 6,593 141,823 Scottish 1 2,193 42,848
Gaelic
Basque 1 8,993 121,443 Irish 1 1,763 40,572 Serbian 1 4,384 97,673
Belarusian 1 637 13,325 Italian 6 35,481 811,522 SkoltSa´mi 1 36 321
Bhojpuri 1 254 4,881 Japanese 4 67,117 1,498,560 Slovak 1 10,604 106,043
Breton 1 888 10,054 Karelian 1 228 3,094 Slovenian 2 11,188 170,158
Bulgarian 1 11,138 156,149 Kazakh 1 1,078 10,536 Spanish 3 34,693 1,004,443
Buryat 1 927 10,185 Komi 1 49 399 Swedish 3 12,269 206,855
Permyak
Cantonese 1 1,004 13,918 KomiZyrian 2 327 3,463 SwedishSign 1 203 1,610
Language
Catalan 1 16,678 531,971 Korean 3 34,702 446,996 SwissGerman1 100 1,444
Chinese 5 12,449 285,127 Kurmanji 1 754 1,0260 Tagalog 1 55 292
Classical 1 15,115 74,770 Latin 3 41,695 582,336 Tamil 1 600 9,581
Chinese
Coptic 1 1,575 40,034 Latvian 1 13,643 219,955 Telugu 1 1,328 6,465
Croatian 1 9,010 199,409 Lithuanian 2 3,905 75,403 Thai 1 1,000 22,322
Czech 5127,507 2,222,163 Livvi 1 125 1,632 Turkish 3 9,437 91,626
Danish 1 5,512 100,733 Maltese 1 2,074 44,162 Ukrainian 1 7,060 122,091
Dutch 2 20,916 306,503 Marathi 1 466 3,849 Upper 1 646 11,196
Sorbian
English 7 35,791 620,509 Mbya´ 2 1,144 13,089 Urdu 1 5,130 138,077
Guaran´ı
Erzya 1 1,550 15,790 Moksha 1 65 561 Uyghur 1 3,456 40,236
Estonian 2 32,634 465,015 Naija 1 948 12,863 Vietnamese 1 3,000 43,754
Faroese 1 1,208 10,002 NorthS´ami 1 3,122 26,845 Warlpiri 1 55 314
Finnish 3 34,859 377,619 Norwegian 3 42,869 666,984 Welsh 1 956 16,989
French 7 45,074 1,157,171 OldChurch 1 6,338 57,563 Wolof 1 2,107 44,258
Slavonic
Galician 2 4,993 164,385 OldFrench 1 17,678 170,741 Yoruba 1 100 2,664
3 Studies in Dependency Parsing
ThetreebanksreleasedbytheUDprojecthavebeenwidelyusedinNLPresearch
over the past ﬁve years and now constitute the natural benchmark data sets for
dependencyparsinginmostlanguages.Thistendencyhasbeenfurtherreinforced
by the CoNLL shared tasks on UD parsing organized in 2017 and 2018 [42,44].
These tasks are very similar in spirit to the old tasks from 2006 and 2007,
designed to evaluate dependency parsing models on data from multiple lan-
guages, but there are two important diﬀerences. The ﬁrst diﬀerence is that the
new tasks focus on the entire task of mapping raw text to rich morphosyntactic18 J. Nivre
representations with no prior segmentation or annotation of the input, whereas
the old tasks used gold standard segmentation and morphological annotation
as input to dependency parsing. The second diﬀerence is that annotations are
now standardized across languages thanks to the UD framework, which facil-
itates cross-linguistic comparisons. In addition, the number of treebanks and
languages has greatly increased since the pioneering eﬀorts in 2006 and 2007.
The CoNLL 2017 shared task featured 81 test sets from 49 languages (including
4 surprise languages), and the CoNLL 2018 shared task added 8 new languages.
Figure2 is an attempt to visualize the impact of these shared tasks on the lan-
guages appearing in both tasks. The x axis represents the amount of annotated
dataavailableandtheyaxisrepresentsthetoplabeledattachmentscore(LAS).
Foreachlanguage,anorangedotandareddotconnectedbyanarrowrepresent
the situation before and after the shared tasks, respectively. It is clear that the
overwhelming majority of languages have seen an increase both in the amount
ofannotatedresourcesandinparsingaccuracy,sometimesverysubstantiallyso.
TheCoNLL2017and2018sharedtasksareonlythetipoftheicebergwhenit
comestoparsingresearchbasedonUDresources,whichincludesawiderangeof
studiesofparsingmodels forindividual languages, aswell aswork oncross-and
multilingualparsingmodels.Iwillmakenoattemptatsurveyingthislargebody
of literature here. Instead, I will present three speciﬁc studies by the Uppsala
parsing group in order to illustrate how the availability of cross-linguistically
consistent annotation for multiple languages enables us to study the interplay
between parsing techniques and language structure in a more informed way.
Fig.2. Impact of the CoNLL 2017 and 2018 shared tasks on amount of data (x axis)
andtopLAS(yaxis)forUDlanguages.FigurecreatedbyFilipGinterandpreviously
published in Nivre et al. [31].Multilingual Dependency Parsing 19
3.1 Parsing Framework
Two of the three studies that will be discussed below make use of a generic
frameworkfordependencyparsingoriginallyproposedbyKiperwasserandGold-
berg [11]. The core component of the architecture is a BiLSTM encoder:
BiLSTM(x1,...,xn)=v1,...,vn
The input here is a sequence of vectors x1,...,xn representing the input words
w1,...,wn of a sentence. The output is a corresponding sequence of vectors
v1,...,vn, where each vi is a contextualized representation of the word wi, that
is, a representation that combines information from the input representation xi
with information from both the left context x1,...,xi−1 and the right context
xi+1,...xn.ThesecontextualizedwordrepresentationsarethenfedintoanMLP
that scores alternative hypotheses about the dependency structure. If a graph-
based parsing model is assumed, this may mean scoring a potential dependency
arc wi →wj:
S(wi →wj)=MLP(vi,vj)
A full parse can then be computed by ﬁnding the maximum spanning tree over
a complete score matrix [22].
If instead a transition-based parsing model is assumed, the model will score
a potential transition t out of the current parser conﬁguration c, represented by
a small number of word vectors vi,...vk extracted from the parser’s stack and
buﬀer:
S(c,t)=MLP(vi,...vk,t)
A parse tree can in this case be constructed by repeatedly applying the highest-
scoring transition to the current conﬁguration until a terminal conﬁguration is
reached, using a greedy deterministic parsing algorithm [25].
OneofthemainachievementsofKiperwasserandGoldbergwastoshowthat
this comparatively simple architecture can lead to state-of-the-art parsing accu-
racyforbothgraph-basedandtransition-basedmodelsiftrainedadequately[11].
Thismodelhassincebeendevelopedfurtherbyseveralresearchersandcurrently
underlie most of the state-of-the-art models for dependency parsing [3,5,6,12].
OneoftheevolvedversionsisUUParser[17],originallydevelopedfortheCoNLL
2017 and 2018 shared tasks [18,35], which involves two types of modiﬁcations.
The ﬁrst modiﬁcation is the implementation of models that can handle non-
projective dependency trees. In the transition-based case, this amounts to an
extension of the arc-hybrid transition system [13] with a SWAP transition for
online reordering [19,26]. In the graph-based case, this is achieved by using the
Chu-LiuEdmondsmaximumspanningtreealgorithm[22].Thesecondmodiﬁca-
tionconcernstheinputrepresentationsx1,...,xn andwillbediscussedindetail
in the following sections.
3.2 Representing Word Types
Our ﬁrst study, presented at EMNLP 2018 [36], concerns how neural transition-
based dependency parsers beneﬁt from diﬀerent input representations, speciﬁ-20 J. Nivre
cally pre-trained word embeddings, character-based representations, and part-
of-speechtags.Allofthesetechniqueshavepreviouslybeenshowntobehelpful,
but there is a lack of systematic studies investigating how they compare to each
other and whether the techniques are complementary or redundant. Another
important goal, in line with the goals of this paper, is to ﬁnd out how the use-
fulness of these techniques vary across languages.
Forthisstudy,weusethetransition-basedversionofUUParserandonlyvary
the way in which the input words w1,...,wn are represented. In the simplest
model,thevectorxi representinginputwordwi issimplyarandomlyinitialized
word embedding with a dimensionality of 100:
xi =er(wi)
In the most complex model, xi is the concatenation of three vectors:
xi =et(wi)◦BiLSTM(c1:m)◦e(pi)
The ﬁrst vector et(wi) is a word embedding of the same dimensionality as
er(wi), but initialized using the pre-trained word embeddings trained using
word2vec [23] and released for the CoNLL 2017 shared task on universal depen-
dencyparsing[44].ThesecondvectorBiLSTM(c1:m)isacharacter-basedrepre-
sentationobtainedbyrunningaBiLSTMoverthecharactersequencec1,...,cm
ofwi,withadimensionalityof100fortheoutputvector.Thevectore(pi),ﬁnally,
is an embedding of the part-of-speech tag pi assigned to wi by an independent
part-of-speech tagger [6] (dimensionality 20, random initialization).
Table 3. Mean LAS across nine languages for a baseline system employing randomly
initialisedwordembeddingsonly,comparedtothreeseparatesystemsusingpre-trained
wordembeddings(+ext),acharactermodel(+char),andpart-of-speechtags(+pos).
Scores are also shown for a combined system that utilises all three techniques and
corresponding systems where one of the three techniques is ablated (−ext, −char
and −pos).
baseline 67.7 combined 81.0
+ext 76.1 −ext 79.9
+char 78.3 −char 79.2
+pos 75.9 −pos 80.3
In order to study the usefulness of pre-trained word embeddings, character
representations,andpart-of-speechtagsacrossdiﬀerentlanguages,wetrainand
evaluate the parser with diﬀerent combinations of these input representations
on a sample of nine languages, selected for diversity with respect to writing
systems, character set sizes, and morphological complexity. Table3 shows the
labeled attachment score (LAS) for some of these systems when averaged over
all nine languages. In the system +ext, we replace the randomly initializedMultilingual Dependency Parsing 21
word embeddings of the baseline system with pre-trained word embeddings;
in the systems +char and +pos, we concatenate the random embeddings with
character representations and part-of-speech tag embeddings, respectively. The
results for these three systems show clearly that either of the three techniques
byitselfimprovesparsingaccuracyby8–10%pointsforoursampleoflanguages,
although the character representations appear to be a little more eﬀective than
the other two techniques. The combined system, where we combine all three
techniques (and omit random embeddings), adds another 5% points to the best
score, which is a substantial improvement but far from what we should expect
if the improvements were completely independent. Finally, omitting any of the
three techniques from the combined system, which gives us the systems −ext,
−char and −pos, shows similar drops in accuracy of 1–2% points for all tech-
niques(withthecharacterrepresentationsagainbeingslightlymoreimportant).
Fig.3.BinnedHDLASbywordfrequency(top),HDLASbypart-of-speechcategories
(bottom left), and LAS per language (bottom right).22 J. Nivre
The conclusion so far is that all three techniques are helpful but their con-
tributions somewhat redundant with diminishing returns when they are added
on top of each other. However, these results are averaged over all words and
sentences in all languages. In order to get a more ﬁne-grained picture, Fig.3
presentsabreakdownoftheresultsforthebaseline,+ext,+charand+pos
systems by word frequency (in equally sized bins), by part-of-speech category,
and by language. The results per language are standard LAS scores; the other
two diagrams plot HDLAS, a harsher metric that considers a word correctly
analyzedonlyifitssyntacticheadandallitsdependentsarecorrect.Thebreak-
down by frequency shows, not surprisingly, that all three techniques give the
greatest improvements for low-frequency words, including unknown words, and
that character representations are especially eﬀective here. However, it is worth
noting that we see improvements over the whole range of frequencies, and that
character representations gradually lose their advantage, so that the other two
techniquesareinfactmoreeﬀectiveinthebinwiththehighestfrequency.When
looking at accuracy for diﬀerent part-of-speech categories, we see the largest
improvements for open classes like nouns, proper nouns, verbs and adjectives,
which is consistent with the results for frequency, since these classes contain
theoverwhelming majority oflow-frequencywords,andwealsoseethatcharac-
ter representations give the largest improvement for these categories, with the
notable exception of proper nouns.
Whenwebreakdowntheresultsbylanguage,ﬁnally,someinterestingdiﬀer-
ences start to emerge. First of all, the magnitude of the improvement over the
baseline varies considerably, with morphologically rich languages like Russian,
Finnish and Korean beneﬁtting the most. In addition, while character represen-
tations give the largest improvements when averaged over all languages, they
are in fact the most eﬀective technique only for four out of nine languages (the
three languages just mentioned together with Ancient Greek). For most of the
other languages, they are outperformed by both pre-trained word embeddings
and part-of-speech tags. This is a good illustration of the need to test hypothe-
ses about parsing technology on a wide range of languages to guard against
unfounded generalizations.
3.3 Representing Word Tokens
The word embeddings used in the ﬁrst study, whether pre-trained or not, rep-
resent word types. Recent work has shown that results can be further improved
by using contextualized embeddings, which provide distinct representations for
diﬀerenttokensofthesamewordtype,andmodelslikeELMo[33]andBERT[4]
havequicklybecomeubiquitousinNLP.Oursecondstudy,presentedatEMNLP
2019 [14], explores how these token embeddings aﬀect the behavior of parsers
belonging to the two main paradigms for dependency parsing: graph-based and
transition-based. We knew from previous research that, although graph-based
and transition-based parsers often achieve similar accuracy on average, they
used to have distinct error proﬁles due to their inherent strengths and weak-
nesses [20,21,45]. Broadly speaking, transition-based parsers did better on localMultilingual Dependency Parsing 23
dependencies and short sentences, thanks to richer feature models, but strug-
gled with non-local dependencies and longer sentences, due to greedy decoding
and error propagation. Conversely, graph-based parsers were less accurate on
local structures, because of lower structural sensitivity, but degraded less as
dependencies and sentences get longer, thanks to exact decoding. However, this
comparativeerroranalysiswasalldonebeforetheswitchtoneuralnetworktech-
niques, so we thought it was high time to replicate the old analysis in the new
methodological framework with three main research questions:
1. Do the old error proﬁles persist after the switch to neural networks?
2. How do contextualized word embeddings change the picture?
3. Are the patterns consistent across diﬀerent languages?
To answer these questions, we train and evaluate graph-based and transition-
based versions of UUParser with three diﬀerent input representations, yielding
a total of six parsing models, each of which is applied to 13 diﬀerent languages.
The baseline parsers gr (graph-based) and tr (transition-based) use the same
input representations as the −pos system in the ﬁrst study, that is, a com-
bination of pre-trained word embeddings and character representations (but no
part-of-speechtags).Fortheotherfourmodels,weconcatenatethebaselinerep-
resentation with a token embedding from either ELMo (gr+E and tr+E) or
BERT (gr+B and tr+B). For ELMo, we make use of the pre-trained models
provided by Che et al. [3], who train ELMo on 20 million words randomly sam-
pled from raw WikiDump and Common Crawl datasets for 44 languages. For
BERT,weemploythepretrainedmultilingualcasedmodelprovidedbyGoogle,8
which is trained on the concatenation of WikiDumps for the top 104 languages
with the largest Wikipedias.9
Fig.4. Labeled F1 for six parsing models, averaged over 13 languages.
Figure4 plots the labeled F1 of all six systems as a function of dependency
length, one of the most striking dimensions previously for diﬀerentiating graph-
8 Except for Chinese, for which we make use of a separate, pretrained model.
9 https://github.com/google-research/bert.24 J. Nivre
basedandtransition-basedparsers.Comparingﬁrstthetwobaselinemodels(gr
and tr), we ﬁnd that the old error proﬁles persist only partially. It is still true
thatthetransition-basedparserdegradesmoreasdependenciesgetlonger,butit
no longer has an advantage for the shortest dependencies, because both models
now beneﬁt from an unbounded feature model thanks to the BiLSTM encoder.
On the other hand, we see that the addition of contextualized word embeddings
beneﬁt the transition-based parser more than its graph-based counterpart, pre-
sumably because it mitigates error propagation by giving the parser a better
representation of the global sentence structure. The eﬀect is strongest for the
BERTmodels(gr+Bandtr+B),wherethecurvesarevirtuallyindistinguish-
able, showing that the two parsers do not only make a similar number of errors
butalsosimilartypesoferrors.Thepatternsobservedbothforthebaselinemod-
els and their extended counterparts are remarkably consistent across languages,
as can be seen from the ﬁrst six columns of Table4, which shows the LAS for
all models and all languages. In particular, despite large variations in parsing
accuracy across languages, the claim that transition-based parsers beneﬁt more
from contextualized word embeddings holds not only on average but for every
single language with both ELMo and BERT embeddings.
Table4.Column1–6:LASforparsingmodelswith/withoutdeepcontextualizedword
representations[14].Column7–8:UASforsupervisedparserandstructuralprobe[15].
Language LAS UAS
tr gr tr+E gr+E tr+B gr+B gr+B sp+B
Arabic 79.1 79.9 82.0 81.7 81.9 81.8 88.3 63.9
Basque 73.6 77.6 80.1 81.4 77.9 79.8 86.0 68.2
Chinese 75.3 76.7 79.8 80.4 83.7 83.4 87.0 61.9
English 82.7 83.3 87.0 86.5 87.8 87.6 92.0 73.9
Finnish 80.0 81.4 87.0 86.6 85.1 83.9 91.1 72.8
Hebrew 81.1 82.4 85.2 85.9 85.5 85.9 91.3 71.0
Hindi 88.4 89.6 91.0 91.2 89.5 90.8 95.0 80.0
Italian 88.0 88.2 90.9 90.6 92.0 91.7 95.1 80.4
Japanese 92.1 92.2 93.1 93.0 92.9 92.1 94.7 75.7
Korean 79.6 81.2 82.3 82.3 83.7 84.2 88.5 66.7
Russian 88.3 88.0 90.7 90.6 91.5 91.0 94.6 79.4
Swedish 80.5 81.6 86.9 86.2 87.6 86.9 90.9 74.9
Turkish 57.8 61.2 62.6 63.8 64.2 64.9 74.4 58.1
Average 80.5 81.8 84.5 84.6 84.9 84.9 89.9 71.3Multilingual Dependency Parsing 25
3.4 Parserless Parsing?
The previous study shows a convergence in behavior of diﬀerent parsing models
when equipped with deep contextualized word representations, which suggests
that the parsingmodel is less important than it usedtobeandthat most ofthe
workisdonebytherichpre-trainedinputrepresentations.Thisleadsinevitably
to the question of whether we need a dedicated parsing model at all, or whether
we can extract syntactic representations directly from the contextualized word
representations provided by models like ELMo and BERT. Our third study,
presented at ACL 2020 [15], explores this question using the probing technique
proposed by Hewitt and Manning [10]. The question asked in the original paper
is whether the vector spaces of token embeddings encode parse trees implicitly,
and the authors approach this question by trying to learn linear transforms of a
vectorspacesuchthat,givenasentenceanditsparsetree,thedistancebetween
two token vectors in the new space encodes their distance in the parse tree, on
the one hand, and the norm of a token vector encodes the depth of the token in
the parse tree, on the other. Information about distance and depth is suﬃcient
to extract undirected dependency trees, and experiments on English shows that
itispossibletoreachanundirectedunlabeledattachmentscore(UUAS)ofover
80% when evaluating against existing treebanks.
In the new study, we extend this work in two directions. First, we propose
a method for deriving directed dependency trees from the depth and distance
measures, so that we can evaluate accuracy using the standard metric of unla-
beled attachment score (UAS). We ﬁrst derive scores for all possible directed
dependency arcs wi →wj:
(cid:2)
S(wi →wj)= −−d∞ist(wi,wj)iofthdeerpwtihse(wi)<depth(wj)
The idea is that shorter distances correspond to higher arc scores, and that
arcs from lower to higher nodes are excluded (by giving them a score of −∞).
Given the arc scores, a directed dependency tree can be obtained by extracted
the maximum spanning tree over the complete score matrix [22]. Our second
extensionistogobeyondEnglishandapplythemethodtothesame13languages
as in the previous study, to see whether there are interesting cross-linguistic
diﬀerences.10
The last two columns of Table4 show the UAS scores obtained for diﬀerent
languages when using a weighted average of the predictions from the 12 layers
of the multilingual BERT model. For comparison, we include the UAS scores
achieved by the gr+B parser from the previous study. On the one hand, it is
impressive that we can achieve an average UAS of 71.3 using a simple linear
model on top of a language model that has not been trained with a parsing
objective and has not itself been exposed to parse trees. On the other hand,
10 The published paper contains a third extension, which we omit here because of
space constraints, where we investigate whether the models exhibit a preference for
diﬀerent syntactic frameworks.26 J. Nivre
we see that this result is still very far below the average UAS of 89.9 obtained
with a dedicated parsing model that has access to the same language model
representations. Finally, it is worth noticing that diﬀerences in accuracy across
languages are strongly correlated between the two models, with a Pearson cor-
relation coeﬃcient of 0.88 (p≤0.05).
4 Conclusion
The use of deep neural language models that can be pre-trained on very large
data sets and ﬁne-tuned for particular tasks is prevalent in contemporary NLP,
and dependency parsing is no exception. There is no question that models like
ELMo and BERT learn aspects of syntax (although it is still far from clear
exactly what they learn), which makes them very useful for the parsing task,
although we are still not at the point where a dedicated parser is superﬂuous.
The adoption of continuous representations and neural network techniques in
general has led to a convergence across models and algorithms, where graph-
based and transition-based dependency parsers now share most of their archi-
tecture and also exhibit very similar performance in terms of parsing errors.
Thanks to research on multilingual parsing and initiatives like UD, there is also
some convergence in parsing accuracy across languages, although we still see
signiﬁcant diﬀerences. It is therefore important to maintain a multilingual per-
spective going forward, and we hope that the resources provided by the UD
community can continue to play a role as a touchstone for parsing and probing
studies.
Acknowledgments. I want to thank (present and former) members of the Uppsala
parsinggroup–AliBasirat,MiryamdeLhoneux,ArturKulmizev,PaolaMerlo,Aaron
Smith and Sara Stymne – colleagues in the core UD group – Marie de Marneﬀe,
FilipGinter,YoavGoldberg,JanHajiˇc,ChrisManning,RyanMcDonald,SlavPetrov,
Sampo Pyysalo, Sebastian Schuster, Reut Tsarfaty, Francis Tyers and Dan Zeman –
andallcontributorsintheUDcommunity.Iacknowledgethecomputationalresources
provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu).
References
1. Andrews, A.D.: The major functions of the noun phrase. In: Shopen, T. (ed.)
Language Typology and Syntactic Description. Volume I: Clause Structure, 2nd
edn., pp. 132–223. Cambridge University Press, Cambridge (2007)
2. Buchholz, S., Marsi, E.: CoNLL-X shared task on multilingual dependency pars-
ing. In: Proceedings of the 10th Conference on Computational Natural Language
Learning (CoNLL), pp. 149–164 (2006)
3. Che,W.,Liu,Y.,Wang,Y.,Zheng,B.,Liu,T.:TowardsbetterUDparsing:deep
contextualized word embeddings, ensemble, and treebank concatenation. In: Pro-
ceedingsoftheCoNLL2018SharedTask:MultilingualParsingfromRawTextto
Universal Dependencies, pp. 55–64 (2018)Multilingual Dependency Parsing 27
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep
bidirectionaltransformersforlanguageunderstanding.In:Proceedingsofthe2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (2019)
5. Dozat, T., Manning, C.D.: Deep biaﬃne attention for neural dependency parsing.
In: Proceedings of the 5th International Conference on Learning Representations
(2017)
6. Dozat,T.,Qi,P.,Manning,C.D.:Stanford’sgraph-basedneuraldependencyparser
attheCoNLL2017sharedtask.In:ProceedingsoftheCoNLL2017SharedTask:
Multilingual Parsing from Raw Text to Universal Dependencies, pp. 20–30 (2017)
7. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: cross-
lingual parameter sharing in a neural network parser. In: Proceedings of the 53rd
AnnualMeetingoftheAssociationforComputationalLinguistics(ACL),pp.845–
850 (2015)
8. Futrell, R., Mahowald, K., Gibson, E.: Quantifying word order freedom in depen-
dency corpora. In: Proceedings of the Third International Conference on Depen-
dency Linguistics (Depling), pp. 91–100 (2015)
9. Guo, J., Che, W., Yarowsky, D., Wang, H., Liu, T.: Cross-lingual dependency
parsing based on distributed representations. In: Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 1234–1244
(2015)
10. Hewitt,J.,Manning,C.D.:Astructuralprobeforﬁndingsyntaxinwordrepresen-
tations. In: Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies
(2019)
11. Kiperwasser, E., Goldberg, Y.: Simple and accurate dependency parsing using
bidirectional LSTM feature representations. Trans. Assoc. Comput. Linguist. 4,
313–327 (2016)
12. Kondratyuk, D., Straka, M.: 75 languages, 1 model: parsing Universal Dependen-
cies universally. In: Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 2779–2795 (2019)
13. Kuhlmann, M., G´omez-Rodr´ıguez, C., Satta, G.: Dynamic programming algo-
rithmsfortransition-baseddependencyparsers.In:Proceedingsofthe49thAnnual
Meeting of the Association for Computational Linguistics (ACL), pp. 673–682
(2011)
14. Kulmizev,A.,deLhoneux,M.,Gontrum,J.,Fano,E.,Nivre,J.:Deepcontextual-
izedwordembeddingsintransition-basedandgraph-baseddependencyparsing-a
tale of two parsers revisited. In: Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP), pp. 2755–2768 (2019)
15. Kulmizev, A., Ravishankar, V., Abdou, M., Nivre, J.: Do neural language models
show preferences for syntactic formalisms? In: Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 4077–4091
(2020)
16. Levshina, N.: Token-based typology and word order entropy: a study based on
Universal Dependencies. Linguist. Typology 23, 533–572 (2019)
17. de Lhoneux, M.: Linguistically informed neural dependency parsing for typologi-
cally diverse languages. Ph.D. thesis, Uppsala University (2019)28 J. Nivre
18. de Lhoneux, M., et al.: From raw text to Universal Dependencies - look, no tags!
In: Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw
Text to Universal Dependencies, pp. 207–217 (2017)
19. deLhoneux,M.,Stymne,S.,Nivre,J.:Arc-hybridnon-projectivedependencypars-
ing with a static-dynamic oracle. In: Proceedings of the 15th International Con-
ference on Parsing Technologies, pp. 99–104 (2017)
20. McDonald,R.,Nivre,J.:Characterizingtheerrorsofdata-drivendependencypars-
ing models. In: Proceedings of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pp. 122–131 (2007)
21. McDonald,R.,Nivre,J.:Analyzingandintegratingdependencyparsers.Comput.
Linguist. 37(1), 197–230 (2011)
22. McDonald,R.,Pereira,F.,Ribarov,K.,Hajiˇc,J.:Non-projectivedependencypars-
ingusingspanningtreealgorithms.In:ProceedingsoftheHumanLanguageTech-
nologyConferenceandtheConferenceonEmpiricalMethodsinNaturalLanguage
Processing (HLT/EMNLP), pp. 523–530 (2005)
23. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word repre-
sentations in vector space. arXiv preprint arXiv:1301.3781 (2013)
24. Nivre,J.:Aneﬃcientalgorithmforprojectivedependencyparsing.In:Proceedings
of the 8th International Workshop on Parsing Technologies (IWPT), pp. 149–160
(2003)
25. Nivre, J.: Algorithms for deterministic incremental dependency parsing. Comput.
Linguist. 34, 513–553 (2008)
26. Nivre,J.:Non-projectivedependencyparsinginexpectedlineartime.In:Proceed-
ings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP
(ACL-IJCNLP), pp. 351–359 (2009)
27. Nivre, J.: Towards a universal grammar for natural language processing. In: Gel-
bukh, A. (ed.) CICLing 2015. LNCS, vol. 9041, pp. 3–16. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-18111-0 1
28. Nivre, J., Boguslavsky, I.M., Iomdin, L.L.: Parsing the SynTagRus treebank of
Russian. In: Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pp. 641–648 (2008)
29. Nivre,J.,etal.:TheCoNLL2007sharedtaskondependencyparsing.In:Proceed-
ings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pp. 915–932 (2007)
30. Nivre, J., et al.: Universal Dependencies v1: a multilingual treebank collection.
In: Proceedings of the 10th International Conference on Language Resources and
Evaluation (LREC) (2016)
31. Nivre, J., et al.: Universal Dependencies v2: an evergrowing multilingual tree-
bankcollection.In:Proceedingsofthe12thInternationalConferenceonLanguage
Resources and Evaluation (LREC) (2020)
32. O¨stling, R.: Word order typology through multilingual word alignment. In: Pro-
ceedings of the 53rd Annual Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pp. 205–211 (2015)
33. Peters, M.E., et al.: Deep contextualized word representations. In: Proceedings of
the 2018 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long Papers),
pp. 2227–2237 (2018)
34. Schubert, K., Maxwell, D.: Metataxis in Practice: Dependency Syntax for Multi-
lingual Machine Translation. Mouton de Gruyter, Berlin (1987)Multilingual Dependency Parsing 29
35. Smith, A., Bohnet, B., de Lhoneux, M., Nivre, J., Shao, Y., Stymne, S.: 82 tree-
banks, 34 models: universal dependency parsing with multi-treebank models. In:
Proceedingsofthe2018CoNLLSharedTask:MultilingualParsingfromRawText
to Universal Dependencies (2018)
36. Smith, A., de Lhoneux, M., Stymne, S., Nivre, J.: An investigation of the inter-
actions between pre-trained word embeddings, character models and POS tags in
dependencyparsing.In:Proceedingsofthe2018ConferenceonEmpiricalMethods
in Natural Language Processing (2018)
37. Stevenson, M., Greenwood, M.A.: Dependency pattern models for information
extraction. Res. Lang. Comput. 7, 13–39 (2009). https://doi.org/10.1007/s11168-
009-9061-2
38. Tesni`ere, L.: E´l´ements de syntaxe structurale. Editions Klincksieck (1959)
39. Thompson, S.A.: Discourse motivations for the core-oblique distinction as a lan-
guageuniversal.In:Kamio,A.(ed.)DirectionsinFunctionalLinguistics,pp.59–82.
John Benjamins, Amsterdam (1997)
40. Tiedemann,J.:Cross-lingualdependencyparsingwithUniversalDependenciesand
predicted PoS labels. In: Proceedings of the Third International Conference on
Dependency Linguistics (Depling), pp. 340–349 (2015)
41. Tsarfaty, R., Seddah, D., Ku¨bler, S., Nivre, J.: Parsing morphologically rich lan-
guages: introduction to the special issue. Computat. Linguist. 39, 15–22 (2013)
42. Zeman,D.,etal.:CoNLL2018sharedtask:multilingualparsingfromrawtextto
universal dependencies. In: Proceedings of the CoNLL 2018 Shared Task: Multi-
lingual Parsing from Raw Text to Universal Dependencies (2018)
43. Zeman,D.,etal.:UniversalDependencies2.5(2019).http://hdl.handle.net/11234/
1-3105, LINDAT/CLARIN digital library at the Institute of Formal and Applied
Linguistics (U´FAL), Faculty of Mathematics and Physics, Charles University.
http://hdl.handle.net/11234/1-3105
44. Zeman,D.,etal.:CoNLL2017sharedtask:multilingualparsingfromrawtextto
universal dependencies. In: Proceedings of the CoNLL 2017 Shared Task: Multi-
lingual Parsing from Raw Text to Universal Dependencies, pp. 1–19 (2017)
45. Zhang, Y., Nivre, J.: Analyzing the eﬀect of global learning and beam-search on
transition-based dependency parsing. In: Proceedings of COLING 2012: Posters,
pp. 1391–1400 (2012)Multimodal Fake News Detection with
Textual, Visual and Semantic Information
B
Anastasia Giachanou1( ) , Guobiao Zhang1,2, and Paolo Rosso1
1 Universitat Polit`ecnica de Val`encia, Valencia, Spain
angia9@upv.es, prosso@dsic.upv.es
2 Wuhan University, Wuhan, China
zgb0537@whu.edu.cn
Abstract. Recentyearshaveseenarapidgrowthinthenumberoffake
news that are posted online. Fake news detection is very challenging
since they are usually created to contain a mixture of false and real
information and images that have been manipulated that confuses the
readers.Inthispaper,weproposeamultimodalsystemwiththeaimto
diﬀerentiatebetweenfakeandrealposts.Oursystemisbasedonaneural
network and combines textual, visual and semantic information. The
textualinformationisextractedfromthecontentofthepost,thevisual
one from the image that is associated with the post and the semantic
refers to the similarity between the image and the text of the post. We
conductourexperimentsonthreestandardrealworldcollectionsandwe
show the importance of those features on detecting fake news.
· ·
Keywords: Multimodal fake news detection Visual features
·
Textual features Image-text similarity
1 Introduction
Recent years have seen a rapid growth in the amount of fake news that are
publishedonline.Althoughfakenewsisnotanewphenomenon,theriseofsocial
media has oﬀered an easy platform for their fast propagation. A large amount
of invalid claims, rumours and clickbaits are posted every day online with the
aim to deceive people and to inﬂuence their opinions on diﬀerent topics. For
example, the outcome of Brexit1 has been into question because of the amount
of fake news that were posted before the referendum.
Fake news detection is not a trivial task since the content and the images
are manipulated in many diﬀerent ways which makes the development of an
eﬀectivesystemdiﬃcult.Severalresearchershavetriedtoaddresstheproblemof
fakenewsdetection.Earlyworksfocusedonusingtextualinformationextracted
fromthetextofthedocument,suchasstatisticaltextfeatures[2]andemotional
1 https://www.theguardian.com/world/2017/nov/14/how-400-russia-run-fake-
accounts-posted-bogus-brexit-tweets.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.30–38,2020.
https://doi.org/10.1007/978-3-030-58323-1_3Multimodal Fake News Detection 31
information[6,9].Apartfromthecontent,researchershavealsoexploredtherole
of users [8,16] and the credibility of the source where the post is published [14].
Although content information is very important for the detection of fake
news, it is not suﬃcient alone. Online articles and posts usually contain images
thatprovideusefulinformationforaclassiﬁcationsystem.Someresearchershave
proposed multimodal approaches for the detection of fake news [12,22]. The
majorityofthosesystemscombinetextualandvisualinformationtoaddressthe
problem. However, in addition to the visual information, the similarity between
the image and the text is very important since it is possible that in some fake
news the image to be contradictory to the content. Although text-image simi-
larity can be an additional useful information, it still remains under-explored.
In this paper we propose a system that uses multimodal information to dif-
ferentiate between fake and real news. To this end, we combine textual, visual
and semantic information. Our main motivation is that information that comes
from diﬀerent sources complement each other in detecting fake news. In addi-
tion, some of the fake news contain manipulated images that do not correspond
to the post’s content. Therefore, we also incorporate semantic information that
referstothesimilaritybetweenthetextandtheimage.Ourexperimentalresults
on three diﬀerent collections show that combining textual, visual and semantic
information can lead to an eﬀective fake news detection.
2 Related Work
Early attempts on fake news detection were based on textual information.
Castillo et al. [2] explored the eﬀectiveness of various statistical text features,
suchascountofwordandpunctuation,whereasRashkinetal.[15]incorporated
various linguistic features extracted with the LIWC dictionary [20] into a Long
Short Term Memory (LSTM) network to detect credible posts.
Some researchers explored the role of emotions on the area of fake news.
Vosoughi et al. [21] investigated true and false rumours on Twitter and found
that false rumours triggered fear, disgust and surprise in their replies, whereas
the true rumours triggered joy, sadness, trust and anticipation. Giachanou et
al. [9] proposed an LSTM-based neural network that leveraged emotions from
text to address credibility detection, whereas Ghanem et al. [6] explored the
impact of emotions regarding the detection of the diﬀerent types of fake news.
Visual information complements the textual one and improve the eﬀective-
ness of systems on fake news detection. Wang et al. [22] proposed the Event
Adversarial Neural Networks (EANN) model that consists of the textual com-
ponentrepresentedbywordembeddingsandthevisualthatwasextractedusing
the VGG-19 model pre-trained on ImageNet. Khattar et al. [12] proposed the
Multimodal Variational Autoencoder (MVAE) model based on bi-directional
LSTMsandVGG-19forthetextandimagerepresentationrespectively.Zlatkova
et al. [24] explored the eﬀectiveness of text-image similarity in addition to other
visual information but on the task of claim factuality prediction with respect to
an image.32 A. Giachanou et al.
Diﬀerenttothepreviouswork,notonlyweexploretheeﬀectivenessofawider
rangeofvisualfeaturesonfakenewsdetectionbutalsoofthesimilaritybetween
the image and the text. Our visual features include image tags generated using
ﬁve diﬀerent models as well as LBP, whereas the similarity is calculated using
the embeddings of the post’s text and the image tags.
Fig.1. Architecture of the multimodal fake news detection model.
3 Multimodal Fake News Detection
In this section, we present our multimodal system that aims to diﬀerentiate
between fake and real news. Our system is based on a neural network and
combines the following three diﬀerent types of information: textual, visual and
semantic. The architecture of our system is depicted in Fig.1.
Forthetextualinformation,wecombinewordembeddingsandsentiment.To
extract the word embeddings, we use the public pre-trained words and phrase
vectors GoogleNews-vectors-negative300 that contains 300-dimensional vectors
for 3 million words and phrases.
Inadditiontothewordembeddings,wealsoestimatethesentimentexpressed
in the posts. Sentiment analysis has in general attracted a lot of research atten-
tion and aims to annotate a text regarding to its polarity. Sentimental infor-
mation has been shown to be useful for fake news detection as well as in other
classiﬁcationtasks[7].Toextractthesentimentscorefromthedocuments,weuse
the Valence Aware Dictionary for sEntiment Reasoning (VADER). VADER [4]Multimodal Fake News Detection 33
is a sophisticated tool that in addition to the terms’ sentiment, takes also into
accountfactorssuchasnegationandemoticonusageandpredictsthenormalized
valence of positive or negative sentiment of the given text.
For the visual component, we combine image tags and Local Binary Pat-
terns (LBP) [13]. The visual information can be very useful in case there are
diﬀerent patterns used in fake and real news or there are images that have been
manipulated. To extract the image tags we use pre-trained CNN-based mod-
els. These models are the VGG16, VGG19 [18], Resnet [10], Inception [19], and
Xception [3]. The models are pre-trained on the visual dataset ImageNet that
containsover14millionhand-annotatedimages[5].Weextractthetoptenimage
tags using the pre-trained models, so every image has in total 50 tags. Then for
eachtag,weusetheword2vecembeddingstoestimatethe300-dimensionvector
by averaging the embeddings.
In addition to the image tags, we also explore the eﬀectiveness of LBP. LBP
isaveryeﬃcienttextureoperatorwhichlabelsthepixelsofanimagebyputting
a threshold on the neighborhood of each pixel and considers the result as a
binary number. LBP has been proved to be very eﬀective in many visual tasks
suchas facerecognition [11].Similar toprevious studies thathave usedLBPfor
other tasks such as multimodal sentiment analysis [23], we reduce the original
256-dimensional LBP feature histogram to a 53-dimensional vector.
Table 1. Label statistics of the collections
Collection Real Fake
Training Test Training Test
MediaEval 4,997 1,202 6,742 2,483
PolitiFact 1,067 266 1,712 428
GossipCop 16,495 4,124 16,495 4,124
Finally,thesemanticinformationreferstothetext-imagesimilarity.Estimat-
ing this similarity is very important since it is possible that fake news contain
images that are not relevant to the text. To calculate the similarity we calculate
the cosine similarity between the word embeddings of the text and the embed-
dingsoftheimagetagsextractedfromthevisualfeatureextraction.Thisfeature
providesa5-dimensionalvector,whereeachvectorreferstooneimagetagmodel
(e.g.,VGG16)andiscalculatedbasedontheaveragesimilaritybetweentheword
embeddings of the text and the embeddings of the image tags.
4 Experimental Setup
In this section we describe the collections and the experimental settings used to
run our experiments.34 A. Giachanou et al.
4.1 Collections
For our experiments we use MediaEval [1] and FakeNewsNet [17] that, to the
best of our knowledge, are the only standard collections that contain tweets in
Englishandthatcanbeusedformultimodalfakenewsdetection.Table1shows
the statistics regarding the labels of the collections.
– MediaEval:ThiscollectionwasreleasedasapartoftheVerifying Multimedia
Use at MediaEval challenge [1]. The aim of the task was to detect fake mul-
timedia content on social media. The collection consists of tweets and each
tweet is provided with textual content, image/video and social context infor-
mation. After removing the tweets that did not have an image, we managed
to have a training set of 11,739 tweets of which 4,997 are real and 6,742 are
fake. Our test set contains 3,685 tweets of which 1,202 are real and 2,483 are
fake.
– PolitiFact: This collection is based on tweets that have been collected with
regards to the posts that are published in PolitiFact2 and is part of the
FakeNewsNetcollection[17].PolitiFactisawebsitethatexpertsinjournalism
annotate news articles and political claims as fake or real. To create the
FakeNewsNetcollectionShuetal.usedtheheadlinesofthosepostsasqueries
to collect relevant tweets. We used the tweet ids provided as part of the
FakeNewsNet and the Twitter API to collect the tweets (text and image)
that were available. In total, we managed to collect 2,140 fake and 1,333 real
tweets posts.
Table 2. Neural network parameters
Layers Neurons Learningrate Dropout Activation Optimiser Epochs
4 1000,500,300,100 0.001 0.6 Sigmoid Adam 50
– GossipCop: Similar to PolitiFact, this collection is based on the FakeNews-
Netcollection[17].Thiscollectionisbasedontweetsthatwerecollectedusing
theheadlinesofarticlesthatwerepostedandannotatedinGossipCop3.Fak-
eNewsNetcontains5,323fakeand16,817realnewspostedinGossipCop.Due
to the imbalance between the classes, we decided to use under-sampling and
we randomly selected 5,323 real news posts. We used the tweet ids and the
Twitter API to collect the tweets (text and image) that were still available.
In total, we managed to collect 20,619 tweets for each class.
4.2 Experimental Settings
For our experiments on the PolitiFact and the GossipCop collections, we use
20% of our corpus of tweets for test and 80% for training. For the MediaE-
val, we use the sets as provided in the original collection, that refer to 23% for
2 https://www.politifact.com/.
3 https://www.gossipcop.com/.Multimodal Fake News Detection 35
test and the rest for training. We initialize our embedding layer with the pre-
trained GoogleNews-vectors-negative300 words and phrase vectors. It is worth
tomentionthatatthebeginningofourexperiments,wetestedalsootherclassi-
ﬁersincludingSupportVectorMachinesandRandomForest.Theoverallresults
showedthattheneuralnetworkperformedbetterforthisparticulartask.Table2
shows the parameters for the neural network. We have experimented with other
hyperparameters, such as diﬀerent hidden layer number, hidden units, learn-
ing rate and dropout. The dropout is applied to each layer. We used the same
parameters for all the three diﬀerent collections.
WeusedkerastobuildtheneuralnetworkandtheVGG16,VGG19,Resnet,
Inception and Xception. Finally, opencv and scikit-image libraries were used to
extract the LBP features4.
5 Results
Table3showstheperformanceresultsoftheexperimentsonPolitiFact,MediaE-
valandGossipCopwithregardstoF1-metric.First,weevaluatethesystemwhen
one type of information is used. From the results, we observe that in this case
the word embeddings achieve the best performance in all the three collections
compared to the other types of information. This is expected given that word
embeddings are usually a strong indicator in many text classiﬁcation tasks. In
addition, wenoticethatinMediaEval,text-image similarity managestoachieve
a high performance as well. With regards to the visual information, we observe
thatimagetagsperformbetterthanLBPonallthecollections.Thiscanbedue
to the fact that image tags represent a larger vector compared to LBP.
Table 3. Performance results of the diﬀerent combinations of information and the
diﬀerentcollectionsonthefakenewsdetectiontask.Thebestresultforeverycollection
is emphasized in bold.
PolitiFact MediaEval GossipCop
Embeddings 0.911 0.885 0.815
Sentiment 0.474 0.352 0.562
Tags 0.718 0.615 0.623
LBP 0.474 0.520 0.551
Similarity 0.474 0.875 0.538
Text-tags 0.924 0.637 0.825
Text-LBP 0.909 0.896 0.814
Text-tags-similarity 0.920 0.636 0.827
Text-LBP-similarity 0.910 0.908 0.816
Text-tags-LBP-similarity 0.925 0.622 0.829
4 https://www.pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-
opencv/.36 A. Giachanou et al.
Next, we explore the eﬀectiveness of our system when the text (embeddings
+sentiment)iscombinedwiththevisualinformation.WeobservethatonPoliti-
FactandGossipCop,thecombinationoftextandimagetags(text-tags)performs
betterthanthecombinationoftextandLBP,whereasincaseofMediaEval,the
text-LBP achieves a higher performance compared to text-tags. Webelieve that
thepoorperformanceoftheimagetagsonMediaEvalhastodowiththeimages
of the collection that refer to natural disasters and tend to be more complex
than the images on PolitiFact and GossipCop.
Finally, we incorporate the text-image similarity into the system to evaluate
itsimpactwhenitiscombinedwiththerestoftheinformation.Fromtheresults,
we observe that incorporating text-image similarity improves the performance.
With regards to GossipCop, the text-tags-LBP-similarity combination improves
theperformanceby1.72%and1.84%comparedtoword embeddings andtotext-
LBP respectively. Similar, in case of PolitiFact, the text-tags-LBP-similarity
combination achieves a 1.51% increase compared to word embeddings.
Finally, regarding MediaEval we observe that the best performance is
achieved by the text-LBP-similarity combination, whereas the text-tags-LBP-
similarity combination is not very eﬀective. When with text-LBP-similarity the
system combines the text, LBP and text-image similarity it manages to outper-
form word embeddings by 2.53%.
6 Conclusions and Future Work
In this paper, we proposed a multimodal system to address the problem of
fake news detection. The proposed system is based on a neural network and
combinestextual,visualandsemanticinformation.Thetextualinformationwas
based on the word embeddings and the sentiment expressed in the post, the
visual information was based on image tags and LBP, whereas the semantic
one referred to the text-image similarity. The experimental results showed that
combiningtextual,visualandtext-imagesimilarityinformationisveryusefulfor
the task of fake news detection. Finally, our results showed that diﬀerent visual
information is eﬀective for the diﬀerent collections.
In future, we plan to investigate more visual features extracted from images
such as the color histogram. In addition, we plan to explore the eﬀectiveness of
the multimodal information on fake news detection across diﬀerent languages.
Acknowledgments. Anastasia Giachanou is supported by the SNSF Early Postdoc
MobilitygrantundertheprojectEarlyFakeNewsDetectiononSocialMedia,Switzer-
land(P2TIP2 181441).GuobiaoZhangisfundedbyChinaScholarshipCouncil(CSC)
from the Ministry of Education of P.R. China. The work of Paolo Rosso is partially
fundedbytheSpanishMICINNundertheresearchprojectMISMIS-FAKEnHATEon
MisinformationandMiscommunicationinsocialmedia:FAKEnewsandHATEspeech
(PGC2018-096212-B-C31).Multimodal Fake News Detection 37
References
1. Boididou, C., et al.: Verifying multimedia use at MediaEval 2015. In: MediaEval
2015 Workshop, pp. 235–237 (2015)
2. Castillo, C., Mendoza, M., Poblete, B.: Information credibility on Twitter. In:
WWW 2011, pp. 675–684 (2011)
3. Chollet, F.: Xception: deep learning with depthwise separable convolutions. In:
CVPR 2017, pp. 1251–1258 (2017)
4. Davidson,T.,Warmsley,D.,Macy,M.,Weber,I.:Automatedhatespeechdetection
and the problem of oﬀensive language. In: ICWSM 2017 (2017)
5. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:alarge-scale
hierarchical image database. In: CVPR 2009, pp. 248–255 (2009)
6. Ghanem, B., Rosso, P., Rangel, F.: An emotional analysis of false information in
socialmediaandnewsarticles.ACMTrans.InternetTechnol.(TOIT)20(2),1–18
(2020)
7. Giachanou, A., Gonzalo, J., Mele, I., Crestani, F.: Sentiment propagation for pre-
dicting reputation polarity. In: Jose, J.M., et al. (eds.) ECIR 2017. LNCS, vol.
10193, pp. 226–238. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
56608-5 18
8. Giachanou, A., R´ıssola, E.A., Ghanem, B., Crestani, F., Rosso, P.: The role of
personality and linguistic patterns in discriminating between fake news spreaders
and fact checkers. In: M´etais, E., Meziane, F., Horacek, H., Cimiano, P. (eds.)
NLDB 2020. LNCS, vol. 12089, pp. 181–192. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-51310-8 17
9. Giachanou,A.,Rosso,P.,Crestani,F.:Leveragingemotionalsignalsforcredibility
detection. In: SIGIR 2019, pp. 877–880 (2019)
10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR 2016, pp. 770–778 (2016)
11. Huang, D., Shan, C., Ardabilian, M., Wang, Y., Chen, L.: Local binary patterns
and its application to facial image analysis: a survey. IEEE Trans. Syst. Man
Cybern. Part C 41(6), 765–781 (2011)
12. Khattar, D., Goud, J.S., Gupta, M., Varma, V.: MVAE: multimodal variational
autoencoder for fake news detection. In: WWW 2019, pp. 2915–2921 (2019)
13. Ojala, T., Pietikainen, M., Maenpaa, T.: Multiresolution gray-scale and rotation
invariant texture classiﬁcation with local binary patterns. IEEE Trans. Pattern
Anal. Mach. Intell. 24(7), 971–987 (2002)
14. Popat, K., Mukherjee, S., Yates, A., Weikum, G.: DeClarE: debunking fake news
and false claims using evidence-aware deep learning. In: EMNLP 2018, pp. 22–32
(2018)
15. Rashkin, H., Choi, E., Jang, J.Y., Volkova, S., Choi, Y.: Truth of varying shades:
analyzinglanguageinfakenewsandpoliticalfact-checking.In:EMNLP2017,pp.
2931–2937 (2017)
16. Shu, K., Wang, S., Liu, H.: Understanding user proﬁles on social media for fake
news detection. In: MIPR 2018, pp. 430–435 (2018)
17. Shu, K., Mahudeswaran, D., Wang, S., Lee, D., Liu, H.: FakeNewsNet: a data
repository with news content, social context and spatialtemporal information for
studying fake news on social media. arXiv:1809.01286 (2018)
18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv:1409.1556 (2014)38 A. Giachanou et al.
19. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: CVPR 2016, pp. 2818–2826 (2016)
20. Tausczik,Y.R.,Pennebaker,J.W.:Thepsychologicalmeaningofwords:LIWCand
computerized text analysis methods. J. Lang. Soc. Psychol. 29(1), 24–54 (2010)
21. Vosoughi, S., Roy, D., Aral, S.: The spread of true and false news online. Science
359(6380), 1146–1151 (2018)
22. Wang, Y., et al.: EANN: event adversarial neural networks for multi-modal fake
news detection. In: KDD 2018, pp. 849–857 (2018)
23. Zhao, Z., et al.: An image-text consistency driven multimodal sentiment analysis
approach for social media. Inf. Process. Manag. 56(6), 102097 (2019)
24. Zlatkova, D., Nakov, P., Koychev, I.: Fact-checking meets fauxtography: verifying
claims about images. In: EMNLP-IJCNLP 2019, pp. 2099–2108 (2019)TextA Twitter Political Corpus of the 2019
10N Spanish Election
B
Javier Sa´nchez-Junquera1( ) , Simone Paolo Ponzetto2, and Paolo Rosso1
1 Universitat Polit`ecnica de Val`encia, Valencia, Spain
jjsjunquera@gmail.com
2 Data and Web Science Group, University of Mannheim, Mannheim, Germany
Abstract. WepresentacorpusofSpanishtweetsof15Twitteraccounts
ofpoliticiansofthemainﬁveparties(PSOE,PP,Cs,UPandVOX)cov-
eringthecampaignoftheSpanishelectionof10thNovember2019(10N
Spanish Election). We perform a semi-automatic annotation of domain-
speciﬁc topics using a mixture of keyword-based and supervised tech-
niques. In this preliminary study we extracted the tweets of few politi-
cians ofeachpartywiththe aim toanalyse their oﬃcial communication
strategy.Moreover,weanalysesentimentsandemotionsemployedinthe
tweets. Although the limited size of the Twitter corpus due to the very
shorttimespan,wehopetoprovidewithsomeﬁrstinsightsonthecom-
munication dynamics of social network accounts of these ﬁve Spanish
political parties.
· · ·
Keywords: Twitter Political text analysis Topic detection
Sentiment and emotion analysis
1 Introduction
Inrecentyears,automatedtextanalysishasbecomecentralforworkinsocialand
political science that relies on a data-driven perspective. Political scientists, for
instance, have used text for a wide range of problems, including inferring policy
positionsofactors[6],anddetectingtopics[13],tonameafew.Atthesametime,
researchersinNatural Language Processing(NLP) have addressedrelatedtasks
such as election prediction [11], stance detection towards legislative proposals
[16],predictingrollcalls[5],measuringagreementinelectoralmanifestos[8],and
policy preference labelling [1] from a diﬀerent, yet complementary perspective.
Recent attempts to bring these two communities closer have focused on shared
evaluation exercises [10] as well as bringing together the body of the scholarly
literatureofthetwocommunities[4].Theeﬀectsofthesetwostrandsofresearch
coming together can be seen in political scientists making use and leveraging
major advances in NLP from the past years [12].
The contributions of this paper are the following ones: (i) we introduce a
corpus of tweets from all major Spanish political parties during the autumn
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.41–49,2020.
https://doi.org/10.1007/978-3-030-58323-1_442 J. S´anchez-Junquera et al.
2019 election; (ii) we present details on the semi-automated topic and senti-
ment/emotionannotationprocess;and(iii)weprovideapreliminaryqualitative
analysis of the dataset over diﬀerent addressed topics of the election campaign.
Building thispreliminary resourceofSpanishpolitical tweets, weaim atprovid-
ing a ﬁrst reference corpus of Spanish tweets in order to foster further research
in political text analysis and forecasting with Twitter in languages other than
English.
In the rest of the paper we will describe how each tweet was annotated with
topic information together with sentiments and emotions. Moreover, we will
illustratethepreliminaryexperimentswecarriedoutontopicdetection.Finally,
wewillpresentsomeinsightaboutsentimentandemotiontopic-relatedanalyses.
2 Related Works
TwitterhasbeenusedasasourceoftextsfordiﬀerentNLPtaskslikesentiment
analysis [3,9]. One work that is very related to our study is [7]. They collected
a dataset in English for topic identiﬁcation and sentiment analysis. The authors
used distant supervision for training, in which topic-related keywords were used
toﬁrstobtainacollectionofpositiveexamplesforthetopicidentiﬁcation.Their
results show that the obtained examples could serve as a training set for clas-
sifying unlabelled instances more eﬀectively than using only the keywords as
the topic predictors. However, during our corpus development we noticed that
keyword-based retrieval can produce noisy data, maybe because of the content
andthetopicsofourtweets,andwethenusedacombinationofbothakeyword-
based and a supervised approach.
3 Political Tweets in the 10N Spanish Election
In this paper, we focus on the Spanish election of November 10th, 2019 (10N
Spanish Election, hereafter). For this, we analyse tweets between the short time
span of October 10, 2019, and November 12, 2019. We focus on the tweets from
15 representative proﬁles of the ﬁve most important political parties (Table 1)1:
i.e.,UnidasPodemos(UP);Ciudadanos(Cs);PartidoSocialistaObreroEspan˜ol
(PSOE); Partido Popular (PP); and VOX.
3.1 Topic Identiﬁcation
Topic Categories. We ﬁrst describe how we detect the topic of the tweets on
thebasisofakeyword-basedandsupervisedapproach.Inthecontextofthe10N
Spanish Election, we focused on the following topics that were mentioned in the
political manifestos of the ﬁve main Spanish parties: Immigration, Catalonia,
Economy (and Employment), Education (together with Culture and Research),
Feminism, Historical Memory, and Healthcare. We additionally include a cate-
gory label Other for the tweets that talk about any other topic.
1 The dataset is available at https://github.com/jjsjunquera/10N-Spanish-Election.A Twitter Political Corpus of the 2019 10N Spanish Election 43
Table 1. Number of tweets of the ﬁve political parties. For each party, we use its
oﬃcial Twitter account, its leader, and the female politician that took part in the 7N
TV debate.
Parties The main proﬁles Tweets
UP @ahorapodemos, @Irene Montero , @Pablo Iglesias 671
Cs @CiudadanosCs, @InesArrimadas, @Albert Rivera 789
PSOE @PSOE, @mjmonteroc, @sanchezcastejon 527
PP @populares, @anapastorjulian, @pablocasado 684
Vox @vox es, @monasterior, @santi abascal 749
Total 3582
Table 2. Total number of labelled tweets: the training set (i.e., manually annotated,
and using keywords), and using automatic annotation. The last column has the total
number of labelled tweets considering the training set and the classiﬁer results.
Topic Manualannotated Keywordannotated Automaticallyannotated Totalannotated
Catalonia 115 130 370 615
Economy 71 39 506 616
Education 2 19 23 44
Feminism 10 52 82 144
Healthcare 4 12 7 23
HistoricalMemory 12 16 30 58
Immigration 9 16 36 61
Other 541 153 1037 1731
Pensions 1 24 55 80
Total 765 461 2146 3372
Manual Topic Annotation. Weﬁrstmanuallyannotate1,000randomlysam-
pled tweets using our topic labels.
Table2 summarizes the label distribution across all parties. After removing
the noisy tweets, we are left with only 765 posts. Many tweets in our corpus
are not related to any of the topics of interest, and were assigned to the Other
category. Moreover, during the annotation, we noticed in the manifestos of the
ﬁvepartieslittleinformationabouttopicssuchasresearch,corruption,renewable
energy, and climate change.
Keyword-Based Topic Detection. Due to the manual annotation is time
consuming, we complement it by using topic-related keywords to collect tweets
about each topic. We ranked the words appearing in the sections corresponding
to the topics of interest with the highest Pointwise Mutual Information (PMI).
PMI makes it possible to select the most relevant words for each topic, and is
computed as: PMI(T,w) = log p(T,w) . Where p(T,w) is the probability of a
p(T)p(w)
wordtoappearinatopic,p(T)istheprobabilityofatopic(weassumethetopic
distribution to be uniform), and p(w) is the probability of w. For each topic, we
collect the top-10 highest ranked keywords and manually ﬁlter incorrect ones
(Table 3).44 J. S´anchez-Junquera et al.
Table 3. Keywords used for collecting training data for topic identiﬁcation.
Topic Keywords
Catalonia autono´mica; catalun˜a; civil
Economy bienestar; discapacidad; energ´ıa;
ﬁscalidad; impuesto; innovacio´n;
inversio´n; tecnolo´gico
Education cultura; cultural; educacio´n; lenguas;
mecenazo
Feminism conciliacio´n; familia; machismo;
madres; discriminacio´n; mujeres;
sexual; violencia
Healthcare infantil; sanitario; salud; sanidad;
sanitaria; universal
HistoricalMemory historia; memoria; reparacio´n;
v´ıctimas
Immigration ceuta; extranjeros; inmigrantes;
ilegalmente
Pension pensiones; toledo
Supervised Learning of Topics. For each topic, we collect all tweets in our
corpus in which at least one of its keywords appears. All retrieved tweets are
thenmanuallycheckedtoensurethattheannotatedtweetshaveaground-truth.
Inspired by the work of [7], we use the topic-related keywords to obtain a
collection of “positive” examples to be used as a training set for a supervised
classiﬁer. However, in our dataset, we noticed that keyword-based retrieval can
produce much noisy data. Therefore, the keyword-based collected tweets are
manually checked before training the classiﬁer.
While our solution still requires the mentioned manual checking, the advan-
tage of using keywords is that the labelling is more focused on tweets that are
likely to be in one of the topics of interest, thus reducing the annotation eﬀort
associated with tweets from the Other category.
Table2 summarizes in the second and third columns the number of tweets
that we used as a training set. The second column represents the results after
manually evaluating the tweets labelled by using the keywords. It is interest-
ing that the annotated data reveal most attention towards some topics such
as Catalonia, Feminism and Economy. Finally, the dataset used for training is
composed of all the labelled tweets. To avoid bias towards the most populated
categories we reduce their number of examples to 100 for training, for which
we balance the presence of manually annotated and keyword-based annotated
tweets.
We employ a SVM2 to classify the still unlabeled tweets and leave-one-out
cross-validation because of the small size of the corpus. We represent the tweets
2 Weusedtheimplementationfromsklearn usingdefaultparametervaluesforwitha
linear kernel.A Twitter Political Corpus of the 2019 10N Spanish Election 45
with unigrams, bigrams and trigrams, and use the tf-idf weighting scheme after
removing the n-grams occurring only once.
EvaluationofTopicDetection. Table4ashowsthestandardprecision,recall,
and F scores. Table2 shows in the fourth column the number of tweets anno-
1
tated using our supervised model. The last column shows instead the total of
labelled tweets for each of the topics – i.e., the overall number of labelled tweets
obtained by combining manual, keyword-based annotations with the SVM clas-
siﬁer. We break down the numbers of these overall annotated tweets per party
in Table 4b. The topic distributions seem to suggest that each party is biased
towardsspeciﬁctopics.Forinstance,Immigration seemstobealmostonlymen-
tionedbyVOX,whereaspartieslikePPandCsaremainlyfocusedonCatalonia
and Economy.
3.2 Sentiment Analysis
We next analyse the sentiment expressed by the parties about each topic. For
this, we use SentiStrength to estimate the sentiment in tweets since it has been
eﬀectively used in short informal texts [15]. We compute a single scale with
values from −4 (extremely negative) to 4 (extremely positive).
Table 4. Results on topic classiﬁcation the total number of labelled tweets.
Resultsontopicclassiﬁcation
Totaloflabelledtweets
Topic Precision Recall F1-score
Topic UP Cs PP PSOE VOX
Catalonia 0.72 0.86 0.78
Catalonia 40 198 110 50 72
Economy 0.56 0.7 0.62
Economy 114 117 203 84 88
Education 0.83 0.48 0.61
Education 12 12 11 5 4
Feminism 0.8 0.73 0.77
Feminism 44 30 8 29 31
Healthcare 1 0.38 0.55
Healthcare 10 2 3 6 2
HistoricalMemory 0.82 0.5 0.62
HistoricalMemory 25 7 2 8 16
Immigration 0.92 0.44 0.59
Immigration 4 1 – 7 49
Other 0.56 0.6 0.58
Other 258 262 200 174 243
Pensions 0.85 0.68 0.76
Pensions 17 2 14 37 10
Macroavg. 0.78 0.6 0.65
Fig.1. Expressed sentiment for each topic and party.46 J. S´anchez-Junquera et al.
In order to compare for each topic the sentiment expressed by a party, we
compute the average of the scores for the party on that topic. Only the topics
with a precision greater than 0.6 (Table4a), and the parties that wrote more
than 10 tweets on the corresponding topic, were considered in this compari-
son. It means that we ignore, for instance, the sentiment showed towards Econ-
omy (precision lower than 0.6), and Healthcare (only UP wrote 10 tweets, see
Table4b, and the sentiment that Csshowed towards Pensions (only two tweets,
see Table4b).
Figure1 shows the expressed sentiment for the parties for each topic. Senti-
ment scores seem to reveal some common dynamics of political communication
from political parties in social networks in that generally, even when the party
is known to be negative or have a critical stance with respect to a certain topic
(e.g., a populist party on immigration), tweets receive a positive score. Specif-
ically, we see that VOX was the only party addressing the Immigration topic,
andweobservethatingeneral,itssentimentispositive(i.e.,solutionswerecom-
mented). Also, just two parties show mainly negative sentiments, they are VOX
and PP towards Feminism and Pensions respectively.
3.3 Emotion Analysis
Weﬁnallyanalysetheemotionsexpressedbythepartiesfordiﬀerenttopicsusing
the Spanish Emotion Lexicon (SEL) [14]. SEL has 2,036 words associated with
the measure of Probability Factor of Aﬀective (PFA) concerning to at least one
Ekman’s emotions [2]: joy, anger, fear, sadness, surprise, and disgust. For each
tweet, we compute the ﬁnal measure for each of the ﬁve emotions by summing
the PFA and dividing by the length of the tweet. We then compute the average
PFA of all the emotions for each party and each topic.
Figure2 (top image on the left) shows the emotions that the parties present
in their tweets when talking about diﬀerent topics. We analyse the emotions of
thesamepairsofpartiesandtopicsweanalysedbeforeinSect.3.2.Diﬀerentlyto
thecaseofsentiment,thereisageneraltrendsharedinthatjoyandsadnessare
verymuchpresentacrossall parties.This couldbedueto severalreasons.First,
there is a bias in SEL towards joy (668 words related to joy vs. 391 for sadness,
382foranger,211forfear,209fordisgust,and175forsurprise),andsecond,the
termsthathelptocomputetheSentiStrengthscorearenotnecessarilythesame
that are in SEL. Another interesting thing is the presence of joy and sadness
in the same topic by the same parties. We attribute this behaviour to the fact
that there are tweets describing the current problems and feelings present in
the context of the election – e.g., using words like sufrir (to suﬀer), muerte
(death), triste (sad), grave (grave), but also there are others with a propositive
discourse about the problems – e.g., using words like esperanza (hope), ´animo
(encouragement), unio´n (union), ﬁesta (party).
In Figure2 we also highlight that PSOE shows contrasting emotions about
Catalonia; andCsshowshighscoreofjoyabouttopicsrelatedtofeminism.The
distribution of the emotions from VOX towards Immigration was omitted dueA Twitter Political Corpus of the 2019 10N Spanish Election 47
Fig.2. Emotions distribution across topics.
to the space. However, despite the positive sentiment that VOX showed in this
topic, the predominant expressed emotion was sadness.
4 Conclusions
Inthispaperwepresentedaﬁrststudyaboutthemostrelevanttopicsthathave
beenaddressedinTwitterinthecontextofthe10NSpanishelectionfortheﬁve
main political parties, together with their sentiments and emotions.
On the basis of the above analysis, we noticed that each party focused more
on speciﬁc topics, expressing diﬀerent sentiments and emotions. Our analysis,
although preliminary, indicates potentially interesting dimensions of political
communicationsonsocialnetworkssuchasthetendencytowardspositivetweets,
as well the contrasted presence of problems vs. solutions. This work provides a
ﬁrst attempt towards analysing the political communication by the ﬁve main
politicalpartiesinSpainonsocialnetworksusingNLPtechniques.Althoughwe
are aware of the limitations of this preliminary study due to the very short time
spanandthesizeofthecorpus,wehopethatthisﬁrstanalysiscouldcontribute
to understand how sentiments and emotions were expressed in Twitter by the
politicians of the main ﬁve parties with respect to the topics mentioned in their
manifestos during the political campaign of the 10N Election in Spain.
As future work we plan also to consider additional parties and languages
(e.g. Catalan, Basque and Galician) to provide a more comprehensive resource
as well as a comparative analysis.48 J. S´anchez-Junquera et al.
Acknowledgements. The work of the authors from the Universitat Polit`ecnica de
Val`encia was funded by the Spanish MICINN under the research project MISMIS-
FAKEnHATEonMisinformationandMiscommunicationinsocialmedia:FAKEnews
and HATE speech (PGC2018-096212-B-C31).
References
1. Abercrombie,G.,Nanni,F.,Batista-Navarro,R.,Ponzetto,S.P.:Policypreference
detectioninparliamentarydebatemotions.In:Proceedingsofthe23rdConference
on Computational Natural Language Learning (CoNLL), Hong Kong, China, pp.
249–259. Association for Computational Linguistics, November 2019
2. Ekman, P., et al.: Universals and cultural diﬀerences in the judgments of facial
expressions of emotion. J. Pers. Soc. Psychol. 53(4), 712 (1987)
3. Gao,W.,Sebastiani,F.:Tweetsentiment:fromclassiﬁcationtoquantiﬁcation.In:
2015IEEE/ACMInternationalConferenceonASONAM,pp.97–104.IEEE(2015)
4. Glavaˇs, G., Nanni, F., Ponzetto, S.P.: Computational analysis of political texts:
bridging research eﬀorts across communities. In: Proceedings of the 57th Annual
MeetingoftheAssociationforComputationalLinguistics:TutorialAbstracts,Flo-
rence, Italy, pp. 18–23. Association for Computational Linguistics, July 2019
5. Kornilova, A., Argyle, D., Eidelman, V.: Party matters: enhancing legislative
embeddings with author attributes for vote prediction, pp. 510–515. Association
for Computational Linguistics, July 2018
6. Lowe, W., Benoit, K., Mikhaylov, S., Laver, M.: Scaling policy preferences from
coded political texts. Legis. Stud. Q. 36(1), 123–155 (2011)
7. Marchetti-Bowick,M.,Chambers,N.:Learningformicroblogswithdistantsuper-
vision:politicalforecastingwithTwitter.In:Proceedingsofthe13thConferenceof
theEuropeanChapteroftheAssociationforComputationalLinguistics,Avignon,
France, pp. 603–612. Association for Computational Linguistics, April 2012
8. Menini, S., Nanni, F., Ponzetto, S.P., Tonelli, S.: Topic-based agreement and dis-
agreement in US electoral manifestos. In: Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, Copenhagen, Denmark, pp.
2938–2944. Association for Computational Linguistics, September 2017
9. Nakov, P., Ritter, A., Rosenthal, S., Sebastiani, F., Stoyanov, V.: Semeval-2016
task 4: sentiment analysis in Twitter. arXiv preprint arXiv:1912.01973 (2019)
10. Nanni, F., et al.: Findings from the hackathon on understanding euroscepti-
cism through the lens of textual data. European Language Resources Association
(ELRA), May 2018
11. O’Connor, B., Balasubramanyan, R., Routledge, B.R., Smith, N.A.: From tweets
to polls: linking text sentiment to public opinion time series. In: Proceedings of
theFourthInternationalConferenceonWeblogsandSocialMedia,ICWSM2010,
Washington, DC, USA, 23–26 May 2010 (2010)
12. Rheault, L., Cochrane, C.: Word embeddings for the analysis of ideological place-
ment in parliamentary corpora. Polit. Anal. 1–22 (2019)
13. Roberts, M.E., et al.: Structural topic models for open-ended survey responses.
Am. J. Polit. Sci. 58(4), 1064–1082 (2014)
14. Sidorov,G.,etal.:Empiricalstudyofmachinelearningbasedapproachforopinion
mining in tweets. In: Batyrshin, I., Gonz´alez Mendoza, M. (eds.) MICAI 2012.
LNCS (LNAI), vol. 7629, pp. 1–14. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-37807-2 1A Twitter Political Corpus of the 2019 10N Spanish Election 49
15. Thelwall,M.,Buckley,K.,Paltoglou,G.,Cai,D.,Kappas,A.:Sentimentstrength
detection in short informal text. J. Am. Soc. Inform. Sci. Technol. 61(12), 2544–
2558 (2010)
16. Thomas,M.,Pang,B.,Lee,L.:Getoutthevote:determiningsupportoropposition
fromcongressionalﬂoor-debatetranscripts.In:Proceedingsofthe2006Conference
on Empirical Methods in Natural Language Processing, pp. 327–335. Association
for Computational Linguistics, July 2006Mining Local Discourse Annotation
for Features of Global Discourse Structure
B
Lucie Pola´kova´( ) and Jiˇr´ı M´ırovsky´
Faculty of Mathematics and Physics, Charles University, Malostransk´e n´amˇest´ı 25,
118 00 Prague, Czech Republic
{polakova,mirovsky}@ufal.mff.cuni.cz
http://ufal.mff.cuni.cz
Abstract. Descriptive approaches to discourse (text) structure and
coherencetypicallyproceedeitherinabottom-uporatop-downanalytic
way. The former ones analyze how the smallest discourse units (clauses,
sentences)areconnectedintheirclosestneighbourhood,locally,inalin-
ear way.Thelatter ones postulate ahierarchical organization of smaller
and larger units, sometimes also represent the whole text as a tree-like
graph. In the present study, we mine a Czech corpus of 50k sentences
annotatedinthelocalcoherencefashion(PennDiscourseTreebankstyle)
for indices signalling higher discourse structure. We analyze patterns of
overlappingdiscourserelationsandlook intohierarchiestheyform.The
typesanddistributionsofthedetectedpatternscorrespondtotheresults
forEnglishlocalannotation,withpatternsnotcomplyingwiththetree-
likeinterpretationatverylownumbers.Wealsodetecthierarchicalorga-
nization of local discourse relations of up to 5 levels in the Czech data.
·
Keywords: Local and global discourse coherence Discourse
· · ·
relations Hierarchy Rhetorical Structure Theory Penn Discourse
·
Treebank Prague Dependency Treebank
1 Introduction
Since the establishment of the discipline of text linguistics (and its equivalent
discourse analysis), the various approaches to coherence and cohesion aiming at
formalizablelinguisticdescriptioncanbecharacterizedintermsofmethodology
as local coherence models and global coherence models.
Globalcoherenceisconnectivitybetweenthemaineventsofthetext(scripts,
plans and goals) and the global relations hold independently of the local coher-
ence relations between discourse segments. In the NLP area and in particular in
automatic discourse processing, the local and the global coherence models are
alsoreferredtoasshallowanddeepdiscourseanalyses/parsing,respectively[13].
Both types of approaches deal with determination and description of seman-
tic and pragmatic relations between individual text units; these relations are
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.50–60,2020.
https://doi.org/10.1007/978-3-030-58323-1_5Mining Local Discourse Annotation 51
typically called coherence relations, discourse relations or rhetorical relations,
depending on the theoretical backgrounds and focus of the analyses.1
At present, there is a publicly available large corpus of local discourse anno-
tation for Czech, the Prague Dependency Treebank 3.5 (PDT 3.5; [1], identi-
ﬁcation of discourse connectives and their scopes, see Sect. 2). However, there
is so far no annotation of global coherence for Czech. Also internationally, the
numberofcorporaannotatedforlocalcoherencecoveralargerangeoflanguages
– mostly following the ﬁrst and most prominent Penn Discourse Treebank style
(PDTB, [12]), whereas there are only few corpora with global annotations.2 In
spite of this, recent advances in discourse analysis show that global coherence
modeling (often in addition to local coherence models), apart from its obvious
application in automatic coherence evaluation [4,7] and global discourse pars-
ing,cansigniﬁcantlycontributetocomplexNLPtasks,e.g.summarization,text
generation, textual entailment, text mining and others.
The need to proceed further up from the local coherence analysis to the
global one (more sentences as a separate unit, paragraphs, larger text blocks)
becamemotivatednotonlybytheapplicabilitymentionedabove.Azoomed-out
look on a locally annotated discourse resource revealed even here some patterns
typical for (some type of) global coherence analysis, i.e. even the local anno-
tation already displays features of global text structure: we found hierarchical
organizationofsmallerandlargerdiscourserelations,connectivesandotherdis-
course cues operating between larger blocks of texts, long-distance relations,
genre-related patterns and so on.
The goal of the present paper is to systematically exploit an existing local
discourse annotation of Czech for possible signs or features of global discourse
structure. As this includes a wide range of phenomena, we focus in this study
on the issues of structure, or “the shape” of a text, in other words: we investi-
gatemutualconﬁgurationsofdiscourserelations(pairwise)andtheircomplexity
within locally annotated texts. We relate the detected settings to:
1. a similar research conducted on English Penn Discourse Treebank [5] - with
the aim to compare the conﬁgurations of relations typical for discourse,
2. the principles of a global coherence analysis, namely, to those of Rhetori-
cal Structure Theory, which represents any text document as a single tree-
like structure (see below in Sect.1.1). On these grounds we try to demon-
stratewherethelocalandglobalanalyticperspectivesmeetandinteract.We
also contribute some empirical material to the burning scientiﬁc debate on
whether projective trees are descriptively adequate to represent structure of
texts (e.g. [3,6,9,17]).
1 In this study, we use the term discourse relations, according to the Penn Discourse
Treebank’s terminology.
2 Bothwaysoftextanalysisforthesamedataarerare,yettheyexist,e.g.forEnglish
WallStreetJournaltexts[2,13,18]andforGermannewscommentaries[14].ForGer-
man, even a mapping procedure between the two annotation layers was introduced
in [15].52 L. Pol´akov´a and J. M´ırovsky´
1.1 Rhetorical Structure Theory, the Tree-Like Global Model
OneofthemostinﬂuentialframeworksamongtheglobalmodelsistheRhetorical
Structure Theory (RST, [8]). The main principle of RST is the assumption that
coherent texts consist of minimal units, which are linked to each other, recur-
sively, through rhetorical relations and that coherent texts do not exhibit gaps
or non-sequiturs [16]. The RST represents the whole text document as a single
(projective) tree structure. Basic features of these structures are the rhetorical
relationsbetweentwotextual units(smallerorlargerblocksthatareinthevast
majority of cases adjacent) and the notion of nuclearity. For the classiﬁcation
of RST rhetorical relations, a set of labels was developed, which originally con-
tained24relations.TheRSThasgainedgreatattention,itwasfurtherdeveloped
andtested,languagecorporawerebuiltwithRST-likediscourseannotation.On
the other hand, the framework was criticized in some of its theoretical claims,
aboveall,inthequestionofadequacy/suﬃciencyofrepresentationofadiscourse
structure as a tree graph.3 Linguistically, the strong constraints on the struc-
ture (no crossing edges, one root, all the units interconnected etc.) gave rise to
a search for counter-examples in real-world texts. It was shown that not only
adjacent text units exhibit coherence links and that there are even cue phrases,
which connect non-adjacent units and thus support the claim that a tree graph
is too restricted a structure for an adequate discourse representation [17] and
others.
1.2 Complexity of Discourse Dependencies in PDTB
Leeetal.[5]studiedvarioustypesofoverlapsofdiscourserelationsinthelocally
annotated Penn Discourse Treebank 2. They encountered a variety of patterns
betweenpairsofdiscourserelations,includingnested(hierarchical),crossedand
other non-tree-like conﬁgurations. Nevertheless, they conclude that the types of
discourse dependencies are highly restricted since the more complex cases can
befactoredoutbyappealingtodiscoursenotionslikeanaphoraandattribution,
andarguethatpurecrossingdependencies,partiallyoverlappingargumentsand
a subset of structures containing a properly contained argument should not be
consideredpartofthediscoursestructure.TheauthorschallengeCzechdiscourse
researchers to introduce a similar study (footnote 1 in their paper) in order to
observe and compare the complexity of discourse and syntax dependencies in
two typologically diﬀerent languages.4 We take advantage of the fact that the
corpora used in their and our studies are comparable in genre (journalism), size
(50k sentences), discourse annotation style (PDTB vs. PDTB-like) and number
of annotated explicit discourse relations (approx. 20 thousand).
3 Basically a constituency tree, which is in its nature projective and does not allow
crossing edges, in comparison to the basic mathematical deﬁnition of a tree graph.
4 Due to the limited range of this paper, we only compare our results to theirs for
discourserelations.Theimplicationsforsyntax(levelofcomplexity)isnotexplicitly
discussed.Mining Local Discourse Annotation 53
2 Data and Method
The data used in our study come from the Prague Dependency Treebank 3.5
(PDT 3.5; [1]), a corpus of approx. 50 thousand sentences of Czech journalistic
texts manually annotated for morphology, surface syntax (analytics), deep syn-
tax (tectogrammatics) and other phenomena. Discourse relations are annotated
in the PDTB fashion, in a form of binary relations between two text spans (dis-
course arguments).5 For explicit relations, all connectives in a given text were
identiﬁed(therewasnocloselistofconnectives),thentheirtwoargumentswere
detected and a semantic/pragmatic relation between them assigned to the rela-
tion. Although the minimality principle [12] was taken into account like in the
PDTB, the annotators could also mark large argument spans and non-adjacent
arguments,ifjustiﬁed(compareSect.3.1).ThediscourseannotationofthePDT
consists of 21 223 explicit discourse relations, i.e. relations signalled by explicit
discourse connectives (both primary and secondary), and 361 list relations, i.e.
relationsbetweensubsequentmembersofenumerativestructures.Forthepresent
study, we only take into consideration the explicit discourse relations.
FromthewholePDT,weﬁrstcollectedandclassiﬁedclosediscourserelation
pairs.Wedeﬁneaclose discourse relation pair aspairofdiscourserelationsthat
are either adjacent (the left argument of one relation immediately follows the
right argument of the other relation), or they overlap, see Table 1 for various
patterns. Second, we used pairs of nested relations where one of the relations
is as a whole included in one argument of the other relation (lines 12 and 13
from Table 1) to recursively construct tree structures out of pairs of the nested
relations, see Table 2. Based on the quantitative results, we inspected selected
samples of the detected patterns manually, in order to check the script outcome
and to provide a linguistic description and comparison.
3 Analysis
Inthissection,weﬁrstanalyzethedetectedpatternsforcloserelationpairs,next,
a speciﬁc subsection is devoted to the description of the detected hierarchical
structures (Sect.3.1).
We were able to detect 17 628 close relation pairs and for each such pair, we
investigateditspattern,themutualarrangementofthetworelations.InTable1,
these patterns are also graphically illustrated.6 The table shows ﬁgures for all
explicit relation pairs and, in brackets, only for inter-sentential relation pairs.
Themostcommonsettingforallrelationsare“fullembeddings”,inotherwords
two-level hierarchies, 7 134 in total (lines 12 and 13), which is even more
than pure adjacency (succession) of two relations (6 572, line 1). These two
5 Technically,theannotationisnotcarriedoutonrawtexts,butontopofthesyntactic
trees.
6 We have obtained so much data that we must only select certain aspects for this
study. We therefore concentrate on the patterns studied by Lee et al., and on hier-
archical structuring of discourse relations.54 L. Pol´akov´a and J. M´ırovsky´
Table 1. Patterns of adjacent or overlapping pairs of discourse relations; the total
number of such close relations was 17 628; 109 of them did not ﬁt any of the listed
patterns.(Numbersinbracketsmeanfrequenciesifonlyinter-sententialdiscourserela-
tionsweretakenintoaccount;intotal,therewere2984suchcloserelations,85ofthem
did not ﬁt the listed patterns.)
Line Frequency Pattern Visualization
1 6572 (983) Adjacency <---------> <--------->
<---------> <--------->
2 1923 (865) Pprogress <---------> <--------->
(sharedargument) <---------> <--------->
3 51 (21) Totaloverlap <---------> <--------->
<---------> <--------->
4 266 (116) Leftoverlap <---------> <--------->
rightadjacency <---------> <--------->
5 25 (10) Leftoverlap <---------> <--------->
rightcontained <---------> <--->
6 31 (10) Rightoverlap <---------> <--------->
leftadjacency <---------> <--------->
7 17 (8) Rightoverlap <---------> <--------->
leftcontained <---> <--------->
8 250 (146) ContainmentI <---------> <--------->
<---> <--------->
9 190 (150) ContainmentII <---------> <--->
(opposite) <---------> <--------->
10 83 (14) Bothargs <---------> <--------->
containedI <---> <--->
11 73 (8) Bothargs <---------> <--->
containedII <---> <--------->
12 3591 (299) Lefthierarchy <---------> <--------->
<---> <--->
13 3543 (56) Righthierarchy <---------> <--------->
<---> <--->
14 10 (10) Crossing <---------> <--------->
<---------> <--------->
15 883 (196) Envelopment <---------> <--------->
<---------> <--------->
16 11 (7) Partialoverlap <---------> <--------->
<---------> <--------->
(<---------------> <--------->)
conﬁgurationsrepresenttogetherslightlymorethan3/4ofalldetectedpatterns.
Theyarealsoreferredtoasvery“normal”structuralrelationships in[5],(p.82).
Thenext-largestgroupisprogress(line2),asharedargumentinthePDTB
terminology,with1923instancesor10.9%ofallpatterns.Leeet.al.report7.5%
of this type, which is fairly comparable. Total overlap (line 3) is caused by the
possibility to annotate two diﬀerent relations between the same segments for
cooccurring connectives, as in because for example or but later.
Theenvelopmentpattern(line15)concernsinvastmajorityanon-adjacent
(long-distance) relation and a relation in between its arguments. Generally, the
enveloped relation is a sentence with some inner syntactic structure annotated,Mining Local Discourse Annotation 55
which also explains the drastic diﬀerence between all detected envelopment pat-
terns(883)andonlytheinter-sententialones(196).Linguistically,someofthese
cases are sentences headed by two attribution spans (verbs of saying) and some
structure in the reported content in between, also cases of two linked reporter’s
questions in an interview and the inner structuring of the interviewee’s answer,
but also texts with no striking structural reasons for such an arrangement.
The pattern represents ca. 5% of all settings and it certainly needs further
observation.
The patterns with properly contained arguments, either one of them
(lines 5, 7, 8 and 9) or both (10 and 11), very often involve “skipping one level”
in the syntactic tree of a sentence, see Example 17 of the type 8 (containment
I),i.e.theexclusionofsomegoverningclausefromtheargument,thatmakesits
syntacticallydependentclause(mostlya“reported-content-argument”)toasub-
set of the other represented (mostly) by a whole sentence. Besides the discussed
attribution (introductory statements), this is also the case of the annotation
of some secondary connectives forming whole clauses (like This means that ...,
Example2,type9, containment II).Theseverbphrasesarenottreated as parts
ofanyoftheargumentstheyrelateto,8 andposeamethodologicalissue.Athird
settingconcernsmulti-sentencearguments,wherethecontainedargumentistyp-
ically a single sentence. Patterns with properly contained arguments represent
in total 3.6% (638) of all patterns.
(1) Thegapinthestandardoflivingthatappearsbetweenthequaliﬁedscientiﬁc
elite and the business sphere, right now, at the beginning of the transforma-
tion of the society, will leave traces. It is therefore appropriate to pamper
youngresearchersandnotmisusethefactthat a young researcher works
with enthusiasm for science, regardless of salary. But a person
who begins to ﬁnd his mission in research also starts a family,
wants to live at a good place and live with dignity.
(2) Thisbriefoverviewessentiallyexhauststheareasofnotarialactivitieswithin
the framework of free competition between notaries. This means that in
these notarial agendas, the client has the option of unlimited
choice of notary at his own discretion,asthe notary is not bound
to the place of his work when providing these services.
A(pure)crossingisasettingwheretheleft-sidedargumentoftherightrelation
comes in between the two arguments of the left relation, compare line 14 in
Table 1. Pure crossings violate the RST constraints most visibly, with crossing
edges, so the debate on tree adequacy often circles around the acceptability
of crossings in discourse analysis. Lee et al. [5] identify only 24 cases (0.12%)
of pure crossings in their corpus. We detected only 10 such cases, which is a
7 DuetospacelimitweonlypresenttheEnglishtranslationsofthePDTCzechorigi-
nalshere.Relation1ishighlightedinitalics,relation2inbold.Theconnectivesare
underlined.
8 IntherepresentationinExample2,theclauseThis means that isnotinitalics,not
a part of any argument of the left relation.56 L. Pol´akov´a and J. M´ırovsky´
negligible proportion. Manual inspection nevertheless revealed several diﬀerent
scenarios,fromclearlyincorrectannotation,moreinterpretationspossible,across
caseswithattributionspansinbetween,toafew,inouropinion,perfectlysound
analyses, as exempliﬁed by Example 3.9
(3) (a) What can owners and tenants expect, what should they prepare for?
(b) The new legislation should allow all owners to sell apartments. (c) It
is most urgent for ﬂats owned by municipalities, as they manage
about a quarter of the housing stock of the Czech Republic and
some are - mainly for ﬁnancial reasons - interested in monetizing
a part of their apartments. (d) The law should also allow to complete
transfersofhousingassociationapartmentsandtheirsaletoitsmembers.(e)
This is also not a “small portion”, but a ﬁfth of the total number
of dwellings.
Ifweacceptedthepossibilitythatnotonly(b),butalarger(b+c)unittorelates
to (c) in the “also”-relation, which would be a completely ﬁne interpretation in
the Prague annotation, the relation of (e) – the “neither”-relation – cannot
accept just (d) as its left-sided argument. We also think this case cannot be
factoredoutduetoanaphora.Thereis,forsure,roomfordiﬀerentinterpretations
within diﬀerent theories, we just oﬀer our data, state our view and admit that
crossing structures are extremely rare even in our empirical data.
Partial overlapisatypeofstructurethatviolatestheRSTtreeconstraint,
too. In the PDTB, there were only 4 such cases. In the Czech data we detected
11 cases (line 16 of Table 1). They often include large arguments of untypical
range (2,5 sentences etc.) which can be questioned. Some of the relations also
includesecondaryconnectives withstronganaphoriclinks(in this respect, given
the fact that etc.). These relations can be factored out, yet, again, there was a
small number of cases that are linguistically acceptable, compare Example 4.
(4) The responsibility of the future tenant of this 103,000-m2 area will be to
care for all properties, including their maintenance and repairs. The tenant
will also have to resolve the parking conditions for market visitors and to
meet the conditions of the Prague Heritage Institute during con-
struction changes due to the fact that the complex is a cultural
monument. The capital city at the same time envisages preserv-
ing the character of the Holeˇsovice market.
3.1 Hierarchies
TheresultsofapplyingthesecondstepoftheproceduretothewholePDTdata
aredisplayedinTable2,arrangedaccordingtotheschemeofsuchhierarchytrees
(identical structures are summed and represented by the hierarchy scheme). We
9 The “also-not” connective is originally in Czech ani, in the meaning of neither. Lit.
translation: “Neither here is concerned a small portion...”.Mining Local Discourse Annotation 57
Table 2. Selected schemes of hierarchies of discourse relations. Numbers in brackets
mean frequencies of hierarchy schemes if only inter-sentential discourse relations were
takenintoaccount(nootherinter-sententialhierarchieswereencounteredinthedata).
Line Frequency Depth Hierarchy
1 381 (10) 3 A ( B ( C ))
2 64 (1) 3 A ( B ( C ) D )
3 50 3 A ( B C ( D ))
4 23 (1) 3 A ( B ( C ) D ( E ))
5 20 3 A ( B ( C D ))
6 0 (1) 3 A ( B ( C ) D E )
7 0 (1) 3 A ( B C ( D ) E )
8 19 4 A ( B ( C ( D )))
9 7 4 A ( B ( C ( D )) E )
10 5 4 A ( B C ( D ( E )))
11 5 4 A ( B ( C ( D )) E ( F ( G )))
...
12 2 5 A ( B ( C ( D ( E ))) F )
13 1 5 A ( B C ( D ( E ( F ))) G )
14 1 5 A ( B ( C ( D ) E ( F ( G ))) H I J ( K ))
15 1 5 A ( B ( C ( D ( E ))))
16 1 5 A ( B ( C ( D E ( F ))))
only mention cases where there are at least three levels in the tree, as two-level
hierarchies are part of Table 1.
For explanation: The scheme “A ( B )” means that the whole relation B is
includedinoneoftheargumentsoftherelationA(thisis,ofcourse,onlyatwo-
level tree). The scheme “A ( B C ( D ))” means that relations B and C are all
included in the individual arguments of relation A (without specifying in which
argument they are, so they can be both in one argument or each in a diﬀerent
argument) and the relation D is completely included in one of the arguments of
relation C. It is a three-level hierarchy. Generally, we count the depth (number
of levels) of a hierarchy tree as a number of nodes in the longest path from the
root to a list.
Therearemanysub-hierarchiesinalarge/deephierarchy,forexample“B(C
(D))”isasub-hierarchyof“A(B(C(D))E)”,howeversuchsub-hierarchies
arenotcountedinTable2,i.e.,eachhierarchyisonlycountedinthetableinits
largest and deepest form as it appeared in the PDT data.10
The purpose of looking for such hierarchical structures in the PDT data is
todiscovertowhatextentalocalannotationshowssignsofsomestructure,too.
10 This also explains the zeros in Table 2.58 L. Pol´akov´a and J. M´ırovsky´
We do not claim that the trees detected by us are the trees a global analysis
like RST would discover, but we demonstrate the existence of some hierarchi-
cal text structure in local annotation. Some of it could perhaps partially match
to RST-formed subtrees (and deﬁnitely there would be an intersection of sepa-
rate relations, compare e.g. intersections in Wall Street Journal local and global
annotations[10],butthisisyettobeinvestigated.Wearealsoaware,aspointed
out in [3], that minimal, local annotations cannot form a connected graph.11
In the PDT data, local discourse relations form hierarchies up to ﬁve levels.
We have identiﬁed 5 patterns of 5-level hierarchies (5-LH), with the total of 6
instances, see Table 2. There is also a number of 3- and 4-level hierarchies. An
analysis of random samples (and of all the deepest ones) revealed, surprisingly,
that there can be a 4-level hierarchy spanning 11 sentences, but also a 5-LH
spanning only two sentences, from which one is typically a more complex com-
pound sentence. The “longest” of the 5-LHs includes also 11 sentences (line 14)
and it also exhibits branching (D, G and K as lists, where the G-path is the
deepest). One of the 5-LHs should be in fact one level ﬂatter (line 12), as the
lowest two relations are three coordinated clauses with two and-connectives:
“the troops protected them and fed them and gave them the impression that they
were invulnerable...”. Such structures are notoriously hard to interpret for any
framework, yet in Prague annotation, the annotation is incorrectly hierarchical
where it should have been ﬂat.
To ﬁnd out how much structure is involved only within individual sentences,
i.e. how much of sentential syntax forms the hierarchies, in a second phase we
ﬁltered out all intra-sentential relations. The numbers in brackets give counts
for patterns of hierarchies, if only inter-sentential relations are accounted for.
The hierarchies of this type are much less frequent and their maximum depth
is just 3, which implies that beyond the sentence boundary, local annotation of
explicit connectives does not represent hierarchical text structuring very often.
Ahypothesisforthelackofhierarchiesbuildbyonlyinter-sententialrelationsis
thatonlysomeoftheconnectivesoperatingathigherdiscourselevelswereiden-
tiﬁed and annotated as such, some of them were assigned local coherence links
due to the minimality principle. This issue was recently discussed in [11], where
the annotated non-adjacency of left arguments of paragraph-initial connectives
was partially interpretable as higher discourse structuring, and the relations in
question were in fact adjacent. Moreover, there surely are other, non-connective
cues operating between larger text blocks in our data.
4 Conclusion
In the present study, we have investigated conﬁgurations of pairs of discourse
relations in a large corpus annotated for local discourse coherence of Czech,
in order to detect possible features of global discourse structure (higher text
structuring) and to describe the complexity of semantic/pragmatic relations in
11 And the more so, as we do not include implicit and entity-based relations into our
study.Mining Local Discourse Annotation 59
discourse. We have identiﬁed patterns typical (adjacency, progress, hierarchy,
etc.), less typical (argument containment patterns, envelopment) and quite rare
(total overlap, crossing, partial overlaps etc.) in our data and analyzed them
linguistically. We have compared our ﬁndings to those of a similar study con-
ducted on English locally annotated texts [5], learning that the proportions of
occurrence of individual patterns roughly correspond in both corpora, although
our study distinguishes some more subtle conﬁgurations. Frequent patterns in
ourdatacomplywiththeRSTtreestructurerules.Lessfrequentpatternsinthe
PDTmostlydealwithattributionspans,butalsowiththeannotationstrategies
for secondary connectives in cases where they form a whole clause (It means
that...) or they are anaphoric (in this respect). In some rare patterns, where,
in our opinion, there is a violation of the tree structure in the sense of RST,
we have found a small number of linguistically defensible interpretations with
no anaphora or attribution. Nevertheless, such speciﬁc settings would still need
to be compared within a true RST interpretation. Second, we have investigated
hierarchies built by the local relations and we have detected even 5-level hierar-
chies. On the other hand, much of the structure is intra-sentential: beyond the
sentenceboundary,localannotationofexplicitconnectivesdoesnotexposehier-
archicaltextstructuringveryoften.Webelievethatforalocalannotationofthe
Penn Discourse Treebank type, hierarchical interpretation beyond the sentence
structure may be advantageous, especially when prompted by connectives oper-
atingathigherlevels.Thisperspectiveisparticularlyimportant,asaconsistent
application of the minimality principle can lead to a possible misinterpretation
of such higher relations, which otherwise a local annotation scenario is perfectly
ﬁt to incorporate.
Inthefuture,weplantoanalyzeespeciallyparagraph-initialconnectivesand
othersignalscollectedduringtheimplicitrelationannotation,and,moreimpor-
tantly, in a RST-like pilot annotation we will compare and evaluate, whether
the identiﬁed local and the global hierarchies actually match and represent the
same types of relations.
Acknowledgments. The authors gratefully acknowledge support from the Grant
Agency of the Czech Republic, project no. 20-09853S. The work described herein has
been using resources provided by the LINDAT/CLARIAH-CZ Research Infrastruc-
ture,supportedbytheMinistryofEducation,YouthandSportsoftheCzechRepublic
(project no. LM2018101).
References
1. Hajiˇc,J.,etal.:PragueDependencyTreebank3.5.Data/software.InstituteofFor-
mal and Applied Linguistics, Charles University, LINDAT/CLARIN PID (2018).
http://hdl.handle.net/11234/1-2621
2. Carlson, L., Okurowski, M.E., Marcu, D.: RST Discourse Treebank. Linguistic
Data Consortium, University of Pennsylvania (2002)
3. Egg, M., Redeker, G.: How complex is discourse structure? In: Proceedings of
LREC 2010, Malta, pp. 619–1623 (2010)60 L. Pol´akov´a and J. M´ırovsky´
4. Feng,V.W.,Lin,Z.,Hirst,G.:Theimpactofdeephierarchicaldiscoursestructures
intheevaluationoftextcoherence.In:ProceedingsofCOLING,pp.940–949(2014)
5. Lee,A.,Prasad,R.,Joshi,A.,Dinesh,N.:Complexityofdependenciesindiscourse:
aredependenciesindiscoursemorecomplexthaninsyntax?In:Proceedingsofthe
TLT 2006, Prague, Czech Republic, pp. 79–90 (2006)
6. Lee, A., Prasad, R., Joshi, A., Webber, B.: Departures from tree structures in
discourse: shared arguments in the Penn Discourse Treebank. In: Proceedings of
the Constraints in Discourse III Workshop, pp. 61–68 (2008)
7. Lin, Z., Ng, H.T., Kan, M.Y.: Automatically evaluating text coherence using dis-
course relations. In: Proceedings of the 49th Annual Meeting of the ACL: Human
Language Technologies-Volume 1, pp. 997–1006 (2011)
8. Mann, W.C., Thompson, S.A.: Rhetorical structure theory: toward a functional
theory of text organization. Text-Interdiscip. J. Study Discourse 8(3), 243–281
(1988)
9. Marcu, D.: The Theory and Practice of Discourse Parsing and Summarization.
MIT Press, Cambridge (2000)
10. Pol´akov´a,L.,M´ırovsky´,J.,Synkov´a,P.:Signallingimplicitrelations:aPDTB-RST
comparison. Dialogue Discourse 8(2), 225–248 (2017)
11. Pol´akov´a,L.,M´ırovsky´,J.:Anaphoricconnectivesandlong-distancediscourserela-
tions in Czech. Computaci´on y Sistemas 23(3), 711–717 (2019)
12. Prasad, R., Dinesh, N., Lee, A., et al.: The Penn discourse treebank 2.0. In: Pro-
ceedings of LREC 2008, Morocco, pp. 2961–2968 (2008)
13. Prasad, R., Joshi, A., Webber, B.: Exploiting scope for shallow discourse parsing.
In: Proceedings of LREC 2010, Malta, pp. 2076–2083 (2010)
14. Stede,M.,Neumann,A.:Potsdamcommentarycorpus2.0:annotationfordiscourse
research. In: Proceedings of LREC 2014, pp. 925–929 (2014)
15. Scheﬄer,T.,Stede,M.:MappingPDTB-styleconnectiveannotationtoRST-style
discourse annotation. In: Proceedings of KONVENS 2016, pp. 242–247 (2016)
16. Taboada, M., Mann, W.C.: Rhetorical structure theory: looking back and moving
ahead. Discourse Stud. 8(3), 423–459 (2006)
17. Wolf,F.,Gibson,E.:Representingdiscoursecoherence:acorpus-basedstudy.Com-
put. Linguist. 31(2), 249–287 (2005)
18. Wolf, F., Gibson, E., Fisher, A., Knight, M.: Discourse Graphbank, LDC2005T08
[Corpus]. Linguistic Data Consortium, Philadelphia (2005)Diversiﬁcation of
Serbian-French-English-Spanish
Parallel Corpus ParCoLab
with Spoken Language Data
B
Duˇsica Terzi´c1( ) , Saˇsa Marjanovi´c1 , Dejan Stosic2 ,
and Aleksandra Miletic2
1 Faculty of Philology, University of Belgrade,
Studentski trg 3, 11000 Belgrade, Serbia
{dusica.terzic,sasa.marjanovic}@fil.bg.ac.rs
2 CNRS and University of Toulouse,
5, All´ees Antonio Machado, 31058 Toulouse, France
{dejan.stosic,aleksandra.miletic}@univ-tlse2.fr
Abstract. In this paper we present the eﬀorts to diversify Serbian-
French-English-Spanish corpus ParCoLab. ParCoLab is the project led
byCLLEresearchunit(UMR5263CNRS)attheUniversityofToulouse,
France, and the Romance Department at the University of Belgrade,
Serbia. The main goal of the project is to create a freely searchable
and widely applicable multilingual resource with Serbian as the pivot
language. Initially, the majority of the corpus texts represented written
language. Since diversity of text types contributes to the usefulness and
applicability of a parallel corpus, a great deal of eﬀort has been made
toincludespokenlanguagedataintheParCoLabdatabase.Transcripts
and translations of TED talks, ﬁlms and cartoons have been included
sofar,alongwithtranscriptsoforiginalSerbianﬁlms.Thus,the17.6M-
word database of mainly literary texts has been extended with spoken
language data and it now contains 32.9M words.
· · · ·
Keywords: Parallel corpus Serbian French English Spanish
1 Introduction
ParCoLab1 is a Serbian-French-English-Spanish corpus developed by CLLE
research unit (UMR 5263 CNRS) at the University of Toulouse, France, and
the Department of Romance Studies at the University of Belgrade, Serbia. The
primarygoaloftheParCoLabprojectistocreateamultilingualresourceforthe
Serbian language, searchable via a user-friendly interface that can be used not
onlyinNLPandcontrastivelinguisticresearchbutalsoincomparativeliterature
studies,secondlanguagelearningandteaching,andappliedlexicography[14,17].
1 http://parcolab.univ-tlse2.fr. Last access to URLs in the paper: 20 Apr 2020.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.61–70,2020.
https://doi.org/10.1007/978-3-030-58323-1_662 D. Terzi´c et al.
Another goal of the ParCoLab project is to add several layers of annotation to
thecorpustext,suchaslemmas,morphosyntacticdescriptions(MSDs)andsyn-
tactic relations [10,13,14,17]. Currently, two portions of the Serbian subcorpus
are annotated – a 150K-token literary subcorpus, ParCoTrain-Synt [12], and a
30K-token journalistic subcorpus, ParCoJour [18].2
In the composition of the ParCoLab corpus, quality of the collected data
and the processing of the texts is prioritized over quantity, which requires a
signiﬁcant implication of the human factor in the process [17]. The creation
of the ParCoLab corpus started with written literary texts which, in general,
come with high quality translations. The result, a useful, high-quality corpus
wascreatedbasedonliteraryclassicsandacarefulselectionofgoodtranslations.
However,uniformityofthecorpushasanimportantimpactonNLPapplications.
For instance, the annotation models trained on a single domain corpus are not
particularlyrobustwhenusedtoprocessthetextsofanotherdomain[1,2,6,15].
Thiswasconﬁrmedinaparsingexperimentinwhichaparsingmodelwastrained
on the ParCoTrain-Synt literary treebank and used to parse the ParCoJour
journalistic corpus (see [18]).
ItisnotonlytheuniformityofthedatathathasanimpactontheNLPappli-
cationsbutalsothetypeofthatdata.Itwasshownthatthediﬀerencesbetween
spoken and written language have a signiﬁcant impact on machine translation.
Ruiz and Federico [16] compared 2M words from 2 English-German corpora,
oneofwhichcontainedTEDtalksandtheothernewspaperarticles.Theyfound
that TED talks consisted of shorter sentences with less reordering behavior and
stronger predictability through language model perplexity and lexical transla-
tion entropy. Moreover, there were over three times as many pronouns in TED
corpus than in news corpus and twice as many third person occurrences, as well
as a considerable amount of polysemy through common verbs and nouns [16].
It is therefore necessary to diversify corpus data in order to make them use-
ful for the development of good and robust NLP models. The expansion and
diversiﬁcation of the ParCoLab database represents an important task for Ser-
bian corpus linguistics, considering that Serbian is one of the under-resourced
European languages in terms of both NLP resources and corpora for other spe-
cialists (teachers, translators, lexicographers, etc.). In order to accomplish the
goals of the ParCoLab project, the corpus should be diversiﬁed especially by
adding spoken language data.
However, collecting, transcribing, and translating an authentic spontaneous
speech corpus requires considerable ﬁnancial and human resources. We were
therefore constrained to search for the data closest to the spontaneous speech
that could be collected more eﬃciently. It was decided to introduce TED talks
and ﬁlm and cartoon transcripts and subtitles and the term “spoken language
data”isusedtorefertothistypeofdata.WeareawarethatTEDtalksarewrit-
ten and edited to be spoken in a limited time frame and thus do not represent
spontaneous speech. Film and cartoon transcripts, on the other hand, are more
2 Both corpora can be queried via the ParCoLab search engine and are available for
the download at http://parcolab.univ-tlse2.fr/about/ressources.Diversiﬁcation of Parallel Corpus ParCoLab with Spoken Language Data 63
likelytoresembletranscribednaturalspeechalthoughtheyarealsowrittenand
edited beforehand. Another possible downside to using this type of documents
is the questionable quality of the available transcripts and translations of TED
talksandﬁlms,whichmaycompromisethequalityofthecorpusmaterialandits
usefulness (cf. [7]). The method used to include transcripts and translations of
ﬁlmsintheParCoLabcorpustriestopalliatetheshortcomingsofmassiveinclu-
sion of unveriﬁed data and we present it in this paper. In Sect.2, we introduce
similar corpora in order to demonstrate the position of the ParCoLab corpus
amongst other parallel resources containing Serbian. In Sect.3, we describe the
state of the ParCoLab database before the inclusion of the spoken data. The
ongoing work on including spoken data in the ParCoLab corpus is detailed in
Sect.4.Finally,wedrawconclusionsinSect.5,andpresentplansforfuturework.
2 Related Work
In this section, we present other corpora containing the Serbian language and
oneofthreeotherlanguagesoftheproject–French,EnglishorSpanish.Wealso
discusstheshareofspokendatainthosecorpora.Therearetwobilingualparal-
lel corpora3 developed at the Faculty of Mathematics, University of Belgrade –
SrpEngKorandSrpFranKor.SrpEngKor[8]isa4.4MtokenSerbian-Englishcor-
pusconsistingoflegalandliterarytexts,newsarticles,andﬁlmsubtitles.There
are subtitles of only three English ﬁlms containing approximately 20K tokens.
SrpFranKor [21] is a Serbian-French corpus of 1.7M tokens from literary works
and general news with no spoken data. Texts in both corpora are automatically
aligned on the sentence level and alignment was manually veriﬁed.
TextsintheSerbianlanguagealsoappearinmultilingualcorpora.“1984”[9]
of MULTEXT-East project contains George Orwell’s 1984 and its translation
into several languages including 150K-token Serbian translation. SETimes is a
parallel corpus of news articles in eight Balkan languages, including Serbian,
and English [20]. Its English-Serbian subcorpus contains 9.1M tokens. ParaSol
(Parallel Corpus of Slavic and Other Languages), a corpus originally developed
under the name RPC as a parallel corpus of Slavic languages [22], was subse-
quently extended with texts in other languages [23]. The Serbian part of the
corpus contains 1.3M tokens of literary texts, of which only one novel is orig-
inally written in Serbian. These corpora either do not include spoken data in
Serbian language or the ﬁlm subtitles they contain are neither relevant in size
nor originally produced in the Serbian language.
Thereare,however,twomultilingualcorpora,eachcontainingaSerbiansub-
corpus with ﬁlm subtitles – InterCorp and OPUS. InterCorp4 [5], contains 31M
tokens in Serbian. Texts from literary domain contain 11M tokens, whereas
another 20M tokens come from ﬁlm subtitles. Given that the pivot language
is Czech, sentences in Serbian are paired with their Czech counterparts. It is
3 Consultable at: http://www.korpus.matf.bg.ac.rs/korpus/login.php. It is necessary
to demand authorization to access the interface.
4 The oﬃcial website of the project is: https://intercorp.korpus.cz.64 D. Terzi´c et al.
unclear which portion of the Serbian subcorpus can be paired with the subcor-
pora in languages of the ParCoLab project. According to the information5 on
theoﬃcialwebsite,subtitlesaredownloadedfromtheOpenSubtitles6 database.
OPUS7 [19] also contains subtitles from this database. The Serbian subcorpus
contains 572.1M tokens. Neither alignment nor the quality of the translations
are manually veriﬁed in these two corpora, leading to a signiﬁcant amount of
misaligned sentences and questionable quality of the translations. It is highly
unlikely that these corpora contain ﬁlms originally produced in Serbian.
SerbianspokendatacanalsobefoundinseveralmultilingualcorporaofTED
talks. TED talks are lectures presented at non-proﬁt events in more than 130
countries [24]. They are ﬁlmed and stored in a free online database at https://
www.ted.com/talks. TED provides English transcripts which are translated by
volunteer translators. The translation is then reviewed by another TED trans-
lator, who has subtitled more than 90min of talk content. Finally, the reviewed
translation is approved by a TED Language Coordinator or staﬀ member [24].
Hence,theTEDtalksaresupposedtobeofhigherqualitythanthesubtitlesfrom
OpenSubtitles database, which are not veriﬁed. Free access to hours of spoken
data translated into more than 100 languages has generated works on collect-
ing corpora based on TED talks. WIT8 [4], is an inventory that oﬀers access to
a collection of TED talks in 109 languages. All the texts for one language are
stored in a single XML ﬁle. There are 5.3M tokens in the Serbian ﬁle. In order
to obtain parallel corpus, it is necessary to extract TEDs by their ID and to use
alignment tools since the subcorpus for each language is stored separately [4].
MulTed [24] is a parallel corpus of TED talks which contains an important
amount of material in under-resourced languages such as Serbian. The Serbian
subcorpus comprises 871 talks containing 1.4M tokens. All the translations are
sentence-aligned automatically. Only the English-Arabic alignment was manu-
allyveriﬁed[24].Accordingtotheoﬃcialwebsite9 oftheproject,thecorpuswill
be available for download soon.
AsalreadymentionedinIntroduction,thegoaloftheParCoLabprojectisto
create a parallel corpus of high quality. Even though it is clear that ParCoLab
is not the largest available parallel corpus containing the Serbian language, an
important eﬀort is devoted to ensuring the quality of the alignment. Besides
prioritizing quality over quantity, we pay special attention to including original
Serbian documents. This is also true for ﬁlm subtitles, whose translation we
improve. Another advantage of the ParCoLab corpus is that it contains tran-
scripts of Serbian ﬁlms, providing original Serbian content. In comparison to
othercorporadestinedtoNLPusers,ParCoLabisaccessibleandfreelyavailable
to general public via the user-friendly interface, which widens its applicability.
5 https://wiki.korpus.cz/doku.php/en:cnk:intercorp:verze12.
6 https://www.opensubtitles.org/en/search/subs.
7 http://opus.nlpl.eu/OpenSubtitles2016.php.
8 https://wit3.fbk.eu/#releases.
9 http://oujda-nlp-team.net/en/corpora/multed-corpus.Diversiﬁcation of Parallel Corpus ParCoLab with Spoken Language Data 65
Since 2018, it has been possible to use ParCoLab search engine directly online
without creating an account.
3 ParCoLab Content
The texts included in ParCoLab database are aligned with their translations
using an algorithm integrated in the corpus platform. The alignment process
starts with 1:1 pairing of chapters. It then continues on the level of paragraphs
and, ﬁnally, of sentences. Possible errors are pointed out by the algorithm and
corrected manually afterwards [10,11,17]. Corpus material is stored in XML
format in compliance with TEI P5 (https://tei-c.org/guidelines/p5). XML ﬁles
include standardized metadata – title, subtitle, author, translator, publisher,
publicationplaceanddate,creationdate,source,languageofthetext,language
of the original work, domain, genre, number of tokens, etc. [17].
ParCoLabhasbeengrowingsteadilysinceitsinception.Initially,itcontained
2M tokens [17]. Before the work on diversiﬁcation presented in this paper, it
contained 17.6M tokens, with 5.9M tokens in Serbian, 7.4M in French, 3.9M
in English and 286K in Spanish. All the languages except for Spanish were
representedthroughbothoriginalworksandtranslations.InSpanish,therewere
onlyﬁctiontranslations.Itslowrepresentationisduetothefactthatithasbeen
incorporated recently in order to palliate the lack of existing Serbian-Spanish
corpora. There is ongoing work on including more Spanish texts, both original
and translated.
Regarding the type of texts, the corpus content came from predominantly
literary works [3]. A small portion of the corpus was characterized as web con-
tent, legal and political texts and spoken data, but they were not signiﬁcant
in size – ∼30K tokens of ﬁlm and TV show subtitles and ∼60K tokens from
TED talks [14]. There were some eﬀorts to diversify the corpus by including
domain speciﬁc texts from biology, politics, and cinematography, but this mate-
rialremainedsecondary.Theoriginalnumberoftokenspertypeofdataandper
language is shown in Table1.
Table 1.Tokendistributionperlanguageandtexttypebeforeincludingspokendata.
Texttype Serbian French English Spanish Total %
Literarytexts 5,535,926 6,542,014 3,301,397 286,948 15,666,285 88.77
Non-literarywrittentexts 340,060 761,595 566,656 0 1,668,311 9.45
Spokendata 104,935 125,919 82,504 0 313,358 1.78
Total 5,980,921 7,429,528 3,950,557 286,948 17,647,954
%ofcorpus 33.89 42.10 22.39 1.63
Eventhoughthereweresomediversiﬁcationeﬀorts,theliteraryworksremained
dominant and represented 88.7% of the corpus. ParCoLab corpus consisted
mainly of written texts, apart from only 1.78% of spoken data [11]. As men-
tioned in Introduction, linguistic diﬀerences between written and spoken corpus66 D. Terzi´c et al.
inﬂuence the performance of NLP tools. Therefore, we put in a great deal of
eﬀort to overcome the main shortcoming of the corpus, which we discuss in the
next section.
4 Spoken Language Data in ParCoLab
AswehavealreadydiscussedinIntroduction,oneoftheeasiestwaytodiversify
a corpus by adding spoken language data is to include TED talks and ﬁlm
subtitles even though this material is written and edited before oral production.
This method presents a number of other shortcomings. For instance, some of
the subtitles are translated automatically or by amateur translators without
subsequent veriﬁcation by professional translators. In addition, transcripts and
translations are inﬂuenced by the number of characters that can appear on the
screen. Moreover, the subtitles usually do not represent the translation of the
speech in the ﬁlm, but the translation of the transcripts of that speech, which
areeditedtoﬁtthecharacternumberlimit(see[7]).Inwhatfollows,wedescribe
how these downsides were overcome in the present work.
Although the quality of TED talks translations cannot be guaranteed, they
arereviewedbyexperiencedtranslatorsandaresupposedtobeofhigherquality
then subtitle translations downloaded from the OpenSubtitles database. There-
fore,wedownloadedTEDtalksfromtheoﬃcialTEDsiteinabatch.Wedidnot
use the transcripts existing in other corpora (cf. Sect.2). Transcripts of original
TED talks are included in the database alongside their translations into three
languages of the project – Serbian, French, Spanish. At the time of writing this
paper,2000TEDtalkshavebeenincludedintheParCoLabdatabaseforatotal
of 13,458,193 tokens. A TED talk in ParCoLab corpus contains 1,652 words on
average. The shortest TEDs contain only brief introductions or explanations of
musical or art performances of about 200 words, whereas the longest contain
around 8,000 words. They date from 1984 to 2019.
As for the ﬁlm subtitles, the methodology is slightly diﬀerent. Original
EnglishandFrenchtranscriptsaredownloadedfromtheOpenSubtitlesdatabase.
The Serbian ﬁlms were manually transcribed since it was not possible to down-
load original transcripts or to ﬁnd open source speech-to-text tools for Serbian.
TheinclusionoftheSerbianﬁlmtranscriptsmakestheParCoLabcorpusunique.
TheﬁlmsubtitlestranslationsaredownloadedfromtheOpenSubtitlesdatabase
andthenimprovedbystudentswhoaretranslatorsintrainingandbythemem-
bers of the ParCoLab team who work as professional translators as well. More-
over, the subtitles are compared to the actual speech in the ﬁlm and corrected
accordingly.Thatway,thelimitonthenumberofcharacterstoappearonscreen
does not aﬀect the quality of the transcript and translation.
Apart from ﬁlm transcripts, the transcripts of a large collection of cartoons
arebeingincludedintheParCoLabcorpus.ThedataiscollectedfromtheSmurfs
oﬃcial Youtube channels10 in all four languages of the corpus. The transcripts
10 https://www.youtube.com/channel/UCeY4C8Sbx8B4bIyREPSvORQ/videos.Diversiﬁcation of Parallel Corpus ParCoLab with Spoken Language Data 67
of popular children’s stories produced by Jetlag Productions11 are also included
in the corpus in all four languages. One of the advantages of this approach is
thefactthatthecartoonsaredubbed.Thatway,transcriptsinalllanguagesare
transcripts of the speech in that language and not the translations of the edited
transcripts of that speech. There are currently 19 The Smurfs cartoons and 19
children stories from the Jetlag productions in all four languages.
All the spoken language data is stored in XML ﬁles in compliance with
the TEI P5 guidelines and included in the ParCoLab database using the same
methodology as for the rest of the corpus (see Sect.3). Apart from standardized
metadata, the name of the TED editor is included. Time spans are omitted.
Additional metadata for ﬁlm and cartoon transcripts represent names of char-
acters, gender, and age in order to make it useful for linguistic analysis.
There are now 32.9M tokens in ParCoLab database. The Serbian subcorpus
currentlycontains9.6Mtokens,French11.5M,English7.7M,whereastheSpan-
ish portion contains 4.06M. The current percentage of spoken data is listed in
Table2.
Table 2. Token distribution per language after adding spoken data.
Texttype Serbian French English Spanish Total %ofcorpus
TEDtalks 3,215,129 3,592,230 3,304,572 3,346,261 13,458,193 40.88
Films 292,916 356,749 252,355 0 902,020 2.74
Cartoons 110,865 68,516 173,865 23,324 376,570 1.14
Spoken data 3,618,910 4,017,495 3,730,792 3,369,585 14,736,783 44.77
Written data 5,989,500 7,475,463 4,030,951 687,127 18,183,041 55.23
Total 9,608,410 11,492,958 7,761,743 4,056,712 32,919,824
%ofcorpus 29.19 34.91 23.58 12.32
The percentage of literary works dropped from 88.7% to 55.23% whereas the
spoken data represent 44.77% instead of 1.78% of the corpus before the diversi-
ﬁcation. We can conclude that the inclusion of, what is called here, spoken data
has already demonstrated a substantial progress in diversifying ParCoLab cor-
pus.Allthespokenmaterialcanbequeriedviatheuser-friendlyinterfacewhich
makes this corpus accessible not only to researchers but also to the translators,
lexicographers, teachers, etc. The Spanish section of the corpus rose from 1.63%
to 12.32%.
When it comes to the qualitative evaluation of the corpus, this diversiﬁca-
tion helped to cover certain senses and contexts of speciﬁc words. For instance,
the Serbian adjective doma´ci (Eng. domestic) mostly occurred with the sense
‘related to the home’ in the original corpus [11]. Currently, its dominant sense
is ‘not foreign’, which is in accordance with the monolingual Serbian corpora.
Furthermore, as was supposed previously [10], ﬁlm transcripts contributed to
11 https://en.wikipedia.org/wiki/Jetlag Productions.68 D. Terzi´c et al.
augmenting the number of the examples in which French adjective sale (Eng.
dirty) is ‘used to emphasize one’s disgust for someone or something’.
5 Conclusion and Future Work
The quadrilingual corpus ParCoLab is one of rare parallel resources containing
a Serbian subcorpus, especially when it comes to original Serbian texts. In the
expansion of the corpus, priority was given to quality over quantity. In addition
to continuing work on enlarging the corpus, a great deal of eﬀort has also been
devoted to the diversiﬁcation of the predominantly literary content. This paper
describes the method that allowed us to include transcripts and translations of
2000 TED talks containing 13.5M tokens in ParCoLab. Apart from TED talks,
there are ﬁlm subtitles, among which are those originally produced in Serbian,
aswellasthetranscriptsofdubbedcartoons thatareincludedintheParCoLab
database. By including additional 73 ﬁlm and cartoon transcripts alongside the
aforementioned TED talks, ParCoLab corpus database surpasses 32.9M. Thus
we created the material not only for the development of NLP tools (especially
machinetranslation)butalsoforteachingandlearningFrench,English,Serbian,
and Spanish as foreign languages and for lexicography.
While the ParCoLab content is being diversiﬁed more and more, the anno-
tated portion of the corpus still comes from written documents. Given that the
trainingcorpusfortheannotation toolsneedstobebuiltonthein-domaindata
to perform well, it is necessary to improve the training corpus. A new spoken
language data subcorpus provides us with material to pursue this goal. There-
fore, our next steps in annotating the corpus would be to tag, lemmatize, and
parse added spoken subcorpus.
References
1. Agi´c, Zˇ., Ljubeˇsi´c, N.: Universal dependencies for Croatian (that work for Ser-
bian,too).In:Piskorski,J.(ed.)Proceedingsofthe5thWorkshoponBalto-Slavic
Natural Language Processing (BSNLP 2015), pp. 1–8. INCOMA, Hissar (2015)
2. Agi´c, Zˇ., Ljubeˇsi´c, N., Merkler, D.: Lemmatization and morphosyntactic tagging
of Croatian and Serbian. In: Piskorski, J. (ed.) Proceedings of the 4th Biennial
International Workshop on Balto-Slavic Natural Language Processing (BSNLP
2013), pp. 48–57. Association for Computational Linguistics, Soﬁa (2013)
3. Balvet,A.,Stosic,D.,Miletic,A.:TALC-sefamanually-revisedPOS-taggedliter-
ary corpus in Serbian, English and French. In: LREC 2014, pp. 4105–4110. Euro-
pean Language Resources Association, Reykjavik (2014)
4. Cettolo, M., Girardi, C., Federico, M.: WIT3: web inventory of transcribed and
translatedtalks.In:Proceedingsofthe16thEAMTConference,pp.261–268(2012)
5. Cˇerm´ak, F., Rosen, A.: The case of interCorp, a multilingual parallel corpus. Int.
J. Corpus Linguist. 13(3), 411–427 (2012)
6. Gildea, D.: Corpus variation and parser performance. In: Proceedings of the 2001
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(2001).https://
www.aclweb.org/anthology/W01-0521Diversiﬁcation of Parallel Corpus ParCoLab with Spoken Language Data 69
7. vanderKlis,M.,LeBruyn,B.,deSwart,H.:Temporalreferenceindiscourseand
dialogue (Forth)
8. Krstev, C., Vitas, D.: An aligned English-Serbian corpus. In: Tomovi´c, N., Vuji´c,
J. (eds.) ELLSIIR Proceedings (English Language and Literature Studies: Image,
Identity, Reality), vol. 1, pp. 495–508. Faculty of Philology, Belgrade (2011)
9. Krstev, C., Vitas, D., Erjavec, T.: MULTEXT-East resources for Serbian. In:
Erjavec, T., Gros, J.Z. (eds.) Zbornik 7. mednarodne multikonference “Informa-
cijska druzba IS 2004”, Jezikovne tehnologije, Ljubljana, Slovenija, 9–15 Oktober
2004. Institut “Joˇzef Stefan”, Ljubljana (2004)
10. Marjanovi´c, S., Stosic, D., Miletic, A.: A sample French-Serbian dictionary entry
based on the ParCoLab parallel corpus. In: Krek, S., et al. (eds.) Proceedings of
the XVIII EURALEX International Congress: Lexicography in Global Contexts,
pp. 423–435. Faculty of Arts, Ljubljana (2018)
11. Marjanovi´c,S.,Stoˇsi´c,D.,Mileti´c,A.:ParalelnikorpusParCoLabusluˇzbisrpsko-
francuske leksikograﬁje. In: Novakovi´c, J., Srebro, M. (eds.) Srpsko-francuske
knjiˇzevne i kulturne veze u evropskom kontekstu I, pp. 279–307. Matica srpska,
Novi Sad (2019)
12. Miletic,A.:Untreebankpourleserbe:constitutionetexploitations.Ph.D.thesis.
Universit´e Toulouse Jean Jaur`es, Toulouse (2018)
13. Miletic,A.,Fabre,C.,Stosic,D.:Delaconstitutiond’uncorpusarbor´e´al’analyse
syntaxique du serbe. Traitement Automatique des Langues 59(3), 15–39 (2018)
14. Miletic, A., Stosic, D., Marjanovi´c, S.: ParCoLab: a parallel corpus for Serbian,
FrenchandEnglish.In:Ekˇstein,K.,Matouˇsek,V.(eds.)TSD2017.LNCS(LNAI),
vol. 10415, pp. 156–164. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-64206-2 18
15. Nivre,J.,etal.:TheCoNLL2007sharedtaskondependencyparsing.In:Proceed-
ings of the CoNLL Shared Task Session of EMNLP-CoNLL, pp. 915–932. Associ-
ation for Computational Linguistics, Prague (2007)
16. Ruiz,N.,Federico,M.:Complexityofspokenversuswrittenlanguageformachine
translation.In:Proceedingsofthe17thAnnualConferenceoftheEuropeanAsso-
ciationforMachineTranslation(EAMT),pp.173–180.Hrvatskodruˇstvozajeziˇcne
tehnologije, Zagreb (2014)
17. Stosic,D.,Marjanovi´c,S.,Miletic,A.:Corpusparall`eleParCoLabetlexicographie
bilingue fran¸cais-serbe: recherches et applications. In: Srebro, M., Novakovi´c, J.
(eds.) Serbica (2019). https://serbica.u-bordeaux-montaigne.fr/index.php/revues
18. Terzic,D.:ParsingdestextesjournalistiquesenserbeparlelogicielTalismane.In:
ProceedingsofTALN-RECITAL,PFIA2019,pp.591–604.AfIA,Toulouse(2019)
19. Tiedemann,J.:Paralleldata,toolsandinterfacesinOPUS.In:Calzolari,N.(eds.)
Proceedingsofthe8thInternationalConferenceonLanguageResourcesandEval-
uation (LREC 2012). European Language Resources Association, Istanbul (2014)
20. Tyers,F.M.,Alperen,M.S.:South-EastEuropeantimes:aparallelcorpusofBalkan
languages.In:ProceedingsoftheLRECWorkshoponExploitationofMultilingual
Resources and Tools for Central and (South-) Eastern European Languages, pp.
49–53 (2010)
21. Vitas, D., Krstev, C.: Literature and aligned texts. In: Slavcheva, M., et al. (eds.)
ReadingsinMultilinguality,pp.148–155.InstituteforParallelProcessing,Bulgar-
ian Academy of Sciences, Soﬁa (2006)
22. vonWaldenfels,R.:CompilingaparallelcorpusofSlaviclanguages.Textstrategies,
toolsandthequestionoflemmatizationinalignment.In:Brehmer,B.,Zdanova,V.,
Zimny,R.(eds.)Beitr¨agederEurop¨aischenSlavistischenLinguistik(POLYSLAV)
9, pp. 123–138. Verlag Otto Sagner, Mu¨nchen (2006)70 D. Terzi´c et al.
23. vonWaldenfels,R.:RecentdevelopmentsinParaSol:breadthfordepthandXSLT
based web concordancing with CWB. In: Daniela, M., Garab´ık, R. (eds.) Nat-
ural Language Processing, Multilinguality, Proceedings of Slovko 2011, Modra,
Slovakia, 20–21 October 2011, pp. 156–162. Tribun EU, Bratislava (2011)
24. Zeroual, I., Lakhouaja, A.: MulTed: a multilingual aligned and tagged parallel
corpus. Appl. Comput. Inform. (2018). https://doi.org/10.1016/j.aci.2018.12.003Quantitative Analysis
of the Morphological Complexity
of Malayalam Language
B
Kavya Manohar1,3( ) , A. R. Jayan2,3 , and Rajeev Rajan1,3
1 College of Engineering Trivandrum, Thiruvananthapuram, Kerala, India
sakhi.kavya@gmail.com
2 Government Engineering College Palakkad, Palakkad, Kerala, India
3 APJ Abdul Kalam Technological University, Thiruvananthapuram, Kerala, India
Abstract. This paper presents a quantitative analysis on the morpho-
logical complexity of Malayalam language. Malayalam is a Dravidian
language spoken in India, predominantly in the state of Kerala with
about 38 million native speakers. Malayalam words undergo inﬂections,
derivations and compounding leading to an inﬁnitely extending lexicon.
In this work, morphological complexity of Malayalam is quantitatively
analyzed on a text corpus containing 8 million words. The analysis is
based on the parameters type-token growth rate (TTGR), type-token
ratio (TTR) and moving average type-token ratio (MATTR). The val-
uesoftheparametersobtainedinthecurrentstudyiscomparedtothat
of the values of other morphologically complex languages.
· · ·
Keywords: Morphological complexity Types and tokens TTR
Malayalam language
1 Introduction
Malayalam1 is a language with complex word morphology. Malayalam words
undergo inﬂections, derivations and compounding producing an inﬁnite vocabu-
lary [19]. As a language with high morphological complexity it has a large num-
ber of wordforms derived from a single root word (such as the English words
houses andhousing,whichstemfromthesamerootwordhouse).Morphological
complexity can be measured either in terms of the average number of grammat-
ical features getting encoded into a word or in terms of the diversity of word
forms occurring in the text corpus of a language. The former approach is called
typological analysis and the latter one is called corpus based analysis of mor-
phologicalcomplexity[5].Morphologicalcomplexityofalanguagehasitsimpact
on applications like automatic speech recognition (ASR) where speech to text
conversion depends largely on the underlying language model. A measure of the
1 https://en.wikipedia.org/wiki/Malayalam.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.71–78,2020.
https://doi.org/10.1007/978-3-030-58323-1_772 K. Manohar et al.
complexity is important for improving and adapting the existing methods of
natural language processing (NLP) [10].
This paper analyses the morphological complexity of Malayalam in terms of
corpus based parameters namely, type-token growth rate (TTGR), type-token
ratio (TTR) and moving average type-token ratio (MATTR). These parameters
are formally deﬁned in Sect.5. The study is conducted on a Malayalam text
corpus of 8 million words.
2 Literature Review
Complexity of a natural language can be in terms of morphology, phonology
and syntax [3]. Morphological level complexity of a language implies a large
possibility of inﬂections (by grammatical tense, mood, aspect and case forms)
and agglutinations (of diﬀerent wordforms). The number of possible inﬂection
pointsinatypicalsentence,thenumberofinﬂectionalcategories,andthenumber
of morpheme types are all morphological complexity indicators [4]. It requires
a strict linguistic supervision to analyze each word in terms of its morpheme
types to quantify complexity in this manner. Bentz et al. performed typological
analysis of morphological complexity involving human expert judgement and
compared it with corpus based analysis of morphological complexity and drew
strong correlation between the two [5].
Covingtonetal.suggestedtheuseofMATTRasareliablemeasureoflinguis-
tic complexity independent of the total corpus length and suggested an eﬃcient
algorithm for computing MATTR [6]. Kettunen [13] compared corpus based
parameters like TTR and MATTR with other methods of complexity measures
as deﬁned by Patrick Juola [12] and concluded both TTR and MATTR give a
reliable approximation of the morphological complexity of languages. Ximena
Gutierrez-Vasques et al. suggested estimating the morphological complexity of
a language directly from the diverse wordforms over a corpus is relatively easy
andreproduciblewaytoquantifycomplexitywithoutthestrictneedoflinguistic
annotated data [10].
3 Problem Statement
Malayalam has seven nominal case forms (nominative, accusative, dative, socia-
tive, locative, instrumental and genitive), two nominal number forms (singular
and plural) and three gender forms (masculine, feminine and neutral). These
forms are indicated as suﬃxes to the nouns. Verbs in Malayalam get inﬂected
basedontense(present,pastandfuture),mood(imperative,compulsive,promis-
sive, optative, abilitative, purposive, permissive, precative, irrealis, monitory,
quotative, conditional and satisfactive), voice (active and passive) and aspect
(habitual, iterative, perfect) [16,19]. The inﬂecting suﬃx forms vary depending
on the ﬁnal phonemes of the root words. Words agglutinate to form new words
depending on the context [2]. Table1 gives examples of a few complex word
formation in Malayalam.Morphological Complexity of Malayalam Language 73
Table 1. Complex morphological word formation in Malayalam
MalayalamWord TranslationtoEnglish Remark
( )  inthebox N
tothechild N
babyelephant Compound word formed by agglu-
tination of nouns
phant)and
tothebabyelephants N
plural form of the compound word
donotstayawake Negativeimperativemoodoftheverb
willbesinging Future tense iterative aspect of the
verb
Theproductivewordformation andmorphological complexity ofMalayalam
are documented qualitatively in the domain of grammatical studies. However
a quantitative study on the same is not yet available for Malayalam language.
AdoptionofgeneralNLPsolutionsofhighresourcelanguageslikeEnglishisnot
feasible in the setting of morphologically complex languages. A functional mor-
phology anlayzer, mlmorph addresses the morphological complexity of Malay-
alam applying grammatical rules over root word lexicon [19]. Quantiﬁcation of
linguistic complexity is important to adapt and improve various NLP applica-
tionslikeautomaticspeechrecognition,partsofspeech(POS)taggingandspell
checking [9,14,17,18]. This study aims at quantifying the morphological com-
plexity of Malayalam in terms of corpus parameters.
4 Material
ThisstudyisperformedonMalayalamrunningtextfromWikipediaarticles.The
Malayalam Wikipedia dump is curated and published by Swathanthra Malay-
alam Computing (SMC) as SMC Corpus [1]. It consists of 62302 articles. The
Malayalam running text often has foreign words, punctuation and numerals
present in it. The corpus is ﬁrst cleaned up to eliminate non Malayalam con-
tent and punctuations. It is then unicode normalized [7]. The cleaned up corpus
contained 8.14 million Malayalam words. The nature of the text is formal ency-
clopedic Malayalam.
5 Method
An element of the set of distinct wordforms in a running text is called a type.
Every instance of a type in the running text is called a token. For example,74 K. Manohar et al.
in the sentence, To be or not to be is the question, there are 7 types and 9
tokens.Thetypestoandberepeattwotimeseach.Therelationshipbetweenthe
count of types and tokens is an indicator of vocabulary richness, morphological
complexity and information ﬂow [10]. The type-token ratio (TTR) is a simple
baseline measure of morphological complexity [13]. TTR is calculated by the
formula deﬁned in Eq. (1), where V is the count of types and N is the count of
tokens.
V
TTR= (1)
N
The type count gets expanded due to productive morphology and higher
valuesofTTRcorrespondtohighermorphologicalcomplexity[5].HoweverTTR
isaﬀectedbythetokencount,N[6].Longerthecorpus,itismorelikelythatthe
newtokensbelongtothetypesthathaveoccurredalready.ThevalueofTTRgets
smaller with the increase in token count. Computing TTR over incrementally
larger corpus can indicate how the TTR varies with the token count. In this
study, TTR is computed with diﬀerent token counts starting with 1000 and
increasinguptotheentirecorpussize.ThishasenabledcomparisonofMalayalam
with the morphological complexity of other languages whose TTR values are
available in literature for diﬀerent token counts.
Thetype-tokengrowthrate(TTGR)curveisobtainedbyplottingthegraph
of token count vs. type count. It indicates how many new types appear with the
increase in the token count. If the slope of the growth rate curve reduces and
approacheshorizontalatalowervalueoftokencount,itindicatesasimplemor-
phology [15]. For a morphologically complex language, the type count continues
to grow with the token count [11].
The moving average type-token ratio (MATTR) computes the relationship
between types and tokens that is independent of the text length. Its eﬃcient
implementation by Covington et al. has been used by Kettunen to compare the
morphologicalcomplexityofdiﬀerentEuropeanlanguages[6,13].Thealgorithm
to compute MATTR is as follows [8]:
Algorithm 1: Computation of MATTR
Data: A text Corpus
Result: MATTR
1 N ← length of corpus;
2 L ← length of window (L¡N);
3 start ← initial position of window ;
4 i = start ← index of window position;
5 while i≤(N −L+1) do
6 Vi =type count in the window [i,i+L−1];
7 TTR(i)= Vi;
L
8 i=i+1;
9 end
10 MATTR(L)= (cid:2)Ni=−1L+1TTR(i)
N−L+1Morphological Complexity of Malayalam Language 75
Fig.1. TTGR and TTR plot of Malayalam for SMC Corpus of Wikipedia text
ThecorpuswithN tokensisdividedintotheoverlappedsubtextsofthesame
length, say L, the window length. Window moves forward one token at a time
and TTR is computed for every window. MATTR is deﬁned as the mean of the
entiresetofTTRs[6].InthisworkLischosenas500,enablingcomparisonwith
other languages in the study by Kettunen, where the window length is 500 [13].
6 Result and Discussion
Counting the types and tokens on SMC Corpus, TTGR and TTR curves are
plotted. Figure1 shows the TTGR curve on the left and the TTR on the right.
TTGR curve shows a steep rise initially. As the token count reaches 8 million,
thetypecountisaround1.2million.Butthecurvedoesnotﬂattenevenatthat
tokencount.ThispatternisacommonpropertyofDravidianlanguagesasmany
unseen wordforms appear as the corpus size is increased [15]. TTR is very high
ataround0.82whenthetokencountis1000.TTRreducestoaround0.44when
the token count is 0.1 million and ﬁnally saturates at around 0.16 for the full
corpus of 8 million tokens.
To compare the TTR obtained for Malayalam with that of other languages,
we have used the data reported for European languages by Kettunen and for
Indian languages by Kumar et al. [13,15]. Figures2a and 2b illustrates the com-
parison. Only those languages with the highest reported TTRs in the respective
papers and English are used for comparison. The token size (in millions) used
for computing TTRs used in the comparisons is indicated for each language.
Malayalam clearly shows more morphological complexity than the European
languages, Finnish, Estonian, Czech, Slovak, English and Spanish in terms of
TTRvalues.Valuesof TTRobtained for Malayalam whencompared withother
Indian languages Marathi, Hindi, Tamil, Kannada and Telugu indicate a higher
level of morphological complexity for Malayalam.76 K. Manohar et al.
Fig.2. Comparison of Malayalam TTR with that of European Union Constitution
Corpus [13] and DoE-CIIL Corpus [15]
Fig.3.TTRplottedatdiﬀerentsegmentsoftheSMCcorpusfor1000windowpositions
MATTR is computed with window length, L = 500 over diﬀerent segments
of the SMC corpus. TTR values for the segments with window position index
1–1000, 5001–6000, 15001–16000 and 18001–19000 are plotted in Fig.3. These
segments gave MATTR values 0.834, 0.839, 0.836 and 0.800 respectively. Com-
puting MATTRwith 0.1 million tokens ofSMC corpus resultedinavalue 0.806
for Malayalam. Kettunen has reported MATTR values on European Union con-
stitution corpus with each language having a token count slightly above 0.1
million [13]. A comparative graph of the MATTR values reported by Kettunen
with the values obtained for Malayalam is plotted in Fig.4. It clearly indicates
ahigherdegreeofmorphologicalcomplexityforMalayalamintermsofMATTR
on a formal text corpus. An equivalent comparison with other Indian languages
could not be done due to non availability of reported studies.Morphological Complexity of Malayalam Language 77
Fig.4.ComparisonofMATTRvaluescomputedforMalayalamonSMC Corpus with
that of European Union Constitution Corpus [13]
7 Conclusion
In this paper we have reported a quantitative analysis of the morphological
complexity of Malayalam language on a formal text corpus of 8 million words.
Thecorpusbasedanalysishasrevealedhighdegreesofmorphologicalcomplexity
of Malayalam in terms of TTR and MATTR. It is important that this aspect
of morphological complexity be considered while developing natural language
processing applications like automatic speech recognition, spell checking and
POS tagging for Malayalam. This involves preparing morpheme based language
models and phonetic lexicons for ASR and performing a morphological analysis
of words for POS tagging and spelling correction.
References
1. Malayalam Corpus, by Swathanthra Malayalam Computing, April 2020. https://
gitlab.com/smc/corpus
2. Asher, R.E., Kumari, T.: Malayalam. Psychology Press, London (1997)
3. Baerman, M., Brown, D., Corbett, G.G.: Understanding and Measuring Morpho-
logical Complexity. Oxford University Press, Oxford (2015)
4. Bane, M.: Quantifying and measuring morphological complexity. In: Proceedings
of the 26th West Coast Conference on Formal Linguistics, Cascadilla Proceedings
Project Somerville, MA, pp. 69–76 (2008)
5. Bentz, C., Ruzsics, T., Koplenig, A., Samardzic, T.: A comparison between mor-
phological complexity measures: typological data vs. language corpora. In: Pro-
ceedingsoftheWorkshoponComputationalLinguisticsforLinguisticComplexity
(CL4LC), pp. 142–153 (2016)
6. Covington,M.A.,McFall,J.D.:Cuttingthegordianknot:themoving-averagetype-
token ratio (MATTR). J. Quant. Linguist. 17(2), 94–100 (2010)
7. Davis, M., Du¨rst, M.: Unicode normalization forms (2001)78 K. Manohar et al.
8. Fidler,M.,Cvrˇcek,V.:TamingtheCorpus:FromInﬂectionandLexistoInterpre-
tation, 1st edn. Springer, New York (2018). https://doi.org/10.1007/978-3-319-
98017-1
9. Georgiev, G., Zhikov, V., Osenova, P., Simov, K., Nakov, P.: Feature-rich part-of-
speech tagging for morphologically complex languages: application to Bulgarian.
In:Proceedingsofthe13thConferenceoftheEuropeanChapteroftheAssociation
for Computational Linguistics, EACL 2012, pp. 492–502. Association for Compu-
tational Linguistics (2012)
10. Gutierrez-Vasques, X., Mijangos, V.: Comparing morphological complexity of
Spanish,OtomiandNahuatl.In:ProceedingsoftheWorkshoponLinguisticCom-
plexityandNaturalLanguageProcessing,AssociationforComputationalLinguis-
tics, Santa Fe, New-Mexico, pp. 30–37, August 2018. https://www.aclweb.org/
anthology/W18-4604
11. Htay,H.H.,Kumar,G.B.,Murthy,K.N.:StatisticalAnalysesofMyanmarCorpora.
Department of Computer and Information Sciences, University of Hyderabad pp,
Hyderabad, pp. 1–15 (2007)
12. Juola, P.: Measuring linguistic complexity: the morphological tier. J. Quant. Lin-
guist. 5(3), 206–213 (1998)
13. Kettunen, K.: Can type-token ratio be used to show morphological complexity of
languages? J. Quant. Linguist. 21(3), 223–245 (2014)
14. Kipyatkova, I., Karpov, A.: Study of morphological factors of factored language
modelsforRussianASR.In:Ronzhin,A.,Potapova,R.,Delic,V.(eds.)SPECOM
2014. LNCS (LNAI), vol. 8773, pp. 451–458. Springer, Cham (2014). https://doi.
org/10.1007/978-3-319-11581-8 56
15. Kumar, G.B., Murthy, K.N., Chaudhuri, B.: Statistical analyses of Telugu text
corpora. IJDL. Int. J. Dravidian Linguist. 36(2), 71–99 (2007)
16. Nair, R.S.S.: A grammar of Malayalam. Lang. Ind. 12, 1–135 (2012)
17. Pakoci,E.,Popovi´c,B.,Pekar,D.:Usingmorphologicaldatainlanguagemodeling
for Serbian large vocabulary speech recognition. Comput. Intell. Neurosci. 2019,
8 (2019)
18. Pirinen, T.: Weighted Finite-State Methods for Spell-Checking and Correction.
University of Helsinki, Helsinki (2014)
19. Thottingal, S.: Finite state transducer based morphology analysis for Malayalam
language. In: Proceedings of the 2nd Workshop on Technologies for MT of Low
Resource Languages, European Association for Machine Translation, Dublin, Ire-
land, pp. 1–5, August 2019. https://www.aclweb.org/anthology/W19-6801Labeling Explicit Discourse Relations
Using Pre-trained Language Models
B
Murathan Kurfalı( )
Linguistics Department, Stockholm University, Stockholm, Sweden
murathan.kurfali@ling.su.se
Abstract. Labelingexplicitdiscourserelationsisoneofthemostchal-
lenging sub-tasks of the shallow discourse parsing where the goal is to
identifythediscourseconnectivesandtheboundariesoftheirarguments.
The state-of-the-art models achieve slightly above 45% of F-score by
using hand-crafted features. The current paper investigates the eﬃcacy
of the pre-trained language models in this task. We ﬁnd that the pre-
trainedlanguagemodels,whenﬁnetuned,arepowerfulenoughtoreplace
the linguistic features. We evaluate our model on PDTB 2.0 and report
the state-of-the-art results in extraction of the full relation. This is the
ﬁrst time when a model outperforms the knowledge intensive models
without employing any linguistic features.
· ·
Keywords: Explicit discourse relations Shallow discourse parsing
Argument labeling
1 Introduction
Shallow discourse parsing (SDP) refers to the task of segmenting a text into a
set of discourse relations. A typical discourse relation consists of two arguments
and a discourse connective accompanied with a sense reﬂecting the semantic
relation between the arguments (e.g. cause, precedence). Within the Penn Dis-
course Treebank (PDTB), discourse connectives are assumed to be the lexical
itemswhichconnecttwoabstractobjectssuchasevents,states,andpropositions
following the deﬁnition of [8]. There are two main types of discourse relations,
explicit and implicit, where the diﬀerence is the presence of an overt discourse
connective. Parsing explicit and implicit relations are often treated as separate
tasks,andimplicitdiscourserelationshavereceivedmuchoftheattentiondueto
thechallengesbroughtbyalackofanovertsignal.Inthiswork,weinsteadfocus
on the less studied task of identifying explicit discourse relations. This consists
of identifying discourse connectives and their arguments in text.
Labeling explicit relations is a challenging task due to three main reasons:
(i) connectives do not always assume a discursive role (ii) the arguments can
consist of discontinuous text spans (iii) the same text span can have diﬀerent
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.79–86,2020.
https://doi.org/10.1007/978-3-030-58323-1_880 M. Kurfalı
roles in diﬀerent relations. All three challenges are illustrated in the Example1
and 21.
(1) Although Dr. Warshaw points out that stress and anxiety have their positive
uses, ‘stress perceived to be threatening implies a component of fear and
anxietythatmaycontributetoburnout.’ Healsonotedthatvariouswork
environments, such as night work, have their own stressors.
(2) Although Dr. Warshaw points out that stress and anxiety have their
positive uses, “stress perceived to be threatening implies a component of
fear and anxiety that may contribute to burnout”
Example1 presents a case where a complete discourse relation, which is pro-
vided in Example2, is embedded within another relation. Therefore, the text
span ‘stress and ... uses’ assumes two diﬀerent roles in two diﬀerent relations; it
is part of the ﬁrst argument in the ﬁrst relation, whereas it is the second argu-
ment in Example2. Additionally, the second argument in Example1 consists of
an discontinuous text span as it is interrupted by the connective. Similarly, the
text span (‘Dr. Warshaw ... that’) creates discontinuity between the connective
and the second argument in Example2 as it does not belong to the relation at
all.Finally,thelexicalitemand,whichisthemostcommondiscourseconnective,
in Example2 do not assume any discursive role in this case as it only connects
two noun phrases rather than abstract objects.
Most existing literature heavily relies on feature engineering to deal with
these issues, with [2] and [3] being the only notable exceptions. The current
work follows the latter studies in performing explicit discourse relation labeling
without resorting to any linguistic features. Speciﬁcally, we try to answer the
following question: can pre-trained language models, which have shown signiﬁ-
cant gains in a wide variety of natural language tasks, replace the rich feature
sets used in a standard parser? To this end, we model explicit discourse relation
labeling as a pipeline of two tasks, namely connective identiﬁcation and argu-
ment labeling. Each sub-task is regarded as a token-level sequence prediction
problem and modeled as a simple neural architecture employing BERT [1].
WeevaluatedoursystemonthePDTB2.0corpus.Experimentalresultsshow
thatcontextualembeddingsareindeedpowerfulenoughtoreplacethelinguistics
featuresusedinpreviouswork.Unlikethepreviousfeature-independentmodels,
theproposedsystemmanagestoimproveovertheexistingsystemsbyachieving
8% increase in the extraction of the both arguments2. Besides the performance
gain, the proposed system has the beneﬁt of being directly applicable to any
raw text as it does not require any pre-processing and can straightforwardly
be extended to other languages with available training data and a pre-trained
language model.
1 In the examples, unless otherwise stated, Arg2 is shown in bold, Arg1 is in italics
and the discourse connective is underlined.
2 The source code is available at: https://github.com/MurathanKurfali/shallowdisc.Labeling Explicit Discourse Relations Using Pre-trained Language Models 81
2 Background
Shallow discourse parsing (SDP) aims to uncover the local coherence relations
within text without assuming any tree/graph structure between the relations,
hence the name shallow. It started with the release of PDTB 2.0 [8] and, lately,
it attracted attention thanks to the two subsequent shared tasks at CoNLL
2015[12]and2016[13].MostoftheparticipatingsystemstreatSDPasapipeline
of sub-tasks following [5] which is the ﬁrst full end-to-end PDTB-style discourse
parser.Astandardpipelinestartswithlabelingexplicitdiscourserelationswhich
isfollowedbysenseclassiﬁcationandlabelingImplicitdiscourserelations.Label-
ing explicit relations are further decomposed into a set of sub-tasks which are
connective identiﬁcation, argument position identiﬁcation, extraction of each
argument. Each sub-task is addressed by employing a rich set of linguistics
features, including dependency trees, part-of-speech tags, Brown clusters and
Verbnet classes [12,13].
[2]marks thebeginning ofanewlineofresearchwhichis toperformshallow
discourse parsing without any feature engineering. The authors address labeling
ofexplicitdiscourserelationstaskinasimpliﬁedsettingwherethetaskisreduced
to determining the role of each token within a pre-extracted relation span. The
authorstrainaLSTMonthosespanswhichtakesGloveembeddingsasitsinput
andclassiﬁeseachtokenwithoneofthefourlabelswhichareConn, Arg1, Arg2,
None. The network achieves F-score of 23.05% which is signiﬁcantly lower than
the state-of-the-art models. Nevertheless, the study is of great importance as
it shows that argument labeling is possible without any feature engineering.
[3] extends the idea of [2] to full shallow discourse parsing on raw texts. They
employ a BiLSTM and a sliding window approach, according to which the text
is split into overlapping windows. In each window the system tries to capture
the parts which belong to a discourse relation. The predicted relation spans are
laterassembledusinganovelaggregationmethodbasedonJaccarddistance.The
proposed hierarchy performs considerably better than [2] but still falls short of
matching the state-of-the-art methods.
Method-wise,theclosestworktothecurrentpaperisthatof[6](ToNy)which
employscontextualembeddingstoperformmultilingualRST-stylediscourseseg-
mentation.InsteadofdirectlyﬁnetuningBERT,ToNyusesasimpliﬁedsequence
prediction architecture which consists of an LSTM conditioned on the concate-
nation of the contextual embeddings (either Elmo or BERT) and the character
embeddings obtained by convolution ﬁlters.
3 Method
Unlike previous studies which realize labeling explicit discourse relations as a
long pipeline of tasks which usually consists of 4 to 6 sub-components [9,11],
we propose a simpliﬁed pipeline consisting of only two steps, namely connective
identiﬁcation and argument extraction.
Theconnectiveidentiﬁcationstephelpsustoexploitthelexicalizedapproach
of the PDTB. In the PDTB framework, the discourse connectives function as82 M. Kurfalı
anchor points and without them, determining the number of relations in a text
become highly problematic, especially when the arguments of multiple relations
overlapasinExample1and2.Bypassingconnectiveidentiﬁcationwouldrequire
anextrapost-processingsteptosortdiﬀerentrelationswithcommonarguments
out which is not a trivial task. In order to avoid those problem, we perform
connective identiﬁcation as the ﬁrst task, mimicking the original annotation
schema.
Followingthepreviousstudiesof[2,3],weapproachexplicitdiscourserelation
labeling as an N-way token classiﬁcation problem. To this end, we follow the
standard token classiﬁcation architecture used in sequence prediction tasks, e.g.
named entity recognition, employing BERT [1]. The architecture consists of a
BERT model with a linear layer on top. The linear layer is connected to the
hidden-states of the BERT and outputs the label probabilities for each token
based on the sub-task it is trained on.
3.1 Connective Identiﬁcation
Theaimofthiscomponentistoidentifythelexicalitemswhichassumeadiscur-
sive role in the text. Although connective identiﬁcation seems to be the easiest
step among the other sub-tasks of shallow discourse parsing [4,9], it has its own
challenges. One problem is that discourse connectives in PDTB can be mul-
tiword expressions such as lexically frozen phrases, e.g. on the other hand, or
as modiﬁed connectives which co-occur with an adverb e.g. partly because, par-
ticularly since. Such multi-word connectives pose a challenge because diﬀerent
connectives may also appear in the text consecutively without forming a longer
connective, as illustrated in Example3 and 4.
(3) a. Typically, developers option property, and then once they get the
administrative approvals, they buy it (Conjunction)
b. Typically, developers option property, and then once they get the
administrative approvals, they buy it (Precedence)
c. Typically, developers option property, and then once they get the
administrative approvals, they buy it (Succession) (WSJ 2313)
(4) Consider the experience of Satoko Kitada, a 30-year-old designer of vehicle
interiors who joined Nissan in 1982. At that time, tasks were assigned
strictly on the basis of seniority (Synchrony) (WSJ 0286)
BothExample3and4involveathreewordsequenceannotatedasconnectives
but in Example3 each token signals a diﬀerent relation whereas in the latter
example they are part of the same connective, hence signal only one relation.
Therefore, correct prediction of the boundaries of the connectives is as crucial
as identifying them as such because failing to do so may cause whole system
to miss existing relations or add artiﬁcial ones. To this end, unlike previous
studies [2,3], we assign diﬀerent labels to single and multi token connectives
(#Conn and #MWconn respectively) so at the time of inference, we can decide
whether consecutive tokens predicted as connective is a part of a multiword
connective or signal diﬀerent relations.Labeling Explicit Discourse Relations Using Pre-trained Language Models 83
Table 1.Numberofannotationswithvariousspanlengths,intermsofthenumberof
words in the relation, in the training set which consists of 14722 relations in total.
# of Annotations (%) Span length
6231 (42.32%) <25
12243 (83.16%) <50
13810 (93.81%) <75
14240 (96.73%) <100
14617 (99.29%) <250
Average 36.79
OnedrawbackofusingthepubliclyavailableBERTmodelisthattextspans
longerthan512wordpiecescannotbeencoded[1].Therefore,wedecidedtosplit
thetextintoparagraphstoensurethecoherenceofthetextsegmentsassplitting
into an arbitrary number of sentences would risk having incoherent segments.
Manual inspection of the training data reveals that majority of the relations
(84.94%) have both of their arguments in the same paragraph which further
support our decision.
3.2 Argument Extraction
The argument extractor needs to identify the Arg1 and Arg2 spans of each pre-
dictedconnective.Theextractorsearchesfortheargumentsoftherelationwithin
awindowof100wordscenteredaroundthediscourseconnective3.Followingthe
IOB2 format, the ﬁrst word of each argument is tagged as #ARGX-B while
other words within the argument spans are simply labeled as #ARGX where X
is the argument number (1 or 2). The words outside of the relations are labeled
as #NONE.
The window size is determined by considering the number of relations that
can be covered and the label distribution in the extracted spans as longer win-
dows sizes introduce a high of number of #NONE labels which negatively aﬀect
the training. Based on the span lengths of the relations in the training data
(Table1), window size of 100 presents itself as the best candidate since longer
windows only minimally increase the coverage.
4 Experiments
FollowingtheCoNLL2015setting,weusethePDTBSections2–21,22and23as
the training, development and test set respectively. We use the cased BERTbase
model in our experiments4 and the classiﬁers are implemented using the Hug-
gingface’s Transformer library5. The maximum sequence length is set to 400
3 For multiword connectives, we center the window around the ﬁrst token.
4 https://github.com/google-research/bert.
5 https://github.com/huggingface/transformers.84 M. Kurfalı
for connective identiﬁcation and 250 for argument extraction. We use AdamW
optimizer with the learning rate of 5×10−5 and (cid:2) = 10−8. Both classiﬁers are
ﬁne-tuned for 3 epochs. We train each classiﬁer for 4 runs in order to estimate
the variance and report the average performance.
5 Results and Discussion
We evaluate our model using the oﬃcial evaluation script of the CoNLL 2016
shared task6. The script calculates the exact match scores of the identiﬁed con-
nectives,extractedspansoftheﬁrstandthesecondargumentseparatelyaswell
as the identiﬁcation of the both arguments together (Arg1+Arg2).
Table 2. Exact match results (precision, recall, F-score) of explicit discourse relation
labelingonPDTBtestset.Themodelswithinhorizontallinesarethebestperforming
systemsofCoNLL2015,CoNLL2016andthefeatureindependentsystemsrespectively.
*refers to the results when the gold connectives are provided to the model.
Conn Arg1 Arg2 Arg1+Arg2
P R F P R F P R F P R F
[11] 94.83 93.49 94.16 51.05 50.33 50.68 77.89 76.79 77.33 45.54 44.90 45.22
[10] – – 92.77 – – 50.05 – – 76.23 – – 44.58
[14] 91.8 86.6 89.1 47.5 44.8 46.1 70.5 66.4 68.4 40.0 37.7 38.8
[7] 83.42 92.22 87.6 51.25 56.65 53.81 68.36 75.57 71.79 43.12 47.66 45.28
[9] 92.42 94.88 93.63 49.73 51.06 50.38 75.73 77.75 76.73 44.31 45.49 44.9
[4] 99.67 98.19 98.92 42.47 41.84 42.15 76.06 74.92 75.48 36.51 35.97 36.24
[3] 71.35 62.73 66.76 33.16 29.15 31.03 52.47 46.13 49.09 37.25 32.75 34.86
[3]* – – – 46.69 44.59 45.62 68.94 65.83 67.35 48.16 45.99 47.05
Ours 96.62 96.93 96.77 60.02 60.22 60.12 80.37 80.63 80.50 53.20 53.37 53.28
We compare our results with the top performing systems of CoNLL 2015/16
Sharedtasksaswellaswith[3]whichistheonlyfeature-independentstudythat
can run on raw texts (Table2). We selected the top systems in each sub-task
from CoNLL 2016 whereas for CoNLL 2015 we chose the top 3 ranked systems
as [11] single-handedly achieved the best score in each sub-task that year.
Connective Identiﬁcation: In line with the previous work, connective identi-
ﬁcation is the easiest step where our model achieves almost 97% F-score. Since
the standard deviation among diﬀerent runs is pretty low (<0.5), we randomly
selected one run and manually checked the predicted connectives. In total, 16
unique text spans are incorrectly predicted as discourse connective for a total of
28 times where most of them are and tokens (30.6%). Similarly, and also con-
stitutes the 30% of the false negatives (the connectives which are not labeled as
6 https://github.com/attapol/conll16st.Labeling Explicit Discourse Relations Using Pre-trained Language Models 85
such by the classiﬁer), suggesting that and is more challenging to disambiguate
in term of its discursive role than other connectives. Finally, of all predictions,
only two of them (10 min and end) are not valid connective candidates which
further proves the model’s success on connective identiﬁcation. However, since
there is not any unseen connective in the test set, we cannot draw any conclu-
sions regarding the generalization capabilities of the proposed model which will
be further examined in a future study.
Argument Extraction: The proposed model achieves the state-of-the-art
resultsinseparateextractionoftheargumentsaswellasthefullrelationextrac-
tion.Theincreaseintheextractionoftheﬁrstargumentisofspecialimportance
because the ﬁrst argument is the most challenging component to automatically
predict as it can reside anywhere in the text and do not have any syntactic
bounds with the connective, unlike the second argument.
Manualanalysisofthepredictedﬁrstargumentspansrevealsthat20%ofall
mismatches are only by one or two words and mostly occur in the beginning of
the argument. Several cherry-picked examples are provided in Example5 where
the predicted spans are underlined and the gold spans are shown in bold.
(5) a. I expect the market to open weaker Monday
b. crumbled. Arbitragers couldn’t dump their UAL stock
c. This has both made investors uneasy and the corporations
more vulnerable
To further investigate the performance of the argument extraction, we ran
twoadditionalevaluations.Firstly,weevaluatedtheperformanceontherelations
where the second argument precedes the ﬁrst one in the text (e.g. Example2)
which is quite infrequent (less than 10% the relations have this structure in the
testset).However,theproposedmodelturnedouttobequitesuccessfulinthose
relations and achieves 75.6% F-score in full relation extraction, suggesting that
itlearnedtheargumentstructureofthediscourseconnectivesconsiderablywell.
In the second evaluation, we focused on the relations with discontinuous
spans where there are at least a ﬁve word sequence which do not belong to the
any part of the relation between the ﬁrst argument and the connective. There
are 93 such relations in the test set and they are the most challenging ones
spreading over a text span of 91 words on average. Unfortunately, the proposed
model fails to extract the arguments of those relations by achieving only 14.7%
F-score in the extraction of the full relation, hence extraction of the arguments
which are not located in the immediate vicinity of the connective still remains a
challenge. Yet, it should also be noted that some of these relations falls outside
of the argument extractor’s scope due to its window size (see Sect.3.2).
6 Conclusion
We have shown that labeling explicit discourse relations is possible without
any feature engineering. We achieve state-of-the-art results by ﬁnetuning a pre-
trained language model on PDTB 2.0 which is the ﬁrst time that a feature-
independent system outperforms the existing knowledge intensive systems on86 M. Kurfalı
this task. However, detailed evaluations reveal that there is much room for
improvement, especially in identifying the discontinuous relations where the
arguments are interrupted by various text spans. We see the proposed system
as a ﬁrst step towards a high-performance shallow discourse parser that can be
extended to any language with a suﬃcient annotated data and a pre-trained
language model.
Acknowledgments. IwouldliketothankRobertO¨stlingandAhmetU¨stu¨nfortheir
useful comments and NVIDIA for their GPU grant.
References
1. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:pre-trainingofdeepbidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018)
2. Hooda, S., Kosseim, L.: Argument labeling of explicit discourse relations using
LSTM neural networks. arXiv preprint arXiv:1708.03425 (2017)
3. Knaebel, R., Stede, M., Stober, S.: Window-based neural tagging for shallow dis-
course argument labeling. In: Proceedings of the 23rd Conference on Computa-
tional Natural Language Learning (CoNLL), pp. 768–777 (2019)
4. Li,Z.,Zhao,H.,Pang,C.,Wang,L.,Wang,H.:Aconstituentsyntacticparsetree
based discourse parser. In: Proceedings of the CoNLL-16 Shared Task, pp. 60–64
(2016)
5. Lin, Z., Ng, H.T., Kan, M.Y.: A PDTB-styled end-to-end discourse parser. Nat.
Lang. Eng. 20(2), 151–184 (2014)
6. Muller,P.,Braud,C.,Morey,M.:ToNy:contextualembeddingsforaccuratemul-
tilingual discourse segmentation of full documents (2019)
7. Nguyen,M.:SDP-JAIST:ashallowdiscourseparsingsystem@CoNLL2016shared
task. In: Proceedings of the CoNLL-16 Shared Task, pp. 143–149 (2016)
8. Prasad, R., et al.: The Penn discourse TreeBank 2.0. In: LREC. Citeseer (2008)
9. Qin,L.,Zhang,Z.,Zhao,H.:Shallowdiscourseparsingusingconvolutionalneural
network. In: Proceedings of the CoNLL-16 Shared Task, pp. 70–77 (2016)
10. Stepanov, E., Riccardi, G., Bayer, A.O.: The UniTN discourse parser in CoNLL
2015sharedtask:token-levelsequencelabelingwithargument-speciﬁcmodels.In:
Proceedings of the Nineteenth Conference on Computational Natural Language
Learning-Shared Task, pp. 25–31 (2015)
11. Wang, J., Lan, M.: A reﬁned end-to-end discourse parser. In: Proceedings of
the Nineteenth Conference on Computational Natural Language Learning-Shared
Task, pp. 17–24 (2015)
12. Xue, N., Ng, H.T., Pradhan, S., Prasad, R., Bryant, C., Rutherford, A.: The
CoNLL-2015sharedtaskonshallowdiscourseparsing.In:ProceedingsoftheNine-
teenthConferenceonComputationalNaturalLanguageLearning-SharedTask,pp.
1–16 (2015)
13. Xue,N.,etal.:CoNLL2016sharedtaskonmultilingualshallowdiscourseparsing.
In: Proceedings of the CoNLL-16 Shared Task, pp. 1–19 (2016)
14. Yoshida,Y.,Hayashi,K.,Hirao,T.,Nagata,M.:HybridapproachtoPDTB-styled
discourse parsing for CoNLL-2015. In: Proceedings of the Nineteenth Conference
on Computational Natural Language Learning-Shared Task, pp. 95–99 (2015)EPIE Dataset: A Corpus for Possible
Idiomatic Expressions
B
Prateek Saxena( ) and Soma Paul
International Institute of Information Technology, Hyderabad, India
prateek.saxena@research.iiit.ac.in, soma@iiit.ac.in
Abstract. Idiomaticexpressionshavealwaysbeenabottleneckforlan-
guage comprehension and natural language understanding, speciﬁcally
for tasks like Machine Translation (MT). MT systems predominantly
produce literal translations of idiomatic expressions as they do not
exhibit generic and linguistically deterministic patterns which can be
exploited for comprehension of the non-compositional meaning of the
expressions. These expressions occur in parallel corpora used for train-
ing, but due to the comparatively high occurrences of the constituent
words of idiomatic expressions in literal context, the idiomatic mean-
ing gets overpowered by the compositional meaning of the expression.
State of the art Metaphor Detection Systems are able to detect non-
compositional usage at word level but miss out on idiosyncratic phrasal
idiomaticexpressions.Thiscreatesadireneedforadatasetwithawider
coverageandhigheroccurrenceofcommonlyoccurringidiomaticexpres-
sions,thespansofwhichcanbeusedforMetaphorDetection.Withthis
in mind, we present our English Possible Idiomatic Expressions (EPIE)
corpuscontaining25,206sentenceslabelledwithlexicalinstancesof717
idiomaticexpressions.Thesespansalsocoverliteralusagesforthegiven
set of idiomatic expressions. We also present the utility of our dataset
by using it to train a sequence labelling module and testing on three
independent datasets with high accuracy, precision and recall scores.
· ·
Keywords: Idioms Idiomatic expressions Multiword expressions
1 Introduction
Naturallanguageunderstandingofidiomaticexpressionsembeddedinsentences
has been a complex problem to solve for some time. Idiom handling has been
a problematic area for a variety of NLP tasks. [2,11,14] have discussed the
magniﬁed complexity of this problem with respect to linguistic precision. [12]
provides empirical evidence that state-of-the-art machine translation systems
may achieve only half of the BLEU score on sentences that contain idiomatic
expressions as compared to the ones that do not. This drop in the score occurs
not only due to the comparatively low frequency of the idiomatic phrase with
respect to the frequency of the constituent words, but also due to the lack of
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.87–94,2020.
https://doi.org/10.1007/978-3-030-58323-1_988 P. Saxena and S. Paul
automatically determinable clear patterns in the wide and varied instances of
idioms in data [4]. This makes a regular monolingual training dataset sparse
withrespecttoidiomaticexpressions.Theabsenceofadatasetrichinidiomatic
expressions hampers the possibility of modelling the problem into a machine
learning task.
Any attempt on handling these idiomatic expressions has to follow certain
predeﬁnedstepsasdiscussedin[9].Theﬁrststepistodetectlexicaloccurrences
of idiomatic expressions in a given text. The subsequent steps constitute iden-
tifying the underlying semantics and learning a simpler representation for any
downstream task. In this paper, we attempt the ﬁrst step from the aforemen-
tionedstepsi.e.detectionofpossibleidiomaticexpressionsinagiventext.These
lexical variations can have a literal occurrence as our purpose is to capture the
span of the phrase in order to identify a metaphorical usage as the next step.
We present a dataset of 25,206 sentences which contain lexical occurrences of
717 idiomatic expressions from the IMIL dataset [1]. We identify the detection
of idiomatic expressions as a sequence labelling task and present a two pronged
approachfordetectionoftwodiﬀerentkindsofidioms:StaticandFormal.Static
idiomsdonotundergolexicalchanges,thereforelabellingthemcanbeassimple
asastringsearchinthetext.Formalidioms,ontheotherhand,undergovarious
lexical modiﬁcations, therefore labelling them can be modelled as a supervised
task. We test a model trained on our dataset and test on three datasets, “all
words”and“lexsample”trainingdatasetsofSemEval-2013Task5bDataset[7],
andPIECorpus[5].Alltestsgiveresultswithhighaccuracy,precisionandrecall
scores.
The major contributions of this work can be summarized as follows:
– Wepublicallyreleaseadatasetof25,206sentenceslabelledwithlexicaloccur-
rences of 717 idioms. These labels are done by automatic systems with high
accuracy. Of these, 21,891 sentences contain occurrences of Static idioms
which are 359 in number and 3,135 sentences contain occurrences of Formal
idioms which are 358 in number.1
– Ananalysisofthedistribution(MeanandStandardDeviation)ofidiomsover
the dataset.
2 Related Work
[4] created a distinction in idioms i.e. Formal and Static. Static idioms are the
kind of idioms which do not exhibit internal or morphosyntactic variation. For
example, As soon as possible, no comment, etc. Formal idioms, on the other
hand, undergo inﬂectional changes, pronominal and determiner modiﬁcations,
and internal qualitative modiﬁers (adjectival and adverbial). For example, keep
eye on, race against time etc. StringNet [15] identiﬁed that mapping base forms
of phrases is necessary in order to extract their surface realization. StringNet
used hybrid ngrams and cross indexing to create a resource to extract idiomatic
1 Dataset available at: https://github.com/prateeksaxena2809/EPIE Corpus.EPIE Dataset: A Corpus for Possible Idiomatic Expressions 89
sentences from the British National Corpus [8]. We use StringNet for the ﬁrst
level extraction of sentences for our work. [1] has created the IMIL dataset
which maps 2000 of the highly occurring English idioms to their counterparts
in diﬀerent Indian languages. We use their idiom list as a starting point for our
sentence extraction.
There have been some attempts to extract idiomatic expressions. The VNC-
Tokens Dataset [3], IDIX Corpus [13], PIE Corpus [5] and SemEval-2013 Task
5 Dataset [7] all contain around 3,000 to 4,500 potential idiomatic expressions
instances of 53 to 65 candidate idioms. These datasets, though thorough for
their respective candidate idioms, are small in size and limited in coverage. Our
dataset attempts to provide a wider coverage over a larger dataset.
3 Data
Ouraimistocreateadatasetonlycontainingsentenceswithlexicaloccurrences
ofidiomsfortheIMILdataset.Thisrequiresmultipledataﬁlteringsteps.These
steps are explained in the subsequent subsections.
3.1 StringNet Extraction
Variations in Idiomatic Expressions occurs in the following forms:
– Inﬂectional Modiﬁcations (tense, gender, number, etc):
Bite the dust
(cid:129) The visiting team bit the dust in the football game yesterday.
– Determiner/Pronominal Replacement:
Keep up the good work
(cid:129) Keep up your good work and the promotion will follow.
– Named Entities and Qualitative Modiﬁers inclusions (Adjectival and Adver-
bial)
Keep an eye on
(cid:129) Keep a keen eye on the child while he plays.
Behind his back
(cid:129) People say a lot behind James’ back.
In order to extract all instances of an idiomatic expression, it is important
to account for all the variation in the expression. We use StringNet for this
task.Stringnetcontainstwobillionconnectedhybridngramscross-indexedwith
lexeme information, parts of speech information and various word forms. This
matches an idiomatic expression like keep your eye on to its inﬂectional modiﬁ-
cations like kept your eye on and keeps your eye on. We also utilize StringNet’s90 P. Saxena and S. Paul
uniquefeatureofverticalpruningandhorizontalpruning.Verticalpruningrefers
togeneralizationoflexemesinagivensearchentryinordertosearchoccurrence
of parent ngrams and child ngrams of the entry in the corpus. For example, a
parent ngram of the entry Keep your eye on is keep [pron] eye on as [pron]
constitutesallpronouns.VerticalPruninghelpsinextractionofpronominaland
determiner variation. Horizontal pruning refers to connecting an ngram with
another ngram which diﬀers by one unit or type of ngram. For example, the
entry keep [det] eye on can be connected to keep eye on and keep [det] keen eye
on using horizontal pruning because it diﬀers from these ngrams by a length
of 1. But the entry keep your eye on can also be connected to keep an eye on
using horizontal pruning because both entries diﬀer by 1 ngram type. Horizon-
tal pruning helps in extraction of determiner-pronoun interchangeability and
internal qualitative modiﬁers.
Wetakethe2,000idiomspresentintheIMILdatasetandprocessthemauto-
matically in order to be used as search entries into StringNet. The processing
involves two features; lemmatization, and generalization of pronouns and deter-
miners into generic entries [pron] and [det] respectively. An entry keep an eye
on becomes keep [det] eye on. In addition to searching the term, we also search
the idiom in both directions through one level each of vertical and horizontal
pruning. This results in the extraction of 81562 sentences containing instances
from 758 of the 2,000 idioms.
3.2 Candidate Idioms Selection
In this step, we ﬁlter out redundant idioms from our idioms list Redundant
idioms constitute similar idiom entries in the 758 idioms list like music to my
ears and music to my ear are clubbed into a single entry, removing duplicate
entries of instances from the sentences. This step results in ﬁltering 749 idioms
and 77,894 sentences. The idioms that remain are unique and have idiomatic
usages.
3.3 Candidate Instances Selection
Idiomatic Expressions are also idiosyncratic in the kind of lexical variations
they allow. In this step, we ﬁlter out those lexical variations of idioms, which
willneveroccuridiomatically.Thisrequiresextractionofspeciﬁcpatternswhich
arerelevantexclusivelytoparticularidioms.Forexample,theidiomkeep an eye
on can occur as keep your eye on but give me a hand cannot occur as give me
your hand. In order to eﬃciently extract correct patterns, we manually divide
the idioms list into two categories based on [4].
Static Idioms. Static idioms are idioms which do not undergo any lexical
modiﬁcation. We identify 388 idioms as Static in our idioms list. These idioms
have 45,955 instances in the data. We ﬁlter out sentences which did not have an
exact occurrence of the idiom. If no exact occurrence of an idiom is found, weEPIE Dataset: A Corpus for Possible Idiomatic Expressions 91
reject the idiom altogether. At the end of this step, 21,891 sentences with 359
Static idioms are left.
Formal Idioms. Formal Idioms are idioms which occur in sentences with vari-
ous lexical modiﬁcations. We identify 361 idioms from our idioms list as Formal
idioms based on their occurrences. These idioms have 31,939 instances in the
data.AsthistaskrequiresmoreﬂexibilityandcomplexitythanStaticidioms,an
completely automatic approach is not feasible. At the same time, going through
the whole dataset sentence by sentence is quite ineﬃcient. Thus, in order to
eﬃciently sift through the data, we extract the unique variations of each idiom
and then manually remove the irrelevant occurrence patterns, thus removing
all sentences with those occurrences. This reduces our load by a scale factor of
1/3 as the unique occurrences are around 10,000 in number. This process does
not reduce the number of idioms to large extent (358) but we do ﬁlter out a
considerable number of patterns, resulting in only 3,135 remaining sentences.
3.4 Final Result
Finally we create a dataset of 717 idioms in 25,026 sentences/instances. We
separate the data into two groups; Static and Formal idioms. We create this
distinction in our data because detection of both categories of idioms require
separate steps. Static idioms can be detected by treating them like words-with-
spaces and simply ﬁnding their exact matches in the sentence. Formal idioms
detection requires a more complex approach which can identify the similarities
between instances of the same idiom and their diﬀerence from other phrases.
Number of sentences and idioms left after each step are given in Table 1. The
ﬁrstthreerowsshowtheresultsforthetotaldataextractionwhilethesubsequent
rows show extraction results for Formal and Static idioms separately.
Table 1. Number of Sentences and Idioms left after each extraction step
Extraction step Sentences Idioms
StringNet Extraction 81,562 758
Candidate Idioms Selection (Total) 77,894 749
Candidate Instances Selection (Total) 25,206 717
Candidate Idioms Selection (Static Idioms) 45,955 388
Candidate Instances Selection (Static Idioms) 21,891 359
Candidate Idioms Selection (Formal Idioms) 31,939 361
Candidate Instances Selection (Formal Idioms) 3,135 358
Wearealsointerestedinﬁndingthespreadofeachidiominouridiomslist.In
thiseﬀort,wecalculatethetotalinstancesofeachidiomandcalculatethemean92 P. Saxena and S. Paul
Table 2. Test Results from the model trained on Formal Idioms Training Dataset.
FormalIdiomsTestDatasetis25%splitfromtheFormalIdiomsDataset.Alldatasets
have been tested separately for All Usages and Only Idiomatic usages of potentially
idiomatic expressions in sentences
Test dataset Accuracy Precision Recall
Formal Idioms Test Dataset 0.98 0.95 0.91
SemEval All Words Dataset (all usages) 0.84 0.90 0.85
SemEval All Words Dataset (idiomatic usages) 0.86 0.93 0.86
SemEval Lex Sample Dataset (all usages) 0.89 0.90 0.90
SemEval Lex Sample Dataset (idiomatic usages) 0.92 0.95 0.92
PIE Corpus (all usages) 0.69 0.60 0.69
PIE Corpus (idiomatic usages) 0.88 0.94 0.88
Table 3. Mean and standard deviations of ﬁnal datasets
Idiom type Sentences Mean Std Dev
Formal 3,135 8.75 8.61
Static 21,891 60.9 160
and standard deviation on the resultant counts respectively for Formal idioms
and Static idioms. Table 3 shows the mean and standard deviation of both the
Formal idioms dataset and Static idioms dataset with respect to their number
of occurrences in data. The mean and standard deviation for Formal idioms are
very close which suggests an exponential distribution whereas the Static idioms
show a skewed distribution.
4 Experiments
WeuseourFormalidiomsdatasetcontaining3,135sentencestotrainonatypical
sequencelabellingneuralnetwork.Wedoa75–25train-evalsplitonourdataset
for our training and evaluation. In addition to the Formal idioms test dataset,
we use three independent datasets for testing mentioned as follows:
– “All words” training dataset from [7] containing 1,143 sentences. All sen-
tences contain potentially idiomatic phrases, each usage is labelled with
idiomatic,literal or both usage.
– “Lex sample” training dataset from [7] containing 1,423 sentences. All sen-
tences contain potentially idiomatic phrases, each usage is labelled with
idiomatic,literal or both usage.
– PIE corpus[5] containing 2,239 sentences. All sentences contain potentially
idiomatic phrases, each usage labelled with a sense label, “y” meaning
idiomatic usage and “n” meaning literal usage.EPIE Dataset: A Corpus for Possible Idiomatic Expressions 93
Weevaluateourmodelsontwoversionsofeachofthethreedatasets:Allsamples
and samples labelled with idiomatic usages.
WeuseaBiLSTM-CRF[6]moduleforourtask.Weuse300dimensionalglove
embeddings [10] as our embedding input. We use LSTM hidden representation
of dimension 100 and batch size of 20. We train the model for 25 epochs.
5 Results
The Results can be seen in Table 2. We see that the Formal idioms test dataset
gives the best results because of similarity with the training dataset. However,
the model also gives good results with other independent datasets.
6 Conclusion
In this paper, we present a semi-automatic approach to create a new dataset of
labelled potentially idiomatic expressions in 25,206 English Sentences extracted
from the BNC corpus [8] with high accuracy. We segregate our dataset into
two categories, Formal and Static. This we do because of the diﬀerence in the
potentially idiomatic span detection mechanisms of these categories.
References
1. Agrawal, R., Kumar, V.C., Muralidaran, V., Sharma, D.: No more beating about
thebush:asteptowardsidiomhandlingforIndianlanguageNLP.In:Proceedings
of the Eleventh International Conference on Language Resources and Evaluation
(LREC-2018) (2018)
2. Cap,F.,Nirmal,M.,Weller,M.,ImWalde,S.S.:HowtoaccountforidiomaticGer-
mansupportverbconstructionsinstatisticalmachinetranslation.In:Proceedings
of the 11th Workshop on Multiword Expressions, pp. 19–28 (2015)
3. Cook,P.,Fazly,A.,Stevenson,S.:TheVNC-tokensdataset.In:Proceedingsofthe
LRECWorkshopTowardsaSharedTaskforMultiwordExpressions(MWE2008),
pp. 19–22 (2008)
4. Fillmore, C.J., Kay, P., O’connor, M.C.: Regularity and idiomaticity in grammat-
ical constructions: the case of let alone. Language 64, 501–538 (1988)
5. Haagsma,H.,Nissim,M.,Bos,J.:Castingawidenet:robustextractionofpoten-
tially idiomatic expressions. arXiv preprint arXiv:1911.08829 (2019)
6. Huang,Z.,Xu,W.,Yu,K.:BidirectionalLSTM-CRFmodelsforsequencetagging.
CoRR abs/1508.01991 (2015). http://arxiv.org/abs/1508.01991
7. Korkontzelos, I., Zesch, T., Zanzotto, F.M., Biemann, C.: SemEval-2013 task 5:
evaluatingphrasalsemantics.In:SecondJointConferenceonLexicalandCompu-
tational Semantics (* SEM), Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013), pp. 39–47 (2013)
8. Leech,G.N.:100millionwordsofEnglish:theBritishnationalcorpus(BNC)(1992)
9. Liu, C., Hwa, R.: Phrasal substitution of idiomatic expressions. In: Proceedings
of the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 363–373 (2016)94 P. Saxena and S. Paul
10. Pennington, J., Socher, R., Manning, C.: Glove: global vectors for word represen-
tation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 1532–1543 (2014)
11. Sag,I.A.,Baldwin,T.,Bond,F.,Copestake,A.,Flickinger,D.:Multiwordexpres-
sions:apainintheneckforNLP.In:Gelbukh,A.(ed.)CICLing2002.LNCS,vol.
2276,pp.1–15.Springer,Heidelberg(2002).https://doi.org/10.1007/3-540-45715-
1 1
12. Salton, G., Ross, R., Kelleher, J.: An empirical study of the impact of idioms
onphrasebasedstatisticalmachinetranslationofEnglishtoBrazilian-Portuguese
(2014)
13. Sporleder, C., Li, L., Gorinski, P., Koch, X.: Idioms in context: the IDIX corpus.
In: LREC. Citeseer (2010)
14. Volk, M., Weber, N.: The automatic translation of idioms. machine translation
vs.translationmemorysystems.Sprachwissenschaft,Computerlinguistikundneue
Medien (1), 167–192 (1998)
15. Wible, D., Tsao, N.L.: Stringnet as a computational resource for discovering and
investigating linguistic constructions. In: Proceedings of the NAACL HLT Work-
shop on Extracting and Using Constructions in Computational Linguistics, pp.
25–31. Association for Computational Linguistics (2010)Experimenting with Diﬀerent Machine
Translation Models in Medium-Resource
Settings
Haukur Pa´ll Jo´nsson1 , Haukur Barri S´ımonarson2 ,
V´esteinn Snæbjarnarson2 , Steinþ´or Steingr´ımsson1 ,
B
and Hrafn Loftsson1( )
1 Language and Voice Lab, Reykjavik University, Reykjavik, Iceland
{haukurpj,steinthor18,hrafn}@ru.is
2 Mieind ehf., Reykjavik, Iceland
{haukur,vesteinn}@mideind.is
Abstract. State-of-the-art machine translation (MT) systems rely on
the availability of large parallel corpora, containing millions of sentence
pairs. For the Icelandic language, the parallel corpus ParIce exists, con-
sisting of about 3.6 million English-Icelandic sentence pairs. Given that
parallel corpora for low-resource languages typically contain sentence
pairs in the tens or hundreds of thousands, we classify Icelandic as a
medium-resource language for MT purposes. In this paper, we present
on-goingexperimentswithdiﬀerentMTmodels,bothstatisticalandneu-
ral,fortranslatingEnglishtoIcelandicbasedonParIce.Wedescribethe
corpus and the ﬁltering process used for removing noisy segments, the
diﬀerent models used for training, and the preliminary automatic and
humanevaluation.Weﬁndthat,whileusinganaggressiveﬁlteringapp-
roach, the most recent neural MT system (Transformer) performs best,
obtainingthehighestBLEUscoreandthehighestﬂuencyandadequacy
scoresfromhumanevaluationforin-domaintranslation.Ourworkcould
be beneﬁcial to other languages for which a similar amount of parallel
data is available.
· ·
Keywords: Machine translation Parallel data Evaluation
1 Introduction
Most work in Machine Translation (MT) through the years has mainly either
focused on high-resource or low-resource language pairs. Usually, a language
pair is considered high-resource if a parallel corpus exists consisting of millions
ofsentencepairs.Incontrast,alanguagepairisconsideredlow-resourceifeither
no parallel corpus exists, or the corpus only consists of a few tens or hundreds
of thousands of sentence pairs.
H. P. J´onsson, H. B. S´ımonarson, V. Snæbjarnarson, S. Steingr´ımsson—Equal contri-
bution.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.95–103,2020.
https://doi.org/10.1007/978-3-030-58323-1_1096 H. P. J´onsson et al.
NeuralMachineTranslation(NMT),inparticularsequence-to-sequencemod-
elsbasedonattentionmechanisms,e.g.theTransformer[22],hasinrecentyears
become the dominant paradigm in high-resource settings, replacing the previ-
ously long-standing dominance of Statistical Machine Translation (SMT) [10].
Oneparallelcorpus,ParIce[2],containingabout3.6millionEnglish-Icelandic
(en-is)sentencepairs,currentlyexistsforIcelandic.GiventhesizeofParIce,and
thefactthatwehaveonlybeenabletouseabout1.6millionofitssentencepairs
for training (see Sect.3.2), we currently categorize the en-is pair as a medium-
resource language pair.
In this paper, we present on-going work of experimenting with diﬀerent MT
systems,bothbasedonSMTandNMT,fortranslatingintheen→isdirection.
We describe the ParIce corpus and the ﬁltering process used for removing noisy
segments, the diﬀerent models used for training, and the preliminary evaluation
– both with regard to BLEU scores and human evaluation. We ﬁnd that, while
using an aggressive ﬁltering approach, the most recent NMT system, based on
the Transformer, performs best in our setting, obtaining a BLEU score of 54.71
(6.11pointshigherthanthenextbestperformingsystem,Moses).Furthermore,
the Transformer system also obtained the highest ﬂuency and adequacy scores
from human evaluation, in the in-domain setting. Our work could be beneﬁcial
to other languages for which a similar amount of parallel data is available.
2 Related Work
Inthelastfewyears,researchhasshownthattheNMTapproachhassigniﬁcantly
pushedaheadthestate-of-the-artinMT,whichbeforebelongedtophrase-based
SMT (PBSMT) systems. For example, [3] compared and analysed the output of
three PBSMT systems and one NMT system for English→German and found,
interalia,thati)theoverallpost-editeﬀortneededontheoutputfromtheNMT
system is considerably lower compared to the best PBSMT system; ii) that the
NMT system outperforms the PBSMT on all sentence lengths; and iii) that the
NMT output contains less morphological errors, less lexical errors and less word
order errors.
Even though NMT has emerged as the dominant MT approach, there have
also been reports of poor performance when using NMT under low-resource
conditions. Compared to SMT, [11] found that NMT systems have lower qual-
ity on out-of-domain texts, sacriﬁcing adequacy (how much of the meaning is
transferred between the source and the generated target) for the sake of ﬂu-
ency (a rating of how ﬂuent the generated target language is). They also found
that the NMT systems performed worse in low-resource settings, but better in
high-resource settings.
[5] discuss the quality of NMT vs. SMT. They argue that “so far it would
appearthatNMThasnotfullyreachedthequalityofSMT”,basedonautomatic
and human evaluations for three use cases, and that the results depend on the
diﬀerent domains and on the various language pairs.
Inastudy,usingthemedium-resourcelanguagepairEnglish-Polish,[8]found
that an SMT model achieves a slightly better BLEU score than an NMT modelExperimenting with Diﬀerent Machine Translation Models 97
basedonanattentionmechanism.Ontheotherhand,humanevaluationcarried
out on a sizeable sample of translations (2,000 pairs) revealed the superiority of
the NMT approach, particularly in the aspect of output ﬂuency.
Given the mixed ﬁndings in the literature regarding comparison between
NMTandPBSMT,especiallyinlowormedium-resourcesettings,wedecidedto
include SMT in our experiments.
The only previously published MT results regarding Icelandic are [4,9],
although Icelandic has been included in massively multilingual settings [6]. The
results rely either on rule-based systems or variants of transfer learning. In con-
trast,ourworkconstitutestheﬁrstpublishedMTandNMTresultsforIcelandic
based on direct supervised learning.
3 Corpus and Filtering
In this section, we describe the ParIce corpus and explain which parts of it are
used for training/testing as well as the ﬁltering process for removing segments
not suited for training.
3.1 ParIce
For training, we used ParIce [2], an en-is parallel corpus consisting of roughly
3.6 million translation segments. The corpus data is aligned with hunalign [21]
and ﬁltered using a sentence scoring algorithm based on a bilingual lexicon bag-
of-words method and a comparison between an MT generated translation of a
segment and the original segment.
ParIceisacollectionofdatafromdiﬀerentsources,thelargestbeingacollec-
tion of EEA regulatory texts (48%), data from OpenSubtitles (37%), published
onOPUS[20]butreﬁlteredintheParIcecorpus,andtranslationsegmentsfrom
the European Medicines Agency (EMA; 11%) published in the Tilde MODEL
corpus [17] (other sources amount to 4% of the data). From each of these three
corpora, we sampled roughly 2000 segments to serve as test sets.
3.2 Filtering
Startingfromthe3.6millionsegmentscompiledinParIce,weﬁlteredthecorpus
beforetraininganymodels.Amongtheﬁltersweused,manywereadaptedfrom
the suggestions of [15]. Most of the ﬁlters are proxies for alignment errors, OCR
errors, encoding errors and general text quality.
Primarily, the ﬁlters and post-editing consist of: 1) empty sentence ﬁlter;
2) identical or approximately identical source and target sequence, measured
by absolute and relative edit distance; 3) sentence length ratio ﬁlter, in charac-
tersandtokens;4) maximumandminimumsequencelengthﬁlter,incharacters
and tokens; 5) maximum token length; 6) minimum average token length; 7)
character whitelist; 8) digit mismatch: both sides should have the same set of98 H. P. J´onsson et al.
number sequences; 9) unique sequence pair, after removing whitespace, punctu-
ation,capitalizationandnormalizingallnumbersto0(allnumbersequencesare
equivalent); 10) case mismatches where one side is all uppercase and the other
not; 11) corrupt symbols, e.g. weird punctuation like ? and " inside words; 12)
many other ad-hoc regular expressions for Icelandic and dataset speciﬁc OCR
artifacts and encoding errors (e.g. common words where b replaces, i replaces l,
missing accents); 13) normalizing of quotes, bullets, hyphens and other punctu-
ation; 14) ﬁxing line splits where a word was split due to text reﬂow.
When applicable, we use the numbers provided in [15]. Otherwise the ﬁl-
ters were tuned to ﬁt Icelandic and ParIce speciﬁcally. Roughly half of ParIce
was ﬁltered out with this approach, leaving 1.6 million translation segments for
training, consisting of around 29 million Icelandic tokens and 32 million English
tokens.
4 Models
In this section, we describe the key characteristics of PBSMT and NMT models
and the three diﬀerent systems/models we have experimented with: the SMT
system Moses, and two NMT models, the ﬁrst one based on BiLSTM and the
secondoneontheTransformer.Eachmodelattemptstoestimatetheprobability
p(t|s), the probability of a sentence t in the target language given a sentence s
in the source language.
4.1 PBSMT
In PBSMT, p(t|s) is not modelled explicitly, rather Bayes’ theorem is applied
and t is reached via a translation model p(s|t) and a language model p(t) by
estimating argmax p(s|t)p(t). Furthermore, s and t are segmented into smaller
t
phrases,uponwhichthetranslationmodelisdeﬁned.Thephrasesareextracted
and their probabilities estimated during training using the underlying parallel
corpus. The language model ensures the ﬂuidity of t and can be derived from
thetrainingdataand/orfromaseparatemonolingualcorpus.Forfurtherdetails
see [10].
Moses. We used the standard open source implementation of PBSMT, the
Moses system1. We created a number of diﬀerent Moses models in order to deal
with the morphological richness of Icelandic. For example, we used a large out-
of-domainmonolingualcorpusandtokenizersincludingsubwordtokenizerssuch
as SentencePiece [13] with Byte Pair Encoding (BPE) and Unigram for both is
and en, with a 30k vocabulary for each language. For all models we used the
default alignment heuristic, the default distortion model, and a 5-gram KenLM
[7] language model trained on additional monolingual data, i.e. 6.5 million sen-
tences from the Icelandic Gigaword Corpus [18]. The best performing model,
which uses the Moses tokenizer for both en and is, is evaluated against the
NMT based systems in Sect.5.
1 http://www.statmt.org/moses/.Experimenting with Diﬀerent Machine Translation Models 99
4.2 NMT
An NMT system attempts to model p(t|s) directly using a large modular neural
network that reads s and outputs t, token by token. Instead of representing
the tokens symbolically, like PBSMT systems, the tokens are represented using
vectors(embeddings).ThetypicalNMTsystemisbasedonsequence-to-sequence
learning,andconsistsoftwocomponents:anencoderandadecoder.Thesystem
is trained to maximize p(t|s) by updating the parameters of the network using
stochastic gradient descent to back-propagate the errors from the output layer
to the previous layers. The two dominant NMT architectures over the last few
years are based on 1) LSTM, and 2) self-attention networks (Transformer).
BiLSTM. The general LSTM model for NMT is described in [19]. In this
model, the encoder is an LSTM that converts an input sequence s to a ﬁxed-
sized vector v from which the decoder, another LSTM, generates t. Given the
embedded tokens of s, (x1,...,xT) and v, the model estimates the conditional
probability p(y1,...,yT(cid:2)|x1,...,xT) as follows:
(cid:2)
(cid:2)T
p(y1,...,yT(cid:2)|x1,...,xT)= p(yt|v,y1,...,yt−1) (1)
t=1
where (y1,...,yT(cid:2)) represents the target sentence t, and where T(cid:2) may be dif-
ferent from T. In other words, the prediction of each target token depends on
the encoded version of the whole input sequence, as well as on the previously
predicted target words.
ThemodelisfurtherimprovedbyaddinganadditionalLSTMtotheencoder
which reads the input in the reverse order, i.e. the encoder is bidirectional.
Additionally, during decoding, these networks can be augmented with attention
[1,14] where alignments between target and source tokens can be modeled more
explicitly. We used the standard BiLSTM implementation from OpenNMT2,
medium and large NMT models with Luong attention [14] (4-layer 256 hidden
unit encoder, 4 layer 512 hidden unit decoder; large model has 6 layers and
double the number of hidden units). We used a 16k joint BPE vocabulary.
Transformer. TheTransformer,proposedby[22],buildsonpreviousmodelsin
variousways.Itsdesignprovidesformuchbetterparallelization,anditleverages
GPU architecture more so than LSTMs. In general, it achieves better machine
translation performance for the same training time and data as compared to
LSTMs.
The Transformer consists of stacked transformer blocks, each of which com-
prises 2–3 sublayers, self-attention, decoder-to-encoder attention, and a fully
connected layer. The block operates independently over a sequence of hidden
2 https://opennmt.net/.100 H. P. J´onsson et al.
vectors hi whereby each vector in the sequence can attend to (i.e. receive infor-
mation from) all other hidden vectors in the sequence before being transformed
by the fully connected sublayer. The decoder block has an added attention sub-
layer that allows it to attend to the encoder in addition to itself. Finally, the
last decoder block has a softmax output layer for token probabilties.
The implementation we use is the reference implementation from [22] of the
transformer-base architecture which is part of the Tensor2Tensor package3. It
has 6 layers each for its encoder and decoder with attention head count of 8.
We used shared source and target embeddings. The included subword tokenizer
provided by Tensor2Tensor was used to build a 16k joint subword vocabulary.
5 Evaluation
In this section, we present the results of automatic and human evaluation of
the individual models, Moses, BiLSTM and Transformer, for translating in the
en→is direction.
Neither NMT model was ﬁne-tuned before evaluation, and the Transformer
used checkpoint-averaging (a gain of about 0.5 BLEU). The batch sizes for the
Transformer and the BiLSTM were 1700 (subword) tokens and 32 sequences,
respectively. No other hyperparameter tuning was performed due to computa-
tional restraints.
5.1 BLEU Scores
We use BLEU for automatic evaluation. It is the most widely used MT quality
metric and it has reasonably high correlation with human evaluations. Due to
possiblebiasesthatmaybe“unfair”tosometechnologies[16],theBLEUscores
cannotbetheprimaryevidenceofthequalityofoursystems.Therefore,wealso
rely on human evaluation.
As discussed in Sect.3.1, the test sets consists of about 2000 segments sam-
pled from three parts of the ParIce corpus: EEA, EMA, and OpenSubtitles.
Table 1showstheresultsforthethreesystemandthediﬀerenttestsets,aswell
as the combined sets.
Table 1. BLEU scores for the three systems and the diﬀerent test sets.
Model EES EMA OpenSubtitles Combined
BiLSTM 38.68 41.60 23.32 38.12
Moses 49.70 54.93 26.11 48.60
Transformer 56.31 58.37 34.71 54.71
3 https://github.com/tensorﬂow/tensor2tensor.Experimenting with Diﬀerent Machine Translation Models 101
Table 2. Fluency and adequacy scores from human evaluation.
Test set Model Fluency Adequacy
In-domain BiLSTM 2.49 2.01
Moses 3.64 3.84
Transformer 4.30 4.33
Google 3.80 4.16
Out-of-domain BiLSTM 1.85 1.30
Moses 2.54 2.32
Transformer 3.20 2.86
Google 3.40 3.80
In [22] it was shown that the Transformer is the dominant model in high-
resource settings. Our results indicate that the Transformer also performs best
in medium-resource settings. It is, however, noteworthy that the Moses systems
performs signiﬁcantly better than the BiLSTM model.
5.2 Human Evaluation
We recruited three people with translation experience for adequacy evaluation
andthreeIcelandiclinguists forﬂuencyevaluation. Werandomlychose100sen-
tences from our test set for in-domain evaluation, and 100 sentences from news
for out-of-domain evaluation. The sentence lengths varied substantially, averag-
ing 18.2 words per sentence, with a standard deviation of 13.7. Each sentence
wastranslatedbyourthreesystemsaswellasbyGoogleTranslate,forreference.
We used Keops4 for the evaluation.
The ﬂuency group was given the following instructions: Is the sentence good
ﬂuentIcelandic?Ratethesentenceonthefollowingscalefrom1to5.1–incom-
prehensible; 2 – disﬂuent Icelandic; 3 – non-native Icelandic; 4 – good Icelandic;
5 – ﬂawless Icelandic. The adequacy group was given the following instructions:
Does the output convey the same meaning as the input sentence? Rate the sen-
tence on the following scale from 1 to 5. 1 – none; 2 – little meaning; 3 – much
meaning; 4 – most meaning; 5 – all meaning.
We calculated the Intraclass Correlation Coeﬃcient (ICC) for both groups.
This resulted in ICC of 0.749, with 95% conﬁdence interval (CI) in the range
0.718–0.777 for the ﬂuency group, and ICC of 0.734 and 95% CI in the range
0.705–0.760 for the adequacy group. According to [12], this suggests that inter-
rater agreement is moderate to good for both groups.
Wecalculatedadequacyandﬂuencyonouroriginalscaleresultingintheval-
uesshowninTable2.TheresultsshowthattheTransformerisperceivedtogive
more adequate and more ﬂuent translations than our other two systems, both
forout-of-domaintranslationsandin-domain,whereitevenoutperformsGoogle
4 https://github.com/paracrawl/keops.102 H. P. J´onsson et al.
Translate, although that may of course be because our in-domain translations
are not in Google Translate’s domain. Our SMT system performs decently, not
as good as the Transformer or Google Translate, but outperforms the BiLSTM
system by far.
6 Conclusion
We have described experiments in using three diﬀerent architectures (Moses,
BiLSTM and Transformer) for translating in the en → is direction. Automatic
and human evaluation shows that the Transformer architecture performs best,
followed by Moses and BiLSTM (in that order).
In future work, we intend to experiment with larger model sizes, backtrans-
lation, and bilingual language model pre-training. Explicit handling of named
entities is also a problematic issue, as the available parallel data contains very
few Icelandic names.
Acknowledgments. This project was funded by the Language Technology Pro-
gramme for Icelandic 2019–2023. The programme, which is managed and coordinated
byAlmannar´omur,isfundedbytheIcelandicMinistryofEducation,ScienceandCul-
ture.
References
1. Bahdanau,D.,Cho,K.,Bengio,Y.:Neuralmachinetranslationbyjointlylearning
to align and translate. In: Bengio, Y., LeCun, Y. (eds.) Proceedings of the 3rd
International Conference on Learning Representations, ICLR, San Diego (2015)
2. Barkarson, S., Steingr´ımsson, S.: Compiling and ﬁltering ParIce: an English-
icelandic parallel corpus. In: Proceedings of the 22nd Nordic Conference on Com-
putational Linguistics, NODALIDA, Turku, Finland (2019)
3. Bentivogli, L., Bisazza, A., Cettolo, M., Federico, M.: Neural versus phrase-based
machine translation quality: a case study. In: Proceedings of the Conference on
Empirical Methods in Natural Language Processing, EMNLP, Austin, TX, USA
(2016)
4. Brandt, M.D., Loftsson, H., Sigurþ´orsson, H., Tyers, F.M.: Apertium-IceNLP: a
rule-basedIcelandictoEnglishmachinetranslationsystem.In:Proceedingsofthe
15th Annual Conference of the European Association for Machine Translation,
EAMT, Leuven, Belgium (2011)
5. Castilho, S., Moorkens, J., Gaspari, F., Calixto, I., Tinsley, J., Way, A.: Is neural
machinetranslationthenewstateoftheart?PragueBull.Math.Linguist.108(1),
109–120 (2017)
6. Defauw, A., Vanallemeersch, T., Van Winckel, K., Szoc, S., Van den Bogaert, J.:
Being generous with sub-words towards small NMT children. In: Proceedings of
the12thLanguageResourcesandEvaluationConference,LREC,Marseille,France
(2020)
7. Heaﬁeld, K., Pouzyrevsky, I., Clark, J.H., Koehn, P.: Scalable modiﬁed Kneser-
Ney language model estimation. In: Proceeedings of 51st Annual Meeting of the
Association for Computational Linguistics, ACL, Soﬁa, Bulgaria (2013)Experimenting with Diﬀerent Machine Translation Models 103
8. Jassem, K., Dwojak, T.: Statistical versus neural machine translation - a case
studyforamediumsizedomain-speciﬁcbilingualcorpus.PoznanStud.Contemp.
Linguist. 55(2), 491–515 (2019)
9. Johnson,M.,Firat,O.,Aharoni,R.:Massivelymultilingualneuralmachinetrans-
lation. In: Proceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), NAACL, Minneapolis, MN, USA (2019)
10. Koehn, P.: Statistical Machine Translation. Cambridge University Press, Cam-
bridge (2009)
11. Koehn,P.,Knowles,R.:Sixchallengesforneuralmachinetranslation.In:Proceed-
ings of the First Workshop on Neural Machine Translation, Vancouver, Canada
(2017)
12. Koo, T., Li, M.: A guideline of selecting and reporting intraclass correlation coef-
ﬁcients for reliability research. J. Chiropractic Med. 15, 155–163 (2016)
13. Kudo,T., Richardson,J.: SentencePiece: asimple andlanguage independent sub-
word tokenizer and detokenizer for Neural Text Processing. In: Proceedings of
the Conference on Empirical Methods in Natural Language Processing: System
Demonstrations, EMNLP, Brussels, Belgium (2018)
14. Luong,T.,Pham,H.,Manning,C.D.:Eﬀectiveapproachestoattention-basedneu-
ral machine translation. In: Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP, Lisbon, Portugal (2015)
15. Pinnis,M.:Tilde’sparallelcorpusﬁlteringmethodsforWMT2018.In:Proceedings
of the Third Conference on Machine Translation: Shared Task Papers, Brussels,
Belgium (2018)
16. Reiter,E.:AstructuredreviewofthevalidityofBLEU.Comput.Linguist.44(3),
393–401 (2018)
17. Rozis,R.,Skadin¸ˇs,R.:TildeMODEL-multilingualopendataforEUlanguages.In:
Proceedingsofthe21stNordicConferenceonComputationalLinguistics,NODAL-
IDA, Gothenburg, Sweden (2017)
18. Steingr´ımsson, S., Helgad´ottir, S., R¨ognvaldsson, E., Barkarson, S., Gunason, J.:
Risam´alheild:averylargeicelandictextcorpus.In:Proceedingsofthe11thInter-
national Conference on Language Resources and Evaluation, LREC, Miyazaki,
Japan, May 2018
19. Sutskever,I.,Vinyals,O.,Le,Q.V.:Sequencetosequencelearningwithneuralnet-
works.In:Proceedingsofthe27thInternationalConferenceonNeuralInformation
Processing Systems - Volume 2, NIPS, Montreal, Canada (2014)
20. Tiedemann,J.:Paralleldata,toolsandinterfacesinOPUS.In:Proceedingsofthe
8thInternationalConferenceonLanguageResourcesandEvaluation,LREC2012,
Istanbul, Turkey (2012)
21. Varga,D.,N´emeth,L.,Hal´acsy,P.,Kornai,A.,ViktorTr´on,V.N.:Parallelcorpora
for medium density languages. In: Proceedings of Recent Advances in Natural
Language Processing, RANLP, Borovets, Bulgaria (2005)
22. Vaswani,A.,etal.:Attentionisallyouneed.In:Guyon,I.,etal.(eds.)Advancesin
NeuralInformationProcessingSystems,vol.30,pp.5998–6008.CurranAssociates,
Inc. (2017)FinEst BERT and CroSloEngual BERT
Less Is More in Multilingual Models
Matej Ulˇcar(B) and Marko Robnik-Sˇikonja
Faculty of Computer and Information Science, University of Ljubljana,
Veˇcna pot 113, Ljubljana, Slovenia
{matej.ulcar,marko.robnik}@fri.uni-lj.si
Abstract. Large pretrained masked language models have become
state-of-the-artsolutionsformanyNLPproblems.Theresearchhasbeen
mostly focused on English language, though. While massively multilin-
gual models exist, studies have shown that monolingual models pro-
duce much better results. We train two trilingual BERT-like models,
one for Finnish, Estonian, and English, the other for Croatian, Slove-
nian,andEnglish.Weevaluatetheirperformanceonseveraldownstream
tasks, NER, POS-tagging, and dependency parsing, using the multilin-
gual BERT and XLM-R as baselines. The newly created FinEst BERT
andCroSloEngualBERTimprovetheresultsonalltasksinmostmono-
lingual and cross-lingual situations.
· ·
Keywords: Contextual embeddings BERT model Less-resourced
·
languages NLP
1 Introduction
In natural language processing (NLP), a lot of research focuses on numeric
word representations. Static pretrained word embeddings like word2vec [12] are
recently replaced by dynamic, contextual embeddings, such as ELMo [14] and
BERT [4]. These generate a word vector based on the context the word appears
in, mostly using the sentence as the context.
Large pretrained masked language models like BERT [4] and its derivatives
achievestate-of-the-artperformancewhenﬁne-tunedforspeciﬁcNLPtasks.The
research into these models has been mostly limited to English and a few other
well-resourcedlanguages,suchasChineseMandarin,French,German,andSpan-
ish. However, two massively multilingual masked language models have been
released: a multilingual BERT (mBERT) [4], trained on 104 languages, and
newerevenlargerXLM-RoBERTa(XLM-R)[3],trainedon100languages.While
both, mBERT and XLM-R, achieve good results, it has been shown that mono-
lingualmodelssigniﬁcantlyoutperformmultilingualmodels[11,20].Arkhipovet
al. (2019) [2] trained a four language (Russian, Bulgarian, Polish, Czech) BERT
modelbybootstrappingmBERT.TheyreportedimprovementsovermBERTon
named entity recognition task.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.104–111,2020.
https://doi.org/10.1007/978-3-030-58323-1_11FinEst BERT and CroSloEngual BERT: Less Is More in Multilingual Models 105
In our work, we reduced the number of languages in multilingual models to
three, two similar less-resourced languages from the same language family, and
English. The main reasons for this choice are to better represent each language,
and keep sensible sub-word vocabulary, as shown by Virtanen et al. (2019) [20].
We decided against production of monolingual models, because we are inter-
ested in using the models in multilingual sense and for cross-lingual knowledge
transfer. By including English in each of the two models, we expect to better
transfer existing prediction models from English to involved less-resourced lan-
guages. Additional reason against purely monolingual models for less-resourced
languages is the size of training corpora, i.e. BERT-like models use transformer
architecture which is known to be data hungry.
We thus trained two multilingual BERT models: FinEst BERT was trained
on Finnish, Estonian, and English, while CroSloEngual BERT was trained on
Croatian, Slovenian, and English. In the paper, we present the creation and
evaluationofthesemodels,whichrequiredconsiderablecomputationalresources,
unavailable to most NLP researchers. We make the models which are valuable
resources for the involved less-resourced languages publicly available1.
2 Training Data and Preprocessing
BERT models require large quantities of monolingual data. In Sect. 2.1 we ﬁrst
describe the corpora used, followed by a short description of their preprocessing
in Sect. 2.2.
2.1 Datasets
To obtain high-quality models, we used large monolingual corpora for each lan-
guage, some of them unavailable to the general public. High-quality English
languagemodelsalreadyexistandEnglishisnotthemainfocusofthisresearch,
wethereforedidnotuseallavailableEnglishcorporainordertopreventEnglish
fromoverwhelmingtheotherlanguagesinourmodels.Somecorporaareavailable
online under permissive licences, others are available only for research purposes
or have limited availability. The corpora used in training are a mix of news arti-
cles and general web crawl, which we preprocessed and deduplicated. Details
about the training set sizes are presented in Table 1, while their description can
be found in works on the involved less-resourced languages, e,g., [18].
2.2 Preprocessing
Before using the corpora, we deduplicated them for each language separately,
using the Onion (ONe Instance ONly) tool2. We applied the tool on sentence
1 CroSloEngual BERT: http://hdl.handle.net/11356/1317
FinEst BERT: http://urn.ﬁ/urn:nbn:ﬁ:lb-2020061201.
2 http://corpus.tools/wiki/Onion.106 M. Ulˇcar and M. Robnik-Sˇikonja
Table1.Thetrainingcorporasizesinnum- Table2.Thesizesofcorporasub-
ber of tokens and the ratios for each lan- sets in millions of tokens used to
guage. create wordpiece vocabularies.
Model CroSloEngual FinEst Language FinEst CroSloEngual
Croatian 31% 0% Croatian / 27
Slovenian 23% 0% Slovenian / 28
English 47% 63% English 157 23
Estonian 0% 13% Estonian 75 /
Finnish 0% 25% Finnish 97 /
Tokens 5.9·109 3.7·109
level for those corpora that did have sentences shuﬄed, and on paragraph level
for the rest. As parameters, we used 9-grams with duplicate content threshold
of 0.9.
BERT models are trained on subword (wordpiece) tokens. We created a
wordpiece vocabulary using bert-vocab-builder tool3, which is built upon ten-
sor2tensor library [19]. We did not process the whole corpora in creating the
wordpiece vocabulary, but only a smaller subset. To balance the language rep-
resentation in vocabulary, we used samples from each language. The sizes of
corpora subsets are shown in Table 2. The created wordpiece vocabularies con-
tain 74,986 tokens for FinEst and 49,601 tokens for CroSloEngual model.
3 Architecture and Training
We trained two BERT multilingual models. FinEst BERT was trained on
Finnish, Estonian, and English corpora, with altogether 3.7 billion tokens.
CroSloEngual BERT was trained on Croatian, Slovenian, and English corpora
with together 5.9 billion tokens.
Both models use bert-base architecture [4], which is a 12-layer bidirectional
transformerencoderwiththehiddenlayersizeof768andaltogether110million
parameters. We used the whole word masking for the masked language model
trainingtask.Bothmodelsarecased,i.e.thecaseinformationwaspreserved.We
followed the hyper-parameters settings of Devlin et al. (2018) [4], except for the
batchsizeandtotalnumberofsteps.Wetrainedthemodelsforapproximately40
epochswithmaximumsequencelengthof128tokens,followedbyapproximately
4 epochs with maximum sequence length of 512 tokens. The exact number of
steps was calculated using the expression s = Ntok·E, where s is the number
b·λ
of steps the models were trained for, Ntok is the number of tokens in the train
corpora,E isthedesirednumberofepochs(inourcase40and4),bisthebatch
size, and λ is the maximum sequence length.
We trained FinEst BERT on a single Google Cloud TPU v3 for a total of
1.24 million steps where the ﬁrst 1.13 million steps used the batch size of 1024
3 https://github.com/kwonmha/bert-vocab-builder.FinEst BERT and CroSloEngual BERT: Less Is More in Multilingual Models 107
and sequence length 128, and the last 113 thousand steps used the batch size
256 and sequence length 512. Similarly, CroSloEngual BERT was trained on a
single Google Cloud TPU v2 for a total of 3.96 million steps, where the ﬁrst 3.6
million steps used the batch size of 512 and sequence length 128, and the last
360 thousand steps were trained with the batch size 128 and sequence length
512. Training took approximately 2 weeks for FinEst BERT and approximately
3 weeks for CroSloEngual BERT.
4 Evaluation
Weevaluated the two new BERT models on sensible languages and three down-
stream evaluation tasks available for the four involved less-resourced languages:
namedentityrecognition(NER),part-of-speechtagging(POS),anddependency
parsing (DP). We compared both models with BERT-base-multilingual-cased
model (mBERT). On the NER task we compared also XLM-RoBERTa (XLM-
R) and Finnish BERT (FinBERT).
4.1 Named Entity Recognition
NER is a sequence labeling task, which tries to correctly identify and classify
each token from an unstructured text into one of the predeﬁned named entity
(NE)classes,orasnotNE.ThepubliclyavailableNERdatasetsfortheinvolved
languages that we used have only three NE classes in common. To allow a more
direct comparison between languages, we reduced them to the four labels in
common:person,location,organization,andother.Alltokens,whicharenotNE
or belong to any other NE class were labeled as other.
For Croatian and Slovenian, we used NER data from hr500k [10] and
ssj500k[8],respectively.NotallsentencesinSlovenianssj500kareannotated,so
we excluded those that are not annotated. The English dataset comes from the
CoNLL2013sharedtask[17].ForFinnishweusedtheFinnishNewsCorpusfor
NER [15], and as the Estonian dataset we used the Nimeu¨ksuste korpus [9].
TheimplementationusestheHuggingface’sTransformerlibraryv2.8,andour
codeisbasedonitsNERexample4.Weﬁne-tunedeachofourBERTmodelswith
an added token classiﬁcation head for 3 epochs on the NER data. We compared
the results with mBERT, XLM-R and FinBERT models, which we ﬁne-tuned
withexactlythesameparametersonthesamedata.Weusedmaximumsequence
length of 512 and batch size of 6 for all models and languages.
We evaluated the models in a monolingual setting (training and testing on
the same language), and cross-lingual setting (training on one language, testing
on another). We present the results as macro average F scores of the three NE
1
classes, excluding other label. Results are shown in Table 3.
In monolingual setting, the diﬀerences in performance of tested models on
English data is negligible. In other languages, our models outperform both the
4 https://github.com/huggingface/transformers/tree/v2.8.0/examples/ner.108 M. Ulˇcar and M. Robnik-Sˇikonja
Table 3. TheresultsofNERevaluationtask.ThescoresaremacroaverageF scores
1
ofthethreeNEclasses.NERmodelswereﬁne-tunedfrommBERT(mB),CroSloEngual
BERT (CSE), FinEst BERT (FE), XLM-RoBERTa (XR), and FinBERT (FB).
Train Test mB CSE XR Train Test mB FE XR FB
Croatian Croatian 0.790 0.884 0.817 Finnish Finnish 0.933 0.957 0.930 0.954
Slovenian Slovenian 0.897 0.920 0.914 Estonian Estonian 0.898 0.927 0.908 0.876
English English 0.939 0.944 0.937 English English 0.939 0.945 0.937 0.922
Croatian English 0.807 0.868 0.773 Finnish English 0.688 0.812 0.722 0.573
English Croatian 0.602 0.799 0.641 English Finnish 0.764 0.900 0.823 0.817
Slovenian English 0.745 0.845 0.747 Estonian English 0.774 0.816 0.755 0.641
English Slovenian 0.708 0.833 0.739 English Estonian 0.783 0.832 0.794 0.523
Croatian Slovenian 0.810 0.891 0.855 Finnish Estonian 0.798 0.880 0.825 0.529
Slovenian Croatian 0.765 0.849 0.786 Estonian Finnish 0.819 0.914 0.869 0.823
mBERTandXLM-R,thediﬀerenceisespeciallylargeinCroatian.FinEstBERT
performsonparwithFinBERTonFinnish.Incross-lingualsetting,bothFinEst
and CroSloEngual BERT show a signiﬁcant improvement over both mBERT
andXLM-R.ThisleadsustobelievethatmultilingualBERTmodelswithfewer
languages are more suitable for cross-lingual knowledge transfer.
4.2 Part-of-Speech Tagging and Dependency Parsing
Next, we evaluated the created BERT models on two more syntactic classiﬁ-
cation tasks: POS-tagging and DP. In the POS-tagging task, we predict the
grammaticalcategoryofeachtoken(verb,adjective,punctuation,adverb,noun,
etc). DP models predict the tree structure, representing the syntactic relations
between words in a given sentence.
We trained classiﬁers on universal dependencies (UD) treebank datasets,
using universal part-of-speech (UPOS) tag set. For Croatian, we used the
dataset of Agic and Ljubesic (2015) [1]; for English, we used A Gold Stan-
dard Dependency Corpus [16], and for Estonian we used Estonian Dependency
Treebank [13], converted to UD. The Finnish treebank used is based on the
Turku Dependency Treebank [6]. Slovenian treebank [5] is based on the ssj500k
corpus [8].
We used Udify tool [7] to train both POS tagger and DP classiﬁers at the
sametime.Weﬁne-tunedeachBERTmodelfor80epochsonthetreebankdata,
keeping the tool parameters at default values, except for “warmup steps” and
“start step” values, which we changed to the number of training batches in one
epoch.
We present the results of POS tagging as UPOS accuracy in Table 4. In
themonolingual setting, thediﬀerencesinperformancebetween diﬀerentBERT
modelsaresmallforthistask.FinEstandCroSloEngualBERTsperformslightly
better than mBERT on all languages, except Croatian, where mBERT and
CroSloEngual BERT are equal. On Finnish, FinBERT (acc = 0.984) slightly
outperforms FinEst BERT (acc = 0.981). The diﬀerences are more pronounced
in cross-lingual setting. When training on Slovenian, Finnish, or Estonian andFinEst BERT and CroSloEngual BERT: Less Is More in Multilingual Models 109
Table 4. The performance on the UD POS-tagging task, using UPOS accuracy for
CroSloEngual BERT (CSE), FinEst BERT, and mBERT.
Train Test mBERT CSE Train Test mBERT FinEst
Croatian Croatian 0.983 0.983 English English 0.969 0.970
English English 0.969 0.972 Estonian Estonian 0.972 0.978
Slovenian Slovenian 0.987 0.991 Finnish Finnish 0.970 0.981
English Croatian 0.876 0.869 English Estonian 0.852 0.878
English Slovenian 0.857 0.859 English Finnish 0.847 0.872
Croatian English 0.750 0.756 Estonian English 0.688 0.808
Croatian Slovenian 0.917 0.934 Estonian Finnish 0.872 0.913
Slovenian English 0.686 0.723 Finnish English 0.535 0.701
Slovenian Croatian 0.920 0.935 Finnish Estonian 0.888 0.919
testing on English, CroSloEngual and FinEst BERT signiﬁcantly outperform
mBERT. The exception is training on English and testing on Croatian, where
mBERT outperforms CroSloEngual BERT.
We present the results of DP task with two metrics, the unlabeled attache-
ment score (UAS) and labeled attachment score (LAS). In the monolingual set-
ting, CroSloEngual BERT shows improvement over mBERT on all three lan-
guages(Table5)withthehighestimprovementonSlovenianandonlyamarginal
improvement on English. FinEst BERT outperforms mBERT on Estonian and
Finnish, with the biggest margin being on the Finnish data, while the two mod-
els perform equally on English data. FinBERT again outperforms FinEst on
Finnish, scoring UAS = 0.946 and LAS = 0.930.
In the cross-lingual setting, the results are similar to those seen on
the POS tagging task. Major improvements of FinEst and CroSloEngual
BERT over mBERT are observed in English-Estonian, English-Finnish and
English-Slovenianpairs,minorimprovementsinEstonian-FinnishandCroatian-
Slovenian pairs, while on English-Croatian pair mBERT outperformed CroSlo-
Engual BERT.
Table 5.TheresultsontheDPtaskpresentedwithUASandLASscoresforCroSlo-
Engual BERT, FinEst BERT, and mBERT.
mBERT CroSloEngual mBERT FinEst
Train Test UAS LAS UAS LAS Train Test UAS LAS UAS LAS
Croatian Croatian 0.930 0.891 0.940 0.903 English English 0.917 0.894 0.9180.895
English English 0.917 0.894 0.922 0.899 EstonianEstonian0.880 0.848 0.9090.882
SlovenianSlovenian0.938 0.922 0.957 0.947 Finnish Finnish 0.898 0.867 0.9330.915
English Croatian 0.824 0.724 0.822 0.725 English Estonian0.697 0.531 0.7680.591
English Slovenian0.830 0.719 0.848 0.736 English Finnish 0.706 0.561 0.7810.624
Croatian English 0.759 0.627 0.782 0.657 EstonianEnglish 0.633 0.492 0.7260.567
Croatian Slovenian0.880 0.802 0.912 0.840 EstonianFinnish 0.784 0.695 0.8640.801
SlovenianEnglish 0.741 0.578 0.794 0.648 Finnish English 0.543 0.433 0.6840.558
SlovenianCroatian 0.861 0.773 0.891 0.810 Finnish Estonian0.782 0.691 0.8520.778110 M. Ulˇcar and M. Robnik-Sˇikonja
5 Conclusion
We built two large pretrained trilingual BERT-based masked language models,
Croatian-Slovenian-English and Finnish-Estonian-English. We showed that the
new CroSloEngual and FinEst BERTs perform substantially better than mas-
sively multilingual mBERT on the NER task in both monolingual and cross-
lingual setting. The results on POS tagging and DP tasks show considerable
improvement of the proposed models for several monolingual and cross-lingual
pairs, while they are never worse than mBERT.
In future, we plan to investigate diﬀerent combinations and proportions of
less-resourced languages in creation of pretrained BERT-like models, and use
the newly trained BERT models on the problems of news media industry.
Acknowledgments. The work was partially supported by the Slovenian Research
Agency (ARRS) core research programme P6-0411. This paper is supported by Euro-
peanUnion’sHorizon2020researchandinnovationprogrammeundergrantagreement
No825153,projectEMBEDDIA(Cross-LingualEmbeddingsforLess-RepresentedLan-
guages in European News Media). Research was supported with Cloud TPUs from
Google’s TensorFlow Research Cloud (TFRC).
References
1. Agi´c,Zˇ.,Ljubeˇsi´c,N.:UniversaldependenciesforCroatian(thatworkforSerbian,
too).In:The5thWorkshoponBalto-SlavicNaturalLanguageProcessing,pp.1–8
(2015)
2. Arkhipov,M.,Troﬁmova,M.,Kuratov,Y.,Sorokin,A.:Tuningmultilingualtrans-
formers for language-speciﬁc named entity recognition. In: Proceedings of the 7th
Workshop on Balto-Slavic Natural Language Processing, pp. 89–93. Association
for Computational Linguistics, Florence, August 2019. https://doi.org/10.18653/
v1/W19-3712
3. Conneau, A., et al.: Unsupervised cross-lingual representation learning at scale.
arXiv preprint arXiv:1911.02116 (2019)
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)
5. Dobrovoljc, K., Erjavec, T., Krek, S.: The universal dependencies treebank for
Slovenian. In: Proceeding of the 6th Workshop on Balto-Slavic Natural Language
Processing (BSNLP 2017) (2017)
6. Haverinen,K.,etal.:BuildingtheessentialresourcesforFinnish:theTurkudepen-
dency treebank. LREC 48, 493–531 (2013)
7. Kondratyuk,D.,Straka,M.:75languages,1model:parsinguniversaldependencies
universally. In: Proceedings of the 2019 EMNLP-IJCNLP, pp. 2779–2795 (2019)
8. Krek, S., et al.: Training corpus ssj500k 2.2 (2019). Slovenian language resource
repository CLARIN.SI
9. Laur, S.: Nimeu¨ksuste korpus. Center of Estonian Language Resources (2013)
10. Ljubeˇsi´c, N., Klubiˇcka, F., Agi´c, Zˇ., Jazbec, I.P.: New inﬂectional lexicons and
training corpora for improved morphosyntactic annotation of Croatian and Ser-
bian. In: Proceedings of the LREC 2016 (2016)FinEst BERT and CroSloEngual BERT: Less Is More in Multilingual Models 111
11. Martin, L., et al.: CamemBERT: a tasty French language model. arXiv preprint
arXiv:1911.03894 (2019)
12. Mikolov, T., Le, Q.V., Sutskever, I.: Exploiting similarities among languages for
machine translation. arXiv preprint 1309.4168 (2013)
13. Muischnek, K., Mu¨u¨risep, K., Puolakainen, T.: Estonian dependency treebank:
from constraint grammar tagset to universal dependencies. In: Proceedings of
LREC 2016 (2016)
14. Peters, M.E., et al.: Deep contextualized word representations. arXiv preprint
arXiv:1802.05365 (2018)
15. Ruokolainen,T.,Kauppinen,P.,Silfverberg,M.,Lind´en,K.:AFinnishnewscorpus
for named entity recognition. Lang. Res. Eval. 54(1), 247–272 (2020)
16. Silveira,N.,etal.:AgoldstandarddependencycorpusforEnglish.In:Proceedings
of LREC-2014 (2014)
17. Tjong Kim Sang, E.F., De Meulder, F.: Introduction to the CoNLL-2003 shared
task:language-independentnamedentityrecognition.In:Daelemans,W.,Osborne,
M. (eds.) Proceedings of CoNLL-2003, Edmonton, Canada, pp. 142–147 (2003)
18. Ulˇcar, M., Robnik-Sˇikonja, M.: High quality ELMo embeddings for seven less-
resourcedlanguages.In:Proceedingsofthe12thLanguageResourcesandEvalua-
tion Conference, pp.4731–4738. EuropeanLanguage ResourcesAssociation, Mar-
seille, May 2020
19. Vaswani, A., et al.: Tensor2tensor for neural machine translation. In: Proceedings
of the AMT, pp. 193–199 (2018)
20. Virtanen,A.,etal.:Multilingualisnotenough:BERTforFinnish.arXivpreprint
arXiv:1912.07076 (2019)Employing Sentence Context in Czech
Answer Selection
Marek Medvedˇ(B), Radoslav Sabol, and Aleˇs Hor´ak
Natural Language Processing Centre, Faculty of Informatics, Masaryk University,
Botanick´a 68a, 602 00 Brno, Czech Republic
{xmedved1,xsabol,hales}@fi.muni.cz
Abstract. Question answering (QA) of non-mainstream languages
requires speciﬁc adaptations of the current methods tested primarily
with very large English resources. In this paper, we present the results
of improving the QA answer selection task by extending the input can-
didate sentence with selected information from preceding sentence con-
text. The described model represents the best published answer selec-
tion model for the Czech language as an example of a morphologically
richlanguage.Thetextcontainsthoroughevaluationofthenewmethod
includingmodelhyperparametercombinationsanddetailederrordiscus-
sion.Thewinningmodelshaveimprovedthepreviousbestresultsby4%
reaching the mean average precision of 82.91%.
· · ·
Keywords: Question answering Answer selection Czech Answer
·
context Morphologically rich languages
1 Introduction
The state-of-the-art results in question answering (QA) methods have already
surpassed the estimated human performance1 when trained on very large word-
based datasets of more than 100,000 questions such as SQuAD [12], RACE [4]
or GLUE [18]. This allows for very wide benchmarking and comparison of new
deeplearningtechniquesbutstraightforwardapplicationinnon-mainstreamlan-
guages is diﬃcult.
Nowadays, the answer selection subtask, i.e. identiﬁcation of the one sen-
tence containing the exact answer to a given question, has advanced from the
early works based on measuring sentence similarity according to string overlap-
ping [17] to complex deep neural network architectures ﬁrst introduced in [19]
and later improved and reﬁned [13,16,20]. The latest approaches prevalently
lean on employing advanced language models such as BERT/ALBERT [3,5] or
GPT-2 [10].
In this paper, we show the details of a method adaptation and a new tech-
nique evaluated with Czech as a representative of a small but lexically and
1 ThehumanperformancewiththeSQuADdatabaseis86.8%exactmatch[11]while
the current best results reach more than 90.7% [20].
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.112–121,2020.
https://doi.org/10.1007/978-3-030-58323-1_12Employing Sentence Context in Czech Answer Selection 113
Question Document Answer Answer
Question Answer
processor selection selection extraction
SQAD
Fig.1. AQA pipeline schema
morphologically rich language. All methods are evaluated with a published QA
benchmark dataset SQAD 3.0 [9] which contains more than 13,000 question-
answer pairs with detailed metadata related to the morphology and question
answer typology. The current improvements of the QA answer selection task lie
in extending the input candidate sentence with selected information from pre-
ceding sentence context. The following text contains thorough evaluation of all
model hyperparameter combinations and detailed error discussion. The winning
models have improved the previous best results by 4% reaching mean average
precision of 82.91%.
2 AQA Modules for the Czech Language
The Automatic Question Answering system (AQA) is designed to answer ques-
tions in Slavonic languages with the Czech language selected as their represen-
tative for testing and developing purposes. The whole AQA system consists of
multiple modules organised in one pipeline. The pipeline structure is as follows
(see Fig.1):
1. Theﬁrstmodule,triggeredafterthequestioninput,isthequestion type anal-
ysis module.Thispartofthesystemprovidesinformationaboutwhattypeof
questionthesystemreceivesandwhattypeofansweritshouldlookfor.This
information is exploited later in the pipeline where the system searches for
the ﬁnal answer. The core of this module uses pre-trained bi-LSTM network
that was trained with the training subset of the SQAD database of manually
annotated question and answer types. For in-depth information see [8].
2. Thesecondmoduleinthepipelineisthedocumentselection module.Itsmain
purpose it to pick up a document (or top k documents) from the underlying
documentcollectiontobesearchedforsentenceswithexpectedanswers.This
module is based on weighted TF-IDF scoring with syntax-based similarity
measuresbetweenthequestionandthedocumentcontent.Theresultisalist
of documents ranked according to document relevance to the given question.
For detailed design and evaluation see [9].
3. After a part of the document collection is selected for further analysis, the
answer selection module is employed to select a candidate answer sentence.
The answer selection module is based on attentive bi-directional gated recur-
rent unit architecture that is trained on question-answer pairs and yields a
list of candidate sentences ranked by their relevance to the input question.114 M. Medvedˇ et al.
Fig.2. New answer selection architecture with answer context.
Fordetaileddescriptionsee[9].Inthefollowingsection,wepresentnewdevel-
opments of this module with concentration on adding the answer context to
the network input.
4. Fromthebestscoredsentenceoftheanswerselectionmodule,thelastanswer
extraction moduleselectsthesmallestpartcontaining enoughinformation to
answer the input question. The extraction is based on rules based on the
question-answer type information for identifying the boundaries of the exact
answer. Detailed description of this module can be found in [6].
3 Neural Answer Selection Architecture with Context
The AQA answer selection module is based on a speciﬁc Siamese neural net-
work [2,15] which exploits a bi-GRU attentive recurrent network to learn the
question-answer similarities of correct answers and dissimilarities of related
incorrect answers [9]. In this paper, we present the latest results of both newEmploying Sentence Context in Czech Answer Selection 115
networkhyperparametersetupwithimprovedanswerselectionandanextension
of the technique with answer contexts as another network input.
The extended network architecture requires three sequences as an input.
Besides the question and a candidate answer, the network takes the answer
context as its third input sequence. Currently, the context is presented in the
form of selected noun phrases from the preceding two sentences in the input
document separated by a new "[SEP]" token.2
The new architecture is presented in Fig.2. A shared Bidirectional Gated
Recurrent Unit (bi-GRU) layer is applied to all three input sequences, each
sequenceusingitsindividualhiddenstate.Subsequently,thebi-GRUrepresenta-
tionofcontextisconcatenatedwiththecandidateanswerrepresentationforming
a single sequence for the question-answer attention matrix.
In the following step, a two-way attention mechanism is applied to the
question (Q) and answer-context (K) representations, producing their respec-
tive attention vectors. The question vector contains importance scores of each
word with regard to the answer, while the answer attention vector consists
of importance scores for all words of the answer and the context, potentially
improving the ﬁnal ranking in case the target entity was mentioned in the pre-
vioussentence(s).Usingthedotproductwiththecorrespondingbi-GRUoutput
makestheﬁnalrepresentationstobecomparedusingthevectorcosinesimilarity
measure.
4 Experiments and Results
The SQADv3 [14] dataset consists of almost 13,500 richly annotated question-
answer pairs with full texts of 6,500 Wikipedia articles used as the underlying
Table 1.Theanswerselectionresultsforvarioushyperparametersettingswithacom-
parison of context and non-context model.
Embedding size Hidden size Optimizer Learning rate Non-context Context
MAP MRR MAP MRR
500 400 Adagrad 0.005 81.78 88.07 82.91 88.75
500 300 Adagrad 0.005 81.65 87.98 82.81 88.86
500 200 Adagrad 0.005 81.33 87.66 82.38 88.51
300 300 Adagrad 0.005 80.99 87.55 82.33 88.53
300 200 Adagrad 0.005 80.8 87.37 82.26 88.38
100 300 Adagrad 0.005 78.97 86.02 80.42 87.21
100 200 Adagrad 0.005 78.54 85.67 80.24 87.04
100 300 SGD 0.6 78.87 85.94 79.39 86.35
100 200 SGD 0.9 79.13 86.13 79.35 86.23
2 Similar approach is frequently used in sequence to sequence machine translation
neural architectures.116 M. Medvedˇ et al.
Fig.3. Hyperparameter sensitivity comparison for both architectures via the hidden
layer size.
documentcollection.Thedatasetispartitionedintotraining,validationandtest
set in the ratio of 60:10:30. The validation set is used as an evaluation of the
current model state after each training epoch. The best validated epoch is then
chosen for the ﬁnal evaluation using the test set. The content of the partitions
is the same as in previous experiments with answer contexts added. Current
experiments were run for both the models with and without context to produce
comparable results.
Foreachquestion,thetrainingalgorithmrandomlysamples20negativecan-
didate answers along with the positive run. The input vectors use pre-trained
FastText [1] word embeddings which are prepared in 100- and 300-dimensional
vectors. A dropout layer is applied to the input data to support generalization.
In previous experiments, the dropout probability of 0.2 was the most promi-
nent while other values were degrading the precision with every single setup [7].
Therefore, the dropout probability was kept at this value for all the following
runs.
Table 2. The answer selection accuracy per question and answer types
Question Context Non-ctx Diﬀ. Answer Context Non-ctx Diﬀ.
Type CountMAP(%)MAP(%)(%) Type CountMAP(%)MAP(%)(%)
ABBR. 97 88.66 91.75 −3.09ABBR. 95 88.42 91.58 −2.16
LOC. 498 84.94 83.13 1.81 DENOT. 53 88.68 86.79 1.89
DTTIME592 84.63 83.95 0.68 LOC. 494 88.68 83.20 5.48
ADJ P. 449 83.07 82.63 0.44 DTTIME589 84.55 83.87 0.67
VERB P.678 83.63 82.74 0.89 YES NO 675 83.56 82.81 0.75
PERSON526 83.65 83.84 −0.19 ENTITY 527 82.35 79.70 2.65
ENTITY 738 81.71 79.54 2.17 OTHER 668 80.39 78.44 1.95
NUM. 293 79.52 78.84 0.68 NUM. 298 79.53 79.19 0.34
CLAUSE 139 71.94 68.35 3.59 ORG. 83 74.70 78.31 −3.61Employing Sentence Context in Czech Answer Selection 117
Thesizeofthebi-GRUlayer(hiddensize)alsocorrespondstothedimension-
ality of the attention layer weights thus aﬀecting multiple internal layers of the
network. The following experiments primarily focus on hidden size values from
100 to 500. Models were trained using multiple optimizers, ranging from the
Stochastic Gradient Descent (SGD) with a learning rate scheduler3 to Adagrad,
Adadelta and Adam.
Overall 423 new models were produced while optimizing the parameters for
thenon-contextmodel.ThebestperformingmodelshaveachievedaMeanAver-
age Precision (MAP) of 81.78% using the 300-dimensional embeddings which is
a2.91%increase whencomparedtotheolderresult[9]withtheMAPof78.87%.
The reason behind this increase is a ﬁne-grained optimization of model param-
eters along with larger embedding size. The best model setups are summarized
in Table1.
Inexperimentsoptimizingthenewcontextarchitecture,125modelswerepro-
duced. The number of parameter combinations was reduced to the ones which
had achieved reasonable accuracy with the non-context architecture. The best
setuphasreachedtheMAPof 82.91%,outperformingthebestconﬁgurationof
thenon-contextarchitectureby1.13%(andthepreviousbestresultby4.04%)
using the same parameters. As can be seen in Fig.3, the hyperparameter sensi-
tivityremainsmoreorlessthesamebutwithanincreaseintheoverallprecision.
The parameter combinations aﬀect also the running times of the training
and testing process. The non-context model used an average running time of
242min for 100-dimensional word embeddings.4 For 300-dimensional embed-
dings, the running times increased to 497min on average, raising to 1,100 min
Table 3. A comparison of Precision at k for the best performing context and non-
context models.
k Context Non-context Sum
Num P@k Sum Num P@k Sum Diﬀ.
1 3327 82.91 82.91 3287 81.91 81.91 1.00
2 319 7.95 90.85 351 8.75 90.66 0.19
3 105 2.62 93.47 117 2.92 93.66 −0.19
4 65 1.62 95.09 65 1.62 95.19 −0.10
5 39 0.97 96.06 39 0.97 96.16 −0.10
6 29 0.72 96.79 21 0.52 96.69 0.10
7 21 0.52 97.31 16 0.4 97.08 0.23
8 12 0.3 97.61 16 0.4 97.48 0.13
9 17 0.42 98.03 16 0.4 97.88 0.15
≥10 79 1.97 100.00 85 2.11 100.00
3 For each epoch, the new learning rate is computed by dividing the initial learning
rate by the current epoch.
4 Including all 25 epochs with their respective validation and the ﬁnal evaluation.118 M. Medvedˇ et al.
with 500-dimensional input. The context models required more complex data
preparation with larger input sequences. This is reﬂected in the increase of run-
ning times, where 100-dimensional embeddings raised the time to 550min on
average, 990min for 300-dimensional word embeddings, and 1,800min for 500-
dimensional input.
4.1 Discussion and Error Analysis
The context model proved to be superior to the standard (non-context) model
inalmostalltypesofquestionsandexactanswers,seeTable2fordetails.Major
improvements can be seen with clause question types (more than 3%) and with
answers of type location (more than 5%). Most other categories are improved
except the abbreviations category which shows a decrease of 2–3% and with
answers that provide a name of an organization (decrease of more than 3%).
If we analyze the system improvements regarding the position of the correct
answerdenotedasthePrecisionatk(orP@k)wecanseethatthecontextmodel
speciﬁcally helps to improve the position of the top 3 answers. Since these are
the frequent positions, such step is very important in improving the system as
a whole. See the list of P@k values for the ﬁrst 10 positions in Table3.
Record ID: 003189
Question Jak´e kapely patˇr´ı do tzv. Velk´e thrashov´eˇctyˇrky ?
(Q) (Which bands belong to the “Big Four of Thrash”?)
Non-context Thrash zpopularizovala tzv.0.81 “ Velka´0.64 thrashova´0.57 ˇctyˇrka0.62 ” :
answer with Anthrax, Megadeth, Metallica a Slayer.
attention (The subgenre was popularized by the “Big Four of Thrash”: Anthrax,
Megadeth, Metallica, and Slayer.)
Q withatt. Jak´e kapely patˇr´ı0.51 do0.53 tzv.1.0 Velk´e0.68 thrashov´e0.58 ˇctyˇrky ?
score: 0.587
position: 1st
Context Tyto1.0 dvˇe0.39 kapely, spolu s Death a Obituary, patˇr´ı mezi
answer with nejvy´znamnˇejˇs´ı0.38 skupiny na hlavn´ı deathmetalov´e sc´enˇe, ktera´
attention povstala na Floridˇe v polovinˇe 80. let.
(These two bands, along with Death and Obituary, were leaders of the
major death metal scene that emerged in Florida in the mid-1980s)
Q withatt. Jak´e0.86 kapely patˇr´ı0.53 do tzv.1.0 Velk´e0.73 thrashov´eˇctyˇrky ?
context Morbid Angel; neofaˇsistickou symboliku (neo-fascist symbolism);
phrases: z vy´jimek (of exceptions); Glen Benton; z kapely (from band); Deicide;
pˇri vystoupen´ıch (on stage)
score: 0.569
position: 1st
corr. answer Thrashzpopularizovala1.0tzv.0.37“Velka´thrashova´ˇctyˇrka”:Anthrax,
withattention Megadeth, Metallica a Slayer.
score: 0.469
position: 7th
Fig.4. An example QA pair which is better analyzed with the standard non-context
model.Employing Sentence Context in Czech Answer Selection 119
Detailed error analysis of the achieved results showed that not all question
analysesareimprovedwiththecontextnetworkmodel.Forexample,Fig.4shows
a QA example where the standard non-context model ranks the correct answer
as the ﬁrst one of possible answers whereas the context model places the cor-
rect answer at the 7th position. If we compare the normalized attention scores
between the two models, we can see that the standard model puts an empha-
sis on the words patˇr´ı , do , tzv , Velk´e , and thrashov´e
belongs to socalled Big ofTrash
inthequestionandtzv ,Velka´ ,thrashov´a ,andˇctyˇrka inthe
socalled Big ofTrash Four
answer.Ontheotherhand,thecontextmodelsigniﬁcantlyemphasizesthewords
Jak´e , patˇr´ı , tzv , and Velk´e of the question and Tyto ,
which belongs socalled Big These
dvˇe , and nejv´yznamnˇejˇs´ı in the ﬁrst (incorrect) answer and zpo-
two mostimportant
pularizovala and tzv in the correct answer at the 7th position.
popularized socalled
This information leads us to conclusion that in the case of more names in the
context, thecontextphrasesconfusedtheattention layer andthemodelfocused
on incorrect words.
On the other hand, the context model obviously helps to identify the answer
incandidatesentencesthatrefertoanentityfromthequestionviaanaphoricref-
erence(pronoun)toaprecedingsentence.However,eveninnon-anaphoriccases,
the context model can increase the attentive score of a key phrase, for example
in a question of Kdo dal dohromady koncept v´ystroje? (Who put together the
equipment concept?) the context model assigns a higher combined score to the
phrase koncept v´ystroje which allowed to improve the rank of the
equipmentscore
correct answer sentence.
5 Conclusions
Inthispaper,wehavepresentedanewmethodoftheanswerselectiontaskbased
onemployingbroaderanswercontextintheinputoftherecurrentneuralnetwork
model. The new model is consistently better than the model without context
usingthesamenetworkhyperparameters.Overall,thebestcontextmodeloﬀers
an improvement of 4% when compared to the previous best published result
(from 78.87% to 82.91%).
Since the non-context and context models are (partly) supplemental to each
other, one of the main future directions lies in testing ensemble model architec-
tures building on top of these two models.
References
1. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Trans. Assoc. Comput. Linguist. 5, 135–146 (2017)
2. Bromley, J., Guyon, I., LeCun, Y., S¨ackinger, E., Shah, R.: Signature veriﬁcation
usinga“Siamese”timedelayneuralnetwork.In:AdvancesinNeuralInformation
Processing Systems, pp. 737–744 (1994)
3. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:BERT:pre-trainingofdeepbidi-
rectionaltransformersforlanguageunderstanding.In:ProceedingsoftheNAACL
2019, Volume 1 (Long and Short Papers), pp. 4171–4186 (2019)120 M. Medvedˇ et al.
4. Lai,G.,Xie,Q.,Liu,H.,Yang,Y.,Hovy,E.:Race:large-scalereadingcomprehen-
sion dataset from examinations. arXiv preprint arXiv:1704.04683 (2017)
5. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: a
lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942 (2019)
6. Medvedˇ, M., Hor´ak, A.: AQA: automatic question answering system for Czech.
In: Sojka, P., Hor´ak, A., Kopeˇcek, I., Pala, K. (eds.) TSD 2016. LNCS (LNAI),
vol.9924,pp.270–278.Springer,Cham(2016).https://doi.org/10.1007/978-3-319-
45510-5 31
7. Medvedˇ, M., Hor´ak, A.: Sentence and word embedding employed in open
question-answering. In: Proceedings of the 10th International Conference on
AgentsandArtiﬁcialIntelligence(ICAART2018),Setu´bal,Portugal,pp.486–492.
SCITEPRESS - Science and Technology Publications (2018)
8. Medvedˇ, M., Hor´ak, A., Kuˇsnir´akov´a, D.: Question and answer classiﬁcation in
czechquestionansweringbenchmarkdataset.In:Proceedingsofthe11thInterna-
tional Conference on Agents and Artiﬁcial Intelligence, Prague, Czech Republic,
vol. 2, pp. 701–706. SCITEPRESS (2019)
9. Medvedˇ,M.,Sabol,R.,Hor´ak,A.:ImprovingRNN-basedanswerselectionformor-
phologically rich languages. In: Proceedings of the 12th International Conference
onAgentsandArtiﬁcialIntelligence(ICAART2020),Valleta,Malta,pp.644–651.
SCITEPRESS - Science and Technology Publications (2020)
10. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language
models are unsupervised multitask learners. OpenAI Blog 1(8), 9 (2019)
11. Rajpurkar,P.,Jia,R.,Liang,P.:Knowwhatyoudon’tknow:unanswerableques-
tionsforSQuAD.In:ProceedingsoftheACL2018(Volume2:ShortPapers),Mel-
bourne, Australia, pp. 784–789. Association for Computational Linguistics (2018)
12. Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: Squad: 100, 000+ questions for
machine comprehension of text. CoRR abs/1606.05250 (2016)
13. Rao, J., Liu, L., Tay, Y., Yang, W., Shi, P., Lin, J.: Bridging the gap between
relevance matching and semantic matching for short text similarity modeling. In:
Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pp. 5373–5384 (2019)
14. Sabol,R.,Medvedˇ,M.,Hor´ak,A.:CzechquestionansweringwithextendedSQAD
v3.0 benchmark dataset. In: Proceedings of Recent Advances in Slavonic Natural
Language Processing, RASLAN 2019, pp. 99–108 (2019)
15. dos Santos, C., Tan, M., Xiang, B., Zhou, B.: Attentive pooling networks. arXiv
preprint arXiv:1602.03609 (2016)
16. Shen, Y., et al.: Knowledge-aware attentive neural network for ranking question
answer pairs. In: The 41st International ACM SIGIR Conference on Research &
Development in Information Retrieval, pp. 901–904 (2018)
17. Wang, M., Smith, N.A., Mitamura, T.: What is the jeopardy model? A quasi-
synchronous grammar for QA. In: Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pp. 22–32 (2007)
18. Warstadt, A., Singh, A., Bowman, S.R.: Neural network acceptability judgments.
Trans. Assoc. Comput. Linguist. 7, 625–641 (2019)Employing Sentence Context in Czech Answer Selection 121
19. Yih, S.W., Chang, M.W., Meek, C., Pastusiak, A.: Question answering using
enhanced lexical semantic models. In: Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics. ACL - Association for Compu-
tational Linguistics, August 2013
20. Zhang, Z., Yang, J., Zhao, H.: Retrospective reader for machine reading compre-
hension. arXiv preprint arXiv:2001.09694 (2020)Grammatical Parallelism of Russian
Prepositional Localization and Temporal
Constructions
Victor Zakharov(&) and Irina Azarova
Saint-Petersburg State University, 199034Saint-Petersburg, Russia
{v.zakharov,i.azarova}@spbu.ru
Abstract. In this paper we present a part of corpus-driven semantico-
grammatical ontological description of Russian prepositional constructions.
The main problem of a prepositional ontology is itsinner controversy because
the ontological structure presupposes logical analysis of concepts, however,
prepositions are usually interpreted as non-lexical grammatical language ele-
ments.Inourunderstanding,thisisanontologyoflexico-grammaticalrelations
that are implemented in prepositional constructions. We demonstrate the onto-
logical structure for semantic rubrics of temporal and locative syntaxemes
extracted through the elaborated technique for processing corpus statistics of
prepositional constructions in modern Russian texts. Common and contrastive
traits betweenthis twotopmostsemantic domains areshown.
(cid:1) (cid:1)
Keywords: Russianprepositional constructions Preposition meaning
(cid:1) (cid:1) (cid:1)
Corpus statistics Locative constructions Temporalconstructions Semantic
rubrics
1 Introduction
Thepaperpresentsthenextstageofcorpus-drivensemantic-grammaticaldescriptionof
Russianprepositionalconstructions.Thecollectedstatisticsfromvariouscontemporary
corpora for pairs “preposition – its meaning”, their scholarly description [1, 2] and
existingschemesoflexicalandsyntacticstructuring[3]ledustotheconceptionofthe
prepositional ontology. We consider this notion as a semi-grammatical language
component linking fuzzy lexico-semantic word classes by the hierarchical set of
grammatical relations. These relations are established by a combination of the partic-
ularpreposition,asemantictypeofthelexemeattachingtheprepositionalconstruction,
and a semantic class and a grammatical form of the dependent noun.
Themainproblemofsuchanontologyisitsinnercontroversysincetheontological
structure presupposes logical analysis of concepts. Prepositions, however, are usually
interpretedas non-lexical or notfullylexical languageelements. Currently,there isan
understanding that the meaning of prepositions should be considered as a special type
of relationship inside prepositional constructions. A prepositional ontology has a sig-
niﬁcant difference from a classic one. Our understanding is, that it is an ontology of
lexico-grammatical relations which are implied in prepositional constructions. We
©SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.122–134,2020.
https://doi.org/10.1007/978-3-030-58323-1_13Grammatical Parallelism of RussianPrepositional Localization 123
believethatsuch anontology cannot bebuilt from thetopdown. Weadvocateadata-
drivencorpusapproachfromthebottomupandfocusonpatternsofusage.Thesimilar
approach one can see in the building the Pattern Dictionary of English Prepositions
(PDEP) [4]. The links and relations between objects of our ontology (syntaxemes), in
turn,canalsobeidentiﬁedonthebasisofcorpus-basedapproach.Therelationsofthis
kindareusuallycalculatedbythevectorspacemodel[5].Ourapproachiscloserto[6].
But unlike [6], where machine learning is used, we rely on corpus statistics.
WegroundourresearchontwoobservationsbyM.I. Steblin-Kamenskiy[7]:(1)an
incompleteawarenessofthemotivationforgrammaticalmeanings,whichisexpressed,
forexample,intheattributionofanimatenesstoobviouslyinanimatenominationssuch
as пoкoйник, мepтвeц (‘dead person’) or кyклa (‘a doll’) and the like, and (2) the
particular type of binary grammatical opposition, where one member, so called char-
acteristic category, expresses a grammatical meaning “A”, and its counterpart is not a
simple opposite of “non A” but some sort of a merger between “non A” and “A” [8].
For our approach, this is a fairly clear principle of distinguishing grammatical oppo-
sitions on the basis of corpus statistics. We will use this idea of the characteristic
categoryasaguidelinefordistinguishinggrammaticaloppositionssincepurelylogical
comparisonsofprepositionalmeaningsleadtotheso-called“inconsistency”intheuse
of prepositional-case constructions due to their grammatical nature.
This corpus-based semantic and grammatical description of Russian prepositional
constructionsusesempiricdatafromvariouscontemporaryRussiancorporainorderto
identify and then formalize the basic ontological semantic patterns of “prepositional
grammar”.
2 Prepositional Ontology
It isclaimed in[9]that aprepositional ontologyhasahierarchicalstructure.The most
abstractconceptsaresemanticrubrics,whicharerealizedassyntaxemes.Thistermwas
proposed by G.A. Zolotova [3] as a designation of the minimal syntactic-
morphological prepositional constructions having particular meanings. Syntaxemes
may be divided into subtypes (subsyntaxemes) which convey lexico-grammatical
meanings and may be expressed by primary or secondary prepositions in a variety of
textualforms. Notionsfromontologicallevelshavegrammatical naturethatrequiresa
special quantitative grammatical approach for further structuring.
Quantitative grammatical description is carried out on morphological annotated
corpora using corpus searching tools. Besides, we developed own software to extract
prepositional phrases from the syntactically annotated corpus Taiga [10]. Frequencies
ofprepositionalmeaningsobtainedfromvariouscorporadifferforavarietyofreasons,
the most important being the balance of stylistic and thematic text characteristics.
Frequencies in this paper are derived from a balanced corpus of Russian developed at
the Saint-Petersburg University.
Thecrucial pointofourmethodology[11]isacompilation ofarandomsampleof
contexts with prepositional constructions from corpora. The sample contexts are
annotatedattheﬁrststagebylinguists.Prepositionalmeaningsarerankedaccordingto
thepercentageofaparticularmeaningofapreposition.Thetopranksdemonstratethe124 V.Zakharov and I.Azarova
regular use of prepositional constructions, and the bottom ranks show their irregular
use. The meanings from the top ranks are extrapolated due to the total frequency of a
prepositioninthecorpusandnormalizedtoanumberofmillionsoftokenspresentedin
thecorpusandprocessedasanipmfrequencymeasureofprepositionalmeanings.They
may be used for aligning the pairs “preposition – its meaning” according to the sim-
ilarity of meanings to subsyntaxemes, syntaxemes, and rubrics of prepositional
constructions.
A prepositional syntaxeme is characterized by a morphological arrangement (a
preposition plus a noun case form) which has a unity of the form and the meaning
functioning as a constructive and signiﬁcant component of a phrase or a sentence.
SyntaxemesintheoriginalZolotova’sdescriptionlooklikesemanticrolesorargument
speciﬁcation: locative, temporative, directive, destinative, correlative, quantitative,
mediative, qualitative. A typical syntaxeme is expressed by several prepositional
phrases, some of them are synonyms and some are not.
The prepositional semantic rubrics, as well as syntaxemes and subsyntaxemes are
arranged into cortege sets, that manifest the conceding corpus frequencies. Ratio
enumeration of rubrics according to [9] includes localization (.35); temporative (.22);
objective(.14); derivative (.09); qualiﬁcative (.05); partitive (.03); quantiﬁcative (.02).
Two topmost semantic rubrics of prepositional meanings are localization and
temporative.
In[11]thestructureofthelocalizationsemanticrubricwithappropriatesyntaxemes
and subsyntaxemes is outlined. It is build on the context analysis for 10 topmost
Russian prepositions, that are common for all functional styles and periods from
Russian National Corpus [12]: “в” (‘in’), “нa” (‘on’), “c” (‘with’), “пo” (‘by’), “к”
(‘to’), “из” (‘from’), “y” (‘at’), “зa” (‘behind’), “oт” (‘from’), “o” (‘about’).
We verify the alleged structure of the localization rubric incorporating more fre-
quent prepositions from the list in [12]: “дo” (‘to’), “пpи” (‘at’), “пoд” (‘under’),
“пocлe” (‘after’), “бeз” (‘without’), “чepeз” (‘through’), “пepeд” (‘before’), “мeждy”
(‘between’), “нaд” (‘over’), “из-зa” (out of’), “из-пoд” (‘from under’). The veriﬁed
structure of the localization rubric is described in the next section.
3 The Grammatical Structure of the Localization Rubric
This rubric is informative due to its frequency domination (about 14000 ipm) in text
corpora,thus variousgrammatical “characteristiccategories”arepresented initstypes
and subtypes. The framework of this rubric is – in some way – reproduced by other
semanticrubricsconformingthelocalizationgrammaticaloppositionstotheirparticular
nature. This correspondence is illustrated by the structure of the prepositional tempo-
rative rubric. In [11] 4 syntaxemes are set up: locative, directive, departive, and tran-
sitive. Distribution of ipm frequencies for them and their subtypes (subsyntaxemes) is
shown below in Table 1.Grammatical Parallelism of RussianPrepositional Localization 125
3.1 Locative Syntaxeme
Thegeneralmeaningofthelocativesyntaxemeisthedesignationofthepointorextent
in space. Its 6 subtypes are opposed in corpus frequencies, the case form of the
governee noun, and particularities referring to lexico-semantic classes of governor
words and governee nouns. Governor words in general for all subtypes have verbal
naturedesignatingactions,statesandprocesses.Concretenounsareusedalsobutwith
minor corpus frequencies. The locative_1 formed by “в” (‘in’) and locative_2 formed
by “нa” (‘on’) are combined with nouns in the same case form. This case is placed at
the sixth position in the standard Russian case paradigm, and called “prepositional”.
Governee nouns in these subsyntaxemes are artiﬁcial and natural objects which form
the human environment. They are classiﬁed into two intersecting groups: Place_1 and
Place_2. Usually difference between these groups is associated with the idea of “in-
clusion”fortheformerinthecontrastto“support”and“contiguity”forthelatter[13].
Thisoppositionissupportedinexamplescидeтьвcaдy(‘tositinthegarden’),cидeть
нa cтyлe (‘to sit on the chair’). The locative_2 subsyntaxeme with “нa” (‘on’) con-
cedes the locative_1 with “в” (‘in’) greatly: 1800 ipm to 3700 ipm. It is a part of
Russian grammatical structure, that reﬂects statistical and combinatory characteristics
of modern Russian grammatical usage.
The most frequent variant of the locative_3 involves the genitive case of the
governee noun from both groups Place_1 and Place_2 and designates immediate
proximity. The core preposition is “y” (‘at’) [250 ipm], other synonymous secondary
prepositions designating ‘near’ are“вoзлe” [66 ipm], “oкoлo” [16 ipm], “нeпoдaлeкy
oт”[8ipm],“вблизиoт”[1ipm]:cидeтьy(вoзлe/oкoлo)мopя(‘tositby(near/about)
the sea’); зaнять мecтo y oкнa (‘to take a seat by the window’). There is another
variantincludingtheablativecaseform:“pядoмc”[73ipm],“пoд”[35ipm]:вoeвaть
pядoм c/пoд Mocквoй (‘to ﬁght near Moscow’).
The preposition “зa” (‘behind’) with the ablative case [260 ipm] denotes the
locative_4 designating a dividing limit (кpичaть зa дoмoм ‘to scream behind the
house’;cтoятьзacпинoй‘tostandbehind’),whichcansimultaneouslybeamarkerof
theofferedservices(cтoятьзaпpилaвкoм‘tostandbehindthecounter’,cкyчaтьзa
бapнoйcтoйкoй‘tobeboredbehindthebarcounter’),thelatterconstructionleadsto
an “active” interpretation of the locative_4 (быть пpoдaвцoм, бapмeнoм ‘to be a
seller,abartender’).Prepositionsareoftenincludedinvariousidioms.Inthiscasethey
losetheirprimarygrammaticalmeaning.Inthisarticle,theproblemofidentifyingand
describing such “prepositional” idioms is not discussed. The object standing as a
governeeafter“зa”(‘behind’)maybearealobstacle,hidingfromthesightofaperson
what is behind it cтoять зa двepью/вopoтaми ‘to stand behind the door/gate’,
нaxoдитьcя зa зaбopoм ‘to be behind the fence’. The locative_4 tends to lexicalize:
бытьзaгopoдoм(=нaпpиpoдe)‘tobeinthecountry’,нaxoдитьcязapyбeжoм‘to
be abroad’. This phenomenon indicates the starting point of the threshold range
dividing grammatical and lexical characteristics of a prepositional construction. The
preposition “пepeд” (‘in front of’, ‘before’) with the ablative case denotes a opposite
variant of the locative_4 designating being on this side of a dividing limit from the
sight of a observing person.126 V.Zakharov and I.Azarova
Thelocative_5 isformed bythepreposition “пoд”(‘under’)with theablativecase
[145 ipm]. It designates the position below some marked location: лeжaть пoд
cтoлoм (‘to lie under the table’), cтoять пoд нaвecoм (‘to stand under an awning’).
Another subsyntaxeme locative_6 represents the peripheral usage of the frequent
preposition “пo” (‘over’) taking the dative case [110 ipm]. This preposition is con-
sidered by G.A. Zolotova predominantly as a transitive syntaxeme (see below),
althoughitisalsousedforalocalizationspeciﬁcationdenotingtheboundariesofnon-
directional or chaotic movement: бpoдить пo гopoдy/yлицaм (‘to wander around the
city/the streets’), пyтeшecтвoвaть пo cтpaнe (‘to travel around the country’). This
subtypesignalstheendofthegrammaticalthresholdrange.Lessfrequentprepositional
constructionsarestructuredaccordingtheprinciplesoflexicalorganization,asalexico-
semantic group. Plenty examples are given in [9].
Differentsubtypesofagrammaticalsyntaxememayco-occurinthetextatthesame
time: cидeть в кpecлe нa вepaндe (‘to sit in a chair on the veranda’), гpaбить нa
oткpытыxдopoгaxзaгopoдoм(‘torobonopenroadsoutsidethecity’).Thisfactis
usually considered an evidence that these prepositional constructions have different
semantic roles.
3.2 Directive, Departive, and Transitive Syntaxemes
The next type of localization prepositional groups reﬂects the trajectory of object or
subject movement which is compatible with verbal governors denoting self-propelled
movement or changing object location. Three aspects of this trajectory are usually
speciﬁed in corpus texts: (a) the end point of the trajectory, that is, the goal; (b) the
initial or starting point of the trajectory; (c) the space traversed.
The directive syntaxeme [4575 ipm] speciﬁes the end point of the movement. It
shows the conformity with subtypes of the locative syntaxeme. The topmost frequent
prepositions“в”(‘in’)and“нa”(‘on’)haveimpressiveparallelismintheirgrammatical
speciﬁcity.Theyrequiretheaccusativecaseofnounsandhavetheidenticalselectional
preferences for governee nouns as locative_1 and locative_2. Thus, we can introduce
two syntaxeme subtypes: the directive_1 [2500 ipm] and the directive_2 [1250 ipm]:
пoлoжитьвшкaф(‘toputinthecloset’),пocтaвитьнacтoл(‘toputonthetable’).
The same grammatical shift is valid for less frequent directive_4 with the preposition
“зa” (in this sense ‘over’) [115 ipm] and directive_5 with the preposition “пoд”
(‘under’) [70 ipm]: бpocить зa oгpaдy (‘to throw over the fence’), выйти зa oгpaдy
(‘togooverthefence’),пpыгнyтьпoдвaгoн(‘tojumpunderthewagon’),пoлoжить
пoд кpoвaть (‘to put under the bed’).
The novelty in this group is the preposition “к” (‘to’) taking the dative case [650
ipm] which is analogous to locative_3 with preposition “y” (‘at’) as regards lexical
nature of governee nouns: ycтpeмитьcя к вepaндe (‘to rush to the veranda’), пoд-
винyтькoкнy(‘tomovetothewindow’).Theenumerationofthissyntaxemesubtype
is directive_3. Another variant of this subtype is preposition “дo” (‘to’) taking the
genitivecase[65ipm]:плытьдoocтpoвa(‘tosailtotheisland’).Thisvariantisused
with the preﬁxal verbal derivatives coinciding with the preposition: дoбpaтьcя дo
ocтpoвa (‘to get to the island’).Grammatical Parallelism of RussianPrepositional Localization 127
Thedepartivesyntaxemespeciﬁesthestartingpointofthemovementtrajectory.It
does not look sustainable because these constructions have corpus frequencies com-
parable with those of the directive group, and follow the similar type of lexical pref-
erences for governee nouns. Its subtypes match straightforwardly enumeration of the
directive syntaxeme. Departive subtypes attach governee nouns in the genitive case
form. The departive_1 uses the preposition “из” (‘from’) [660 ipm] and denotes the
movement of an object or a subject opposite to that of directive_1: yйти из caдa (‘to
leave the garden’), вытaщить из шкaфa (‘to pull out of the closet’). The same
oppositioncanbeseeninthedepartive_2 withthepreposition“c”(‘from’)[410ipm]:
yбpaть co cтoлa (‘to clear the table’), yйти c вepaнды (‘to leave the veranda’). The
departive_3expressedbythepreposition“oт”(‘from’)[300ipm]isanoppositetothe
directive_3: oтoдвинyть oт oкнa (‘to move away from the window’), yйти oт
cтoлa (‘to get away from the table’). The departive_4 is expressed by the preposition
“из-зa” (in this sense ‘from’) [40 ipm]. It is the counterpoise of the directive_4:
вытaщитьиз-зaпaзyxи(‘topulloutfromthebosom’),вcтaтьиз-зacтoлa(‘toget
upfromthetable’).Similarlythedepartive_5with“из-пoд”(‘fromunder’)[22ipm]is
thecounteractionofthedirective_5:вытaщитьиз-пoдcтoлa(‘topulloutfromunder
the table’), тopчaть из-пoд cнeгa (‘to stick out of the snow’).
Thetransitivesyntaxemeformstheperipheralsyntaxemeinthelocalizationrubric
duetolessercorpusfrequencyandmoreintricatesubtypeoppositions.Thetransitive_1
isformedbytheonlycorepreposition“пo”(inthissense‘along’)withthedativecase
[360ipm]:пpoйтипoкopидopy/пoлю(‘towalkalongthecorridor/ﬁeld’),cпycкaтьcя
пo лecтницe (‘to go down the stairs’). The secondary synonym is peripheral “вдoль
пo”(‘along’)[2ipm].Theprimarypreposition“чepeз”(‘through’or‘across’)withthe
accusative case of the governee noun [135 ipm] forms the transitive_2 subtype:
пpoйтичepeзxoлл(‘togothroughthehall’),пpoвoзитьчepeзпepeeзд(‘totransport
acrosshighwaycrossing’).Thesecondarypreposition“cквoзь”(‘through’)[38ipm]is
usually considered to belong to the transitive_2 subtype.
Table1. Distribution of ipm frequencies of syntaxemes from the localization rubric in a
balancedcorpus(thesuperscriptfollowingtheprepositiondesignatesthecaseformasitsposition
in thestandard case paradigm)
Locative Directive Departive Transitive
Locative_1 в6 4000 Directive_1 в4 2500 Departive_1 из2 660 Transitive_1 пo3 360
Locative_2 нa6 2500 Directive_2 нa4 1250 Departive_2 c2 410 Transitive_2 чepeз4 135
Locative_3 y2… 449 Directive_3 к5дo2 640 Departive_3 oт2 300 Transitive_2 cквoзь4 38
Locative_4 зa5 260 Directive_4 зa4 115 Departive_4 из-зa2 40
Locative_5 пoд5 145 Directive_5 пoд4 70 Departive_5 из-пoд2 22
Locative_6 пo3 110
ipm 7464 ipm 4575 ipm 1432 ipm 533
The distribution of the frequencies of localization syntaxemes illustrates the
quantitative realization of grammatical oppositions in the ontological structure of
prepositional meanings. Jacobson’s characteristic categories can be seen in the128 V.Zakharov and I.Azarova
syntaxeme group: the locative_1 prevails over the locative_2, and so forth, and this
ratioisrecurrentineverysyntaxemegroup.Thisstructureistransformedsubsequently
into other semantic rubrics, though the fundamental cognitive traits of the localization
rubric are quite clear. The signiﬁcant feature ofthe localization syntaxemestructure is
the fact that they are predominantly unambiguous: they involve different prepositions,
and if they coincide, attached case forms vary. This leads to a situation in which the
juxtapositionofRussianlocalizationsyntaxemescannotbetranslatedintoEnglishasa
validphraseduetotherepetitionofthesamepreposition.Therefore,wemayconsider
enumeratedsyntaxemestobeajointformofgrammaticalexpressionofthelocalization
ontological system. English translations of the localization syntaxemes reﬂect the fact
that in some parts the ontological structures in two languages overlap, and partially
they are totally different.
Contextual examples of Russian localization syntaxemes show the correlation
between preﬁx verbal derivatives and localization syntaxemes in use. The usual
interpretation of this correlation is formulated as follows: preﬁxal verbal derivatives
create constituents of “governed” prepositional constructions, the preﬁx regularly
matching the preposition in such a constituent. However, the investigation of this
phenomenon with a help of corpus statistics analysis in [11] shows that realization of
derivative,departiveandtransitivesyntaxemesforpreﬁxedverbalderivativesishigher
than for those without preﬁxes. The distribution of substantial frequencies of local-
ization syntaxemes over preﬁxal verbal derivatives illustrates that the appearance of
localization syntaxemes to a considerable extent depends on the semantic type of the
governee noun. Localization subsyntaxemes tend to substitute one another, the formal
repetition of the preﬁx by its matching preposition is possible, but not obligatory and
predominant.
4 The Grammatical Structure of the Temporative Rubric
The temporative rubric [6140 ipm] concedes to the localization one in frequency.
Naturally, there is no complete isomorphism of spatial and temporal relations, never-
theless, the loose speciﬁcation of governor words enlarges the number of verbal
derivativescomparingwiththelocalizationrubric.Thetopmostsyntaxeme[2765ipm]
inthetemporativerubricisthetemporalspeciﬁcationofsomeevent.Thegrammatical
opposition in corpus frequencies holds between the principal preposition in the
localizationrubric“в”(‘in’)incombinationwiththeprepositional(locative)case[1150
ipm] and accusative case [920 ipm] in the same manner as the locative_1 versus the
directive_1.However,these prepositionalconstructions inthetemporativerubriceven
atthehighestlevelofthestructureexploitspeciﬁclexicalpreferences.Theﬁrstsubtype
temporal_1 attaches nouns denoting months, years, and longer time intervals, these
timespeciﬁcationsmaybeloadedwithanadditionalmeaningcomponent:в1999гoдy
(‘in 1999’), в aвгycтe (‘in August’), в 19 вeкe (‘in the 19th century’), в нeoлитe (‘in
the Neolithic’), в дeтcтвe (‘in childhood’). The second subtype temporal_2 with
nouns in the accusative case is used to indicate the day of the week or the time of the
day: в пятницy (‘on Friday’), в пять чacoв yтpa (‘at ﬁve in the morning’). The
difference between temporal_1 and temporal_2 reﬂects the distinction betweenGrammatical Parallelism of RussianPrepositional Localization 129
governeenounsdenoting timeintervals(Time_1)andmomentsoftime(Time_2),and
thislinguisticclassiﬁcationappearstobeaslatentasthelocativedivisionintoPlace_1
and Place_2.
The opposition between two groups of governee nouns (Time_1 and Time_2) is
nulliﬁedintheconstructiontemporal_3similartodirective_4“зa”(‘during’)attaching
the accusative case [230 ipm]: yничтoжить зa ceкyндy (‘to destroy in a second’),
пocтaвки зa гoд (‘deliveries per year’). The secondary preposition “в тeчeниe”
(‘during’) with genitive case [120 ipm] is widely used supporting this group: дeйcт-
вoвaтьвтeчeниeдeкaбpя(‘toactduringDecember’),зaeздвтeчeниeпятницы(‘a
check in during Friday’).
The next subsyntaxeme temporal_4 is formed according the model of the loca-
tive_4: “зa” (‘during’) attaching the ablative case [110 ipm]. It provides the temporal
reference by mentioning some action: cидeть pядoм зa oбeдoм (‘to sit nearby at
dinner’), cкaзaть зa зaвтpaкoм (‘to say at breakfast’). This subsyntaxeme is sup-
portedbythesecondarypreposition“вoвpeмя”(‘during’)[138ipm]withnounsinthe
genitive case: вo вpeмя бeceды (‘during the conversation’).
The last subtype temporal_5 in this group is formed on the pattern of the direc-
tive_2: “нa” (in this sense ‘at’) with the accusative case, the governee noun phrase
denotes the date of some event [55 ipm]: гoтoвить втopжeниe нa 12 мaя (‘to
prepare the invasion for May 12’), пoлoжeниe дeл нa 6 фeвpaля (‘the state of affairs
on February 6’). Distribution of ipm frequencies for the syntaxemes from the tempo-
rative rubric is shown in the Table 2.
The co-occurence of temporal subsyntaxemes is possible. There are two basic
types. The standard one: nouns appropriate for temporal_1 stands in genitive after
various subtypes: пpoизoйти в oктябpe 1995 г. (‘to happen in October, 1995’); the
second type is the sequence of several temporal_2 constructions: apecтoвaть в
вocкpeceньe в дeвять чacoв yтpa (‘to arrest on Sunday at nine in the morning’).
The second syntaxeme in the temporative rubric aspective speciﬁes a trajectory
projected on the limits of some time interval: its beginning, accomplishing, duration,
and repetition. In this syntaxeme the opposition of intervals and moments turns in
twofold aspects. The ﬁrst predominant method presents direct time designation and
indirectnominationbyreferencetotheeventasapointwithoutproperduration,thisis
a metaphorical transfer of the space trajectory. They are represented as time “marks”:
the beginning happens after this mark as a point of departure, the accomplishment –
before that mark as a point of arrival. The second method presents time as a time
duration in which it is possible to specify logically the beginning and the end. Sec-
ondaryprepositionsincorporatingthepreposition“в”(‘in’)designatetheseboundaries
of thetime interval: в нaчaлe[27 ipm] июня (‘atthe beginningofJune’), в кoнцe[41
ipm] 1995 гoдa (‘at the end of 1995’), в пepиoд [24 ipm] нepecтa (‘during spawn-
ing’). We divide propositional constructions compatible with aspects of temporal
variation into 4 subtypes: the terminative, the intervallum, the inchoative, and the
repetitive. The description of these subtypes is focused primarily on usage of primary
prepositions, because secondary ones include them as a part attaching lexical items
overtly presenting the subsyntaxeme meaning.
The topmost subtype of the aspective syntaxeme is the terminative [504 ipm], it
designates the time of event accomplishment, it is usually the point-wise method of130 V.Zakharov and I.Azarova
timespeciﬁcation.Itsﬁrstvarianttransformsthesecondvariantofthedirective_3with
the preposition “дo” (in this sense ‘by’) with the genitive case [236 ipm]: зaкoнчить
дo oceни (‘to ﬁnish by fall’), нaлaдить дo 15 мaя (‘to set up by may 15’). This
subsyntaxeme may be included into a sequence with the inchoative producing the
intervallumsubsyntaxeme:“c…дo”[22ipm]:бeгaтьcyтpaдoпoзднeгoвeчepa(‘to
run from morning till late evening’), “oт… дo” [14 ipm] длитьcя oт нecкoлькиx
минyт дo чaca, (‘to last from a few minutes to an hour’). The second variant is the
modiﬁcation of directive_3 “к” (in this sense ‘by’) with dative case [122 ipm]:
зaкoнчитькoceни(‘toﬁnishbyfall’).Anothervariantlookslikethedirective_5:the
preposition “пoд” (in this sense ‘before’) attaching the accusative case [25 ipm] with
lexical restrictions:yйти пoдвeчep(‘toleave intheevening’),пpиexaть пoдHoвый
Гoд (‘to come on New Year’s Eve’). The secondary preposition “в кoнцe” (‘at the
end’)[41ipm]presentstheextendedviewtothetimeinterval:пoлyчитьaттecтaтв
кoнцe гoдa (‘to get a certiﬁcate at the end of the year’).
Theinchoativesubsyntaxeme[367ipm]isopposedtotheterminativeinthepoint-
wise system of time representation and speciﬁes the time of event beginning. The
topmost variant is the transformed departive_2, the preposition “c” (‘from’) with the
genitive case [170 ipm]: пить чaй c yтpa (‘to drink tea in the morning’), нaчaть
paбoтaть c 13 лeт (‘to start working from 13 years old’). The second variant is the
transformed departive_3, the preposition “oт” (‘from’) with the genitive case [110
ipm]: в вoзpacтe oт 7 лeт (‘from the age of 7’), cлeпoй oт poждeния (‘blind from
birth’), yкaз oт 12 мaя (‘Decree of May 12’). Both variants are combined with the
terminatives,andformtheintervallum(“c…дo”).Thefrequentextendedinchoativeis
formed by the secondary preposition “в нaчaлe” (‘at the beginning’).
The intervallum syntaxemes [467 ipm] overlaps with the inchoative and the ter-
minativepresentedinasequenceinthetime.Thetopmostvariantis“зa”(inthissense
‘in’) with the accusative case [230 ipm] (it coincides with the directive_4 in the
localization rubric): пpocмoтpeть тeкcт зa 10 минyт (‘to look through the text in
10 min’). The second variant is a new construction “пo” (in this sense ‘till’) with the
accusative case [180 ipm], it expresses a quantiﬁcative time period: жить пo двa
мecяцaвдepeвнe(‘tolivetwomonthsinthevillage’).Thisconstructionmaybeused
asbinaryexpressionofthetimeperiod:“c…пo”[24ipm]paбoтaтьcмapтaпoмaй
(‘to work from March till May’). The third variant of this subtype is the transformed
directive_2,thepreposition“нa”(inthissense‘for’)withtheaccusativecase[24ipm]:
ocтaвить нa нoчь (‘to leave overnight’), ocвoбoдить нa двa-тpи дня (‘to free for
two-threedays’).Thesecondaryprepositions“впepиoд”and“зaпepиoд”(‘duringthe
period’) present lexically the intervallum subtype.Grammatical Parallelism of RussianPrepositional Localization 131
Table2. Distribution of ipm frequencies of syntaxemes from the temporative rubric in a
balancedcorpus(thesuperscriptfollowingtheprepositiondesignatesthecaseformasitsposition
in thestandard case paradigm)
Temporal Aspective Taxis
Temporal_1 в 1150 Terminative до к 504 Consequent после 857
6 2 3 2
Temporal_2 в4 920 вконце2 через4
под спустя
5 2
Temporal_3 за 350 Intervallum за по 467 Concurrent при 675
4 4 4 6
втечение на
2 4
Temporal_4 за6 290 впериод2 Antecedent до2 340
вовремя2 Inchoative с2от2 367 перед5
вначале
2
Temporal_5 на 55 Repetitive по 180
4 5
ipm 2765 ipm 1518 ipm 1872
Therepetitivesubsyntaxeme[180ipm]isexpressedbythetransformed locative_6
“пo” (‘on’) with the ablative case: игpaть в кapты пo пятницaм (‘to play cards on
Fridays’).Distributionofipmfrequenciesfortheaspectivesyntaxemeanditssubtypes
from the temporative rubric is shown in Table 2.
The time deﬁnition in texts may reffer to the time of some event. In this case the
time speciﬁcation turnes out to be relative, such prepositional constructions form the
third syntaxeme taxis (after R. Jacobson’s terminology for verbal relative time).
Speciﬁcation of taxis is straightforward: the time interval preceding some event –
antecedent, following it – consequent, simultaneous – concurrent.
Thetopmostsubtypeofthissyntaxemeisconsequent[857ipm].Theﬁrstvariantis
formed with the secondary preposition “пocлe” (‘after’) [500 ipm] attaching the gen-
itive case.: пpoизoйти пocлe oкaзaния пoмoщи (‘to happen after assisting’), зaмe-
тить пocлe oбыcкa нa квapтиpe (‘to notice after searching the apartment’). The
second variantisthetransformedtransitive_2“чepeз”(in this sense ‘after’) [300ipm]
with the accusative case, the governee noun phrase is usually designation of standard
time measurements potentially quantiﬁed: пpийти чepeз дeнь (‘to come in a day’),
пpoизoйти чepeз 2 cтoлeтия (‘to happen after 2 centuries’). The secondary prepo-
sition “cпycтя” (‘later’) with the accusative case [57 ipm] forms the equivalent con-
struction: вoзглaвить миниcтepcтвo cпycтя 20 лeт (‘to head the ministry 20 years
later’). This preposition in small number of contexts (5%) stands after the governee
noun phrase in a postposition: нaйти нecкoлькo лeт cпycтя (‘to ﬁnd a few years
later’).Theﬁrstandthesecondvariantscanbeusedinasequencewithaﬁrstvariant:
вepнyтьcя cпycтя| чepeз гoд пocлe poждeния мaлышa (‘to come back a year after
the birth of the baby’).
The next subtype is concurrent, although there is the only primary preposition
“пpи”(‘during’)[675ipm]:иcпoльзoвaтьпpиoкaзaниипoмoщи(‘touseinassisting’
= to use when assist).
The third subtype is antecedent [340 ipm] which attaches governee nouns, des-
ignating actions, events, and soon. The topmost variant transforms thesecond variant
of the directive_3: “дo” (‘before’) with the genitive case [266 ipm]: oцeнивaть132 V.Zakharov and I.Azarova
coтpyдникa дo eгo пoявлeния (‘to evaluate an employee before he appears’). The
second variant is formed by the preposition “пepeд” (‘before’) with the ablative case
[74 ipm]: выcкoчить пepeд зaкpытиeм двepeй (‘to pop out before closing the
doors’), ocмoтpeтьcя пepeд выxoдoм (‘to look around before going out’). A great
number of secondary prepositions for this syntaxeme are listed in [9].
5 Semantico-Grammatical Parallelism of Localization
and Temporative Rubrics
Semantic rubrics are abstract semantic classes divided into syntaxemes by which we
describe the meaning of prepositional constructions. The localization and temporative
semantic rubrics are two topmost rubrics of prepositional meanings. As shown above,
their framework, division into subtypes, and speciﬁcation of subtypes are sometimes
similar. Let’s illustrate this parallelism in a compressed form (Table 3). We suppose
that it is a result of an associative transfer of spatial relations into the realm of imag-
inary location and movement on the timeline of events.
Table3. Proximity betweensubsyntaxemes oftemporative andlocalization rubrics
Temporative Localization
Syntaxeme Subsyntaxeme Example Syntaxeme Subsyntaxeme Example
Temporative Temporal_1 вaвгycтe Locative Locative_1 вcaдy
Temporative Temporal_3 зaгoд,втeчeниe Locative Locative_6 впpeдeлaxпoля,нa
дeкaбpя пpoтяжeниипyти
Temporative Temporal_4 зaзaвтpaкoм,вo Locative Locative_1 вкoмнaтe,нa
вpeмябeceды вepaндe
Aspective Terminative зaкoнчитьк Directive Directive_3 пoдoйтик
oceни,дooceни вepaндe,плытьдo
ocтpoвa
Aspective Inchoative питьчaйcyтpa, Departive Departive_2/Departive_3 yйтиcвepaнды,
нaчaтьc13лeт yйтиoтcтoлa
Aspective Intervallum дoйтизa Directive/Departive Directive3/Departive3 oтдoмaдopeки
5минyт,c
мapтaпoмaй
Taxis Consequent пocлeoбыcкa, Locative Locative_4 чepeз3
чepeздeнь килoмeтpa,зa
гopoдoм
Taxis Concurrent пpиoкaзaнии Locative Locative_3 cидeтьyмopя
пoмoщи
Taxis Antecedent дoпoявлeния, Locative Locative_4 cтoятьпepeд
пepeд двepью
зaкpытиeм
We would like to note that there are not only hierarchical relationships between
syntaxemesandsubsyntaxemesinourontology,buthorizontalrelationsbetweenunits
from different semantic rubrics, too.Grammatical Parallelism of Russian Prepositional Localization 133
6 Conclusion and Further Work
The semantic rubrics presented in our approach help to organize rather vague prepo-
sitional meanings. Their afﬁnity and difference may be explicated through the overlap
of semantic classes of governing and subordinate words. The whole structure of
prepositionalfrequenciesthatsofarhavenotbeeninanystudyandneighborsemantic
distributions are resources for the compilation of the quantitative prepositional gram-
mar for Russian.
We proceed to compile the structure of other semantic rubrics on the basis of
outlined technique.Theframe ofgrammatical oppositionsareorganizedbydispersion
of primary prepositional meanings. Secondary prepositionsare attached to this system
appending their particular lexical connotation. Occasionally they gain the appropriate
position in the grammatical framework, thus shifting to another level of balance
between grammatical and lexical components of meaning.
Further stages of investigation include:
(cid:129) to ﬁnalize the set of syntaxemes and subsyntaxemes of prepositional semantic
rubrics referring to governers and governees semantic types;
(cid:129) to compare sets of prepositional constructions (grammatical syntaxemes and their
lexicalizedversionswithsecondaryprepositions)fromcorporaofdifferentgenresin
order to discover the signiﬁcant variation of statistical parameters;
(cid:129) toinvestigatethecombinatorypotentialoftheextractedgrammaticalsyntaxemesin
order to separate their subtypes and synonymic variants;
(cid:129) to compile rules of the hybrid generative grammar showing the use and interpre-
tation of syntaxemes for the corpus text.
Acknowledgements. This paper has been supported by the Russian Foundation for Basic
Research,project No.17-29-09159.
References
1. Solonitskiy,A.V.:ProblemsofsemanticsofRussianprimitiveprepositions.125p.(inRus)
[Problemy semantiki russkikhpervoobraznykh predlogov]. Vladivostok (2003)
2. Filipenko, M.V.: Problems of the description of prepositions in modern linguistic theories
[Problemy opisaniyapredlogovvsovremennykhlingvisticheskikh teoriyakh].In:Research
onthe semanticsof prepositions. Moscow: Russkieslovari, pp. 12–54(2000)
3. Zolotova, G.A.: Syntactical Dictionary: a Set of Elementary Units of Russian Syntax.
440 p. (in Rus) [Sintaksicheskiy slovar’: repertuar elementarnykh edinits russkogo
sintaksisa], 4th edn.Moscow (2011)
4. Litkowski, K.: Notes on grilled Opakapaka: ontology in preposition patterns. Technical
report 15–01. Damascus, MD:CL Research (2015)
5. Zwarts, J., Winter, Y.S.: A semantic characterization of locative PPs. In: Lawson, A. (ed.)
Proceedings ofSemantics andLinguisticTheory, pp.294–311(1998)
6. Lassen, T.: An ontology-based view on prepositional senses. In: Proceedings of the Third
ACL-SIGSEM Workshop onPrepositions, pp.45–50. ACM(2006)134 V.Zakharov and I.Azarova
7. Steblin-Kamenskiy, M.I.: Controversial in Linguistics. Leningrad: Leningrad University
Publishing House. 144 p. (in Rus.) [Spornoye v yazykoznanii. Leningrad: Izd-vo
Leningradskogo universiteta] (1974)
8. Jakobson, R.O.: Selected Works, Moscow, 1985. 460 p. (In Rus.) Izbrannyye raboty.
Moskva: Progress. 460s.(1985)
9. Zakharov, V., Azarova, I.: Semantic structure of Russian prepositional constructions. In:
Ekštein, K. (ed.) TSD 2019. LNCS (LNAI), vol. 11697, pp. 224–235. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-27947-9_19
10. Gudkov, V., Golovina, A., Mitrofanova, O., Zakharov, V.: Russian prepositional phrase
semanticlabellingwithwordembedding-basedclassiﬁer.In:CEURWorkshopProceedings,
vol.2552, pp.272–284(2020)
11. Azarova,I.,Zakharov,V.,Khokhlova,M.,Petkevič,V.:OntologicaldescriptionofRussian
prepositions. In: CEUR WorkshopProceedings, vol.2552, pp.245–257(2020)
12. Lyashevskaya,O.N., Sharoff,S.A.:Frequency dictionaryof themodern Russianlanguage,
vol. XXII, 1087 p. (in Rus.). Moscow: Azbukovnik [Novyj chastotnyj slovar’ russkoj
leksiki] (2009). http://dict.ruslang.ru/freq.php?act=show&dic=freq_freq&title=%D7%E0%
F1%F2%ED%FB%E9%20%F1%EF%E8%F1%EE%EA%20%EB%E5%EC%EC
13. Herskovits, A.: Semantics and pragmatics of locative expressions. Cogn. Sci. 9, 341–378
(1985)Costra 1.1: An Inquiry into Geometric
Properties of Sentence Spaces
B
Petra Baranˇc´ıkova´( ) and Ondˇrej Bojar
Charles University, MFF U´FAL, Prague, Czech Republic
{barancikova,bojar}@ufal.mff.cuni.cz
Abstract. Inthispaper,wepresentanewdatasetfortestinggeometric
properties of sentence embeddings spaces. In particular, we concentrate
on examining how well sentence embeddings capture complex phenom-
ena such paraphrases, tense or generalization. The dataset is a direct
expansionofCostra1.0[7],whichweextendedwithmoresentencesand
sentence comparisons. We show that available oﬀ-the-shelf embeddings
donotpossessessentialattributessuchashavingsynonymoussentences
embeddedclosertoeachotherthansentenceswithasigniﬁcantlydiﬀer-
entmeaning.Ontheotherhand,someembeddingsappeartocapturethe
linearorderofsentenceaspectssuchasstyle(formalityandsimplicityof
the language) or time (past to future).
· ·
Keywords: Sentence embeddings Sentence transformations
·
Paraphrasing Semantic relations
1 Introduction
Trained vector representations of words and sentences, known as embeddings,
have become ubiquitous throughout natural language processing (NLP). Since
their popularity took oﬀ with the introduction of word2vec word embeddings
[15], numerous diﬀerent methods with diﬀerent properties have emerged, high-
lighting the importance of estimating their quality. However, it is not entirely
clear in which way the embeddings should be evaluated, aside from the perfor-
mance in the task they originate in. Two main classes, extrinsic evaluation and
intrinsic evaluation, are considered [17].
Extrinsic evaluation utilizes word embeddings as feature vectors for machine
learningalgorithmsindownstreamNLPtasks.Itserveswellinchoosingthebest
methodforaparticulartaskbutnotasanabsolutemetricofembeddingquality
as the performances of embeddings do not correlate across diﬀerent tasks [5].
This research was supported by the grants 19-26934X (NEUREM3) of the Czech
Science Foundation, 825303 (Bergamot) of the EU, and SVV project 260 575.
This work has been using language resources stored and distributed by the project
No. LM2015071, LINDAT-CLARIN, of the Ministry of Education, Youth and Sports
of the Czech Republic.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.135–143,2020.
https://doi.org/10.1007/978-3-030-58323-1_14136 P. Baranˇc´ıkov´a and O. Bojar
[16] demonstrate the presence of linguistic regularities in the word2vec
embedding space. Namely, they show that various word analogy tasks can be
solved by simple vector arithmetic in the embedding space, e.g. ﬁnding correct
word D for words A, B, C and their respective embeddings vA,vB,vC by opti-
mizing:
arg max simC(vD,vA−vB +vC), (1)
D∈V\{A,B,C}
where simC represent cosine similarity between two vectors. This works for var-
ious semantic and syntactic relationships, like for example:
A B C D
king man woman queen
Russia Moscow Paris France
walked walk tell told
bigger big small smaller
This lead to a novel approach—intrinsic evaluation—in which word embeddings
arecomparedwithhumanjudgmentonwordrelations.Thereisalargenumberof
available datasets for syntactic and semantic intrinsic evaluation, word analogy
task [15,16] belongs among the most popular methods.
For sentence embeddings this is a diﬀerent story. When Kiros et al. [12]
introduced Skip-Thought vectors, they evaluated their quality in eight super-
vised tasks such as paraphrase detection or sentiment polarity. This extrinsic
evaluation or ‘transfer tasks’ became the de facto standard for evaluation and
comparison of sentence embeddings, despite the fact that even simple bag-of-
words (BOW) approaches often achieve competitive results on transfer tasks
[18].
[1,11] introduce intrinsic evaluation of sentence embeddings, however, most
of the research in interpretation of sentence embeddings consists of probing for
surface linguistic features of the sentence such as its length, verb tense, word
order, etc. Furthermore, [4,14] indicate that strong performance in these tasks
might be caused by test ﬂaws—the test sentences are grammatically too simple.
However, any geometric properties of an embedding space remain a largely
uncharted territory. We attempt to ﬁll this gap, examining whether sentence
representation spaces exhibit regularities with regard to certain kinds of rela-
tionships,inawaysimilartothelinearrelationsobservedinwordvectorspaces.
To this end, we devise a new dataset on the basis of Costra 1.0 [7], which we
extendwithinformationonlinearorderingofembeddedsentenceswithregardto
certainkindsofrelationships.Theseallowustotestempiricallywhetherexisting
sentence embedding models reﬂect analogical relationships between sentences.
The paper is structured as follows: Sect.2 presents existing methods of
semantic evaluation of sentence embeddings and available oﬀ-the-shelf embed-
dings.Section3describesthemethodologyforconstructingourdataset.Section4
details the evaluation of embeddings and Sect.5 presents the results.Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces 137
2 Related Work
2.1 Sentence Embedding Space Evaluation
Zhu et al. [21] compare sentence embeddings from a relational perspective using
automatically generated triplets of sentence variations and explore how syntac-
tic or semantics changes of a given sentence aﬀect the similarities among their
sentence embeddings. The following example sentences illustrate this point:
S1: A pig is eating goulash.
S2: A pig is feeding on goulash.
S3: A pig is not eating goulash.
Synonyms (S1, S2) should be embedded closer to each other in a vector space
thansentenceswithsimilarwordingbutdiﬀerentmeaning(S1,S3)and(S2,S3).
They discover that several embeddings perform surprisingly well in these tasks.
A sentence analogy task was recently introduced in [21]: in template sen-
tencestheysubstituteapairofwordssuchasstate/capital,man/womanorplu-
ral/singular.Totest,whethertheembeddingsarereallyabletoﬁndtheanalogy
correctly, they create incorrect sentences similar in wording to the correct ones
and examine whether Eq. (1) ﬁnds the ‘correct’ sentence.
Similarly, [6] examined sentences that are close in wording but diﬀer in one
keyaspect(e.g.changeofgender,addinganadjective,removinganumeral)and
show that the changes form meaningful clusters in the sentence vector space.
In Costra 1.0 [7], we attempted to move to more sophisticated types of sen-
tencerelations,beyondthosein[6,20].Wepresentadatasetofcomplexsentence
transformationsinCzech.Itiscreatedmanuallywiththeaimtothoroughlytest
how well sentence embeddings capture the meaning and style of sentences. The
dataset contains sentences very diﬀerent in wording with a similar meaning as
well as sentences similar on the surface level but very diﬀerent in meaning.
However,thedatasethascertainlimitations.Forinstance,itcontainsseveral
generalizations of a sentence but their mutual relations are no further studied.
In other words, we do not know, which ones are more general and should be
embedded closer to the original sentence. Our work directly builds upon [7]. We
decided to make the dataset more robust by extending it with more sentences
and also to ensure that sentences are related to each other whenever possible.
Wealsocreatedatooltoautomaticallyevaluatethequalityofembeddingsusing
our dataset and used it to compare several oﬀ-the-shelf Czech embeddings.
2.2 Sentence Embedding Methods
SinceweextendtheCostradataset,westicktotheCzechlanguage.Ourgoalisto
testasmanyoﬀ-the-shelfCzechsentenceembeddingsaspossible.Unfortunately,
toourbestknowledgethereisonlyonedirectlylearnedrepresentationforentire
sentences available for the Czech language: LASER [3].
However,thereareavailablepretrainedlanguagemodelssuchasmultilingual
BERT(mBERT)[10]orFlair[2].Despiteneglectingthewordorder,thesemeth-
odsyieldsurprisinglystrongresultsinmanydownstreamtasks.Inordertomove138 P. Baranˇc´ıkov´a and O. Bojar
from word vector representations towards representations for entire sentences,
we simply average embeddings of hidden states of all tokens in a sentence. For
BERT, we also consider the CLS token as a sentence embedding.
Sentence multilingual BERT1 (SentBERT) is a sentence encoder initialized
with multilingual BERT and ﬁne-tuned using MultiNLI [19] and XNLI [9]
datasets. The recommended sentence representations are mean-pooled token
embeddings, we use the CLS token too.
3 Annotation
We acquired the data in two rounds of annotation. In the ﬁrst one, we were
concentrating on adding more related sentences, i.e., making the sentence space
denser. In order to project sentence transformation to a linear scale, we decided
to collect interpolations and extrapolations. In the second round, we collected
pairwise comparisons of sentences from both Costra 1.0 and our ﬁrst round.
3.1 First Round: Collecting Interpolations and Extrapolations
In the ﬁrst round of annotation, we present annotators with a seed sentence
and its transformation and ask them to write the following two new sentences:
interpolation – a sentence with meaning/style between the two sentences, and
extrapolation–asentencewithmeaning/styleevenfurtherawayfromtheseed
sentence than the transformation in the suggested direction. An example of one
annotation is presented in Fig.1.
Fromthe14transformationtypesavailableinCostra1.0,wedidnotselectall
types of transformation for the ﬁrst round.2 The reason was straightforward: it
doesnotmakesensetocollectinterpolationsorextrapolationsforsomeofthem.
Forexample,meaningofparaphrases shouldbeidenticalorveryclosetooriginal
sentences and searching for interpolation would be a waste of annotators’ time.
Similarly, there is the non-sense transformation, which is created by shuﬄing
content words of a seed sentence, so the ﬁnal sentence is grammatically correct
but has no meaning. There are no interpolations or extrapolations of nonsense.
We manually examined all transformation types and selected only 6 of them
thatlookmostlinearlyscalable:formalsentence,future,generalization,nonstan-
dard sentence, opposite meaning and past. We do not introduce any new type of
transformations.
We collected almost 1,500 annotations from 7 annotators, containing 2,749
unique sentences. Total volume of Costra 1.1 is 6,968 sentences.
1 http://docs.deeppavlov.ai/en/master/features/models/bert.html.
2 Costra 1.0 contains the following 14 diﬀerent transformation types: paraphrase, dif-
ferentmeaning,oppositemeaning,nonsense,minimalchange,generalization,gossip,
formalsentence,non-standardsentence,simplesentence,possibility,ban,future,past.Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces 139
seed “Obˇcas se mi na hlavˇe ma´lo prokrvuje k˚uˇze.”
The skin on my head sometimes ﬁlls with little blood.
interpolation “K˚uˇze na hlavˇe se mi prokrvuje tak akor´at”
The skin on my head ﬁlls with just the right amount of blood.
transformation “Obˇcas se mi na hlavˇe hodnˇe prokrvuje k˚uˇze”
The skin on my head sometimes ﬁlls with too much blood.
extrapolation “Nema´mˇza´dnou k˚uˇzi na hlavˇe”
There is no skin on my head.
Fig.1.Examplefromtheﬁrstroundofannotations.Theannotatorﬁlledtheinterpo-
lation and extrapolation to the seed and its transformation with opposite meaning.
Implied Sentence Comparison. In the second round of annotations, the annota-
tors are sorting sentence pairs. We however know that an interpolation is closer
in meaning or style to the seed sentence than its pre-existing transformation or
theextrapolation.Theseimpliedrelationsprovideuswithalmost7,000sentence
comparisons.
3.2 Second Round: Sentence Comparison
Again, we have manually chosen transformation categories to be compared. We
selected those that are linearly comparable, i.e. changes in tense (future, past),
changes in style (formal sentence, gossip, nonstandard sentence, simple sen-
tence) and signiﬁcant changes in meaning (generalization, opposite meaning).
We merged two categories (non-standard and gossip) because the actual sen-
tences in the collection often realized ‘gossipping’ via non-standard language
and vice versa.
The annotators were presented with a pair of sentences and criteria, how
to compare them.3 Of course, not always are the sentences comparable. Their
meaning might be either very close or very far from each other, both making
them hard to compare. For every pair of sentences S and S , the annotators
1 2
had the following four options:
1. S is more general/formal/in the past/non-standard/... than S .
1 2
2. S is more general/formal/in the past/non-standard/... than S
2 1
3. S and S are too similar, for example: “Byl rozˇcilen´y a hodnˇe mluvil.” (He
1 2
was upset and talked a lot.) and “Ovlivnˇen siln´ymi emocemi ˇr´ıkal ledacos.”
(Inﬂuenced by strong emotions, he said all kind of things.) are so close in
their meaning that it is almost impossible to select the more general one.
4. S and S too dissimilar, for example: neither of the sentences “Vˇsechno
1 2
zl´e je pro nˇeco dobr´e.” (Every cloud has a silver lining; lit. All bad is good
for something.) and “V Asii jsou r˚uzn´a obdob´ı.” (There are diﬀerent seasons
3 Only for opposite meaning the annotators were presented with three sentences: two
candidates and a source sentence. The annotators were then supposed to say which
of the candidates is closer to meaning of the source sentence.140 P. Baranˇc´ıkov´a and O. Bojar
in Asia.) is generalization of the other sentence, even though they both were
created as generalizations of the sentence “Banglad´eˇssk´a monzunov´a sezo´na
pˇrina´ˇs´ı radost, probl´emy i pozoruhodn´e fotograﬁe” (Bangladesh’s monsoon
season brings joy, problems and remarkable photos.)
Wecollectedmorethan25ksentencepairwisecomparisonsfrom7annotators.
Wecomputeinterandintra-annotatoragreementusingaveragepairwiseKohen’s
kappa[8].Thescoresaregenerallygood,notlowerthanothertypesoflinguistic
annotation. Our inter-annotator agreement is 0.62 (κ = 0.49) and our intra-
annotator agreement is 0.77 (κ=0.7).
4 Vector Evaluation
4.1 Sentence Comparison
We combine sentence comparisons obtained in the ﬁrst and second round of the
annotation. A pair of sentences can have multiple annotations in the collection.
We trust the annotation only if there is an option with the majority of votes.
We keep 16,385 sentence pairs with human comparison and 1,620 were dis-
regarded because of a disagreement in annotators’ judgments.
4.2 Sentence Evaluation
We evaluate sentence embeddings in 12 scales grouped into 6 classes for con-
ciseness. Two focus on transformations without an assumed linear scale behind:
basic:paraphrasesshouldbeclosertotheirseedthananytransformation,which
signiﬁcantlychangedthemeaningoftheseed(diﬀerentmeaning,nonsense,min-
imal change), modality: paraphrases should be closer to their seed than any
transformation, which changes modality of the seed (possibility, ban).
Theremainingfourclassesevaluatewhethersentencespacereﬂectstheorder-
ing implied by the collected comparisons: time (how often the mutual order-
ing of all transformation towards future matches the relative distances in the
embeddingspace;similarlybutseparatelyforpast),style(formalsentence,non-
standard sentence, simple sentence), generalization, and opposite.
For categories in the ﬁrst two classes, we compute the accuracy of sentence
embeddings, i.e., how often simC(vseed,vP) > simC(vseed,vT) for every para-
phrase P and every transformation T of the particular category and in the
examined sentence embeddings v•.
For categories in the latter four classes, the evaluation is based on collected
judgments. So if the annotators judge that sentences A, B and C satisfy A < B
andB<C,wetesthowoftensimC(vA,vB)>simC(vA,vC) and simC(vB,vC)>
simC(vA,vC). To make use of the options too similar and too dissimilar, we
check whether simC(vA,vB)>simC(vB,vC) for all sentences A, B, C where the
annotations indicate that A and B are too similar to each other and B and C
are too dissimilar.Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces 141
Table 1. Experimental results: geometric relations in sentence embedding spaces
Basic Modality Time Style Gener. Opposite Avg
SentBERT - mean 0.150 0.251 0.667 0.588 0.718 0.685 0.510
SentBERT - CLS 0.172 0.303 0.654 0.577 0.690 0.654 0.508
Flair - mean 0.145 0.157 0.682 0.627 0.695 0.728 0.506
mBERT - CLS 0.262 0.274 0.616 0.579 0.603 0.640 0.496
mBERT - mean 0.103 0.115 0.674 0.621 0.691 0.727 0.489
LASER 0.255 0.244 0.583 0.533 0.667 0.636 0.486
5 Results
As Table1 shows, none of the examined sentence embeddings are particularly
good in the basic requirement of paraphrases being embedded closer to each
other than sentences with a signiﬁcantly diﬀerent meaning. The best perform-
ing method mBERT-CLS reach the accuracy of 26%. This contrasts with [13],
which shows that LASER is particularly good at identifying related sentences
in Polish. However, we must emphasize that transformations in the basic class
werepurposefullyselectedtoposeadiﬃcultchallenge4 –onlyverysophisticated
embedding method can achieve high accuracy, which is precisely the purpose of
this testing dataset.
As one can expect, the ﬁrst two tasks turned out too hard for all BOW
embeddings that use mean to calculate the ﬁnal vector. On the other hand,
LASER and mBert-CLS perform surprisingly well with more than one-fourth of
paraphrases embedded close to their seeds.
The linearity of time, style, level of generality or the level of opposition
are reﬂected considerably better: 63–74% of tested sentence triples satisfy the
expectation. Mean-based embeddings (Flair and mBert in particular) achieve
the best performance in this evaluation of linear relations.
6 Conclusion
WepresentedanextensionofCOSTRA1.0,acorpusofsentencetransformations,
providingnewtransformations andrelations inordertoexaminetowhatextent
embeddingspacesreﬂectlinearorderingwithregardtocertainkindsofsentence
relationships.
We ﬁnd that paraphrases are often embedded too far from each other and
many meaning-altering transformations lie in a closer range. This conﬁrms that
the selected transformations are not easy to capture since all BOW methods
perform very poorly on them. The natural ordering of sentences with respect to
4 Diﬀerent meaning, nonsense and minimal change are all very similar in wording to
a seed sentence unlike its paraphrases, which must use diﬀerent words to express
similar meaning. For more details see [7].142 P. Baranˇc´ıkov´a and O. Bojar
time, style and level of generalization or opposition is embedded considerably
better.
Interestingly, the only directly learned sentence embedding LASER shows
on average the worst results from all tested methods. However, the diﬀerences
between all methods are very small.
Our hope is that Costra 1.1 will help to develop new better sentence embed-
ding for the Czech language. It is freely available at the following link:
http://hdl.handle.net/11234/1-3248
Easy-to-use Czech sentence embeddings quality evaluator is available here:
https://github.com/barancik/costra
References
1. Adi, Y., et al.: Fine-grained analysis of sentence embeddings using auxiliary pre-
diction tasks. CoRR abs/1608.04207 (2016)
2. Akbik, A., Blythe, D., Vollgraf, R.: Contextual string embeddings for sequence
labeling. In: COLING (2018)
3. Artetxe, M., Schwenk, H.: Massively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. CoRR abs/1812.10464 (2018)
4. Bacon,G.,Regier,T.:Probingsentenceembeddingsforstructure-dependenttense.
In: EMNLP BlackboxNLP (2018)
5. Bakarov, A.: A survey of word embeddings evaluation methods. CoRR
abs/1801.09536 (2018)
6. Baranˇc´ıkov´a, P., Bojar, O.: In search for linear relations in sentence embedding
spaces. In: ITAT SloNLP (2019)
7. Baranˇc´ıkov´a,P.,Bojar,O.:COSTRA1.0:adatasetofcomplexsentencetransfor-
mations. In: LREC (2020)
8. Carletta,J.:Assessingagreementonclassiﬁcationtasks:thekappastatistic.Com-
put. Linguist. 22(2), 249–254 (1996)
9. Conneau, A., et al.: XNLI: evaluating cross-lingual sentence representations. In:
EMNLP (2018)
10. Devlin, J., et al.: BERT: pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR abs/1810.04805 (2018)
11. Ettinger,A.,Elgohary,A.,Resnik,P.:Probingforsemanticevidenceofcomposition
by means of simple classiﬁcation tasks. In: ACL RepEval (2016)
12. Kiros, R., et al.: Skip-thought vectors. In: NIPS (2015)
13. Krasnowska-Kiera´s, K., Wr´oblewska, A.: Empirical linguistic study of sentence
embeddings. In: ACL (2019)
14. Linzen, T., Dupoux, E., Goldberg, Y.: Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. In: TACL, vol. 4 (2016)
15. Mikolov, T., Chen, K., Corrado, G.S., Dean, J.: Eﬃcient estimation of word rep-
resentations in vector space (2013)
16. Mikolov,T.,Yih,W.T.,Zweig,G.:Linguisticregularitiesincontinuousspaceword
representations. In: NAACL/HLT (2013)
17. Schnabel,T.,Labutov,I.,Mimno,D.,Joachims,T.:Evaluationmethodsforunsu-
pervised word embeddings. In: EMNLP (2015)Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces 143
18. Wieting, J., Bansal, M., Gimpel, K., Livescu, K.: Towards universal paraphrastic
sentence embeddings. CoRR abs/1511.08198 (2015)
19. Williams, A., Nangia, N., Bowman, S.R.: A broad-coverage challenge corpus for
sentence understanding through inference. In: NAACL-HLT (2018)
20. Zhu,X.,Li,T.,deMelo,G.:Exploringsemanticpropertiesofsentenceembeddings.
In: ACL (2018)
21. Zhu, X., de Melo, G.: Sentence analogies: exploring linguistic relationships and
regularities in sentence embeddings (2020)Next Step in Online Querying
and Visualization of Word-Formation
Networks
Jona´ˇs Vidra(B) and Zdenˇek Zˇabokrtsky´
Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics,
Charles University, Malostransk´e n´amˇest´ı 25, 11800 Prague 1, Czech Republic
{vidra,zabokrtsky}@ufal.mff.cuni.cz
Abstract. In this paper, we introduce a new and improved version of
DeriSearch,asearchengine andvisualizer for word-formationnetworks.
Word-formationnetworksaredatasetsthatexpressderivational,com-
pounding and other word-formation relations between words. They are
usuallyexpressedasdirectedgraphs,inwhichnodescorrespondtowords
and edges to the relations between them. Some networks also add other
linguistic information, such as morphological segmentation of the words
or identiﬁcation of the processes expressed by the relations.
Networks for morphologically rich languages with productive deriva-
tion or compounding have large connected components, which are dif-
ﬁcult to visualize. For example, in the network for Czech, DeriNet 2.0,
connected components over 500 words large contain 1⁄8 of the vocabu-
lary, including its most common parts. In the network for Latin, Word
FormationLatin,over10000words(1⁄3ofthevocabulary)areinasingle
connected component.
WiththerecentreleaseoftheUniversalDerivationscollectionofword-
formationnetworksforseverallanguages,thereisaneedforasearching
and visualization tool that would allow browsing such complex data.
· ·
Keywords: Derivational morphology Word formation Graph
·
visualization Search engine
1 Introduction
Aword-formationnetworkisadatasetcapturinginformationaboutderivational,
compounding, conversional and other processes, through which words can be
created. The networks come in many forms [4], but a typical one we focus on in
this paper is a directed graph structure in which the nodes represent individual
lexemes(generallyrepresentedbyalemma,whichstandsforasetofinﬂectional
forms) and edges represent the word-formation relations between them.
Both the lexemes and the relations can be further annotated in the dataset.
Lexemes are typically listed with their part of speech and morphological infor-
mation such as nominal gender or verbal conjugation paradigm, while relations
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.144–152,2020.
https://doi.org/10.1007/978-3-030-58323-1_15Querying and Visualization of Word-Formation Networks 145
Fig.1.Anexcerptfromaword-formationalfamilyofthewordvraˇzdit (“tomurder”),
as present in DeriNet 2.0 in UDer [5]. It shows morph segmentation (roots delimited
bydots),relationlabelsandcompounding.Renderingwasmadebythepresentedtool.
canbeannotatedwiththetypeoftherepresentedprocess(e.g.derivation,com-
pounding, conversion or clipping) or with the change occurring between the
related words (e.g. addition of the -er suﬃx). An example of a graph structure
from DeriNet 2.0, a word-formation network for Czech [14], is given in Fig.1.
A collection of 10 word-formation networks for 10 diﬀerent languages has
been published recently under the name Universal Derivations 0.5 (UDer) [5].
Severalresourcesinthecollectioncontainlargeandcomplexgraphsthataredif-
ﬁculttobrowseandvisualize without specializedtools. Forexample, thelargest
connected component from DeriNet 2.0 contains 56362 lexemes and over 12%
of the vocabulary is found in components over 500 lexemes large. Similarly, the
largest component in Word Formation Latin [6] contains 10514 lexemes.
Theaimofthisarticleistopresentasearchenginethatiscapableofprocess-
ing and visualizing the graphs in a user-friendly manner. A demo of the search
engine runs at https://quest.ms.mﬀ.cuni.cz/derisearch2/v2/databases/.
2 Related Work
Visualization and searching methods are useful when developing any linguistic
resources whose annotations specify structured data, and word-formation net-
works are no exception. A good visualization tool, which allows its users to gain
insights into the structure and ﬁnd interesting phenomena in both the annota-
tions and the raw data, helps prevent annotation errors and inconsistencies and
discover more annotation possibilities. Such tools are in use for a long time in
syntactic analysis or in Wordnet creation.
2.1 Comparison with Visualization of Syntactic Trees
Both the syntactic annotation and the word-formation annotation are graph-
based in nature. The practical diﬀerence is the size of the trees. The number of146 J. Vidra and Z. Zˇabokrtsky´
nodes in syntactic trees corresponds to the number of tokens in the annotated
sentence-verycloselyfordependencytrees,butevenintheoriesthathavemany
nonterminalnodesinthestructure,thenumberofnonterminalsgenerallydoesn’t
grow too much. Although sentences can be arbitrarily long, typical ones are
usually shorter than a couple dozen tokens. Longer sentences, whether created
withartist’sintentorasaresultofstylisticclumsiness,areatypical.Incontrast,
languageswithrichderivation,suchasRussianorCzech,havemanyderivational
families that contain several thousands of lexemes.
Forexample,manytoponymsinCzechhavecorrespondingcompoundadjec-
tives with north-, east-, south- and west-, such as jihoamerick´y (“South Ameri-
can”)orza´padolond´ynsk´y (“ofwestLondon”),whichmakesthemallmembersof
a single huge connected component through the words jih (“the south”), za´pad
(“the west”) etc. This diﬀerence in sizes requires diﬀerent approaches to visual-
ization and therefore diﬀerent tools.
2.2 Comparison with Visualization of Wordnets
Wordnet creation is another ﬁeld similar to word-formation networks, perhaps
more so than syntactic analysis. The authors of various wordnets have, too,
created tools to support browsing and searching their datasets.
One example is the VisDic/DEBVisDic/DEBGrid family of tools [3]. Here,
we can again see diﬀerences in requirements - word-formation networks have a
biggerfocusontheoverallstructureandconnectionsbetweenthelexemes,while
DEBVisDic was created to support dictionary editing and browsing (hence the
DEB in the name), which focuses more on the individual dictionary entries. As
a result, for a long time, the tool only visually showed the tree of the hypero-
and hyponyms, other relations had to be browsed a single node at a time, with
hyperlinks pointing to the node’s neighbors. A way of showing the structure of
the synsets as a graph was added recently [9], but even this visualization only
shows neighboring synsets and their member words, not transitive relations to
further synsets and members.
2.3 Visualizers of Word-Formation Networks
As noted by creators of the Word Formation Latin resource a few years ago,
visualization toolsforword-formation networksarestillintheirinfancy[2].The
six searching and visualization tools from ﬁve projects, which we list below in
alphabetical order, are known to us:
ThecreatorsofCroDeriV[10]haveapublicsearchenginethatallowslooking
a word up by its morphological composition, for example, by its root, part of
speech or preﬁxes, and an unpublished version that also shows its derivational
family as a graph. It doesn’t allow searching by lemma.
DeriNet[14]hastwodiﬀerentbrowsersandvisualizers.Bothcanonlyprocess
and display trees, not more complex graphs with e.g. compounding. DeriNet
Viewerismorelimitedinitssearchingabilities,onlyallowingbrowsingbylemma.
It has two modes: Either a display of a complete derivational tree for a givenQuerying and Visualization of Word-Formation Networks 147
size, or statistical overviews of tree shapes. DeriSearch allows searching by any
conjunction of regular expressions over lemmas and parts of speech, but doesn’t
allow querying other attributes [12]. It is also able to search for structures,
allowingquerieslike“allnounsendingin-t´ı derivedfromverbs”.Itsvisualization
capabilities include several graph layouts optimized for large trees [13].
AnothersearchenginewascreatedforanotherCzechresource,Derivancze[7].
It allows searching by lemma and shows the immediate derivational neighbors
ofthefoundword,annotatedwiththeirlogarithmicfrequencyinacorpusanda
tagspecifyingthetypeofrelationbetweenthefoundwordandtheneighbor.The
neighbors are hyperlinked, allowing the user to explore the whole derivational
family one word at a time by clicking on the links.
A basic search engine is provided by the DerIvaTario project [11]. It allows
queryingbypart-of-speechandbyaﬃxandvariousaﬃxalproperties,butnotby
wordformorlemma.Theoutputisatextuallistingofthematchinglexemeswith
their properties and base words, and tables of statistics of the word-formational
processes visible in the set of results. Since the base words are not hyperlinked
and derived words are not listed, browsing the dataset as a graph is diﬃcult.
The last search engine known to us was made for the Word Formation Latin
project[6].Thesearchengine[2]allowslookupbylemma(usingaregularexpres-
sion), part of speech, aﬃx (chosen from a list) and word-formational rule. The
results can be further trimmed down: for example, when looking a word up by
lemma, one can view only derivational roots (words without a base word) or
only derived words, and in aﬃx search, include or exclude words that contain
the aﬃx internally, not as the last word-formational step. Visualization is done
using graphs and the user can choose whether to only show the derivational
family, or compounding relations as well.
ThetoolweintroduceinthispaperisareimplementationofDeriSearch,with
an extended query language and improved visualizations aimed at large graphs.
3 Query Language
DeriSearch uses a query language called DCQL [12]. As the name suggests, it is
based on the Corpus Query Language (CQL) used by Manatee, SketchEngine
and other text corpora query engines and descended from a common root in
theIMSCorpusWorkbench[1].TheoriginalDCQLusedinpreviousversionsof
DeriSearch(calledDCQL-1forthepurposesofthispaper)hadtobeextendedto
support querying the kinds of information not found in DeriNet versions up to
1.7, such as compounding, morpheme segmentation or relation labels. In the
following two sections, we ﬁrst give a tutorial on DCQL-1 and then introduce
our extensions as DCQL-2.
3.1 Original Query Language: DCQL-1
The DCQL-1 query language was designed with accessibility to new users in
mind. You can search for a lexeme by typing its lemma into the search box, no148 J. Vidra and Z. Zˇabokrtsky´
syntaxneeded.Theresultshowsyouanexcerptofthelexeme’sword-formational
family, with the lexeme itself highlighted. More complex queries need syntactic
constructions, which are expressed in a language similar to CQL. An overview
of the syntax and a discussion of its limits are given below.
Individual lexemes are queried using their attributes. In DCQL-1, these are
the “lemma”, “pos” (the part-of-speech tag) and several resource-speciﬁc ones,
such as “techlemma” used for storing bits of extra morphological and seman-
tic annotations in the DeriNet network. A bare-word query means exact string
matching against the “default attribute” (selectable in the graphical user inter-
face, defaulting to lemma).
To query other attributes, or several of them at once, a square bracket nota-
tion familiar from CQL, “[attribute="regex"]”, is used. It searches for all
lexemes in which the value of “attribute” matches the regular expression
“regex”. Multiple queries in conjunction can be connected with “and”. For
example, to search for all nouns ending in -er, one would write “[pos="NOUN"
and lemma=".*er"]”. As a special case, the empty brackets “[]” match any
lexeme.
DCQL-1 only allows to specify each attribute at most once and connect the
individual properties with logical AND. This makes some queries more awk-
ward to express - e.g. a query for all nouns or adjectives must be expressed as
“[pos="NOUN|ADJ"]” instead of the, perhaps more natural, “[pos="NOUN" or
pos="ADJ"]”-andotherqueriesinexpressible,suchas“allnounsendingin-ater
or verbs ending in -at”. Approximating this query as “[pos="NOUN|VERB" and
lemma=".*ater|.*at"]” would erroneously include e.g. the noun seat.
Derivational structurecanbequeriedbyassemblingatree-shapedqueryout
ofindividuallexemequeriesusingconcatenation,similartohowonesearchesfor
consecutive words in CQL, and parentheses with commas. Neighboring lexeme
queries correspond to base-derivative relations between the lexemes. For exam-
ple, to search for lexeme B derived from lexeme A, one would write “A B”, e.g.
“[pos="VERB"] [lemma=".*er"]” (“words ending in -er derived from a verb”).
Several derivatives at the same level of the tree are written in parentheses sep-
arated by commas, e.g. “[pos="VERB"] ([pos="NOUN"], [pos="NOUN"])” (“a
verb with at least two nouns immediately derived from it”).
DCQL-1 limits queries to tree-shaped ones, because it was designed for use
with DeriNet 0.9, which only contained tree-shaped derivational families. Other
networksandnewerversionsofDeriNetcancontainnon-tree-shapedsubgraphs.
3.2 Extended DCQL Language: DCQL-2
To support the increased variety and complexity found in the UDer collection,
we extended the DCQL-1 language in several ways, described below. We refer
to the extended language as DCQL-2
We extended the set of queryable attributes of lexemes by all attributes
stored in UDer databases, such as nominal gender (for Czech, French, German
and Latin), verbal aspect (for Czech) or nominal declension (for Latin). AnyQuerying and Visualization of Word-Formation Networks 149
attributes present in the resources can be queried by string comparison and
regularexpressionswithout requiringspeciﬁccodesupportinthesearchengine.
A second extension is the ability to query relations. The UDer collection
has introduced relations as an explicit object in the database, which can carry
annotation such as semantic labels or word-formation process type. The query
syntax is similar to lexeme queries, but with angle brackets instead of square
ones. The relation query must occur between two lexeme queries, like this: “[]
<SemanticLabel="Possessive"> []” (“all derived possessives”).
A third extension is the ability to query general contiguous graphs instead
of just trees. To facilitate this, we introduced labeled nodes with references and
allowed the user to search for several interlinked trees at once by coindexing
the nodes. The user can label an arbitrary node by prepending a user-selected
textual label and a colon before the node or relation deﬁnition.
Nodes and relations are coindexed if they are labeled by the same string. A
node can’t be coindexed with a relation and labeling a relation identically as a
node is an error. Coindexed nodes or relations only match if they can point to
thesamelexemeorrelationinthesearchresults.Forexample,thequery“a: []
[] a: []”matchesallnodes(labeleda)thatareinabi-directionalrelationwith
anothernode(themiddleunlabeledone).Thiscurrentlydoesnothappeninany
datasetfromUDer,butitcanoccurwhentwolexemesarederivationallyrelated,
but the direction of derivation is unclear, as with some neoclassical formations
[8], and the annotators decide to include both directions of derivation in the
dataset. When coindexing nodes with constraints in them, the resulting query
must match all of them, as if they were connected by logical “and”.
Notalldirectedstructurescanbedescribedbyasingletreethatloopsbackon
itself using labels. For example, querying compounding requires specifying two
independentparentsthatneedn’tbeconnectedbyacycle.Anygeneralgraphcan
be, however, described as a union of several directed trees. The user can specify
suchaunionbyconnectingseveraltreequerieswith“&”,e.g.“[lemma=="week"]
weekday:[lemma=="weekday"] & [lemma=="day"] weekday:[]”.
A ﬁnal extension is the ability to query morphological segmentation under
the attribute “morfeman”. This is done with regular expressions over a string
representing the segmentation using vertical bars and colons. Each segment
is speciﬁed as morph:morpheme:type, where type is R for root, P for pre-
ﬁx, S for suﬃx, I for inﬁx, X for interﬁx and U for unknown. The seg-
ments are separated by vertical bars, with extra bars at the start and end of
the whole string. For example, the word revalidation could be expressed as
“|re:re:P|valid:valid:R|at:ate:S|ion:ion:S|”.
We’ve also made a single backwards-incompatible change: The meaning of
regularexpressionswaschangedtomatchthewholestring,insteadofasubstring.
Thismeansthate.g.theexpression“"er$"”fromDCQL-1shouldbewrittenas
“".*er"” in DCQL-2. The change makes the search engine behavior identical
to the behavior of popular corpus managers using CQL. All examples in this
paperaregiveninthenewformat.Otherwise,theregularexpressionsofDCQL-
2 conform to the speciﬁcation of Java’s util.regex.Pattern.150 J. Vidra and Z. Zˇabokrtsky´
4 Visualization of Results
SinceDCQL-1workedwithtree-shapedderivationaldataonly,thesearchengine
could use algorithms for display of trees that produce tidy, readable layout.
It allowed the user to choose from four diﬀerent ways of displaying the trees
[13]. DCQL-2 can be used to search arbitrary graphs, limiting our choice of
visualization methods. Therefore, we decided to use a force-directed 2D physics
simulation,inwhichword-formationlinkscorrespondtospringsofpresetlength
and stiﬀness and lexemes repel one another with an electromagnetic force.
The challenge with large connected components found in recent versions of
several word-formation networks is that we want to allow users to explore the
whole component at their leisure, but at the same time focus at the results of
their query. With huge graphs, this means showing only the part of the graph
with the result, but allowing the user to view other parts on demand easily.
When there are multiple results in a single connected component, which can
easily happen with general queries such as “all adjectives derived from words
ending in -at”, we have several options:
1. unroll enough of the cluster to show them all at the same time,
2. show a limited selection, possibly indicating that there are more results that
are hidden and giving the user an option to view them on demand,
3. display multiple independent views of the same cluster, each focusing on a
diﬀerent part.
Each of these options has its drawbacks and each can be confusing in some
situations.Option1leadstotoomuchdatabeingdisplayedatonce,particularly
for queries such as “[]” (“all lexemes”). With option 2, it can become unclear
howmanyresultsthereareandhowtoseethem.Withverydeepclusters,giving
the user an option to quickly show enough of the cluster to see multiple results
reverts to option 1. Option 3 is overwhelming in another way - for queries such
as “[morfeman=".*\|slav:[^:]*:R\|.*"]” (“words with root morph ‘slav’”)
it shows many result panels, all showing the same cluster, just focusing on a
diﬀerent part.
Inthecurrentimplementation,wechosemethod2,withtheoptiontohideor
showneighborsofarbitrarylexemesbyclickingonthem.Bydefault,weshowall
nodes up to the root of the derivational family of the result and the immediate
neighbors of nodes in the result.
The notions of “neighbors” and “root of derivational family” are, however,
somewhatcomplicatedhere.Theresultmaycontainnodesbelongingtomultiple
families,andsimplyhidingthechildrenofalexemeonclickisnotsuﬃcient,nor
istransitivelyshowingalltheparentsbydefault,becausethegraphsmaycontain
cycles and in those cases it is unclear what to hide, and the default view may
havetoomanynodesvisible.Therefore,weusethefollowingalgorithm:Wepick
one of the nodes of the selected result as its anchor. For results that happen to
be trees, this is its root; otherwise the selection is arbitrary. We consider the
result to belong to the derivational family given by this anchor, and on click,
we show or hide nodes further away from the anchor than the clicked node.Querying and Visualization of Word-Formation Networks 151
By doing all operations relative to a single node, they become well deﬁned,
because the graph can be converted to a tree via breadth-ﬁrst search from this
node.
5 Conclusions
We presented a new search engine and visualizer for word-formation networks.
It extends the query language of an existing tool to support querying non-tree
structuresandattributesfoundinmanyexistingresources,andaddsanimproved
visualization that allows for browsing even very large graphs.
Acknowledgments. This work was supported by the Grant No. GA19-14534S of
the Czech Science Foundation, by the Charles University Grant Agency project No.
1176219 and by the SVV project No. 260 575. It uses language resources devel-
oped, stored, and distributed by the LINDAT/CLARIAH CZ project (LM2015071,
LM2018101).
References
1. Christ, O., Schulze, B.M., Hofmann, A., K¨onig, E.: The IMS Corpus Workbench:
CorpusQuery Processor(CQP)User’sManual. UniversityofStuttgart,Germany
(1999)
2. Culy, C., Litta, E., Passarotti, M.: Visual exploration of Latin derivational mor-
phology. In: Proceedings of FLAIRS 2017, pp. 601–606 (2017)
3. Hora´k, A., Pala, K., Rambousek, A., Povolny´, M.: DEBVisDic - ﬁrst version of
newclient-serverWordnetbrowsingandeditingtool.In:ProceedingsoftheThird
International WordNet Conference (GWC 2006), pp. 325–328 (2005)
4. Kyj´anek, L.: Morphological resources of derivational word-formation relations.
Technical report U´FAL TR-2018-61, U´FAL MFF UK, Prague, Czechia (2018)
5. Kyj´anek,L.,Zˇabokrtsky´,Z.,Sˇevˇc´ıkov´a,M.,Vidra,J.:Universalderivationskickoﬀ:
acollectionofharmonizedderivationalresourcesforelevenlanguages.In:Proceed-
ings of DeriMo 2019, Prague, Czechia, pp. 101–110 (2019)
6. Litta,E.,Passarotti,M.,Culy,C.:Formatioformosaest.Buildingawordformation
lexicon for Latin. In: Proceedings of CLiC-IT 2016, pp. 185–189 (2016)
7. Pala, K., Sˇmerk, P.: Derivancze — derivational analyzer of Czech. In: Kr´al, P.,
Matouˇsek, V. (eds.) TSD 2015. LNCS (LNAI), vol. 9302, pp. 515–523. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-24033-6 58
8. Panocov´a, R.: Internationalisms with the suﬃx -´acia and their adaptation in Slo-
vak. In: Proceedings of DeriMo 2017, Milano, Italy, pp. 129–139 (2017)
9. Rambousek,A.,Hor´ak,A.,Klement,D.,Kleteˇcka,J.:NewfeaturesinDEBVisDic
for WordNet visualization and user feedback. In: Proceedings of RASLAN 2017
(2017)
10. Sˇojat, K., Srebaˇci´c, M., Tadi´c, M., Paveli´c, T.: CroDeriV: a new resource for pro-
cessing Croatian morphology. In: Proceedings of LREC 2014 (2014)
11. Talamo, L., Celata, C., Bertinetto, P.M.: DerIvaTario: an annotated lexicon of
Italian derivatives. Word Struct. 9(1), 72–102 (2016)
12. Vidra,J.:ImplementationofasearchengineforDeriNet.In:ProceedingsofITAT
2015, Prague, Czechia, pp. 100–106 (2015)152 J. Vidra and Z. Zˇabokrtsky´
13. Vidra, J., Zˇabokrtsky´, Z.: Online software components for accessing derivational
networks. In: Proceedings of DeriMo 2017, Milano, Italy, pp. 129–139 (2017)
14. Vidra,J.,Zˇabokrtsky´,Z.,Sˇevˇc´ıkov´a,M.,Kyj´anek,L.:Derinet2.0:towardsanall-
in-one word-formation resource. In: Proceedings of DeriMo 2019, Prague, Czechia
(2019)Evaluating a Multi-sense Deﬁnition
Generation Model for Multiple Languages
B
Arman Kabiri( ) and Paul Cook
Faculty of Computer Science, University of New Brunswick,
Fredericton, NB E3B 5A3, Canada
{arman.kabiri,paul.cook}@unb.ca
Abstract. Most prior work on deﬁnition modelling has not accounted
for polysemy, or has done so by considering deﬁnition modelling for a
target word in a given context. In contrast, in this study, we propose a
context-agnostic approach to deﬁnition modelling, based on multi-sense
wordembeddings,thatiscapableofgeneratingmultipledeﬁnitionsfora
targetword.Infurthercontrasttomostpriorwork,whichhasprimarily
focused on English, we evaluate our proposed approach on ﬁfteen dif-
ferent datasets covering nine languages from several language families.
To evaluate our approach we consider several variations of BLEU. Our
results demonstrate that our proposed multi-sense model outperforms a
single-sense model on all ﬁfteen datasets.
· ·
Keywords: Deﬁnition modelling Multi-sense embeddings Polysemy
1 Introduction
The advent of pre-trained distributed word representations, such as [12], led to
improvements in a wide range of natural language processing (NLP) tasks. One
limitationofsuchwordembeddings,however,isthattheyconﬂateallofaword’s
senses into a single vector. Subsequent work has considered approaches to learn
multi-senseembeddings,inwhichawordisrepresentedbymultiplevectors,each
correspondingtoasense[3,10].Morerecentworkhasconsideredcontextualized
word embeddings, such as [5], which provide a representation of the meaning of
a word in a given context.
Deﬁnitionmodelling,recentlyintroducedby[16],isaspeciﬁctypeoflanguage
modelling which aims to generate dictionary-style deﬁnitions for a given word.
Deﬁnitionmodellingcanprovideatransparentinterpretationoftheinformation
representedinwordembeddings,andhasthepotentialtobeappliedtogenerate
deﬁnitions for newly-emerged words that are not yet recorded in dictionaries.
The approach to deﬁnition modelling of [16] is based on a recurrent neural
network (RNN) language model, which is conditioned on a word embedding for
thetargetwordtobedeﬁned,speciﬁcallypre-trainedword2vec[12]embeddings.
As such, this model does not account for polysemy. To address this limitation,
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.153–161,2020.
https://doi.org/10.1007/978-3-030-58323-1_16154 A. Kabiri and P. Cook
a number of studies have proposed context-aware deﬁnition generation mod-
els [4,7,9,11,15]. In all of these approaches, the models generate a deﬁnition
corresponding to the usage of a given target word in a given context.
In contrast, in this paper we propose a context-agnostic multi-sense deﬁni-
tion generation model. Given a target word type (i.e., without its usage in a
speciﬁc context) the proposed model generates multiple deﬁnitions correspond-
ing to diﬀerent senses of that word. Our proposed model is an extension of [16]
that incorporates pre-trained multi-sense embeddings. As such, the deﬁnitions
that are generated are based on the senses learned by the embedding model on
a background corpus, and reﬂect the usage of words in that corpus. Under this
setup—i.e.,generatingmultipledeﬁnitionsforeachwordcorrespondingtosenses
present in a corpus—the proposed deﬁnition generation model has the poten-
tial to generate partial dictionary entries. In order to train the proposed model,
pre-trained sense vectors for a word need to be matched to reference deﬁnitions
for that word. We consider two approaches to this matching based on cosine
similarity between sense vectors and reference deﬁnitions.
Recently, [20] propose a multi-sense model for generating deﬁnitions for the
varioussensesofatargetword.Thismodelutilizeswordembeddingsandcoarse-
grained atom embeddings to represent senses [1], in which atoms are shared
across words. In contrast, we only rely on ﬁne-grained multi-sense embeddings.
To match sense vectors to reference deﬁnitions during training, [20] propose a
neuralapproach,andalsoconsideraheuristic-basedapproachthatincorporates
cosine similarity between senses and deﬁnitions. Our proposed approach to this
matching is similar to their heuristic-based approach, although we explore two
variationsofthismethod.Furthermore,[20]onlyconsiderEnglishforevaluation,
whereas we consider ﬁfteen datasets covering nine languages.
Following[20]weevaluateourproposedmodelusingvariationsofBLEU[17].
We evaluate our model on ﬁfteen datasets covering nine languages from several
families. Our experimental results show that, for every language and dataset
considered,ourproposedapproachoutperformsthebenchmarkapproachof[16]
which does not model polysemy.
2 Proposed Model
Here we brieﬂy describe the model of [16], referred to as the base model, and
then present our proposed multi-sense model which builds on the base model.
ThebasemodelisanRNN-basedlanguagemodelwhich,givenatargetword
tobedeﬁned(w∗),predictsthetargetword’sdeﬁnition(D =[w1,...,wT]).The
probability of thetthword of the deﬁnition sequence,wt, is calculated based on
the previous words in the deﬁnition as well as the word being deﬁned, as shown
in Eq.1.
(cid:2)T
P(D|w∗)= p(wt|w1,...,wt−1,w∗) (1)
t=1Evaluating a Multi-sense Deﬁnition Generation Model 155
The probability distribution is estimated by a softmax function. The model
further incorporates a character-level CNN to capture knowledge of aﬃxes. A
full explanation of this model is in [16].
In the base model, the target word being deﬁned (w∗) is represented by
its word2vec word embedding. This reliance on single-sense embeddings limits
the model’s ability to generate deﬁnitions for diﬀerent senses of polysemous
target words. To address this limitation, we propose to extend the base model
by incorporating multi-sense embeddings, in which each word is represented by
multiplevectorswhichcorrespondtodiﬀerentmeaningsorsensesforthatword.
Speciﬁcally, we replace w∗ in Eq.1 by a sense of the target word, represented as
a sense vector.
Most prior work on deﬁnition modelling has considered polysemy through
context-aware approaches [4,7,9,11,15] that require an example of the target
word in context for deﬁnition generation. In contrast, the model we propose is
contextagnostic(asisthebasemodel)andisabletogeneratemultipledeﬁnitions
for a target word without requiring that speciﬁc contexts of the target word be
given in order to generate deﬁnitions.
The base model is trained on instances consisting of pairs of a word—
representedbyaword2vecembedding—andoneofitsdeﬁnitions,i.e.,fromadic-
tionary.Ourproposedapproachistrainedonpairsofawordsense—represented
as a sense vector—and one of the corresponding word’s deﬁnitions. In order to
train our proposed approach, we require a way to associate pre-trained sense
vectors with dictionary deﬁnitions, where the number of sense vectors and deﬁ-
nitions is often diﬀerent for a given word.
We consider two approaches to associating sense vectors with deﬁnitions:
deﬁnition-to-sense and sense-to-deﬁnition. For both approaches we require a
representation of deﬁnitions. We represent a deﬁnition as the average of its
wordembeddings,afterremovingstopwords.Foreachwordinthetrainingdata,
we then calculate the pairwise cosine similarity between its sense vectors and
deﬁnitions. For deﬁnition-to-sense, each deﬁnition is associated with the most
similar sense vector for the corresponding word. For sense-to-deﬁnition, on the
other hand, each sense is associated with the most similar deﬁnition. For both
approaches, the selected sense–deﬁnition pairs form the training data.
These approaches to pairing senses and deﬁnitions are only used to create
training instances. At test time, to generate deﬁnitions for a given target word,
each sense vector for the target word is fed to the deﬁnition generation model,
which then generates one deﬁnition for each of the target word’s sense vectors.
3 Materials and Methods
In this section, we describe the datasets, word and sense embeddings, and eval-
uation metrics used in our experiments.156 A. Kabiri and P. Cook
Table 1. The number of words, and proportion of polysemous words (PPW) in each
dataset.
Language Omega Wiktionary WordNet
#Words PPW #Words PPW #Words PPW
Dutch 13093 0.18 – – – –
English 17000 0.20 17000 0.27 20000 0.18
French 15869 0.17 20000 0.26 – –
German 13338 0.12 16000 0.26 – –
Greek – – – – 11517 0.26
Italian 18351 0.21 – – 16290 0.22
Japanese – – – – 20000 0.30
Russian – – 15000 0.17 – –
Spanish 17000 0.19 – – 18934 0.12
3.1 Datasets
Inthiswork,weconductamulti-lingualstudyofdeﬁnitionmodelling.Weextract
monolingual dictionaries for nine languages covering several language families,
from three diﬀerent sources: Wiktionary,1 OmegaWiki,2 and WordNet [13].
Wiktionary is a free collaboratively-constructed online dictionary for many
languages.ThestructureofWiktionarypagesisnotconsistentacrosslanguages.
Extracting word–deﬁnitions pairs from Wiktionary pages for a given language
requires a carefully-designed language-speciﬁc parser, which moreover requires
some knowledge of that language to build. We therefore use publicly-available
Wiktionary parsers. We use WikiParsec for English, French, and German,3 and
Wikokit for Russian,4 to extract word–deﬁnition pairs for these languages.
OmegaWiki, like Wiktionary, is a free collaborative multilingual dictionary.
In OmegaWiki data is stored in a relational database, and so language-speciﬁc
parsers are not required to automatically extract words and deﬁnitions. We
extract the word–deﬁnition pairs from OmegaWiki for English, Dutch, French,
German, Italian, and Spanish—the six languages with the largest vocabulary
size in OmegaWiki—using the BabelNet Java API [14].
Finally, we consider WordNets. We only use WordNets for which the words
and deﬁnitions are in the same language. We again use the BabelNet Java API
to extract the word–deﬁnition entries from English [13], Italian [2], and Spanish
[6] WordNets. We separately extract word–deﬁnition pairs from Greek [19] and
Japanese [8] WordNets.
Properties of the extracted datasets are shown in Table1. Each dataset is
partitioned into train (80%), dev (10%), and test (10%) sets. We ensure that,
1 https://en.wiktionary.org.
2 http://www.omegawiki.org.
3 https://github.com/LuminosoInsight/wikiparsec.
4 https://github.com/componavt/wikokit.Evaluating a Multi-sense Deﬁnition Generation Model 157
for each word in each dataset, all of its deﬁnitions are included in only one of
thetrain,dev,ortestsets,sothatmodelsareonlyevaluatedonwordsthatwere
not seen during training.
3.2 Word and Sense Embeddings
Following[16],weuseword2vecembeddingsinthesinge-sensedeﬁnitiongenera-
tionmodel(i.e.,thebasemodel).Fortheproposedmulti-sensemodels,weutilize
AdaGram embeddings [3]. AdaGram is a non-parametric Bayesian extension of
Skip-gram which learns a variable number of sense vectors for each word, unlike
many multi-sense embedding models which learn a ﬁxed number of senses for
every word. Note that although here we use AdaGram, any multi-sense embed-
ding method could potentially be used.5
For each language, word2vec and AdaGram embeddings are trained on the
most recent Wikipedia dumps as of January 2020.6 We extract plain text from
these dumps, and then pre-process and tokenize the corpora using tools from
AdaGram,7 modiﬁed for multilingual support, except in the case of Japanese
where we use the Mecab tokenizer.8 The resulting corpora range in size from
roughly 86 million tokens for Greek to 3.7 billion tokens for English. The same
pre-processing and tokenization is also applied to the datasets of words and
deﬁnitions extracted from dictionaries.
Wetrainword2vecembeddingsusingGensim[18]withitsdefaultparameters.
We also use the default parameter settings for AdaGram. To obtain represen-
tations for words, as opposed to senses, from AdaGram sense embeddings, as
required to form representations for deﬁnitions (Sect.2), we take the most fre-
quentsensevectorofeachword(asindicatedbyAdagram)astherepresentation
of the word itself.
3.3 Evaluation Metrics
BLEU [17] has been widely used for evaluation in prior work on deﬁnition mod-
elling [9,15,16]. BLEU is a precision-based metric that measures the overlap of
a generated sequence (here a deﬁnition) with respect to one or more references.
Formulti-sensemodels,wecalculateBLEUastheaverageBLEUscoreovereach
generated deﬁnition.
While BLEU is appropriate for evaluation of single-sense deﬁnition gener-
ation models, it does not capture the ability of a model to produce multiple
deﬁnitions corresponding to diﬀerent senses of a polysemous word. We there-
fore also consider a recall-based variation of BLEU, known as rBLEU, in which
the generated and reference deﬁnitions are swapped [20], i.e., the overlap of a
5 InpreliminaryexperimentswithMUSEembeddings[10]wefoundMUSEtoperform
poorly compared to AdaGram, and so only report results for AdaGram here.
6 https://dumps.wikimedia.org.
7 https://github.com/sbos/AdaGram.jl/blob/master/utils/tokenize.sh.
8 https://github.com/jordwest/mecab-docs-en.158 A. Kabiri and P. Cook
reference deﬁnition is measured with respect to the generated deﬁnition(s). For
each target word, we calculate rBLEU as the average rBLEU score for each of
its reference deﬁnitions (for both single and multi-sense models).
Inadditiontoprecision-basedBLEU,andrecall-basedrBLEU,wereportthe
harmonic mean of BLEU and rBLEU, referred to as fBLEU.
Table 2. BLEU, rBLEU, and fBLEU for the single-sense deﬁnition generation model
(base) and the proposed multi-sense models using sense-to-deﬁnition (S2D) and
deﬁnition-to-sense (D2S) for each dataset. The best result for each evaluation met-
ric and dataset is shown in boldface.
Lang. Model OmegaWiki Wiktionary WordNet
BLEU rBLEU fBLEU BLEU rBLEU fBLEU BLEU rBLEU fBLEU
DE Base 12.12 11.55 11.83 11.35 08.80 09.91 – – –
S2D 12.43 16.26 14.09 15.00 15.82 15.40 – – –
D2S 12.44 16.83 14.31 14.07 16.54 15.21 – – –
EL Base – – – – – – 13.21 12.06 12.61
S2D – – – – – – 12.44 12.85 12.64
D2S – – – – – – 13.08 13.63 13.35
EN Base 14.74 14.32 14.53 20.21 16.88 18.40 13.78 12.77 13.26
S2D 14.23 16.02 15.07 18.88 16.99 17.89 12.85 13.09 12.97
D2S 15.22 17.80 16.41 21.49 19.78 20.60 13.84 14.84 14.32
ES Base 17.68 17.70 17.69 – – – 26.46 24.69 25.54
S2D 16.52 19.00 17.67 – – – 25.80 28.14 26.92
D2S 17.54 20.28 18.81 – – – 25.68 27.97 26.78
FR Base 12.58 12.66 12.62 63.48 59.87 61.62 – – –
S2D 11.70 14.26 12.85 63.56 60.00 61.73 – – –
D2S 11.94 14.82 13.23 64.12 60.41 62.21 – – –
IT Base 12.29 11.93 12.11 – – – 21.33 20.65 20.98
S2D 11.43 13.61 12.43 – – – 20.35 23.67 21.88
D2S 11.74 13.95 12.75 – – – 21.96 25.10 23.43
JA Base – – – – – – 10.13 08.50 09.24
S2D – – – – – – 11.53 11.96 11.74
D2S – – – – – – 09.42 09.37 09.39
NL Base 14.37 14.04 14.20 – – – – – –
S2D 13.49 15.88 14.59 – – – – – –
D2S 14.46 17.07 15.66 – – – – – –
RU Base – – – 47.04 46.04 46.53 – – –
S2D – – – 46.24 46.69 46.46 – – –
D2S – – – 47.52 48.09 47.80 – – –Evaluating a Multi-sense Deﬁnition Generation Model 159
4 Results
In this section, we present experimental results comparing the proposed multi-
sense deﬁnition generation models against the single-sense base model [16]. All
modelsaretrainedusingparametersettingsfrom[16],i.e.,atwo-layerLSTMas
the RNN component with 300 units in each level; a character-level CNN with
kernels of length 2–6 and size {10,30,40,40,40} with a stride of 1; and Adam
optimization with a learning rate of 0.001.
To generate deﬁnitions at test time, for each word and sense for the single-
sense and multi-sense models, respectively, we sample tokens at each time step
from the predicted probability distribution with a temperature of 0.1. We com-
pute BLEU, rBLEU, and fBLEU for each word, and then the average of these
measuresoverallwordsinadataset.Werepeatthisprocess10times,andreport
the average scores over these 10 runs.
ResultsareshowninTable2.FocusingonfBLEU,foreverydataset,thebest
results are obtained using a multi-sense model—i.e., sense-to-deﬁnition (S2D),
ordeﬁnition-to-sense(D2S).Moreover,foreverydataset,D2Simprovesoverthe
base model. These results show that deﬁnition modelling can be improved by
accounting for polysemy through the incorporation of multi-sense embeddings.
Toqualitativelycomparethebasemodelandtheproposedmodel,weconsider
the deﬁnitions generated for the word state. The following three deﬁnitions are
generated for this word by the base model: (1) a state of a government, (2) to
make a certain or permanent power,and(3)to make a certain or administrative
power. In contrast, the proposed multi-sense model using D2S generates the
following three deﬁnitions, which appear to capture a wider range of the usages
ofthewordstate:(1)a place of government,(2)a particular region of a country,
and (3) a particular place of time.
Comparing S2D and D2S in terms of fBLEU, we observe that D2S often
performs better. The number of sense vectors learned by Adagram for a given
word is on average higher than the number of reference deﬁnitions available for
that word, for every dataset. We hypothesize that the poor performance of S2D
relative to D2S could therefore be due to sense vectors being associated with
inappropriate deﬁnitions.
rBLEU is a recall-based evaluation metric that indicates the extent to which
the reference deﬁnitions are covered by the generated deﬁnitions. A multi-sense
deﬁnition generation model—which produces multiple deﬁnitions for a target
word—is therefore particularly advantaged compared to a single-sense model—
such as the base model—which produces only one, with respect to this met-
ric. Indeed, we see that for every dataset, both S2D and D2S, outperform the
base model in terms of rBLEU. BLEU, on the other hand, is a precision-based
metric that indicates whether a generated deﬁnition contains material present
in the reference deﬁnitions. The improvements of the multi-sense models over
the base model with respect to rBLEU do not substantially impact BLEU—as
observedbytheoverallhigherfBLEUobtainedbythemulti-sensemodels.Over-
all,theseresultsindicatethatamulti-sensemodelisabletogeneratedeﬁnitions160 A. Kabiri and P. Cook
that better reﬂect the various senses of polysemous words than a single-sense
model, without substantially impacting the quality of the individual generated
deﬁnitions.
5 Conclusions
Deﬁnition modelling is a recently-introduced language modelling task in which
theaimistogeneratedictionary-styledeﬁnitionsforagivenword.Inthispaper,
we proposed a multi-sense context-agnostic deﬁnition generation model which
employed multi-sense embeddings to generate multiple senses for polysemous
words. In contrast to most prior work on deﬁnition modelling which focuses
on English, we conducted a multi-lingual study including nine languages from
several language families. Our experimental results demonstrate that our pro-
posed multi-sense model outperforms a single-sense baseline model. Code and
datasets for these experiments is available.9 In future work, we intend to con-
sider incorporating alternative approaches to learning multi-sense embeddings
into our model, as well as alternative approaches to associating sense vectors to
deﬁnitions for constructing training instances.
References
1. Arora,S.,Li,Y.,Liang,Y.,Ma,T.,Risteski,A.:Linearalgebraicstructureofword
senses, with applications to polysemy. TACL 6, 483–495 (2018)
2. Artale,A.,Magnini,B.,Strapparava,C.:WordnetforItaliananditsuseforlexical
discrimination.In:Lenzerini,M.(ed.)AI*IA97:AdvancesinArtiﬁcialIntelligence,
pp. 346–356. Springer, Berlin, Heidelberg (1997)
3. Bartunov,S.,Kondrashkin,D.,Osokin,A.,Vetrov,D.:Breakingsticksandambi-
guities with adaptive skip-gram. In: Proceedings of AISTATS 2016. pp. 130–138.
Cadiz, Spain (2016)
4. Chang, T.Y., Chen, Y.N.: What does this word mean? Explaining contextualized
embeddings with natural language deﬁnition. In: Proceedings EMNLP-IJCNLP
2019. pp. 6064–6070. Hong Kong, China (2019)
5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding.In:ProceedingsofNAACL
2019. pp. 4171–4186. Minneapolis, Minnesota (2019)
6. Fern´andez-Montraveta, A., V´azquez, G., Fellbaum, C.: The Spanish version of
WordNet 3.0. In: Text Resources and Lexical Knowledge. Selected Papers from
KONENS 2008. pp. 175–182. Mouton de Gruyter (2008)
7. Gadetsky,A.,Yakubovskiy,I.,Vetrov,D.:Conditionalgeneratorsofwordsdeﬁni-
tions. In: Proceedings of ACL 2018. pp. 266–271. Melbourne, Australia (2018)
8. Isahara, H., Bond, F., Uchimoto, K., Utiyama, M., Kanzaki, K.: Development of
theJapaneseWordNet.In:ProceedingsofLREC2008.Marrakech,Morocco(2008)
9. Ishiwatari, S., Hayashi, H., Yoshinaga, N., Neubig, G., Sato, S., Toyoda, M., Kit-
suregawa,M.:Learningtodescribeunknownphraseswithlocalandglobalcontexts.
In: Proceedings of NAACL 2019. pp. 3467–3476. Minneapolis, Minnesota (2019)
9 https://github.com/ArmanKabiri/Multi-sense-Multi-lingual-Deﬁnition-Modeling.Evaluating a Multi-sense Deﬁnition Generation Model 161
10. Lee, G.H., Chen, Y.N.: MUSE: modularizing unsupervised sense embeddings. In:
Proceedings EMNLP 2017. pp. 327–337. Copenhagen, Denmark (2017)
11. Mickus, T., Paperno, D., Constant, M.: Mark my word: A sequence-to-sequence
approach to deﬁnition modeling. In: Proceedings of the First NLPL Workshop on
Deep LearningforNaturalLanguage Processing.pp. 1–11.Turku,Finland (2019)
12. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-
sentations of words and phrases and their compositionality. Advances in Neural
Information Processing Systems 26, 3111–3119 (2013)
13. Miller, G.A.: WordNet: An electronic lexical database. MIT press (1998)
14. Navigli,R.,Ponzetto,S.P.:Babelnet:Theautomaticconstruction,evaluationand
applicationofawide-coveragemultilingualsemanticnetwork.ArtiﬁcialIntelligence
193, 217–250 (2012)
15. Ni,K.,Wang,W.Y.:Learningtoexplainnon-standardEnglishwordsandphrases.
In: Proceedings of IJCNLP 2017. pp. 413–417. Taipei, Taiwan (2017)
16. Noraset, T., Liang, C., Birnbaum, L., Downey, D.: Deﬁnition modeling: Learning
to deﬁne word embeddings in natural language. AAAI 2017, 3259–3266 (2017)
17. Papineni,K.,Roukos,S.,Ward,T.,Zhu,W.J.:Bleu:amethodforautomaticeval-
uationofmachinetranslation.In:ProceedingsofACL2002.pp.311–318.Philadel-
phia, Pennsylvania, USA (2002)
18. Rˇeh˚uˇrek, R., Sojka, P.: Software Framework for Topic Modelling with Large Cor-
pora. In: Proceedings of the LREC 2010 Workshop on New Challenges for NLP
Frameworks. pp. 45–50. ELRA, Valletta, Malta (May 2010), https://is.muni.cz/
publication/884893/en
19. Stamou, S., Nenadic, G., Christodoulakis, D.: Exploring balkanet shared ontol-
ogy for multilingual conceptual indexing. In: Proceedings of LREC 2004. Lisbon,
Portugal (2004)
20. Zhu, R., Noraset, T., Liu, A., Jiang, W., Downey, D.: Multi-sense deﬁnition mod-
eling using word sense decompositions. arXiv preprint arXiv:1909.09483 (2019)Combining Cross-lingual and Cross-task
Supervision for Zero-Shot Learning
Matu´ˇs Pikuliak(B) and Maria´n Sˇimko
Slovak University of Technology in Bratislava, Bratislava, Slovakia
{matus.pikuliak,marian.simko}@stuba.sk
Abstract. In this work we combine cross-lingual and cross-task super-
visionforzero-shotlearning.Ourmaincontributionisthatwediscovered
thatcouplingmodels,i.e.modelsthatshareneitheratasknoralanguage
with the zero-shot target model, can improve the results signiﬁcantly.
Coupling models serve as a regularization for the other auxiliary mod-
els that provide direct cross-lingual and cross-task supervision. We con-
ducted a series of experiments with four Indo-European languages and
fourtasks(dependencyparsing,languagemodeling,namedentityrecog-
nition and part-of-speech tagging) in various settings. We were able to
achieve32%errorreductioncomparedtousingcross-lingualsupervision
only.
· ·
Keywords: Transfer learning Cross-lingual learning Parameter
·
sharing Zero-shot learning
1 Introduction
Despite the recent advances in neural methods for NLP, we can not realistically
train deep models for many low resource languages, mainly because we lack the
annotated data [20]. This inspired researchers to use transfer learning [16] to
bootstrap the learning from auxiliary sources. We can use either cross-lingual
supervision, with data from other languages annotated for the same task, or we
can use cross-task supervision, with data in the same language, but annotated
for other tasks. However, various forms of supervision are currently only rarely
combined.Webelievethatthisisunfortunate,sincevariouscombinationsmight
complement each others and improve the results even further.
In this work, we combine these two approaches for zero-shot learning, i.e. for
asituationwhenwedonothaveany annotateddataforthetargetmodelandwe
needtorelyonlyonthetransfersupervisionfromauxiliarysources.Asconﬁrmed
previously[13],acombinationofcross-lingualandcross-tasksupervisionisquite
viable. However, unlike previous works, we supervise the zero-shot target model
with a full grid of models, as depicted in Fig. 1.
ThisworkwaspartiallysupportedbytheScientiﬁcGrantAgencyoftheSlovakRepub-
lic,grantsNo.VG1/0725/19andVG1/0667/18andbytheSlovakResearchandDevel-
opment Agency under the contracts No. APVV-15-0508, APVV-17-0267 and APVV
SK-IL-RD-18-0004.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.162–170,2020.
https://doi.org/10.1007/978-3-030-58323-1_17Combining Cross-lingual and Cross-task Supervision for Zero-Shot Learning 163
Fig.1. A grid of models. Each model solves a speciﬁc task in a speciﬁc language. If
model A is our target model, we can use cross-task supervision from model B and/or
cross-lingual supervision from model C. Model D does not share neither a task nor
a language with A. Instead it serves as a regularization connecting models B and C.
We call models like D coupling models. Color coded is the parameter sharing strategy
described in Sect. 3 - layers with the same color have identical parameters.
We can provide each model with both cross-lingual and cross-task supervi-
sion.Ontopofthat,wealsohavemodelsthatshareneitheratasknoralanguage
with the zero-shot target. We found out that these “unrelated” models signiﬁ-
cantlyimprovetheperformanceinzero-shotsetting.Wehypothesize,thatthese
models–wecallthemthecoupling models –servemainlyasaregularizationfor
the other auxiliary models that provide direct cross-lingual or cross-task super-
vision. Coupling models enable a communication between the other models and
help them ﬁnd a common ground that can be then exploited by the zero-shot
target model.
Thisworkisasteptowardsunderstandinghowtousecross-lingualandcross-
tasksupervisionatthesametime.Themaincontributionofourworkisthatwe
are to the best of our knowledge the ﬁrst to describe the regularization power
of the coupling models and show that they improve performance for zero-shot
learning. We evaluate this claim with a full grid of 16 models (4 tasks and
4 languages). We contrast these results with a low resource scenario, where only
a limited amount of training data is available for the target model. We also
releaseourcodeanddata,thatcanbeusedasabenchmarkforfutureresearch1.
2 Related Work
Parameter Sharing.Parametersharingisatechniqueformultitasklearningthat
is based on an idea of having the same parameter values (and thus the same
1 https://github.com/matus-pikuliak/crosslingual-parameter-sharing.164 M. Pikuliak and M. Sˇimko
behavior)fordiﬀerentmodels[4].ItwasrecentlysuccessfullyusedindeepNLP.
Most often, whole layers (e.g. embedding layers or recurrent layers) are shared
across the models [5,8,19].
Multilingual Learning. Multilingual learning can be perceived as a speciﬁc
type of multitask learning, where the samples come from diﬀerent languages.
Recently, parameter sharing based approaches were used to solve dependency
parsing [14], machine translation [1], sequence tagging [21] as well as other NLP
tasks.[13]isthemostsimilarworktoours.Theyexplorevariousparametershar-
ingstrategiesforcross-lingualPOStaggingandnamedentityrecognition(NER)
and are able to beat the baseline trained without transfer learning. Compared
to this work we focus on zero-shot learning and the concept of coupling models,
which proved to further improve the results. We also work with more tasks and
languages at the same time using the full grid of 16 models.
3 Transfer Learning Model
Hereweproposeaneuralmodelthatwillbeusedtotestvariousformsoftransfer
learning.Weneedamodelthatisabletosolveword-leveltasksandthatcanuse
similar encoder architecture for all the tasks, i.e. dependency parsing, language
modeling, NER and POS tagging. Based on these requirements we propose an
LSTM-based model with following parts:
1. Word embeddings. We use multilingual word embeddings [17] as word repre-
sentations.
2. Bi-directional LSTM encoder. We use an LSTM encoder [9] to get a contex-
tualized representation for each word. This part contains the main bulk of
trainable parameters.
3. Task-speciﬁc decoders. We designed a task-speciﬁc decoder for each task. We
use conditional random ﬁelds [12] based decoders for POS tagging and NER.
Parsingisdoneviagraph-basedparser[22].Forlanguagemodelingwepredict
the word from the previous states of both forward and backward pass of the
encoder, i.e. we do a leave-one-out language modeling.
To implement parameter sharing, we create a model for each task-language
pair, e.g. Spanish NER has its own model. One of the models is then designed
to be the target model. Our goal is to get the best possible performance for
this model. During training we use supervision from other auxiliary models via
parametersharing.Foreachlearningstepwesampleamodelfromallthemodels
in the grid, we update its parameters with one training batch and then we
propagate the updated parameters to all the other models which are bound to
this model. Each epoch consists of a ﬁxed number of these steps.
Wordembeddingsarenaturallysharedacrossmodelswiththesamelanguage
and they are ﬁxed during the training, i.e. their parameters are not changing.
Encoder parameters are shared across all the models. This is where most of the
transferlearninghappens.Finally,decoderparametersaresharedacrossmodels
with the same task. With this setup, we can transfer the parameters to eachCombining Cross-lingual and Cross-task Supervision for Zero-Shot Learning 165
layer of the target model from some auxiliary model(s). This schema is depicted
in Fig. 1 with color-coded blocks representing the layers of the models. Layers
with the same color eﬀectively have the same parameters.
4 Experiments
We have done a series of experiments with various forms of supervision. Our
goal is to ﬁnd out how well do the coupling models work compared to and
combined with the other forms. We evaluate their performance in zero-shot and
low resource setting on four tasks and four languages.
4.1 Data
We have a dataset for each task in each language (Czech, English, German and
Spanish). Table 1 shows the size of the training sets.
Table 1. Number of sentences in training datasets.
Czech German English Spanish
DP & POS 67.9k 13.8k 12.0k 14.0k
NER 7.1k 24.8k 38.4k 6.9k
LM 6.2M 7.3M 7.4M 6.3M
Dependency Parsing and Part-of-speech Tagging. We use Universal Depen-
dencies [15] dataset for these two tasks. Data from all languages are annotated
with universal tagging schemata for both tasks. POS tagset contains 17 word
tags, tagset for dependency parsing contains 37 syntactic relation tags.
Named Entity Recognition. We combined several datasets: Groningen Mean-
ing Bank [3] for English, GermEval 2014 [2] for German, CoNLL 2002 [18] for
Spanish and Czech Named Entity Corpus [11] for Czech. We uniﬁed the tag-
ging schemata to the standard IOB schema with four types of named entities:
persons, locations, organizations and miscellaneous.
Language Modeling. We use dumps from Wikipedia as corpora for language
modeling.
Word Embeddings. We use pre-trained multilingual MUSE [6] word embed-
dings that are available online2.
2 https://github.com/facebookresearch/MUSE.166 M. Pikuliak and M. Sˇimko
4.2 Evaluation Measures
We use accuracy for POS tagging, chunk F1 score for NER and LAS (labeled
attachmentscore)fordependencyparsingasevaluationmetrics.Languagemod-
eling was used as an auxiliary task only.
Weconsidertwotrainingsettings:(1)4×4setting,inwhichcasewedesignate
one model to be the target model and the 15 remaining models are auxiliary
(three models provide cross-lingual supervision, three models provide cross-task
supervision and 9 models are coupling). (2) 2×2 setting, where we randomly
sampled 20 target models and with random auxiliary task and language. In this
case we have one model for each kind of supervision.
We show the performance either for a speciﬁc task and setting, e.g. POS is
4
anaveragePOSperformancein4×4setting.Orweshowtheoverallperformance
across multiple tasks by calculating the average error reduction (AER), which
is calculated as AER = M−B, where M is the method being compared to
U−B
the baseline B and U is the upper bound of the performance. We calculate U
as a performance of the model trained on its training data without any form of
transfersupervision.AERshowshowmucherrorareweabletoremovecompared
tothebaseline.Wereporttheperformanceonatestsetduringtheepochwiththe
highestvalidationsetperformance.Validationsetisalsousedforearlystopping.
Tuning hyperparameters for each training run we have done would be too
expensive for us. Instead we opted out for ﬁxed hyperparameters tuned before-
handusingseveralmodels(CzechNER,GermanPOS,Englishparsing)without
any form of auxiliary supervision. All the hyperparameters we use will be pub-
lished alongside the code.
4.3 Results
We divided the auxiliary models into three categories: models providing cross-
lingualsupervision(CL),modelsprovidingcross-tasksupervision(CT)andcou-
pling models (Co) as illustrated in Fig. 1. The left part of Table 2 shows the
resultsforzero-shotlearningapproach,i.e.whenwedidnotuseanydataforthe
target model and all we use was auxiliary supervision.
Note, that cross-lingual supervision is used in all zero-shot learning exper-
iments, since we need it to train the decoder parameters. Having only cross-
lingual supervision is therefore our baseline in zero-shot learning. Combining
cross-lingualandcross-tasksupervisionimprovestheresults,aspreviouslyshown
in [13]. Adding coupling tasks to the baseline did not help very much, on con-
trary, the results even deteriorated for 4 × 4 setting. However, using both
cross-task supervision and coupling tasks had an interesting synergic eﬀect.
Even though the coupling tasks proved to be ineﬀective by themselves (CL-Co),
when combined with cross-task supervision (CL-CT-Co) the resulting perfor-
mance improvement is even greater than the performance improvement gained
byaddingcross-tasksupervisionalone.Wewereabletoremovealmostonethird
of the error rate simply by adding additional auxiliary sources.Combining Cross-lingual and Cross-task Supervision for Zero-Shot Learning 167
Table2.Performancefortransferlearningwithvariousauxiliarymodels:cross-lingual
(CL), cross-task (CT) and coupling models (Co).
Zero-shot Lowresource
DP4 NER4 POS4 AER4 AER2 DP4 NER4 POS4 AER4 AER2
Notransfer – – – – – 57.26 53.91 87.35 0.00 0.00
CL 38.75 49.30 69.05 0.00 0.00 64.30 60.16 90.10 29.54 17.32
CT – – – – – 65.56 61.91 91.64 41.35 29.28
CL-CT 37.38 53.13 77.34 12.62 7.01 70.78 64.82 92.23 54.29 37.89
CL-Co 36.70 47.84 68.95 −3.47 0.28 – – – – –
CL-CT-Co 46.25 56.62 84.09 32.40 18.11 71.05 63.02 92.51 53.63 39.17
The right side of Table 2 shows the results for low resource target model.
In this case, we provided 200 training samples for the target model. The target
model is being used during the training, along with all the other models. The
baseline in this case is a model trained with these 200 samples and without
any auxiliary supervision. We can see that both cross-lingual and cross-task
supervision improve the performance, and their combination even more so. We
wereabletoremovemorethan50%oftheerrorin4×4settingcomparedtothe
baseline.However,notethatthecouplingtaskshardlyprovideanyimprovement
(from 54.29 to 53.63 AER ). The performance seems to be on par with the
4
trainingwithoutthem.Comparethistothehugeperformanceimprovementthat
coupling models brought for zero-shot learning (from 12.62 to 32.40 AER ).
4
Based on this behavior of coupling tasks supervision, we hypothesize that
their role in actually to perform as a sort of regularization between models
providing cross-lingual and cross-task supervision. By using coupling tasks we
force these two types of models to ﬁnd a common ground. This common ground
canthenbeexploitedduringthetargetmodelevaluation.Evenwithlittletarget
model data, this external regularization is no longer needed. The target model
by itself is able to couple cross-lingual and cross-task models and perhaps even
better so, considering that it directly optimizes the evaluation objective.
5 Discussion and Conclusions
The conducted experiments are limited by the tasks and language we have
selected. We used high resource language for which we could gather signiﬁcant
datasets for both training and evaluation. This might have posed unrealistic
performance expectation [10]. We mainly focus on the basic research of NLP
supervision types and their combination. The application of these techniques to
truly low resource languages is left to future work. We are working mainly with
low-level syntactic tasks here. We believe that it is possible to extend our app-
roach to other, more semantics oriented tasks, e.g. natural language inference,
question answering, etc. It is also possible that this method could be used in
other domains than NLP, where there are multiple tasks being solved for mul-
tiple domains, e.g. in computer vision various tasks can be solved on real-life168 M. Pikuliak and M. Sˇimko
photography data and rendered 3D images. In that case, a grid similar to ours
could be constructed.
It is practically impossible to compare our results to the previous state-of-
the-art,becauseweworkwithuniquedatarequirements.Previousworkismainly
focusedoneithercross-lingualorcross-tasksupervision.Usingthesameauxiliary
datasetsisnecessaryforthecomparabilityoftheresults.Webelievethatitisan
important future work to create a standardized benchmark to better compare
results combining various forms of supervision. The data we plan to release
might be a step towards this goal. As far as supervision techniques go, our
cross-lingual supervision baseline used for zero-shot learning is similar to what
iscurrentlybeingusedasstate-of-the-artmethodsandourcomparisonwiththis
baseline should be suﬃcient to prove the merit of our approach. The only work
that combines cross-lingual and cross-task supervision in similar fashion to ours
is[13].Weconﬁrmtheirresultsandextendtheirworkbyusingcouplingmodels.
Weshowinthisworkthatitispossibletoeﬃcientlycombinevariousformsof
supervisionforzero-shotlearning.Wealsoshowthatpreviouslyoverlookedcou-
plingmodelsprovideinmanycasesasigniﬁcantperformanceboost.Discovering
their regularizational power is the main contribution of our work.
Weplantoextendourworkwithevaluationonadditionaltextrepresentation
techniques.Rightnowweusemultilingualwordembeddings,butwebelievethat
our method would still apply even if we used non-aligned monolingual word
embeddings or even if we used pre-trained large-scale language models [7]. It
would be also interesting to explore how well our method works with various
data requirements, e.g. with low-level target languages or target tasks. In the
future, it might be possible to construct a large-scale repository of many NLP
datasetsthatcanformasupervisiongridsimilartothe4×4gridthatweusedin
ourexperiments.Traininganewtaskinanewlanguagecouldberelativelyeasy,
even if this grid would be sparse. We believe that constructing such grids and
working on methods of training with them is an interesting research direction.
References
1. Arivazhagan, N., et al.: Massively multilingual neural machine translation in the
wild: ﬁndings and challenges. CoRR abs/1907.05019 (2019)
2. Benikova, D., Biemann, C., Reznicek, M.: NoSta-D named entity annotation for
German: guidelines and dataset. In: Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland,
26–31 May 2014, pp. 2524–2531. ELRA (2014)
3. Bos,J.,Basile,V.,Evang,K.,Venhuizen,N.J.,Bjerva,J.:Thegroningenmeaning
bank. In: Ide, N., Pustejovsky, J. (eds.) Handbook of Linguistic Annotation, pp.
463–496. Springer, Dordrecht (2017). https://doi.org/10.1007/978-94-024-0881-
2 18
4. Caruana, R.: Multitask learning: a knowledge-based source of inductive bias. In:
Proceedings of the Tenth International Conference Machine Learning, University
of Massachusetts, Amherst, MA, USA, 27–29 June 1993, pp. 41–48 (1993)Combining Cross-lingual and Cross-task Supervision for Zero-Shot Learning 169
5. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.P.:
Naturallanguageprocessing(almost)fromscratch.J.Mach.Learn.Res.12,2493–
2537 (2011)
6. Conneau, A., Lample, G., Ranzato, M., Denoyer, L., J´egou, H.: Word translation
without parallel data. In: 6th International Conference on Learning Representa-
tions, Vancouver, Canada (2018)
7. Devlin, J., Petrov, S.: Multilingual bert (2019). https://github.com/google-
research/bert/blob/master/multilingual.md. Accessed 14 Mar 2020
8. Hashimoto, K., Xiong, C., Tsuruoka, Y., Socher, R.: A joint many-task model:
growing a neural network for multiple NLP tasks. In: Proceedings of the 2017
Conference on Empirical Methods in Natural Language Processing, Copenhagen,
Denmark, pp. 1923–1933. ACL, September 2017
9. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
10. Kann,K.,Cho,K.,Bowman,S.R.:Towardsrealisticpracticesinlow-resourcenat-
ural language processing: the development set. In: Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
Hong Kong, China, pp. 3340–3347. ACL, November 2019
11. Kravalova, J., Zabokrtsky, Z.: Czech named entity corpus and SVM-based rec-
ognizer. In: Proceedings of the 2009 Named Entities Workshop: Shared Task on
Transliteration (NEWS 2009), pp. 194–201. ACL (2009)
12. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., Dyer, C.: Neural
architecturesfornamedentityrecognition.In:Proceedingsofthe2016Conference
of the North American Chapter of the ACL: Human Language Technologies, San
Diego, California, pp. 260–270. ACL, June 2016
13. Lin, Y., Yang, S., Stoyanov, V., Ji, H.: A multi-lingual multi-task architecture for
low-resourcesequencelabeling.In:Proceedingsofthe56thAnnualMeetingofthe
ACL, Melbourne, Australia, vol. 1, pp. 799–809. ACL (2018)
14. McDonald, R., Petrov, S., Hall, K.: Multi-source transfer of delexicalized depen-
dency parsers. In: Proceedings of the 2011 Conference on Empirical Methods in
Natural Language Processing, Edinburgh, Scotland, UK, pp. 62–72. ACL (2011)
15. Nivre,J.,etal.:Universaldependenciesv1:amultilingualtreebankcollection. In:
Proceedings of the Tenth International Conference on Language Resources and
Evaluation LREC 2016, Portoroˇz, Slovenia. ELRA (2016)
16. Pan,S.J.,Yang,Q.:Asurveyontransferlearning.IEEETrans.Knowl.DataEng.
22(10), 1345–1359 (2010)
17. Ruder,S.,Vulic,I.,Søgaard,A.:Asurveyofcross-lingualwordembeddingmodels.
J. Artif. Intell. Res. 65, 569–631 (2019)
18. Sang, E.F.T.K.: Introduction to the CoNLL-2002 shared task: language-
independent named entity recognition. CoRR cs.CL/0209010 (2002)
19. Søgaard,A.,Goldberg,Y.:Deepmulti-tasklearningwithlowleveltaskssupervised
at lower layers. In: Proceedings of the 54th Annual Meeting of the ACL, Berlin,
Germany, vol. 2, pp. 231–235. ACL (2016)
20. Tsvetkov, Y.: Opportunities and Challenges in Working with Low-Resource Lan-
guages. Carnegie Mellon University (2017), lecture notes170 M. Pikuliak and M. Sˇimko
21. Yang,Z.,Salakhutdinov,R.,Cohen,W.W.:Transferlearningforsequencetagging
withhierarchicalrecurrentnetworks.In:5thInternationalConferenceonLearning
Representations (2017)
22. Zhang,X., Cheng, J., Lapata, M.: Dependency parsingashead selection. In: Pro-
ceedingsofthe15thConferenceoftheEACL,Valencia,Spain,vol.1,pp.665–676.
ACL (2017)Reading Comprehension in Czech via
Machine Translation and Cross-Lingual
Transfer
B
Kateˇrina Mackova´ and Milan Straka( )
Faculty of Mathematics and Physics, Charles University,
Malostransk´e n´amˇest´ı 25, 118 00 Prague 1, Czech Republic
katerina.mackova@mff.cuni.cz, straka@ufal.mff.cuni.cz
Abstract. Reading comprehension is a well studied task, with huge
trainingdatasetsinEnglish.Thisworkfocusesonbuildingreadingcom-
prehensionsystemsforCzech,withoutrequiringanymanuallyannotated
Czech training data. First of all, we automatically translated SQuAD
1.1 and SQuAD 2.0 datasets to Czech to create training and develop-
mentdata,whichwereleaseathttp://hdl.handle.net/11234/1-3249.We
then trained and evaluated several BERT and XLM-RoBERTa baseline
models. However, our main focus lies in cross-lingual transfer models.
We report that a XLM-RoBERTa model trained on English data and
evaluatedonCzechachievesverycompetitiveperformance,onlyapprox-
imately 2% points worse than a model trained on the translated Czech
data. This result is extremely good, considering the fact that the model
has not seen any Czech data during training. The cross-lingual transfer
approach is very ﬂexible and provides a reading comprehension in any
language, for which we have enough monolingual raw texts.
· · · ·
Keywords: Reading comprehension Czech SQuAD BERT
Cross-lingual transfer
1 Introduction
The goal of a reading comprehension system is to understand given text and
return answers in response to questions about the text. In English, there exist
many datasets for this task, some of them very large. In this work, we consider
the frequently used SQuAD 1.1 dataset [12], an English reading comprehension
datasetwitharound100,000question-answerpairs,whichiswidelyusedtotrain
manydiﬀerentmodelswithrelativelygoodaccuracy.WealsoutilizeSQuAD2.0
dataset[11],whichcombinesSQuAD1.1datasetwith50,000unanswerableques-
tionslinkedtoalreadyexistingparagraphs,makingthisdatasetmorechallenging
for reading comprehension systems.
In this paper, we pursue construction of a reading comprehension system for
Czech without having any manually annotated Czech training data, by reusing
English models and English datasets. Our contributions are:
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.171–179,2020.
https://doi.org/10.1007/978-3-030-58323-1_18172 K. Mackov´a and M. Straka
– We translated both SQuAD 1.1 and SQuAD 2.0 to Czech by state-of-the-art
machine translation system [10] and located the answers in the translated
text using MorphoDiTa [13] and DeriNet [14], and released the dataset.
– We trained several baseline systems using BERT and XLM-RoBERTa archi-
tectures,notablyasystemtrainedonthetranslatedCzechdata,andasystem
whichﬁrsttranslatesatextandaquestiontoEnglish,usesanEnglishmodel,
and translates the answer back to Czech.
– We train and evaluate cross-lingual systems based on BERT and XLM-
RoBERTa,whicharetrainedonEnglishandthenevaluateddirectlyonCzech.
We report that such systems have very strong performance despite not using
any Czech data nor Czech translation systems.
2 Related Work
There exist many English datasets for reading comprehension and question
answering, the readers are referred for example to [12] for a nice overview.
Currently, the best models for solving reading comprehension are based on
BERT architecture [4] (which is a method of unsupervised pre-training of con-
textualized word embeddings from raw texts), or on some follow-up models like
ALBERT [7] or RoBERTa [9].
Most BERT-like models are trained on English, with two notable excep-
tions. Multilingual BERT (mBERT), released by [4], is a single language model
pre-trained on monolingual corpora in 104 languages including Czech; XLM-
RoBERTa (XLM-R) [2] is a similar model pre-trained on 100 languages, and is
available in both base and large sizes, while only base mBERT is available.
Cross-lingual transfer capability of mBERT has been mentioned in 2019 by
manyauthors,forexamplebyKondratyuketal.[6]formorphosyntacticanalysis
or by Hsu et al. [5] for reading comprehension.
Verysimilar toourpaperistheparallelindependentworkofLewisetal.[8],
who perform cross-lingual transfer evaluation of reading comprehension models
on six non-English languages (neither of them being Czech).
3 Constructing Czech Reading Comprehension Dataset
The SQuAD 1.1 dataset consists of 23,215 paragraphs belonging to 536 articles.
Attached to every paragraph is a set of questions, each with several possible
answers,resultinginmorethan100,000questions.Whilethetrainandthedevel-
opmentdatasetsarepublic,thetestsetishidden.Wereferthereadersto[12]for
details about the dataset construction, types of answers and reasoning required
to answer the questions.
The SQuAD 2.0 dataset [11] extends SQuAD 1.1 with more than 50,000
unanswerable questions linked to the existing paragraphs.Reading Comprehension in Czech via MT and Cross-Lingual Transfer 173
3.1 Translating the Data and Locating the Answers
WeemployedtheEnglish-Czechstate-of-the-artmachinetranslationsystem[10]
to translate the SQuAD data.1 Translation of all texts, questions and answers
of SQuAD 2.0 took 3days.
Because the answers are subsequences of the given text in SQuAD, we also
neededtolocatethetranslatedanswersinthetext.Weconsideredseveralalter-
natives:
– Estimate the alignment of the source and target tokens using attention of
the machine translation system, and choose the words aligned to the source
answer.Unfortunately,wecouldnotreliablyextractalignmentfromtheatten-
tion heads of a Transformer-based machine translation system.
– Mark the answer in the text before the translation, using for example quo-
tation marks, similarly to [8]. Such an approach would however result in
a dataset with every question linked to a custom text, which would deviate
from the SQuAD structure.
– Locate the answer in the given text after the translation, without relying on
the assistance from the machine translation system.
Wechosethethirdalternativeandlocatedthetranslatedanswersinthetexts
as follows:
1. We lemmatized the translated text and answer using MorphoDiTa [13].
2. We replaced the lemmas by roots of their word-formation relation trees
according to the DeriNet 2.0 lexicon [14].
3. ThenwefoundallcontinuoussubsequencesofthetextwiththesameDeriNet
roots as the answer, but with any word order.
4. Finally,ifseveraloccurrenceswerelocated,wechosetheonewiththerelative
position in the text being the most similar to the relative position of the
original answer in the original text.
We believe the proposed algorithm has substantially high precision (after
manually verifying many of the located answers), and we also ﬁnd its recall
satisfactory. Notably, in the SQuAD 2.0 training dataset, we have preserved
107,088 questions (which is 82.2% of the English ones) and in the development
datasetwekept10,845questions,91.3%oftheoriginaldataset.Thedetailedsizes
ofthecreatedCzechdatasetsarepresentedinTable1.Notethattheratioofthe
keptdatainSQuAD1.1islower,becauseunanswerablequestionsofSQuAD2.0
are always preserved.
Thedatasetisavailablefordownloadathttp://hdl.handle.net/11234/1-3249.
3.2 Evaluation Metrics
TheSQuADdatasetisusuallyevaluatedusingtwometrics:exactmatch,which
istheaccuracyofexactly predictedanswers,andF1-scorecomputedoverindi-
vidual words of the answers.
1 Available on-line at https://lindat.mﬀ.cuni.cz/services/translation/.174 K. Mackov´a and M. Straka
Table 1. Size of the translated Czech variant of SQuAD 1.1 and SQuAD 2.0.
Dataset English questions Czech questions Percentage kept
SQuAD 1.1 Train 87,599 64,164 73.2%
Test 10,570 8,739 82.7%
SQuAD 2.0 Train 130,319 107,088 82.2%
Test 11,873 10,845 91.3%
Given that Czech is a morphologically rich language, we performed lemma-
tization and then replaced lemmas by DeriNet roots (as in Sect.3.1) prior to
evaluation with the oﬃcial evaluation script.
4 Model Training and Evaluation
Considering that the current best SQuAD models are all BERT based, we also
employaBERT-likearchitecture.Wereferreadersto[4]fordetaileddescription
of the model and the ﬁne-tuning phase.
Because our main goal is Czech reading comprehension, we consider such
BERT models which included Czech in their pre-training, notably Multilingual
BERT (mBERT), released by [4], both cased and uncased, and also XLM-
RoBERTa (XLM-R) [2], both base and large. As a reference, we also include
English BERT base, both cased and uncased.
Weﬁnetunedallmodelsusingthe transformerslibrary[15].Forallbase
models,weusedtwo training epochs,learning rate2e−5withlinearwarm-upof
256 steps and batch size 16; for XLM-RoBERTa we increased batch size to 32
andforXLM-RoBERTalarge wedecreasedlearningrateto1.5e−5andincreased
warm-up to 500.
All our results are presented in Table2 and also graphically in Fig.1.
English. For reference, we trained and evaluated all above models on English
SQuAD 1.1 and SQuAD 2.0. The results are consistent with the published
results. It is worth noting that the only large model reaches considerably better
performance, and that mBERT achieves better results than English BERT.
Czech Training, Czech Evaluation. Our ﬁrst baseline model is trained
directly on the Czech training data and then evaluated on the development set.
The relative performance of the BERT variants is very similar to English, but
the absolute performance is considerably lower. Several facts could contribute
to the performance decrease – a smaller training set, noise introduced by the
translation system and morphological richness of the Czech language.Reading Comprehension in Czech via MT and Cross-Lingual Transfer 175
Table 2. Development performance ofEnglish andCzechmodels onSQuAD 1.1, 2.0.
Model Train Dev SQuAD 1.1 SQuAD 2.0
EM F1 EM F1
BERT cased EN EN 81.43% 88.88% 72.85% 76.03%
BERT uncased EN EN 80.92% 88.59% 73.35% 76.59%
mBERT cased EN EN 81.99% 89.10% 75.79% 78.76%
mBERT uncased EN EN 81.98% 89.27% 74.88% 77.98%
XLM-R base EN EN 80.91% 88.11% 74.07% 76.97%
XLM-R large EN EN 87.27% 93.24% 83.21% 86.23%
BERT cased EN CZ 9.53% 21.62% 53.48% 53.84%
BERT uncased EN CZ 6.16% 21.75% 54.78% 54.83%
mBERT cased EN CZ 59.49% 70.62% 58.28% 62.76%
mBERT uncased EN CZ 62.09% 73.89% 59.59% 63.89%
XLM-R base EN CZ 64.63% 75.85% 62.09% 65.93%
XLM-R large EN CZ 73.64% 84.07% 73.50% 77.58%
BERT cased EN CZ-EN-CZ 64.06% 76.78% 64.35% 69.11%
BERT uncased EN CZ-EN-CZ 63.57% 76.61% 65.26% 69.86%
mBERT cased EN CZ-EN-CZ 65.09% 77.47% 67.40% 71.96%
mBERT uncased EN CZ-EN-CZ 65.00% 77.38% 66.20% 70.72%
XLM-R base EN CZ-EN-CZ 64.52% 76.91% 65.62% 70.00%
XLM-R large EN CZ-EN-CZ 69.04% 81.33% 72.82% 78.04%
mBERT cased CZ CZ 59.49% 70.62% 66.60% 69.61%
mBERT uncased CZ CZ 62.11% 73.94% 64.96% 68.14%
XLM-R base CZ CZ 69.18% 78.71% 64.98% 68.15%
XLM-R large CZ CZ 76.39% 85.62% 75.57% 79.19%
English Models, Czech Evaluation via Machine Translation. Our sec-
ond baseline system (denoted CS-EN-CS in the results) reuses English models
to perform Czech reading comprehension – the Czech development set is ﬁrst
translatedtoEnglish,theanswersarethengeneratedusingEnglishmodels,and
ﬁnally translated back to Czech.
The translation-based approach has slightly higher performance for base
models, which may be caused by the smaller size of the Czech training data.
However, for the large model, the direct approach seems more beneﬁcial.
Cross-Lingual Transfer Models. The most interesting experiment is the
cross-lingual transfer of the English models, evaluated directly on Czech (with-
out using any Czech data for training). Astonishingly, the results are very com-
petitive with the other models evaluated on Czech, especially for XLM-R large,
wheretherearewithin1.6%pointsinF1scoreand2.75%pointsinexactmatch
of the best Czech model.176 K. Mackov´a and M. Straka
Fig.1. Development set performance of all models for English and Czech SQuAD 1.1
and SQuAD 2.0 datasets.
4.1 Main Findings
Why Does Cross-Lingual Transfer Work. The performance of the cross-
lingualtransfermodelisstriking.EvenifthemodelneversawanyCzechreading
comprehensiondataanditneversawanyparallelCzech-Englishdata,itreaches
nearly the best results among all evaluated models.
ThisstrongperformanceisanindicationthatmBERTandXLM-Rrepresent
diﬀerentlanguagesinthesamesharedspace,withoutgettinganexplicittrainingReading Comprehension in Czech via MT and Cross-Lingual Transfer 177
signal in form of parallel data. Instead, we hypothesise that if there is a large-
enough similarity among languages, the model exploits it by reusing the same
partofthenetworktohandlethisphenomenonacrossmultiplelanguages.Thisin
turnsavescapacityofthemodelandallowsreachinghigherlikelihood,improving
the quality of the model. In other words, greedy decrease of a loss function
performed by SGD is good enough motivation for representing similarities in
a shared way across languages.
Furthermore,wordembeddingsfordiﬀerentlanguagesdemonstratearemark-
able amount of similarity even after a simple linear transformation, as demon-
stratedfor exampleby[1]or[3].Suchsimilarities aredeﬁnitely exploitable (and
asindicatedbytheresultsalsoexploited)byBERT-likemodelstoachieveshared
representation of multiple languages.
Pre-trainingonCzechIsRequired. Thestrongperformanceofcross-lingual
models does not necessarily mean the models can “understand” Czech – the
named entities could be similar enough in Czech and English, and the model
could be capable of answering without understanding the question.
Therefore,wealsoconsideredanEnglishreadingcomprehensionmodelbased
onEnglishBERT,whichdidnotencounteranyotherlanguagebutEnglishdur-
ingpre-training.EvaluatingsuchamodeldirectlyonCzechdeliverssurprisingly
goodperformanceonSQuAD2.0–themodelisunexpectedlygoodinrecognizing
unanswerablequestions.However,theperformanceofsuchmodelonSQuAD1.1
is rudimentary – 9.53% exact match and 21.62% F1-score, compared to 62.90%
exact match and 73.89% F1-score of an mBERT uncased model.
Cased Versus Uncased. Consistently with intuition, cased models seem to
perform generally better than uncased. However, in the context of cross-lingual
transfer, we repeatedly observed uncased models surpassing the cased ones. We
hypothesise that this result could be caused by larger intersection of Czech and
Englishsubwordsoftheuncasedmodels(whichdiscardnotonlycasinginforma-
tion, but also diacritical marks), because larger shared vocabulary could make
the cross-lingual transfer easier.
5 Conclusion
Inthispaper,wehaveexploredCzechreadingcomprehensionwithoutanyman-
ually annotated Czech training data. We trained several baseline BERT-like
modelsusingtranslateddata,butmostimportantlyweevaluatedacross-lingual
transfer model trained on English and then evaluated directly on Czech. The
performance of this model is exceptionally good, despite the fact that no Czech
training data nor Czech translation system was needed to train it.
Acknowledgements. The work was supported by the Grant Agency of the Czech
Republic, project EXPRO LUSyD (GX20-16819X) and by the SVV 260 575 grant178 K. Mackov´a and M. Straka
of Charles University. This research has also been using data and services provided
bytheLINDAT/CLARIAH-CZResearchInfrastructure(https://lindat.cz),supported
by the Ministry of Education, Youth and Sports of the Czech Republic (Project No.
LM2018101).
References
1. Artetxe, M., Labaka, G., Agirre, E.: A robust self-learning method for fully unsu-
pervised cross-lingual mappings of word embeddings. In: Proceedings of the 56th
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers), Melbourne, Australia, pp. 789–798. Association for Computational Lin-
guistics, July 2018
2. Conneau, A., et al.: Unsupervised cross-lingual representation learning at scale.
arXiv e-prints arXiv:1911.02116, November 2019
3. Conneau, A., Lample, G., Ranzato, M., Denoyer, L., J´egou, H.: Word translation
without parallel data. arXiv e-prints arXiv:1710.04087, October 2017
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep
bidirectionaltransformersforlanguageunderstanding.In:Proceedingsofthe2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Minneapolis, Minnesota, vol. 1, pp.
4171–4186. Association for Computational Linguistics, June 2019
5. Hsu,T.Y.,Liu,C.L.,Lee,H.Y.:Zero-shotreadingcomprehensionbycross-lingual
transferlearningwithmulti-linguallanguagerepresentationmodel.arXive-prints
arXiv:1909.09587, September 2019
6. Kondratyuk, D., Straka, M.: 75 languages, 1 model: parsing universal dependen-
cies universally. In: Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Nat-
uralLanguageProcessing(EMNLP-IJCNLP),HongKong,China,pp.2779–2795.
Association for Computational Linguistics, November 2019
7. Lan,Z.,Chen,M.,Goodman,S.,Gimpel,K.,Sharma,P.,Soricut,R.:ALBERT:a
lite BERT for self-supervised learning of language representations. arXiv e-prints
arXiv:1909.11942, September 2019
8. Lewis,P.,O˘guz,B.,Rinott,R.,Riedel,S.,Schwenk,H.:MLQA:evaluatingcross-
lingual extractive question answering. arXiv e-prints arXiv:1910.07475, October
2019
9. Liu,Y.,etal.:RoBERTa:arobustlyoptimizedBERTpretrainingapproach.arXiv
e-prints arXiv:1907.11692, July 2019
10. Popel, M.: CUNI transformer neural MT system for WMT18. In: Proceedings
of the Third Conference on Machine Translation: Shared Task Papers, Belgium,
Brussels, pp. 482–487. Association for Computational Linguistics, October 2018
11. Rajpurkar,P.,Jia,R.,Liang,P.:Knowwhatyoudon’tknow:unanswerableques-
tions for SQuAD. In: Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), Melbourne, Australia,
pp. 784–789. Association for Computational Linguistics, July 2018
12. Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuAD: 100,000+ questions
for machine comprehension of text. In: Proceedings of the 2016 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,Austin,Texas,pp.2383–2392.
Association for Computational Linguistics, November 2016Reading Comprehension in Czech via MT and Cross-Lingual Transfer 179
13. Strakov´a, J., Straka, M., Hajiˇc, J.: Open-source tools for morphology, lemmatiza-
tion, POS tagging and named entity recognition. In: Proceedings of 52nd Annual
MeetingoftheAssociationforComputationalLinguistics:SystemDemonstrations.
JohnsHopkinsUniversity,USA,pp.13–18.AssociationforComputationalLinguis-
tics, Stroudsburg (2014)
14. Vidra, J., Zˇabokrtsky´, Z., Sˇevˇc´ıkov´a, M., Kyj´anek, L.: DeriNet 2.0: towards an
all-in-one word-formation resource. In: Proceedings of the Second International
WorkshoponResourcesandToolsforDerivationalMorphology,CharlesUniversity,
Facultyof Mathematics andPhysics, Institute ofFormal andApplied Linguistics,
Prague, Czechia, pp. 81–89, September 2019
15. Wolf,T.,etal.:HuggingFace’stransformers:state-of-the-artnaturallanguagepro-
cessing. arXiv e-prints arXiv:1910.03771, October 2019Measuring Memorization Eﬀect
in Word-Level Neural Networks Probing
B
Rudolf Rosa( ) , Tom´aˇs Musil , and David Mareˇcek
Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics,
Charles University, Malostransk´e n´amˇest´ı 25, 118 00 Praha, Czechia
{rosa,musil,marecek}@ufal.mff.cuni.cz
https://ufal.mﬀ.cuni.cz/
Abstract. Multiple studies have probed representations emerging in
neural networks trained for end-to-end NLP tasks and examined what
word-levellinguisticinformationmaybeencodedintherepresentations.
In classical probing, a classiﬁer is trained on the representations to
extract the target linguistic information. However, there is a threat of
theclassiﬁersimplymemorizingthelinguisticlabelsforindividualwords,
insteadofextractingthelinguisticabstractionsfromtherepresentations,
thusreportingfalsepositiveresults.Whileconsiderableeﬀortshavebeen
made to minimize the memorization problem, the task of actually mea-
suring the amount of memorization happening in the classiﬁer has been
understudied so far. In our work, we propose a simple general method
formeasuringthememorizationeﬀect,basedonasymmetricselectionof
comparablesetsoftestwordsseenversusunseenintraining.Ourmethod
can be used to explicitly quantify the amount of memorization happen-
inginaprobingsetup,sothatanadequatesetupcanbechosenandthe
results of the probing can be interpreted with a reliability estimate. We
exemplifythisbyshowcasingourmethodonacasestudyofprobingfor
part of speech in a trained neural machine translation encoder.
· ·
Keywords: Probing Memorization Neural networks
1 Introduction
In recent years, there has been a considerable amount of research into linguistic
abstractions emerging in neural networks trained for various natural language
processing (NLP) tasks. It has been found that, to some degree, neural net-
worksoftencaptureabstractionswhichseemtocorrespondtoclassicallinguistic
notions known from the linguistic studies of morphology, syntax or semantics,
even if they were not explicitly trained to do so. The common hypothesis is
This work has been supported by grant 18-02196S of the Czech Science Foundation.
It has been using language resources and tools developed, stored and distributed by
the LINDAT/CLARIAH-CZ project of the Ministry of Education, Youth and Sports
of the Czech Republic (project LM2018101).
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.180–188,2020.
https://doi.org/10.1007/978-3-030-58323-1_19Measuring Memorization Eﬀect in Word-Level Neural Networks Probing 181
thatmodernneuralnetworksaresuﬃcientlypowerfultounravelmanylinguistic
properties and regularities of language, and that they do so if this is useful for
solving the task for which they are trained.
In this work, we focus on the subﬁeld of identifying word-level linguistic
abstractions, such as part-of-speech (POS) labels, in word-level representations,
such as static or contextual word embeddings.
The usual method of assessing the amount to which linguistic abstractions
are captured by a neural network is to use probing, which we review in Sect.2.
In word-level probing, we take representations of words from a trained neural
network (such as word embeddings or hidden states from an encoder) and train
a classiﬁer to predict linguistic labels (such as POS) from the representations
correspondingtothewords,usinglinguisticallyannotateddata(suchasatagged
corpus). The common assumption is that if the classiﬁer learns to predict the
linguistic labels with a high accuracy, it is an indication that the neural word
representations contain a latent abstraction similar to the linguistic notion (e.g.
that contextual word embeddings encode POS of the words).
1.1 The Memorization Problem
Amajorthreatassociatedwiththeprobingapproachisthatofmemorization.As
theprobingclassiﬁerlearnstoassignlabelstowords,itcansucceedintwoways.
Either, it learns to extract an abstraction from the word representation which
correspondstothelabeltoassign;thisistheintendedcase,whichwerefertoas
generalization. Or, it simply memorizes the label associated with each word; we
refer to this as memorization. If memorization occurs, the result of the probing
can be misinterpreted as the representations capturing some linguistic abstrac-
tions, while the actual underlying mechanism is that the representations simply
capture the word identity. The probing classiﬁer thus only learns to extract the
word identity from the representation and memorizes the label for the word.1 A
crucial problem is that, without taking additional measures, there is no way of
distinguishing the true positive result from the false positive result.
Withcontext-independentwordrepresentations(staticwordembeddings),it
is of course possible to avoid the problem by splitting the vocabulary into two
disjoint sets of words, training the classiﬁer on a train set and testing it on a
test set. However, for contextual representations, this cannot be done easily, as
the representations need to be computed for whole sentences, not for individual
words, and the train and test sets thus need to be composed of full sentences,
whichunavoidablyhavealargewordoverlap.Whilewemightevaluatetheprobe
only on test set words unseen in the training data, these are not representative
ofthelanguage, assuchasetoftestwordswill bebiasedtowardslow-frequency
words. We argue that we rather need to evaluate on the full test set while mea-
suring and minimizing the memorization eﬀect.
1 Unlike static word embeddings, contextual representations of the same word in dif-
ferentsentencesarediﬀerent,whichmakesmemorizationharder,butnotimpossible:
theidentityofthewordisstillstronglyencodedinthecontextualrepresentationand
can be extracted from it, especially when a stronger classiﬁer is used.182 R. Rosa et al.
1.2 Measuring Memorization
In this paper, we suggest a general method of measuring the amount of memo-
rizationoccurringinword-levelprobingofneuralnetworkrepresentations,based
on comparing the probing classiﬁer accuracy on sets of seen and unseen words.
Although a standard test set contains both words seen and unseen in training
data,theseenwordstendtobefrequentwhiletheunseenonesaretypicallyrare
words;wethusregardanapproachofcomparingaccuraciesonthesesetsofwords
as inadequate and uninformative. Instead, we propose a method which samples
the seen and unseen words in a symmetric way to ensure their comparability.
Wedonotpresentanewmethodforprobingitself;ourmethodisdesignedto
complementexistingprobingapproachesbyexplicitlymeasuringtheirreliability
withrespecttothememorizationproblem.Thiscanhelptheresearchertoselect
an adequate probing setup by providing means for quantifying the magnitude
of the memorization problem, allowing for a trustworthy interpretation of the
probing results.
Asacasestudy,weapplyourmethodtomeasuretheamountofmemorization
in probing for POS in word representations from a neural machine translation
system.
2 Related Work
A comprehensive survey of word embeddings evaluation methods was compiled
by Bakarov [2]. An overview can also be found in the survey of methodology for
analysis of deep learning models for NLP by Belinkov and Glass [4]. Another
overview [12] mentions “[n]o standardized splits & overﬁtting” as one of the
problems of evaluating word embeddings with similarity tasks.
There are various strategies when it comes to the train/dev/test splitting in
probing.
Whenitispossibletopredicttheprobedpropertyfromthewordtypeitself,
the vocabulary may be split into train/test sets. This strategy is used e.g. in
[19,21] to evaluate POS tag and other morphological features prediction.
Some works split the dataset into train/dev/test sets, without regard to the
same words occuring in both. These include predicting syntactic and semantic
labels (including POS) from hidden states on sentences [3,5,11,18,22] or tree-
banks [7,15].
Bisazza and Tump [6] address the problem with the overlap. They observe
that even a dummy random feature can be predicted with high accuracy when
thesamewordsoccurbothinthetrainandthetestdata.Theyextractonevector
per token from the NMT encoder. They randomly split the vocabulary into two
partsanduseonetoﬁlterthetrainingdataandtheothertoﬁlterthetestdata.
They repeat the experiments several times and report mean accuracies.
Another approach to evaluating words in context of sentences is presented
by [10]. They propose the word content task that tests whether it is possible to
recoverinformationabouttheoriginalwordsinthesentencefromitsembedding.Measuring Memorization Eﬀect in Word-Level Neural Networks Probing 183
They pick 1000 mid-frequency words from the source corpus vocabulary and
sampleequalnumbersofsentencesthatcontainoneandonlyoneofthesewords.
The words can then be partitioned into train and test sets without the risk of
their overlapping.
The ability of deep neural networks to memorize is a challenge for the the-
ory of deep learning [1]. It also has implications for the applications of neural
networks,becauseitmaybeproblematicifaportionofthetrainingdatacanbe
reconstructed from the trained model [9].
Inconnectionwithprobingneuralnetworks,memorizationwasaddressedby
Hewitt and Liang [14], who propose control tasks to complement the linguistics
tasks. A control task associates word types with random labels. If the classiﬁer
performs well on the control task, this means that it is able to memorize the
training set. However, the data distribution aﬀects the generalization ability
of deep neural networks and they tend to learn simple patterns when possible
[16]. Our approach diﬀers from [14] by using the original data to measure the
memorization eﬀect, evading the problem created by altering the distribution in
a control task.
3 Method
In the usual probing approach, we operate with two sets of sentences, a training
set and a test set, both labelled with the word-level labels corresponding to the
linguistic abstraction for which we are probing the neural word representations
(e.g. POS). The training set is used to train a probing classiﬁer to predict the
labelsfromthewordrepresentations.Theclassiﬁeristhenevaluatedonthetest
set, and its accuracy, compared to a baseline, is used to estimate to what extent
the given linguistic abstraction is encoded in the word representations.
The goal of our method is to measure to what extent the probing classiﬁer
onlymemorizeswordidentitiesinsteadofmeasuringthegeneralizationcaptured
by the word representations. The main idea is to compare the probing classiﬁer
accuraciesonwordsthatarepartofthetrainingdata(seen words)andonwords
that are not (unseen words), while keeping the sets of seen and unseen words
otherwise comparable (as discussed in Sect.1), which we ensure by a symmetric
way of creating these sets.
We propose the following approach:
1. Randomlysplitthetrainingsetintotwohalves,whichwewillrefertoasseen
sentences and unseen sentences.
2. Train the probing classiﬁer only on the seen sentences.
3. Apply the probing classiﬁer to the test set.
4. Deﬁnethesetofseen words aswordsthatarecontainedintheseensentences
but not in the unseen sentences.
5. Deﬁne the set of unseen words as words that are contained in the unseen
sentences but not in the seen sentences.184 R. Rosa et al.
6. Evaluate the accuracy of the probing classiﬁer separately on seen words and
on unseen words, ignoring words that are neither seen nor unseen.2
Usingthisapproach,wecannowquantifythemagnitudeofthememorization
eﬀect occurring in the probing setup as the diﬀerence between the classiﬁer
accuracy on seen and on unseen words. If the memorization problem is not
present, these accuracies should be identical, as the classiﬁer only extracts the
linguistic abstraction from the representation, regardless of the word identity;
in this case, the classiﬁer accuracy reliably measures the amount of linguistic
informationencodedbytherepresentation.Ontheotherhand,ahigheraccuracy
onseenwordsthanonunseenwordssignalizesthattheclassiﬁermemorizedsome
of the seen words’ identities to some extent, instead of extracting the linguistic
abstractions from them.
To stabilize the evaluation, we propose to sample the seen and unseen sen-
tences and train the classiﬁer multiple times, and to compute the microaverage
accuracy.
We deﬁne our method as operating on words and word representations, as
this makes the subsequent word-level probing straightforward. Our method is
in principle applicable even for setups using subwords. However, in such cases,
it is up to the researcher to decide whether for the given language and setup,
subword-level memorization is a problem or not, as our method only deals with
word-level memorization.
3.1 Which Words Are Selected for Evaluation?
It is important to note that the distribution of words selected for evaluation
by our method is strongly biased towards lower-frequency words. Very frequent
words are never selected for evaluation, and medium-frequency words are rarely
selected, as they always or nearly always appear in both seen and unseen sen-
tences, and our method is thus unable to measure the memorization eﬀect for
such words.
Speciﬁcally, the probability Psel(w) of a word w being selected as unseen
((cid:2)or seen) fol(cid:3)lows a hypergeometric distribution: Psel(w) ∼ Hypergeometric
|S|,|S|,|Sw| , where S is the set of training sentences, out of which its subset
2
Sw contains the word w. For most words,3 it is similar to the binomial distri-
bution Bi(|Sw|,0(cid:4).5)(cid:5), and Psel(w) is thus inversely exponentially proportional to
|Sw|: Psel(w)≈ 1 |Sw|.
2
2 Note that words which occur in both seen and unseen sentences are neither seen
words nor unseen words. We also need to remove words that are part of the devel-
opment set if one is used for training the probing classiﬁer. Technically, words that
do not appear in the test set can also be removed from the sets of seen and unseen
words as they do not inﬂuence the results.
3 For frequent words, the actual probability is even lower than the (already negligi-
ble) approximated value; for words that appear in more than half of the training
sentences, the probability is 0. The probability is also technically 0 for words that
do not appear in the test set.Measuring Memorization Eﬀect in Word-Level Neural Networks Probing 185
Webelievethatforvery frequent words(especiallyfunctionwordssuchas
commonprepositions,pronouns,determinersandpunctuation),avoidingmemo-
rizationishard–asetofsentencesconstructednottocontainagivenwordfrom
this class would typically not be very representative of the language. Moreover,
the probed neural network is typically not very likely to meaningfully abstract
over such words, as it is usually more economical for the network to simply
memorize the most frequent words and treat them as special cases.4,5
For medium-frequency words, such as common nouns and verbs, we see
their underrepresentation as a shortcoming of our method which we intend to
focus on in future work. We speciﬁcally plan to further investigate the approach
of Bisazza and Tump [6], reviewed in Sect.2, who train the probing classiﬁer on
representationsofonlysomewordsinthetrainingsentencesandregardtheother
words as unseen. We appreciate the approach, but we believe that it must be
analyzed to what degree it may be inﬂuenced by the contextual representations
of the seen words containing information about surrounding words regarded as
unseen.6
Our method mostly focuses on lower-frequency words, which we believe
to be reasonable, as the lower the frequency of the word, the stronger is the
network forced to abstract over the word. We are thus mostly interested in such
wordsinprobing,asifthenetworkcapturestheabstractionsthatweareprobing
for,theyshouldbemostprominent inrepresentations oflower-frequencywords.
Still, we also omit very rare words, which either do not appear in the test
sentencesorinthetraining sentences(or,obviously, innoneofthose). Forthese
words, the memorization eﬀect is very unlikely to occur.
4 Case Study
As a case study, we apply our method to probing representations from a
neural machine translation model for POS. We study the memorization phe-
nomenon along three dimensions, varying the train set size, the contextuality of
the representation (static word embeddings versus encoder output states), and
thepoweroftheprobingclassiﬁer,usingeitheralinearclassiﬁeroramulti-layer
perceptron classiﬁer (MLP).
We analyze a Transformer model [24] implemented within the Neural Mon-
key framework7 [13], trained for the task of machine translation from Czech to
4 Which they often are, as frequent words tend to behave irregularly in language [23,
p. 116].
5 Arguably, it is sane to memorize very frequent words rather than abstracting over
them. Nevertheless, we should be able to measure this reliably, not mistaking one
for the other.
6 Intheirmethod,unseen wordsarepartofthetrainingsentencesandcanthusinﬂu-
ence the contextual representations of the seen words which are used for training
theprobingclassiﬁer,whereasinourmethod,thetrainingsentencesdonotcontain
the unseen words at all.
7 https://github.com/ufal/neuralmonkey.186 R. Rosa et al.
Table 1. Case study evaluation on POS prediction, varying the number of training
sentences,theprobedrepresentations,andtheprobingclassiﬁer.Thediﬀerencebetween
theaccuracyoftheprobeonseenversusunseenwordsrepresentsthemagnitudeofthe
memorization problem. Micro-average over 10 repetitions, in percentage points, with
standard deviations.
Accuracy Stand.dev. Accuracy Stand.dev.
Trainsent. seen unseendiﬀ seen unseen Trainsent. seen unseen diﬀ seen unseen
Encoderoutputstates,linearclassiﬁer Encoderwordembeddings,linearclassiﬁer
50 90.5 87.3 3.3 3.4 5.6 50 98.5 74.3 24.1 0.9 7.6
100 89.1 86.8 2.3 1.8 2.0 100 97.0 78.0 19.0 0.8 2.3
500 93.9 92.8 1.1 0.9 1.1 500 97.6 80.5 17.1 0.7 3.2
1,000 94.7 93.9 0.8 0.9 0.8 1,000 97.0 82.8 14.2 1.0 1.5
5,000 95.5 94.9 0.7 0.5 0.6 5,000 96.2 84.7 11.4 0.5 1.7
10,000 95.7 95.5 0.2 0.8 0.8 10,000 95.2 85.3 10.0 0.8 1.0
30,000 95.8 95.9 0.0 0.4 0.4 30,000 93.5 88.0 5.4 0.6 1.3
Encoderoutputstates,MLP Encoderwordembeddings,MLP
50 97.7 93.3 4.4 1.5 3.2 50 98.5 76.6 21.8 0.9 6.9
100 96.2 93.6 2.7 1.0 1.4 100 97.0 81.4 15.6 0.7 3.0
500 97.2 94.5 2.7 0.3 0.9 500 97.8 87.4 10.3 0.4 1.9
1,000 96.8 94.9 1.9 0.7 0.7 1,000 97.7 89.8 7.9 0.5 1.4
5,000 97.6 95.7 1.9 0.4 0.5 5,000 98.4 92.7 5.6 0.2 1.0
10,000 98.0 96.2 1.8 0.7 0.7 10,000 98.7 93.5 5.2 0.2 1.0
30,000 97.7 96.1 1.6 0.6 0.7 30,000 98.4 94.2 4.1 0.6 1.2
English on the CzEng dataset8 [8]. The setup is based on [17], with the excep-
tion of splitting the sentences into words instead of subwords, as explained in
Sect.3;weuseavocabularyof25,000wordsthataremostfrequentintheparallel
training data.
We probe the source word embeddings and source encoder output states
for Universal POS with a linear classiﬁer (softmax) or a MLP with one hidden
layer of dimension 512, using the Universal Dependencies 1.4 version of the
Czech Prague Dependency Treebank [20]. We use the ﬁrst 500 sentences from
thetreebanktrainingdataastuningdatafortheprobingclassiﬁer,therestofthe
trainingdataisusedtocreatetheseenandunseensentencesets,usingeitherthe
fulldataorsubsamplingsmallersubsets.Theprobingclassiﬁeristhenevaluated
using the development part of the treebank using token-based evaluation. For
eachsetup,werepeattheexperiment10timeswithdiﬀerentsamplesoftheseen
and unseen sentences and report micro-average results.
By comparing the accuracies of the probing classiﬁer on seen and unseen
words in Table1, we can see that the memorization problem is clearly most
pronouncedwithstaticwordembeddings,wherethemagnitudeoftheeﬀect(the
diﬀerenceintheaccuracies)rangesfrom4pointsforthefulltrainingsetupto24
pointsforatrainingsetof50sentences,whileforthecontextualrepresentations,
8 http://ufal.mﬀ.cuni.cz/czeng.Measuring Memorization Eﬀect in Word-Level Neural Networks Probing 187
theeﬀectdoesnotsurpass5points.Thememorizationeﬀectismorepronounced
withthestrongerclassiﬁer,anddisappearsonlywiththelinearclassiﬁerapplied
to contextual representations when trained with the largest train set.
5 Conclusion
Wepresentedamethodformeasuringthememorizationeﬀectinword-levelprob-
ing of neural representations of words, based on a comparison of the accuracy
of the probing classiﬁer on symmetrically sampled comparable sets of seen and
unseen words. As we showed in a case study on probing for POS, our method
can measure the magnitude of the memorization problem and can thus serve as
ameansforselectinganappropriateprobingsetup,aswellasforestimatingthe
reliability of the ﬁndings of the probing experiment with respect to the threat
of mistaking memorization for generalization.
In future, we intend to tackle the shortcoming of our method of underrepre-
senting medium-frequency words. We also plan to apply the method to a wider
rangeofword-basedprobingtasks,aswellastomeasurethememorizationeﬀect
forexistingpreviousprobingworksandreassessresultsreportedbytheirauthors
from this perspective.
References
1. Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.S., et
al.:Acloserlookatmemorizationindeepnetworks.In:ProceedingsofICML,pp.
233–242 (2017)
2. Bakarov, A.: A survey of word embeddings evaluation methods. arXiv preprint
arXiv:1801.09536 (2018)
3. Belinkov,Y.,Durrani,N.,Dalvi,F.,Sajjad,H.,Glass,J.:Whatdoneuralmachine
translation models learn about morphology? In: Proceedings of ACL, Vancouver,
Canada, pp. 861–872 (2017)
4. Belinkov, Y., Glass, J.: Analysis methods in neural language processing: a survey.
In: TACL, vol. 7, pp. 49–72 (2019)
5. Belinkov,Y.,M`arquez,L.,Sajjad,H.,Durrani,N.,Dalvi,F.,Glass,J.:Evaluating
layersofrepresentationinneuralmachinetranslationonpart-of-speechandseman-
tic tagging tasks. In: Proceedings of IJCNLP, Taipei, Taiwan, pp. 1–10 (2017)
6. Bisazza, A., Tump, C.: The lazy encoder: a ﬁne-grained analysis of the role of
morphology in neural machine translation. In: Proceedings of EMNLP, Brussels,
Belgium, pp. 2871–2876 (2018)
7. Blevins,T.,Levy,O.,Zettlemoyer,L.:DeepRNNsencodesofthierarchicalsyntax.
In: Proceedings of ACL, Melbourne, Australia, pp. 14–19 (2018)
8. Bojar, O., et al.: CzEng 1.6: enlarged Czech-English parallel corpus with process-
ingtoolsdockered.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2016.
LNCS(LNAI),vol.9924,pp.231–238.Springer,Cham(2016).https://doi.org/10.
1007/978-3-319-45510-5 27
9. Carlini,N.,Liu,C.,Erlingsson,U´.,Kos,J.,Song,D.:Thesecretsharer:evaluating
and testing unintended memorization in neural networks. In: USENIX Security.
Santa Clara, CA, pp. 267–284 (2019)188 R. Rosa et al.
10. Conneau,A.,Kruszewski,G.,Lample,G.,Barrault,L.,Baroni,M.:Whatyoucan
cram into a single vector: probing sentence embeddings for linguistic properties.
arXiv preprint arXiv:1805.01070 (2018)
11. Dalvi, F., Durrani, N., Sajjad, H., Belinkov, Y., Vogel, S.: Understanding and
improving morphological learning in the neural machine translation decoder. In:
Proceedings of IJCNLP, Taipei, Taiwan, pp. 142–151 (2017)
12. Faruqui,M.,Tsvetkov,Y.,Rastogi,P.,Dyer,C.:Problemswithevaluationofword
embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276 (2016)
13. Helcl, J., Libovicky´, J., Kocmi, T., Musil, T., C´ıfka, O., Variˇs, D., et al.: Neural
monkey: the current state and beyond. In: Proceedings of AMTA, Boston, MA,
pp. 168–176 (2018)
14. Hewitt, J., Liang, P.: Designing and interpreting probes with control tasks. In:
Proceedings of EMNLP-IJCNLP, Hong Kong, China, pp. 2733–2743 (2019)
15. K¨ohn, A.: What’s in an embedding? Analyzing word embeddings through multi-
lingual evaluation. In: Proceedings of EMNLP, pp. 2067–2073 (2015)
16. Krueger, D., Ballas, N., Jastrzebski, S., Arpit, D., Kanwal, M.S., Maharaj, T.,
et al.: Deep nets don’t learn via memorization. In: Proceedings of ICLR, p. 4
(2017)
17. Libovicky´, J., Rosa, R., Helcl, J., Popel, M.: Solving three Czech NLP tasks end-
to-end with neural models. In: Proceedings of SloNLP (2018)
18. Linzen, T., Dupoux, E., Goldberg, Y.: Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. In: TACL, vol. 4, pp. 521–535 (2016)
19. Musil, T.: Examining structure of word embeddings with PCA. In: Ekˇstein, K.
(ed.) TSD 2019. LNCS (LNAI), vol. 11697, pp. 211–223. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-27947-9 18
20. Nivre,J.,etal.:Universaldependencies1.4,LINDAT/CLARIAH-CZdigitallibrary
at U´FAL MFF UK, Charles University (2016)
21. Qian, P., Qiu, X., Huang, X.: Investigating language universal and speciﬁc prop-
erties in word embeddings. In: Proceedings of ACL, pp. 1478–1488 (2016)
22. Shi, X., Padhi, I., Knight, K.: Does string-based neural MT learn source syntax?
In: Proceedings of EMNLP, Austin, Texas, pp. 1526–1534 (2016)
23. Stubbs, M.: Language corpora. In: Davies, A., Elder, C. (eds.) The Handbook of
Applied Linguistics, Chap. 4, pp. 106–132. Blackwell Publishing, Malden (2004)
24. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,etal.:
Attention is all you need. In: Guyon, I., et al. (eds.) NeurIPS 30, pp. 6000–6010.
Curran Ass (2017)Semi-supervised Induction of Morpheme
Boundaries in Czech Using a
Word-Formation Network
Jan Bodna´r(B) , Zdenˇek Zˇabokrtsky´ , and Magda Sˇevˇc´ıkova´
Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics,
Charles University, Prague, Czech Republic
jan.bodnar@seznam.cz, {zabokrtsky,sevcikova}@ufal.mff.cuni.cz
Abstract. This paper deals with automatic morphological segmenta-
tionofCzechlemmascontainedintheword-formationnetworkDeriNet.
Capturing derivational relations between base and derived lemmas, and
segmentinglemmasintosequencesofmorphemesaretwocloselyrelated
formalmodelsofhowwordscomeintoexistence.Thusweproposeanovel
segmentationmethodthatbeneﬁtsfromtheexistenceofthenetwork;our
solution constitutes new state-of-the-art for the Czech language.
· ·
Keywords: Morphological segmentation Morpheme Word
formation
1 Introduction
Morphological segmentation in a standard task in NLP, whose aim is to decom-
pose a word into a sequence of minimal meaning-bearing units called mor-
phemes (e.g., unreachable → un-reach-able). It has been tackled by a variety
of approaches (ranging from rule-based methods to modern machine learning
methods), and several shared tasks focused on it.
In this paper, we report a work in progress focused on morphological seg-
mentation of Czech lemmas. Czech is a morphologically rich language, both in
inﬂection and derivation. Large-coverage NLP tools for Czech inﬂection have
been developed since the 1990s (such as the MorfFlex CZ dictionary, currently
coveringaround1Mlemmas[3]),anddataresourcesforderivationareavailable,
too; cf. the word-formation network DeriNet [15]. However, to our knowledge,
thereis no publicly available large-scale machine-tractable segmentation lexicon
for Czech. We aim to build one.
The task of modeling inﬂection and derivation of a natural language and the
task of morphological segmentation are closely connected. Thus we hypothesize
that the information stored in derivational trees of DeriNet (see Fig.1 (a) for
a sample) could be used – in combination with deep learning methods – for
improving performance in morphological segmentation.
Relying on the existence of a derivational resource might look as a rather
exoticbottleneck,asderivationaldataaremuchscarcerthan,e.g.,dataresources
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.189–196,2020.
https://doi.org/10.1007/978-3-030-58323-1_20190 J. Bodn´ar et al.
forinﬂection.However,accordingto[7]thereareatleast22languagesforwhich
reasonably-scaled derivational databases exist, out of which data for 11 lan-
guages are available in the tree-shaped scheme used in our approach [6]. Thus
our approach could be viable also for several languages other than Czech.
2 Related Work
2.1 Morphological Segmentation as a Linguistic Task
Even if a morpheme is deﬁned simply as a phoneme/grapheme sequence asso-
ciated with a particular meaning that cannot be further subdivided, i.e., as the
smallestlinguisticsign,delimitationofmorphemesinCzechisdiﬃcultespecially
because individual morphemes are often attested in multiple variants (morphs)
due to allomorphy; cf. the morphs of the morpheme br (as in the verb br-a´-t ‘to
take’) in vy-b´ır-a-t ‘to choose’, v´ı-bˇer ‘choice’, and v´y-bor ‘committee’.
In the linguistic description of Czech, morphological segmentation as iden-
tiﬁcation of all morphemes within the word structure is considered a task of
morphology; e.g., lod’-k-a ’small boat’ is cut into the root morpheme lod’, the
(derivational)suﬃxk andthe(inﬂectional)endinga.Arelatedtaskofdelimiting
morphemesthatdistinguishawordfromanimmediatelysimplerword(e.g.lod’-
ka ’smallboat’fromlod’ ‘boat’)fallsunderderivationaspartofword-formation,
which is subsumed under lexicology in the Czech linguistic tradition.
These two perspectives are documented in existing NLP accounts and data
resources, too. Slav´ıˇckov´a’s retrograde dictionary [13], Weisheitelova´ et al. [16],
Osolsobˇe and Pala [9], or Skoumalova´ [12] aim at providing complete morpho-
logical segmentation, whereas Sˇiˇska’s dictionary of root allomorphs [11] and
Sˇimandl’s dictionary of aﬃxes [10] focus rather on word-formation analysis.
2.2 Approaches to Automatic Morphological Segmentation
The task of morphological segmentation is recognized in NLP for a long time,
and solutions have been proposed as early as in the 1950s. Diverse approaches
have been developed since, based, e.g., on (i) lists of aﬃxes and grammar rules,
(ii)supervisedmachinelearningmodels,(iii)unsupervisedmodelsthatoptimize
heuristiccriteriasuchasminimaldescriptionlength,or(iv)unsupervisedmodels
that optimize probability such as Bayesian models (see [1] for references).
One can ﬁnd empirical performance comparisons of various approaches in
publications related to shared tasks on morphological segmentation, such as [8]
or[5],butCzechisnotamongthestudiedlanguages. Asshownin[14],eventhe
modernmodelsconsideredasstate-of-the-artinthesesharedtasks,donotreach
reasonable accuracy for Czech.Semi-supervised Induction of Morpheme Boundaries in Czech 191
3 Our Annotated Data
3.1 Sample of Completely Segmented Lemmas
For training and evaluation purposes, we selected 2,100 lemmas from the Der-
iNet lemma set as follows: (1) 1,000 lemmas were sampled randomly with uni-
form probabilities, (2) additional 1,100 lemmas were sampled randomly with
probabilities proportional to their frequencies in the Czech National Corpus [4].
Morphemeboundariesinbothsetsweremanuallyannotated.Thebasicprin-
ciple of delimiting morphemes through their recurrence in words is challenged
byallomorphy,inparticular,ofrootmorphemes.InCzech,rootsoftenstartand
end in consonants, having a CVC (consonant-vowel-consonant), CC and other
structures, with the ﬁnal consonant alternated due to some suﬃxes (h´ak ‘hook’
> h´aˇc-ek ‘small hook’) and the middle vowel (if present) dropped or alternated
in individual allomorphs (cf. the allomorphy of the br root above).
While segmentation of preﬁxes is rather easy due to their limited number
andrelativelyregularpatternsdocumentedacrossthepart-of-speechboundaries
(e.g.,preﬁxvowellengtheninginverb-to-nounderivationvy-br-a-t ‘tochoose’>
v´y-bˇer ‘choice’), suﬃx parts of many lemmas allow for multiple analyses based
on diﬀerent analogies. For instance, the ova suﬃx is delimited in kup-ova-t ‘to
buy.imperf’ in contrast to koup-i-t ‘to buy.perf’; if propagated to kup-ova´-va-t
‘to buy.imperf-iter’, one obtains a lengthened variant (ov´a), which is not found
inotheriteratives. Analternative, moresubtlesegmentation (kup-ov-a-t >kup-
ov-a´v-a-t) which is applicable also to other iteratives (plav-a-t ‘to swim.imperf’
> plav-a´v-a-t ‘to swim.imperf-iter’) was thus preferred in the annotation.
Theresultingsetofcompletelysegmentedlemmaswasrandomlydividedinto
atrainingportion(1,050lemmas),adevelopmenttestportion(525lemmas),and
an evaluation test portion (525 lemmas).
3.2 Sets of Root Allomorphs for Selected DeriNet Trees
The complete manual segmentation drew our attention to native, high frequent
roots with individual allomorphy, which is hard to capture (cf. the analysis d´a-t
‘to give.perfective’ vs. d-a´-v-a-t ‘to give.imperfective’ provided by [13]).
In order to annotate these lemmas as consistently as possible, a simple
method was proposed on how to detect root morpheme boundaries in groups
of derivationally related words gathered in individual derivational trees in the
DeriNetnetwork,stillwithminimalannotationeﬀort.First,1,760biggestderiva-
tional trees were selected in DeriNet. Then an annotator added a list of all root
morpheme variants to each tree as a whole (actually the list was pre-generated,
inordertomaketheannotationevenfaster),withoutannotatingindividuallem-
mas in the tree. Then the longest-matching root allomorph was marked as the
rootmorphemeineachlemma.Thisresultedin240klemmaswithhighlyreliable
root-morpheme boundaries.192 J. Bodn´ar et al.
4 Morphological Segmentation Procedure
In general, our segmentation approach is constructed as a combination of rule-
basedandmachinelearningmethodsoperatingonseparatewords,withmethods
thatdetectandpropagatemorpheme-boundariesalongderivationaledgesinthe
DeriNet 2.0 derivational trees.
Our algorithm consists of four main parts, detection, propagation, prun-
ing, and post-processing. In the detection phase, the morpheme boundaries are
detectedseparatelyoneachword,withuseofsimplerulesandneuralclassiﬁers.
Inthisphase,wealsoaddmanuallyannotatedrootmorphemes.Inthepropaga-
tionphase,weoperateonDeriNet2.0derivationaltrees,andusethemtoinduce
new morphological boundaries, as well as to propagate already known bound-
aries to other words. The following pruning phase uses two neural classiﬁers for
removing wrong boundaries. In the ﬁnal post-processing phase, a small set of
rulesisusedtocorrectsystematicerrorsoftheclassiﬁers,andweonceagainuse
the manual root annotations, this time to add the annotated boundaries which
may have been removed, and to remove any further segmentation of annotated
roots.
4.1 Inducing Morpheme Boundaries from Derivational Trees
This approach consists of two techniques, boundary detection and boundary
propagation. Boundary detection tries to detect new morpheme boundaries by
examining the changes along derivational edges. For instance, the change in the
parent–child pair lod’ka ‘small boat’ > lodiˇcka ‘tiny boat’ can be used to reveal
the child’s internal structure. We ﬁrst use the edit-distance measure to ﬁnd the
most likely way how the words are aligned, and then examine the diﬀerence
between them. We see that lodiˇcka was created as [“lod’”, “+iˇc”, “ka”], i.e.,
lod’ remained the same, iˇc was inserted, and ka was repeated. The most likely
scenario is that iˇc is either a morpheme or a group of morphemes, and that
it was inserted between two morphemes, i.e., where a morpheme boundary is
assumed to be located. The induced segmentations are lod’-ka and lod-iˇc-ka.
Theboundarypropagationpropagatestheknownboundaries(beitfromthe
previousalgorithmor,e.g.,fromneuralnetworks),betweentwowordsconnected
with a derivational edge. For instance, if we have an edge between roz-dˇelit ‘to
distribute’>pˇrerozdˇelit ‘toredistribute’,wewouldliketotransfertheboundary
roz- into the second word. For this, we create the exact same mapping between
two words as above: pˇrerozdˇelit = [“+pˇre”, “rozdˇelit”]. We can conclude from
this mapping that the boundary can safely be translated into the second word
since the subword rozdˇelit, which contains our boundary, remained unchanged.
Theboundaryjustneedstobeshiftedinsidethewordbecausethreeletterswere
added in front of the boundary. This way, we get the boundary pˇreroz-dˇelit.
The actual algorithm applies both rules at the same time, while iterating
trough edges in a speciﬁc order: We start with leaves and create and propagate
boundariestotheirparents.Onceallchildrenofanodeareprocessed,boundaries
are propagated from this node to its parent. When the tree root is reached, theSemi-supervised Induction of Morpheme Boundaries in Czech 193
secondphasestartsandboundariesarepropagatedintheoppositedirection,i.e.,
fromparentstotheirchildren.Thisapproachensuresthatthewholepropagation
is handled in O(N), where N is the number of words in the tree.
This method helps us to ﬁnd boundaries that would otherwise remain unde-
tected, but it also leads to a relatively high number of false positives. For this
reason,thediﬀerencescausedbychangesinletteraccentsareignored,whilecom-
paring two words. This may cause some false negatives but the overall impact is
highly positive. Boundaries in the ﬁnal output are further pruned by classiﬁers
to increase precision.
4.2 Deep Learning Component
We use two neural network classiﬁers. Both of them are convolutional networks
withcharacterlevelembeddingstrainedaltogetherwiththeclassiﬁers.1 Foreach
position of the input word, they return values 0 to 1 signalizing whether there
is a morpheme-boundary on a given position of the word. The ﬁrst network
was trained on the manually annotated dataset mentioned in Sect.3.1, whereas
the second one was trained in a semi-supervised manner using a combination of
the manually annotated and of a synthetically generated dataset. The synthetic
dataset was generated by the complete algorithm as described in this paper,
without the ﬁnal post-processing, and cleanup phase. The goal was to make
the second classiﬁer learn from the tree-based algorithm and to smooth its out-
puts. Both classiﬁers are used with two thresholds, one for the addition of new
boundaries and the second one for removal of suspicious boundaries.
4.3 Adding Further Information on Morphemes
Our procedure also attempts to classify the identiﬁed morphemes as preﬁxes,
suﬃxes, or roots. Given a derivational tree with segmented lemmas, the root
morpheme of the lemma is identiﬁed in the root node of the tree, and the infor-
mation about the root morpheme is then propagated down the tree. With the
root being identiﬁed in all nodes, we distinguish preﬁx and suﬃx morphemes
(simply by their relative position with respect to the root morpheme), which is
certainly an oversimpliﬁcation that completely disregards the existence of inter-
ﬁx morphemes in compounds and other issues.
In the last step, a representative allomorph is assigned to each morpheme,
which could be considered “allomorph lemmatization”. We align morpheme
sequences of a parent lemma and a child lemma, allowing only links 1-1, 1-0,
and 0-1. Aligned morphemes are either written identically, or they are allo-
morphs. For each group of allomorphs, we chose their representative as the
topmost-appearing allomorph (the one that appears closest to the root node
of the derivational tree).
1 TensorFlow, http://tensorﬂow.org/.194 J. Bodn´ar et al.
Table1.Comparisonofourmethodwithotherapproachesonthedictionarydata[13]
Method Morphemes Boundaries Words
Precision Recall F1 Precision Recall F1 Accuracy
Our method 58.94% 62.62% 63.74% 96.11% 91.63% 93.81% 72.15%
EMmh, i1 64.82% 66.54% 65.67% 91.38% 87.42% 89.36% 35.45%
FlatCat unsup. 37.19% 21.57% 27.31% 97.98% 64.92% 78.10% 1.17%
FlatCat sup. 66.20% 57.72% 61.67% 92.59% 81.15% 86.49% 31.10%
5 Experiments and Evaluation
5.1 Evaluated Setups
Similarly to [14], we use three measures to evaluate the quality of generated
segmentations by comparing them to manually annotated data: (1) lemma cor-
rectness –thepercentage(accuracy)oflemmaswhosesegmentationiscompletely
correct, (2) morpheme correctness – the precision, recall, and f-measure of cor-
rectly recognized morphemes, and (3) boundary correctness – precision, recall,
and f-measure of correctly recognized boundaries between adjacent morphemes.
Wehaveexperimentedwithvarioussetsofrulesaswellaswithmultipleneu-
ral network architectures based on recurrent, convolutional and deconvolutional
networks with various hyper-parameters. Only the conﬁguration that achieved
the best results on the development set is presented here. In general, recurrent
neural networks did not work, probably due to the small training set, while the
bestresultswereachievedbyconvolutionalnetworkswith2convolutionallayers
(400ﬁlters,kernelsize4,stride1,ReLUactivation)andasinglefullyconnected
layerwithoneoutput(sigmoidactivation).However,thisarchitectureismoreof
an example, since slightly diﬀerent architectures had comparable results. With
thedeconvolutionsweexpectedtohelpspreadinginformationaboutmorphemes
through the word, but it did not yield any improvements.
The pipeline required certain structural optimization: we have experimented
with omitting various layers and also with using the same layer on multiple
places. Special care was needed to set-up the classiﬁers. Each of them has two
thresholds – one for adding a new boundary, and one for removing a boundary
from a place where it likely should not be. The ﬁrst threshold was set indepen-
dently for each classiﬁer in such a way that classiﬁer has the highest possible
recall, while not dropping bellow 95% precision. The second – removal – thresh-
oldswereconﬁguredtogetheronbothclassiﬁers.Theperformanceofthepipeline
wasevaluatedforvariouscombinationsofthresholds,andthevalueswhichhave
suﬃcient precision and a reasonable precision-recall balance were chosen. The
architecture of the classiﬁers was chosen on the basis of the maximum recall at
95% precision too, which resulted in choosing a simple CNN with two hidden
convolutional layers.Semi-supervised Induction of Morpheme Boundaries in Czech 195
5.2 Results
The performance of our algorithm was evaluated on the part of our manually
annotated dataset which was not used during previous development. We evalu-
atedtheperformanceinthreeways:Wemeasuredhowaccuratelythealgorithm
marksboundaries(Precision93.3%,Recall81.5%)andmorphemes(Prec.64.5%,
Rec.58.6%),andhowbigapercentageofwordswascorrectlysegmented(58.9%).
In comparison, semisupervised Morfessor Flatcat achieved results: Boundaries -
P:83.7%, R:35.7%; Morphemes - P:36.4%, R:21.2%, Words 26.7%. To compare
our method with Czech state of the art we evaluate it also on the retrograde
dictionary data. [13], on which we can compare it with EMmh method [14] as
well as with Morfessor FlatCat [2], as also evaluated in [14]; see Table1.
Figure1 (b) shows how precision and recall change as data passes through
the layers of the pipeline. Interestingly, classiﬁer 2 during its training learned to
partially mimic the behavior of the Tree Propagation. Because of this, it looks
as if all the work was done by the classiﬁers, and the Tree Propagation was not
useful at all. The plot was evaluated on our manually segmented dataset.
The outputs of the previous versions of the algorithm have undergone a
linguisticinspection,andmostoftheobservedsystematicerrorshavebeenﬁxed
by adding specialized rules into the post-processing phase. Yet there are still
someknownsourcesoferrorsremaining,suchascompounds,whichweareunable
tohandleinstandardways,andthereforewillneedaspecializedapproach.There
is also an issue with clean-up done by classiﬁers. As can be seen in Fig.1 (b),
their presence is essential, yet they tend to remove many correct segments.
Fig.1. (a) The derivational tree with the base noun most ‘bridge’ from DeriNet 2.0.
(b) The changes of precision, recall and F1 on diﬀerent layers of the pipeline.
6 Conclusions
Wehavepresentedanovelapproachtomorphologicalsegmentation,whichbene-
ﬁtsfromtheavailabilityofderivationaltreesandusesdeeplearningcomponents.
OursystemoutperformsprevioussolutionsdevelopedfortheCzechlanguage.A
natural extension of the task would be to segment also inﬂected word forms.196 J. Bodn´ar et al.
Acknowledgments. ThisworkwassupportedbytheGrantNo.GA19-14534Softhe
CzechScienceFoundation.Ituseslanguageresourcesdeveloped,stored,anddistributed
by the LINDAT/CLARIAH CZ project (LM2015071, LM2018101). We thank Michal
Kˇren for helping us query morpheme segmentation patterns in the Czech National
Corpus.
References
1. Goldsmith, J.A., Lee, J.L., Xanthos, A.: Computational learning of morphology.
Ann. Rev. Linguist. 3, 85–106 (2017)
2. Gr¨onroos, S.A., Virpioja, S., Smit, P., Kurimo, M.: Morfessor FlatCat: an HMM-
based method for unsupervised and semi-supervised learning of morphology. In:
Proceedings of COLING 2014, the 25th International Conference on Computa-
tional Linguistics: Technical Papers. pp. 1177–1185. Dublin City University and
Association for Computational Linguistics, Dublin, August 2014
3. Hajiˇc, J., Hlav´aˇcov´a, J.: MorfFlex CZ. LINDAT/CLARIN digital library at Insti-
tute of Formal and Applied Linguistics, Charles University in Prague (2016).
http://hdl.handle.net/11234/1-1673
4. Kˇren, M., et al.: SYN2015: representative corpus of written Czech (2015). http://
hdl.handle.net/11234/1-1593,LINDAT/CLARIAH-CZdigitallibraryattheInsti-
tute of Formal and Applied Linguistics (U´FAL). Faculty of Mathematics and
Physics, Charles University
5. Kurimo,M.,Virpioja,S.,Turunen,V.T.,Blackwood,G.W.,Byrne,W.:Overview
and results of morpho challenge 2009. In: Peters, C., et al. (eds.) CLEF 2009.
LNCS, vol. 6241, pp. 578–597. Springer, Heidelberg (2010). https://doi.org/10.
1007/978-3-642-15754-7 71
6. Kyj´anek,L.,Zˇabokrtsky´,Z.,Sˇevˇc´ıkov´a,M.,Vidra,J.:Universalderivationskickoﬀ:
acollectionofharmonizedderivationalresourcesforelevenlanguages.In:Proceed-
ings of DeriMo, vol. 2019, pp. 101–110 (2019)
7. Kyjnek,L.:Morphologicalresourcesofderivationalword-formationrelations.Tech.
rep. TR-2018-61, Faculty of Mathematics and Physics, Charles University (2018)
8. Liu, C.H., Liu, Q., Dublin, G.: Introduction to the shared tasks on cross-lingual
wordsegmentationandmorphemesegmentation.In:ProceedingsofMLP,pp.71–
74 (2017)
9. Osolsobˇe, K., Pala, K.: Czech stem dictionary for IBM PC XT/AT. In: Confer-
ence on Computer Lexicography, pp. 163–172. Hungarian Academy of Sciences,
Balatonfu¨red (1991)
10. Sˇimandl, J.: Slovn´ık aﬁx˚u uˇz´ıvany´ch vˇceˇstinˇe. Karolinum, Praha (2016)
11. Sˇiˇska, Z.: B´azovy´ morfematicky´ slovn´ıkˇceˇstiny. UPOL, Olomouc (2005)
12. Skoumalov´a,H.:ACzechmorphologicallexicon.In:ProceedingsoftheThirdMeet-
ing of the ACL Special Interest Group in Computational Phonology, Madrid, pp.
41–47. ACL (1997)
13. Slav´ıˇckov´a,E.:Retrogr´adn´ımorfematicky´slovn´ıkˇceˇstiny.Academia,Praha(1975)
14. Vidra, J.: Morphological segmentation of Czech words. Master Thesis (2018)
15. Vidra,J.,Zˇabokrtsky´,Z.,Kyj´anek,L.,Sˇevˇc´ıkov´a,M.,Dohnalov´a,Sˇ.:DeriNet2.0.
LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics,
Charles University in Prague (2019). http://hdl.handle.net/11234/1-2995
16. Weisheitelov´a, J., Kr´al´ıkov´a, K., Sgall, P.: Morphemic Analysis of Czech. MFF
UK, Prague (1982)Interpreting Word Embeddings Using
a Distribution Agnostic Approach
Employing Hellinger Distance
B
Tam´as Ficsor1( ) and Ga´bor Berend1,2
1 Institute of Informatics, University of Szeged, Szeged, Hungary
{ficsort,berendg}@inf.u-szeged.hu
2 MTA-SZTE Research Group on Artiﬁcial Intelligence, Szeged, Hungary
Abstract. Word embeddings can encode semantic and syntactic fea-
tures and have achieved many recent successes in solving NLP tasks.
Despite their successes, it is not trivial to directly extract lexical infor-
mation out of them. In this paper, we propose a transformation of the
embedding space to a more interpretable one using the Hellinger dis-
tance. We additionally suggest a distribution-agnostic approach using
KernelDensityEstimation.Amethodisintroducedtomeasuretheinter-
pretability of the word embeddings. Our results suggest that Hellinger
basedcalculationgivesa 1.35%improvementonaverageovertheBhat-
tacharyya distance in terms of interpretability and adapts better to
unknown words.
· ·
Keywords: Word embeddings Interpretability Computational
semantics
1 Introduction
There have been many successes in the ﬁeld of NLP due to the application of
word embeddings [3]. There is a new forefront as well called contextual embed-
dings (e.g., BERT), which further increases the complexity of models to gain
better performance. [2] showed there is only a small performance increase on
average regard to complexity, but this performance varies on each employed
task. Thus static embeddings still serve a good ground for initial investigations
about the interpretability.
Prior research by [12] has investigated the issue of semantic encoding in
word embeddings by assuming that the coeﬃcients across each dimensions of
theembeddingspacearedistributednormally.Thisassumptionmayormaynor
holdforaparticularembeddingspace(e.g.thenormalityassumptionisunlikely
to hold for sparse word representations), hence we argue for the necessity of
similaralgorithmsthatoperateinandistribution-agnosticmanner.Weintroduce
such a model that allows the word embedding coeﬃcients to follow arbitrary
distributionsbyrelyingonKernelDensityEstimation(KDE).Afurthernovelty
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.197–205,2020.
https://doi.org/10.1007/978-3-030-58323-1_21198 T. Ficsor and G. Berend
of our work is that we propose the application of the Hellinger distance – as
opposed to the Bhattacharyya distance – which could be a more suitable choice
due to its bounded nature. We also make our source code publicly available1 in
order to foster the reproducibility of our experiments.
2 Related Work
Word embeddings can capture the semantic and syntactic relationships among
words [9]. [15] was one of the ﬁrst providing a comparison of several word
embedding methods and showed that incorporating them into established NLP
pipelines can also boost their performance.
There are several ways to incorporate external knowledge into NLP mod-
els. Related methods include the application of auto-encoders [16], embedding
information during training [1] or after the training phase, called retroﬁtting
[5]. One way to understand the semantic encoding of a dimension in embed-
ding spaces is to link them to human interpretable features. [12] introduced
the SEMCAT dataset and a method that relies on the Bhattacharyya distance
for doing so. Their proposed method can produce a more interpretable space
where each dimension encodes a predeﬁned semantic category from the SEM-
CATdataset,whichwastestedonGloVe[11]wordembedding.Therehavebeen
various approaches to nd these semantic categories. Such an approach is to con-
struct datasets in a way which involves human participants only [8], or in a
semi-automated manner where the construction is based on statistics to make
the connections between the members of semantic categories and curated later
by human participants [13].
Our proposed approach relies on the application of the Hellinger distance,
whichhasalreadybeenusedinNLPforconstructingwordembeddings[7].Note
thatthewaywerelyontheHellingerdistanceisdiﬀerentfrompriorworkinthat
we use it for improving the interpretability of some arbitrarily trained embed-
ding, whereas in [7] the Hellinger distance served as the basis for constructing
the embeddings.
3 Our Approach
In this paper we follow a process to produce interpretable word vectors which is
similarto[12].Unlike[12],whotrainedtheirownGloVeembeddings,inorderto
mitigatethevariabilityduetotraining,weareusingthepre-trainedGloVewith
6 billion token as our embedding space with 300 dimensions. Furthermore the
SEMCAT dataset is going to serve as the deﬁnition of the semantic categories.
Instead of GloVe and SEMCAT other kinds of embeddings (e.g., fastText) and
datasets incorporating semantic relations (e.g., the McRae dataset [8]) can be
integrated into our framework.
1 https://github.com/ﬁcstamas/word embedding interpretability.Interpreting Word Embeddings Using a Distribution Agnostic Approach 199
3.1 Information Encoding of Dimensions
Theassumptionofnormalityoftheembeddingdimensionsisstatisticallyacon-
venient, however, empirically not necessarily a valid approach. As the normal
distribution is simple and well-understood, it is also frequently used in predic-
tive models, however, assuming normality could have its own ﬂaws [14]. The
assumption of normality plays an essential role in the method proposed by [12],
that we relax in this paper.
If we try to express the information gain from a dimension regarding some
concept, we can do so by measuring the distance between the concept’s and
dimension’s distribution. In order to investigate the semantic distribution of
semantic categories across all dimensions, we deﬁne WD ∈R|≥d0|×|c|, with |d| and
|c| denoting the number of dimensions of the embedding space and the number
of semantic categories, respectively.
Inthispaper,werelyontwometrics,BhattacharyyaandHellingerdistances.
The suggestion of Hellinger distance is an important step, as it is more sensi-
tive to small distributional diﬀerences when the ﬁdelity (overlap) of the two
distributions is close to 1, which can be utilized in case of dense embeddings.
Furthermore it is bounded on interval [0,1], which could be beneﬁcial for sparse
embeddings where the ﬁdelity has a higher chance of being close to 0 (causing
the Bhattacharrya distance to approach inﬁnity).
First we separate the ith dimension’s coeﬃcients into category (Pi,j) and
out-of-category (Qi,j) vectors. A coeﬃcient belongs to the Pi,j vector if the
associated word to that coeﬃcient is an element of the jth semantic category,
and it belongs to the Qi,j otherwise. It is going to be denoted for P and Q for
short.
ByassumingthatP andQarenormallydistributed,wecanderivetheclosed
form deﬁnitions for the Bhattacharyya and Hellinger distances as included in
Eqs.(1) and (2), respectively. In the below formulas μ and σ denote the mean
and standard deviation of the respective distributions.
(cid:2) (cid:2) (cid:3)(cid:3) (cid:2) (cid:3)
1 1 σp2 σq2 1 (μp−μq)2
DB(P,Q)= ln + +2 + (1)
4 4 σ2 σ2 4 σ2+σ2
q p p q
(cid:4)
(cid:5) (cid:7)
DH(P,Q)=(cid:5)(cid:6)1− 2σpσq e−14·(μσpp2−+μσqq2)2 (2)
σ2+σ2
p q
By discarding the assumption that P and Q are distributed normally, the
more general formulas are included in Eqs.(3) and (4) for the Bhattacharyya
and Hellinger distances
(cid:8)∞
(cid:9)
DB(p,q)=−ln p(x)q(x) dx (3)
−∞200 T. Ficsor and G. Berend
I
E Standardize ES × WNSD SignCorrection
SEMCAT + Distance WD Normalize WND
Fig.1.TheﬂowchartofthegenerationoftheinterpretablespaceI.Ereferstotheinput
wordembeddings,whereasWD denotesthematrixdescribingthesemanticdistribution
of the embedding. WD constructed from the distances of distributions of semantic
category (from SEMCAT) - dimension pairs.
(cid:4)
(cid:5)
(cid:5) (cid:8)∞(cid:9)
(cid:5)
DH(p,q)=(cid:6)1− p(x)q(x) dx (4)
−∞
with the integrand being the Bhattacharyya coeﬃcient, also called ﬁdelity. In
order to calculate the ﬁdelity, we can apply Kernel Density Estimation (KDE)
[6]forturningtheempiricaldistributionsofcoeﬃcientsP andQintocontinuous
(andnotnecessarilynormallydistributed)probabilitydensityfunctionspandq.
By calculating either the closed or the continuous form of distances, we can
calculateWD(i,j)=D(Pi,j,Qi,j),whereDisanyoftheabovedeﬁneddistances.
3.2 Interpretable Word Vector Generation
We normalize WD so, that each semantic category vector in WND sum up to 1
((cid:3) norm). This step is important because otherwise the dominance of certain
1
semantic categories could cause an undesired bias. Additionally, WNSD(i,j) =
sgn(Δi,j)WND(i,j), where Δi,j = μpi,j −μqi,j and sgn is the signum function.
This form of sign correction is useful as a dimension can encode a semantic
category in negative or positive direction and we have to keep the mapping of
the words in each dimension.
We standardize the input word embeddings in a way that each dimension
haszeromeanandunitvariance.WedenotethestandardizedembeddingsasES
and obtain the interpretable space of embeddings I as the product of ES and
WNSD.
3.3 Word Retrieval Test
In order to measure the semantic quality of I, we used 60% of the words
from each semantic category for training and 40% for validation. By using the
training words, we are calculating the distance matrix WD using either one of
the Bhattacharyya or the Hellinger distance. We select the largest k weights
(k ∈ {15,18,30,37,62,75,125,150,250,300}) for each category and replace the
other weights with 0 (WS). We are doing that, so we can inspect the strongest
DInterpreting Word Embeddings Using a Distribution Agnostic Approach 201
encoding dimensions generalization ability. Then in the calculation pipeline
(Fig.1) we are going to use WDS instead of WD, and we continue the rest of the
calculations as it was deﬁned earlier, by that we are going to obtain the inter-
pretablespaceIS.Wearegoingtorelyonthevalidationsetandseewhetherthe
words of a semantic category are seen among the top n, 3n or 5n words in the
correspondingdimensioninIS,wherenisthenumberofthetestwordsvarying
across the semantic categories. The ﬁnal accuracy is the weighted mean of the
accuracy of the dimensions, where the weight is the number of words in each
category for the corresponding dimension.
3.4 Measuring Interpretability
Tomeasuretheinterpretabilityofthemodel,wearegoingtouseafunctionally-
grounded evaluation method [4], which means it does not involve humans in the
process of quantiﬁcation. Furthermore we use continuous values to express the
level of interpretability [10]. The metric we rely on is an adaptation of the one
proposedin[12].Wedesiretohaveametricthatisindependentfromthedimen-
sionalityoftheembeddingspace,somodelswithdiﬀerentnumberofdimensions
can be easily compared.
IS+ = |Sj ∩Vi+(λ×nj)| (5)
i,j
nj
− |Sj ∩Vi−(λ×nj)|
IS = (6)
i,j
nj
In the same way we deﬁned the interpretability score for the positive
(5) and negative (6) directions. In both equations i represents the dimension
(i ∈ {1,2,3,...,|d|}) and j the semantic categories (j ∈ {1,2,3,...,|c|}). Sj
represents the set of words belonging to the jth semantic category, nj the num-
ber of words in that semantic category. V+ and V− gives us the top and bot-
i i
tom words selected by the magnitude of their coordinate respectively in the ith
dimension.λ×nj isthenumberwordsselectedfromthetopandbottomwords,
henceλ∈Nistherelaxationcoeﬃcient,asitcontrolshowstrictwemeasurethe
interpretability.Astheinterpretabilityofadimension-categorypa(cid:10)ir,wetaket(cid:11)he
maximum of the positive and negative direction, i.e. ISi,j =max ISi+,j,ISi−,j .
Oncewehavetheoverallinterpretability(ISi,j),wearegoingtocalculatethe
categoricalinterpretabilityEq.(7).Wethoughtthatitisatoooptimisticmethod
to decide the interpretability level based on the maximum value in each selec-
tion. It is apparent from ISi = maxjISi,j, taking the max for every dimension
would overestimate the true interpretability, because it would take the best-
case scenario. Instead, we calculate Eq.(7), where we have a condition on the
selected i which is deﬁned by Eq.(8). We are going to select from the given
interpretability scores provided by ISi,j (where j is ﬁxed) the ith value where i
is the maximum in the jth concept in WD(i,j). This condition Eq.(8) ensures
thatwearegoingtoobtaintheinterpretabilityscorefromthedimensionswhere202 T. Ficsor and G. Berend
the semantic category is encoded. This method is more suitable to obtain the
interpretability scores, because it is relying on the distribution of the semantic
categories, instead of the interpretability score from each dimension.
ISj =ISi∗,j ×100 (7)
j
i∗j =i(cid:3) WD(i(cid:3),j). (8)
Finally,togettheoverallinterpretabilityoftheembeddingspace,wehaveto
calculatetheaverageoftheinterpretabilityscoresacrossthesemanticcategories,
where C is the number of categories.
Accuracy of the word embedding Interpetability of the word embedding
a b
Fig.2. Values from Table1 with n test words in word retrieval test in a and Table2
with 60% of the categories used in b.
4 Results
Weloadthemostfrequent50,000wordsfromthepre-trainedembeddingssimilar
to[12]andtestedfortheirnormalityusingtheBonferronicorrectedKolmogorov-
Smirnov test for multiple comparisons. Our test showed that 183 of the dimen-
sions are normally distributed (p > 0.05). [12] reported more dimensions to
behave normally, which could be explained by the fact that the authors trained
theirownGloVeembeddings.Wedeemthisasanindicationfortheneedtowards
thekindofdistributionagnosticapproachesweproposebyrelyingonKDE.Dur-
ing the application of KDE, we utilized a Gaussian kernel and a bandwidth of
0.2 throughout all experiments.
4.1 Accuracy and Interpretability
Table1 and Table2 contains the quantitative performance of the embeddings
from two complementary angles, i.e. their accuracy and interpretability. TheseInterpreting Word Embeddings Using a Distribution Agnostic Approach 203
Table 1. Performance of the model on word category retrieval test for the top
n,3n and 5n where n is the number of test words varying across the categories.
k(∈{15,18,30,37,62,75,125,150,250,300}) is the number of top weight kept from
WD in each category. The method was discussed in Sect.3.3
k 15 18 30 37 62 75 125 150 250 300
n
ClosedformofBhattacharyya 13.18 13.85 14.84 14.67 15.61 16.05 15.58 15.66 15.69 15.64
ClosedformofHellinger 13.44 13.27 14.46 14.85 15.55 15.34 15.84 15.75 15.99 16.13
BhattacharyyaKDE 12.54 12.86 14.06 14.29 15.23 15.58 16.05 16.08 16.10 16.13
HellingerKDE 13.09 13.71 14.55 15.14 15.43 15.75 16.04 16.04 15.96 16.16
3n
ClosedformofBhattacharyya 25.76 27.25 29.53 30.61 32.92 33.71 34.15 34.30 33.39 33.18
ClosedformofHellinger 25.35 26.87 29.74 30.73 32.36 33.77 34.03 34.56 34.82 34.73
BhattacharyyaKDE 24.76 26.20 29.06 29.82 31.72 32.16 33.59 33.48 33.63 33.57
HellingerKDE 25.32 27.39 29.88 30.38 32.54 33.27 34.27 34.38 34.50 34.41
5n
ClosedformofBhattacharyya 34.53 36.43 39.65 40.56 43.24 43.51 44.21 45.03 44.68 44.30
ClosedformofHellinger 33.92 36.05 39.15 40.41 42.87 43.30 44.59 44.15 45.00 44.94
BhattacharyyaKDE 33.07 34.41 37.90 39.15 42.55 43.01 44.30 44.68 45.18 45.27
HellingerKDE 34.10 35.79 39.33 40.21 42.87 43.39 44.73 44.65 45.00 45.12
Table2.InterpretabilityscoresfortheinterpretablespaceIwithdiﬀerentλparameter
values (λ = 1 the most strict and λ = 10 the most relaxed) using diﬀerent distances.
Ther∈{100,80,60}percentageofthewordskeptfromthesemanticcategoriesrelative
to category centers
λ1 2 3 4 5 6 7 8 9 10
100%ofthewords
GloVe 2.82 4.84 6.83 8.7210.3712.0813.3414.5515.7916.87
ClosedformofBhattacharyya.35.3448.8456.4761.3565.0168.2170.8172.4273.8875.45
ClosedformofHellinger36.3249.9457.6462.7566.7269.5272.0874.0975.5476.72
BhattacharyyaKDE35.4749.0556.6961.6065.3568.3770.5772.5374.0275.31
HellingerKDE36.2449.4957.3562.7366.6369.5671.9274.0475.4276.78
80%ofthewords
GloVe 1.85 3.42 4.91 6.33 7.69 9.0010.2111.3412.2013.07
ClosedformofBhattacharyya23.9636.9945.7051.6655.3759.1361.9664.5066.4067.91
ClosedformofHellinger24.3638.3647.1853.3257.4961.0963.3565.8967.9169.48
BhattacharyyaKDE25.0839.0446.8052.7057.1060.7363.1865.2667.1668.62
HellingerKDE24.5738.3447.1653.0957.2260.5463.3865.7067.8269.38
60%ofthewords
GloVe 1.05 1.87 2.62 3.71 4.71 5.67 6.59 7.47 8.209.08
ClosedformofBhattacharyya12.4422.7630.7236.6141.3845.0047.8950.6452.7855.02
ClosedformofHellinger13.1224.3633.1439.2443.6647.2550.7653.4255.6957.57
BhattacharyyaKDE15.0126.4434.9240.2244.6648.1051.0153.4555.8757.56
HellingerKDE13.3724.3632.7439.5143.9447.3650.6553.3055.8257.95204 T. Ficsor and G. Berend
resultsarebettertobeobservedjointly(Fig.2)sinceitispossibletohaveahigh
score for interpretability but a low value for accuracy suggests that the original
embedding has a high variance regarding to the probed semantic categories.
Figure2a illustrates a small sample of the results where we can observe that
a word’s semantic information is encoded in few dimensions, since relying on
a reduced number of coeﬃcients from WD achieves similar performance to the
applicationofallthecoeﬃcients.Ourresultstendtohaveclosevalues,whichcan
be caused by the high number of normally distributed dimensions. The results
show that the proposed method is at least as good as [12]’s method, but it can
be applied to any embedding space without restrictions.
5 Conclusions
The proposed method can transform any non-contextual embedding into an
interpretable one, which can be used to analyze the semantic distribution which
can have a potential application in knowledge base completion.
We suggested the usage of Hellinger distance, which shows better results
in terms of interpretability when we have more words per semantic categories.
Furthermore,easiertoanalyzetheHellingerdistanceduetoitsboundednature.
By relying on KDE, our proposed method can be applied even in cases when
the normality for the coeﬃcients of the dimensions is not necessarily met. This
allows our approach a broader range of input embeddings to be applicable over
(e.g., sparse embeddings).
The proposed modiﬁcation on interpretability calculation, opened another
dimensionoffreedom.Itletuscomparetheinterpretabilityofwordembeddings
with diﬀerent dimensionality. So for every embedding space, the compression of
semantic categories can be observed and the modiﬁcation gives us a better look
at the encoding of semantic categories, because we probe the category words
from dimensions where they are deemed to be most likely encoded.
Acknowledgements. This research was supported by the European Union and co-
funded by the European Social Fund through the project “Integrated program for
training new generation of scientists in the ﬁelds of computer science” (EFOP-3.6.3-
VEKOP-16-2017-0002) and by the National Research, Development and Innovation
Oﬃce of Hungary through the Artiﬁcial Intelligence National Excellence Program
(2018-1.2.1-NKP-2018-00008).
References
1. Alishahi, A., Barking, M., Chrupa(cid:4)la, G.: Encoding of phonology in a recurrent
neuralmodelofgroundedspeech.In:Proceedingsofthe21stConferenceonCom-
putational Natural Language Learning (CoNLL 2017), pp. 368–378 (2017)
2. Arora,S.,May,A.,Zhang,J.,R´e,C.:Contextualembeddings:whenaretheyworth
it? arXiv preprint arXiv:2005.09117 (2020)
3. Chen,Y.,Perozzi,B.,Al-Rfou,R.,Skiena,S.:Theexpressivepowerofwordembed-
dings (2013)Interpreting Word Embeddings Using a Distribution Agnostic Approach 205
4. Doshi-Velez,F.,Kim,B.:Towardsarigorousscienceofinterpretablemachinelearn-
ing (2017)
5. Faruqui,M.,Dodge,J.,Jauhar,S.K.,Dyer,C.,Hovy,E.,Smith,N.A.:Retroﬁtting
word vectors to semantic lexicons. In: Proceedings of NAACL (2015)
6. Hwang,J.N.,Lay,S.R.,Lippman,A.:Nonparametricmultivariatedensityestima-
tion: a comparative study. Trans. Sig. Proc. 42(10), 2795–2810 (1994)
7. Lebret, R., Collobert, R.: Word embeddings through hellinger PCA. In: Proceed-
ings of the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics (2014)
8. McRae, K., Cree, G., Seidenberg, M., Mcnorgan, C.: Semantic feature production
norms for a large set of living and nonliving things. Behav. Res. Methods 37,
547–59 (2005)
9. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed represen-
tations of words and phrases and their compositionality (2013)
10. Murdoch,W.J.,Singh,C.,Kumbier,K.,Abbasi-Asl,R.,Yu,B.:Deﬁnitions,meth-
ods, and applications in interpretable machine learning. Proc. Natl. Acad. Sci.
116(44), 22071–22080 (2019)
11. Pennington, J., Socher, R., Manning, C.: Glove: global vectors for word represen-
tation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 1532–1543 (2014)
12. Senel, L.K., Utlu, I., Yucesoy, V., Koc, A., Cukur, T.: Semantic structure and
interpretabilityofwordembeddings.IEEE/ACMTrans.AudioSpeechLang.Proc.
26(10), 1769–1779 (2018)
13. Speer, R., Chin, J., Havasi, C.: Conceptnet 5.5: an open multilingual graph of
general knowledge (2016)
14. Taleb, N.N.: The Black Swan: The Impact of the Highly Improbable, 1st edn.
Random House, London (2008)
15. Turian, J., Ratinov, L.A., Bengio, Y.: Word representations: a simple and general
method for semi-supervised learning. In: Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pp. 384–394 (2010)
16. Yin, P., Zhou, C., He, J., Neubig, G.: StructVAE: tree-structured latent variable
models for semi-supervised semantic parsing. In: Proceedings of the 56th Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),
pp. 754–765 (2018)Verb Focused Answering from CORD-19
B
Elizabeth Jasmi George( )
MT-NLP Lab, LTRC, International Institute of Information Technology, Hyderabad,
Hyderabad, India
elizabeth.george@research.iiit.ac.in
Abstract. At this time of a pandemic turning into an infodemic, it is
signiﬁcanttoanswerquestionsaskedontheresearchrelatedtothat.This
paper discusses a method of answering questions leveraging the syntac-
tic structure of the sentences to ﬁnd the verb of action in the context
correspondingtotheactioninthequestion.Thismethodgeneratescor-
rectanswersformanyfactoidquestionsondescriptivecontextpassages.
The proposed method ﬁnds all the sentences in the passage, which has
the same or synonymous verb as the verb in the question, processes the
dependenciesoftheverbsobtainedfromthedependencyparserandpro-
ceeds with further rule-based ﬁltering for matching the other attributes
of the answer span. We demonstrate this method on CORD-19 data [3]
evaluated with free form natural language questions.
1 Introduction
Machine reading comprehension (MRC) is the essential task of textual question
answering, in which each question is given a related context from which the
answershouldbeinferred.Thispaperpresentsamethodforansweringanatural
languagequestionfromasingledescriptivepassage,inwhichtheverbdescribing
theactioninthequestionisidentiﬁed,andthepassageisanalyzedforcandidate
sentenceshavingverbssynonymouswiththeverbinthequestionoutofwhichthe
answer can be deduced. Machine comprehension systems are particularly suited
tohigh-volume,rapidlychanginginformationsources.Themosteﬀectivewayof
understanding a passage is by answering multiple questions on the passage, and
it requires domain knowledge [8].
The human reader starts to comprehend by skimming the passage to get
a general idea about the text, followed by scanning the passage to get some
speciﬁc information. We speculate that one of the approaches during scanning
is attempting to identify the verb in the question and ﬁnding a similar verb in
the passage. A question will often have a main verb in it. Our machine reader
focuses on pruning the passage text based on the required action comparable to
the one mentioned in the question.
Supported by organization Advainet Solutions Private Limited.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.206–213,2020.
https://doi.org/10.1007/978-3-030-58323-1_22Verb Focused Answering from CORD-19 207
2 Related Work
The diﬀerent datasets, their comparison [23], and approaches for MRC are
described by [25]. The methods and trends for MRC are explained by [15] and
an investigation on the popular benchmarks in MRC is done by [12]. There are
popular non-neural methods used in MRC, such as the bag of words [9], sliding
window,logisticregression,TF-IDFboostedmethod,andintegratedtriaging[7].
The neural methods include mLSTM+Ptr, DCN, GA [4], BiDAF, FastQA [5],
and QAnet [24]. There are methods incorporating reading strategies [19] and
discourse relations [11].
Somesyntax-basedcomprehensionmethodslike[22]replacetheoptionsfrom
the multiple-choice answers in the question and compare it with the sentences
in the context passage. While the state-of-the-art results are obtained by neural
models relying on embeddings such as Bert [2] and Electra [6], they hugely
depend on the quality and quantity of training data and the ﬁne-tuning of the
hyper-parameters.WhilethemajorityofquestionansweringsystemsonCORD-
19 [3] depend on BioBERT [14] or SciBERT [10] language models explicitly
trained on biomedical texts and scholarly articles, we present a syntax-based
method which can produce signiﬁcant answers. Our approach can be applied
successfully to multiple datasets, supports free form questions, and does not
require any training with a speciﬁc dataset.
3 Proposed Method
We propose a method for answering free form natural language English ques-
tions on any passage given as context. Our approach begins with dependency
parsing the question and the passage using StanfordCoreNLP [17] to obtain the
typed dependency relations existing in them. The question word is identiﬁed,
and all the nouns and verbs in the question are ﬁltered along with their cor-
responding dependencies. Structural connections establish dependency relations
between words [20]. The Stanford parser [18] converts the parse into a depen-
dency tree. For every element that one has in the utterance at hand, there is
exactlyonenodeinthesyntacticstructurethatcorrespondstothatelement.One
of the advantages of dependency parsers for NLP is that the parse can be eas-
ily encoded in a table. The output obtained from Stanford CoreNLP parser [17]
consistsoftupleslike{‘dep’: ‘nsubj’, ‘governor’: 6, ‘governorGloss’: ‘celebrated’,
‘dependent’: 5, ‘dependentGloss’: ‘Koch’}1 which mainly consist of a governor-
Gloss, dependentGloss and a dependency relation existing between them. In the
dependency tree obtained for a sentence, there will be edges labeled with the
dependency relation from the governorGlosses to the dependentGlosses.
Themodulesinoursyntax-basedMRCisshowninFig.1.TheQuestionhan-
dler doesalltheprocessing,suchasPOStagging,verbandnounﬁltering,ﬁnding
the dependency relation associated with verbs, and question word identiﬁcation
1 This is one of the tuples obtained for the sentence “Christina Koch celebrated with
a thumbs up as she got out of the Souyz capsule”.208 E. J. George
Fig.1. Modules in this syntax-based MRC
onthequestionutterance.ThePassagehandler processesthepassagebyprepro-
cessing and separating the sentences. Finding the dependency relations of verbs
in it, getting the list of lemmatized verbs and POS tagging the sentences in the
passage.TheAnswer handler identiﬁescandidateanswers,whicheithermatches
the verb in the question or has a synonymous verb as that in the question or
otherwise,hasthemaximumtextspanmatchingwiththequestion.TheAnswer
selector module pinpoints the answer to the question by iterating through the
dependency tuples obtained from the dependency parser. For the named enti-
ties that could not be ﬁltered from the candidate answer, the whole candidate
sentence is output as the answer. The following section outlines the answering
strategy.
3.1 Strategy for Answering Questions
The context passage is a scholarly article from the CORD-19 dataset UID:
br33p9xd,titled“PreventiveBehaviorsConveyedonYouTubetoMitigateTrans-
mission of COVID-19”. The question asked is, “Does COVID spread by con-
tact?”.
The algorithm selects the article whose title and abstract contain the named
entitieswhichsigniﬁcantlyoverlapwiththelistofnamedentitiesofthequestion
asked. The answering strategy is explained in the following sections.
IdentifytheQuestionWord,VerbsandNouns. Questionwordisidentiﬁed
fromthequestion,andallthenounsandalltheverbsinthequestionareﬁltered
withtheircorrespondingdependencies.Thedependencyparsedquestionisgiven
in Fig.2. The parts-of-speech tagged question is [(‘does’, ‘VBZ’), (‘COVID’,
‘NN’), (‘spread’, ‘VBN’), (‘by’, ‘IN’), (‘contact’, ‘NN’), (‘?’, ‘.’)]Verb Focused Answering from CORD-19 209
Fig.2. Dependency parsed question
Preprocess the Question and the Passage. The question and the passage
sentences are converted to lowercase and preprocessed by converting the verbs
in the question to their lemmatized form. WordNet Lemmatizer in NLTK [16]
is used for lemmatizing the verbs. The question word is Does, which is a polar
question. The nouns in the question are ‘COVID’: ‘NN’, ‘contact’: ‘NN’, and
the verbs in the question are [(‘do’, ‘VBZ’, 0), (‘spread’, ‘VBN’, 2)].
The 141 sentences in the article given as context contain 185 verbs and 160
nouns as follows: [(‘cross-sectional’, ‘NNP’), (‘study’, ‘NNP’), (‘monitoring’,
‘VBG’)],[(‘travis’,‘NNP’),(‘sanchez’,‘NNP’),(‘reviewed’,‘VBN’)],[(‘author’,
‘NN’), (‘information’, ‘NN’), (‘article’, ‘NNP’), (‘notes’, ‘VBZ’)], [(‘coron-
avirus’, ‘NN’), (‘spreads’, ‘VBZ’)], [(‘system’, ‘NN’), (‘meet’, ‘VBP’)],.....
[(‘surveillance’, ‘NNP’), (‘are’, ‘VBP’), (‘provided’, ‘VBN’)]
Find the Candidate Sentences by Matching the Verbs. The verbs in
the question are compared with the verbs in the passage, by iterating through
the dependency tuples and comparing the V.+ nodes. If they match, then that
sentence from the passage is added to the candidate sentence list. The auxiliary
verbssuchas‘am’, ‘are’, ‘is’, ‘was’, ‘were’, ‘can’, ‘could’, ‘may’, ‘might’, ‘must’,
‘shall’, ‘should’, ‘will’, ‘would’, ‘do’, ‘does’, ‘did’, ‘has’, ‘have’ can frequently
occur in most English passages and they can often overshadow the main verbs.
Sotheauxiliaryverbsaredeprioritizedwhileconsideringthepassagetoidentify
the presence of the verb from the question. The auxiliary verbs are considered
only when there is no main verb. In the question “Does COVID spread by
contact?”, the main verb ‘spread’ is considered instead of ‘does’ for comparison
with verbs in context passage sentences. The possible answer from the original
passage is obtained as ‘COVID-19 is largely spread by contact with respiratory
droplets from an infected individual’. The dependency structure of the sentence
with a matching verb is given in Fig.3.
Find the Candidate Sentences if Synonymous Verbs Match. If a sen-
tence contains a verb that is synonymous with the verb in the question, then
that sentence is added to the candidate sentence list. Thesaurus and Synonyms
are generated using NLPCompromise packages [13]. The sense of the word is
also included while ﬁnding synonyms to avoid confusion between noun and verb
senses.Ifcandidatesentencesarenotobtainedontheexactverbmatch,theyare
retrieved through the synonymous verb match by replacing the question verbs
with each verb in the synonym list and rechecking the passage for a match.
For the verb, ‘spread’, the synonyms ‘grow, increase, escalate, advance, develop,
broaden, proliferate’ are also considered.210 E. J. George
Fig.3. Dependency parsed matching sentence from the passage
Select Sentences with the Highest Cosine Similarity and Longest
Matching Span. Out of the candidate sentences collected, those with the
longest matching span, and the best cosine similarity are selected to be the
answer sentence. For passages describing many actors doing the same action in
diﬀerent contexts, all the passage sentences with the action verb will be match-
ing. The order of their occurrence in the passage is not signiﬁcant for deciding
their candidature for being the answer to the question. So, in that case, the
cosine similarity of the matching passage sentence with the question is evalu-
atedformakingthebestchoice.Thelongestmatchingspanisalsoevaluatedfor
reinforcing the decision.
Find the Candidate Sentences with Matching Predicates. If there is no
matchingverbinthepassage,thenthepredicateofthequestionismatchedwith
the sentences in the passage, to get the answer. The nouns in the best-matched
sentences are compared with the nouns in question to get the most suitable
candidate sentence as the answer sentence.
Obtain Answer Phrase from the Answer Sentence. Once the answer
sentence is identiﬁed, the answer to a ‘Who’ question is found out by iterating
through the dependency relation tuples obtained from the dependency parser
and matching ‘governorGloss’ of the dependency tuple with the verb in the
questionandthehaving‘dep’ as‘nSubj’ or‘amod’.Thesentencewiththeverbs
convertedtotheirlemmatizedformisusedtoidentifytheanswers.Iftheanswer
is a noun compound, the chain of nouns in the answer is obtained by following
the dependency relation ‘compound’ until it reaches the last modiﬁer from the
head noun. To ﬁnd the answer to ‘How’ questions, ‘dependentGloss’ of depen-
dencies‘advmod’and‘amod’arefoundout.Foranswering‘What’questions,the
matching verb’s dependent ‘dependentGloss’ with a dependency relation ‘dobj’
is found out.
4 Results and Evaluation
This approach of utilizing the syntactic structure and matching the verbs is
eﬃcient in the cases where the passage is action-oriented. However, this method
is not suitable if inference calculation, paraphrase understanding, or coreference
resolutionisrequiredforﬁndingtheanswer.Thisapproachshowslessereﬃciency
when a chain of verbs is embedded in the sentence. Comparing to the deep
learningmethodsinMRCrequiringlongpassagesastrainingdata,thisapproach
using syntax obtains answers with fewer data.Verb Focused Answering from CORD-19 211
Table 1. Evaluation results of our machine reader
Data source #Documents #Questions Genre EM p@1 p@3 Hit@3
considered
CORD-19 2 14 Scholarly – 0.64 0.71 0.71
Articles
NewsQA 4 4 Crowd 25% 0.5 0.75 0.75
sourced
questions
on news
articles
Table 2. Context passages, questions and computed answers
Context Question Computed answer
passage
CORD-19 What are the caution and contraindication
UID: 41jqgsv0 precautions needed with chloroquine and
while using hydroxychloroquine
hydroxychloroquine as expectedly, some precautions
a drug? will be need while use both
these drugs that include
frequent monitoring of
hematological parameters
(rbc, wbc and platelet
counts), measurement of
serum electrolytes, blood
glucose (because of
hypoglycemic potential of
hcq) and hepatic as well as
renal functions
CORD-19 What is the retinopathy be a
UID: recommended dose? dose-limiting adverse eﬀect of
cpu3q9o6 hydroxychloroquine, and a
safe daily dose appear to
correspond to 6.5mg/kg of
ideal body weight and
5.0mg/kg of actual body
weight [8]
For evaluating our system, we use some extractive metrics mentioned in a
survey on machine reading comprehension systems [1]. The metrics applied are
(i) Exact Match (EM) or Accuracy - the percentage of answers that exactly
matchwiththecorrectanswers(ii)Precision@K-thenumberofcorrectanswers
in the ﬁrst K returned answers without considering the position of these correct
answers(iii)Hit@K-countofthenumberofsampleswheretheirﬁrstKreturned212 E. J. George
answers include the correct answer. The evaluation results of our reader on a
few samples from CORD-19 [3] and NewsQA [21] are given in Table1, and the
context passages, questions, and computed answers are given in Table2.
5 Conclusion and Future Work
Finding out the answer span is the ﬁrst step in reading comprehension, which
is attained in this work. Even when the verb in the question is not present
in the passage, the matching span congruence and similarity calculations iden-
tify the best candidate answer if the question is answerable. The future work
would incorporate paraphrasing, inferences, and sentence reduction techniques
to answer questions from the syntactic structure obtained as dependency tuples
with enhanced results from Stanza [26].
References
1. Baradaran, R., Ghiasi, R., Amirkhani, H.:. A Survey on Machine Reading Com-
prehension Systems. arXiv preprint arXiv:2001.01582 (2020)
2. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)
3. COVID-19 Open Research Dataset (CORD-19) (2020). Version 2020-03-20.
https://pages.semanticscholar.org/coronavirus-research. https://doi.org/10.5281/
zenodo.3715505. Accessed 11 Apr 2020
4. Dhingra, B., Liu, H., Cohen, W.W., Salakhutdinov, R.: Gated-Attention Readers
for Text Comprehension. ACL (2016). https://arxiv.org/pdf/1606.01549.pdf
5. Weissenborn, D., Wiese, G., Seiﬀe, L.: FastQA: A Simple and Eﬃcient Neural
Architecture for Question Answering. ArXiv abs/1703.04816: n. pag (2017)
6. Clark, K., Luong, M.-T., Le, Q.V., Manning, C.D.: Electra: Pre-training text
encodersasdiscriminatorsratherthangenerators.arXivpreprintarXiv:2003.10555
(2020)
7. Wu,F.,Li,B.,Wang,L.,Lao,N.,Blitzer,J.,Weinberger,K.Q.:Integratedtriaging
forfastreadingcomprehension.In:WWW2018,Lyon,France,23–27April,2018.
ArXiv abs/1909.13128: n. pag (2019)
8. Hirsch, E.: Reading Comprehension Requires Knowledge-of Words and the World
(2003)
9. Hirschman,L.,Light,M.,Breck,E.,Burger,J.D.:Deepread:areadingcomprehen-
sion system. ACL In: Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics (1999). https://doi.org/10.3115/1034678.1034731
10. Beltagy,I.,Lo,K.,Cohan,A.:SciBERT:apretrainedlanguagemodelforscientiﬁc
text.In:Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-
guageProcessingandthe9thInternationalJointConferenceonNaturalLanguage
Processing (EMNLP-IJCNLP), pp. 3606–3611 (2019)
11. Narasimhan, K., Barzilay, R.: Machine comprehension with discourse relations.
In:Proceedingsofthe53rdAnnualMeetingoftheAssociationforComputational
Linguistics and the 7th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pp. 1253–1262 (2015). https://doi.org/10.3115/
v1/p15-1121Verb Focused Answering from CORD-19 213
12. Kaushik, D., Lipton, Z.C.: How Much Reading Does Reading Comprehension
Require?ACriticalInvestigationofPopularBenchmarks.EMNLP(2018).https://
doi.org/10.18653/v1/d18-1546
13. Kelly, S., et al.:. Compromise- modest natural-language processing in Javascript.
https://www.npmjs.com/package/compromise. Accessed 2019/11/09
14. Lee, J., et al.: BioBERT: a pre-trained biomedical language representation model
for biomedical text mining, Bioinformatics 36(4), 1234–1240 (2020). https://doi.
org/10.1093/bioinformatics/btz682
15. Liu,S.,Zhang,X.,Zhang,S.,Wang,H.,Zhang,W.:Neuralmachinereadingcom-
prehension: methods and trends. Appl. Sci. 9(18), 3698 (2019). https://doi.org/
10.3390/app9183698, ArXiv abs/1907.01118 (2019): n. pag
16. Loper,E.,Bird,S.:NLTK:TheNaturalLanguageToolkit(2002).https://doi.org/
10.3115/1118108.1118117. ArXiv cs.CL/0205028: n. pag
17. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S.J., McClosky, D.:
ThestanfordCoreNLPnaturallanguageprocessingtoolkit.In:Proceedingsofthe
52nd Annual Meeting of the Association for Computational Linguistics: System
Demonstrations, pp. 55–60 (2014). https://doi.org/10.3115/v1/p14-5010
18. de Marneﬀe, M.-C., MacCartney, B., Manning, C.D.: Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC (2006)
19. Sun, K., Yu, D., Yu, D., Cardie, C.: Improving Machine Reading Comprehension
withGeneralReadingStrategies.NAACL-HLT(2018). https://doi.org/10.18653/
v1/n19-1270
20. Tesni`ere, L.: Elements of Structural Syntax (2015). https://www.jbe-platform.
com/content/books/9789027269997
21. Trischler,A., et al.: NewsQA:amachine comprehensiondataset. Rep4NLP@ACL
(2016). https://doi.org/10.18653/v1/w17-2623
22. Wang, H., Bansal, M., Gimpel, K., McAllester, D.: Machine comprehension with
syntax,frames,andsemantics.In:Proceedingsofthe53rdAnnualMeetingofthe
Association for Computational Linguistics and the 7th International Joint Con-
ference on Natural Language Processing (Volume 2: Short Papers), pp. 700–706
(2015)
23. Yatskar, M.: A qualitative comparison of CoQA, SQuAD 2.0 and QuAC. In: Pro-
ceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies, Volume 1 (Long
andShortPapers),Minneapolis,MN,USA,3–5June2019,pp.2318–2323(2019).
https://arxiv.org/abs/1809.10735
24. Yu,A.W.,etal.:QANet:CombiningLocalConvolutionwithGlobalSelf-Attention
for Reading Comprehension. ArXiv abs/1804.09541 (2018). n. pag
25. Zhang, X., Yang, A., Li, S., Wang, Y.: Machine Reading Comprehension: a Liter-
ature Review (2019). https://arxiv.org/abs/1907.01686
26. Qi,P.,Zhang,Y.,Zhang,Y.,Bolton,J.,Manning,C.D.:Stanza:apythonnatural
language processing toolkit for many human languages (2020). https://arxiv.org/
abs/2003.07082Adjusting BERT’s Pooling Layer
for Large-Scale Multi-Label
Text Classiﬁcation
Jan Leheˇcka(B) , Jan Sˇvec , Pavel Ircing , and Luboˇs Sˇm´ıdl
Department of Cybernetics, University of West Bohemia in Pilsen,
Pilsen, Czech Republic
{jlehecka,honzas,ircing,smidl}@kky.zcu.cz
Abstract. Inthispaper,wepresentourexperimentswithBERTmodels
inthetaskofLarge-scaleMulti-labelTextClassiﬁcation(LMTC).Inthe
LMTC task, each text document can have multiple class labels, while
the total number of classes is in the order of thousands. We propose a
pooling layer architecture on top of BERT models, which improves the
quality of classiﬁcation by using information from the standard [CLS]
tokenincombinationwithpooledsequenceoutput.Wedemonstratethe
improvements on Wikipedia datasets in three diﬀerent languages using
public pre-trained BERT models.
·
Keywords: Text classiﬁcation BERT model
1 Introduction
Inpresentdays,textclassiﬁcationtaskplaysaveryimportantroleamongNatu-
ral Language Processing (NLP) problems. With increasing amounts of available
electronictexts,thereisanaturalneedtoclassifythosetextsautomaticallyinto
predeﬁned classes. In many real-world problems, however, a text can belong to
more than one class (e.g. news article about hurricane can be classiﬁed into
both weather and disaster classes). The NLP task whereone text document can
belong to more than one class, is known as Multi-label Text Classiﬁcation.
Whenthenumberofclassesthetextcanpossiblybelongtoreachestheorder
of thousands, it is known as Large-scale Multi-label Text Classiﬁcation problem
(LMTC) [2].
In the last few years, deep neural networks based on Transformers [4] has
dominated the research ﬁeld of NLP and NLU (Natural Language Understand-
ing). Self-attention [13] Transformers, known as BERT (Bidirectional Encoder
RepresentationsfromTransformers),hasachievedamazingresultsinmanytasks,
including multi-label text classiﬁcation [1,11], sentiment analysis [9], language
modeling [15] or text summarization [7].
BERT-based classiﬁcation models typically use the feature vector generated
for a special classiﬁcation token (denoted as [CLS]), which was used during pre-
training for next sentence prediction. In this task, the classiﬁcation token was
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.214–221,2020.
https://doi.org/10.1007/978-3-030-58323-1_23Adjusting BERT’s Pooling Layer for LMTC 215
used to distinguish between inputs with two consequent sentences and inputs
with two randomly chosen sentences from a corpus, i.e. it was pre-trained to a
classiﬁcation problem with two classes.
When using only output from the [CLS] token, the information about the
document’s classiﬁcation must be encoded in a single vector, which in the case
of BERT-base architecture has 768 elements. In this paper, we hypothesize that
for LMTC tasks where the number of labels is in the order of thousands, the
classiﬁcationcouldbeinaccurateduetoinformationcompression.Anothermoti-
vationforourproposedsolutionwasthefactthatthetextclassiﬁcationresultis
often based on a presence or absence of a strong keyword in the documents. In
our solution, the information about such strong keywords can be easily pooled
out from the sequence of output features (one feature vector per input token)
and thus it can contribute to the classiﬁcation results more directly. We call the
top layer which converts BERT’s output into classiﬁcation a Pooling Layer.
The paper is organized as follows. Section2 describes related work we are
aware of. In Sect.3, we propose novel architecture of BERT’s pooling layer and
Sect.4oﬀersdetailsaboutexperimentalsetup.ResultsaresummarizedinSect.5
and in the ﬁnal Sect.6, we discuss achieved results.
2 Related Work
The automatic text classiﬁcation problem has been studied intensively during
the last decades with signiﬁcant improvements scored recently by models based
onTransformers[4].SincetextclassiﬁcationtasksarealsopartofGLUEbench-
mark [14], it attracts the most successful researchers in NLU ﬁeld in present
days to compete1.
BERT model was successfully applied to document classiﬁcation tasks for
example in [1], while an exhaustive set of experiments concerning ﬁne-tuning
methods of BERT models for text classiﬁcation was published in [11]. In both
mentioned papers, the number of target classes was rather small (in the order
of tens).
Large-scale multi-label text classiﬁcation with thousands of labels has been
studied in [2]. In this paper, BERT model was used to classify legislative docu-
ments into a set of 4.3 thousand of labels. It was shown that ﬁne-tuned BERT
model signiﬁcantly outperforms all previously published attention-based deep
neural networks models in a LMTC task.
Moreover, when the number of labels raises to extreme values (e.g. order
of millions), the standard approaches using one neuron per label in the output
layer easily reach hardware limitations. This task is known as XMC (extreme
multi-label text classiﬁcation) and BERT-based solution was presented in [3].
The paper proposes X-BERT, a scalable solution to ﬁne-tune BERT models on
the XMC problem by building label representations.
The majority of published works use BERT with default pooling layer, i.e.
the classiﬁcation is based only on the ﬁnal hidden state of the [CLS] token. In
1 https://gluebenchmark.com/leaderboard.216 J. Leheˇcka et al.
Fig.1. Architecture of pooling layer in baseline model (a) and in our proposed model
(b).
[8],theeﬀectofdiﬀerentpoolinglayersontopofBERTmodelswasstudied.The
mean-pooling of hidden states performed the best in this paper experiments. In
[10], another two eﬀective pooling strategies have been proposed. The idea here
was to pool all intermediate representations of the [CLS] token. In this paper,
we propose a pooling layer which combines [CLS] token with pooled sequence
output. As far as we know, there is no published work similar to our proposed
solution for LMTC task.
3 Proposed Solution
We propose a model where the classiﬁcation result is based not only on the
information from the ﬁnal hidden state of the ﬁrst token (i.e. [CLS] token) but
alsoontheinformationfromthefulloutputsequence.Thediﬀerencebetweenthe
standard use of BERT models in classiﬁcation tasks and our proposed solution
is depicted in Fig.1.
As a baseline approach, we took a standard pooling layer used on top of
BERT models for classiﬁcation tasks (model (a) in Fig.1). In this model, theAdjusting BERT’s Pooling Layer for LMTC 217
classiﬁcation is based only on the information encoded in the ﬁnal hidden state
of [CLS] token, which is fully connected to a feed forward layer with as many
neuronsasthenumberofdiﬀerentlabelsinthedataset.Inthisﬁnalclassiﬁcation
layer, each neuron computes output for one label. The output is then squeezed
between 0 and 1 by a sigmoid activation function.
Our proposed model (model (b) in Fig.1) uses additional information from
ﬁnal hidden states of input tokens t1,t2,...tN. The BERT’s sequence output
is fetched into time-distributed fully connected dense layer with the number of
neurons equal to the number of labels. The output of this layer is pooled in
two diﬀerent ways using the max-pooling and the average-pooling. Max-pooling
outputs the maximum of each feature across all tokens, thus it reacts on strong
class-related keywords. The average-pooling, on the other hand, outputs the
average of each feature over the sequence and thus attends to all tokens in
the sequence evenly. To compute average-pooling, we clipped all features into
interval[−1,1]tointentionallysuppresstheinﬂuenceofstrongkeywords.When
computing pooled values, we ignored all padding tokens (denoted as [PAD]).
Finally, the features generated from the pooled output and the output for the
[CLS] token are summed together. The sigmoid functions predict the output of
the model in the form of class probabilities.
As we involved one extra time-distributed dense layer into our solution, we
slightly increased the number of trainable parameters (by BERT’s hidden size
times the number of labels). However, we did not observe any slowdown of the
ﬁne-tuning.
4 Experimental Setup
Since the ﬁrst paragraph of Wikipedia pages typically contains short text sum-
mary (which should be enough to classify the whole text), we limited input
sequences to the ﬁrst 128 tokens.
The output for each classiﬁed document is a vector of per-label scores (soft
predictions), which must be converted into binary hard prediction. We used the
following thresholding strategy:
Given a vector of per-label scores s=(s1,s2,...,sK), assign i-th label, if
s
i ≥p (1)
max(s1,s2,...,sK)
withthresholdpoptimizedondevelopmentdata.Thissimpleyeteﬀectivestrat-
egy was used for example by two winning teams in WISE 2014 Challenge [12].
For each experiment, we used the training dataset to ﬁne-tune the BERT
model, the development dataset for early stopping and to estimate optimal
thresholds and the test dataset to evaluate models. As an evaluation metric,
we used sample-averaged F1 score, which is the harmonic mean of precision P
and recall R:
2PR
F1= , (2)
P +R218 J. Leheˇcka et al.
tp
P = , (3)
tp+fp
tp
R= , (4)
tp+fn
where tp are true positives (number of correctly assigned labels), fp false posi-
tives (incorrectly assigned labels) and fn false negatives (missed labels).
4.1 BERT Models
For all our experiments, we used BERT-base model architecture, i.e. all models
had 12 transformation blocks, 12 attention heads, 110 million trainable param-
eters and the hidden size of 768 neurons.
ForEnglishdataset,weuseduncasedvariantsofpublishedpretrainedBERT-
base model2. For Czech and Slovak, we used recommended cased variant of
published pretrained multi-lingual model3.
Fine-Tuning. We run ﬁne-tuning on a single GPUs using the keras-bert
library4. During the ﬁne-tuning, we used adapters [5] and updated only about
2% of the model’s parameters, namely self-attention normalization layers, feed
forward normalization layers and the adapter layers.
All models were ﬁne-tuned with the same setup: learning rate 1e−4 with
RAdam [6] and categorical entropy loss, maximum sequence length 128 tokens,
batchsize32andmaximum50epochswithearlystoppingafter3non-improving
epochs. The improvement in the mean of the loss function was measured on the
developmentdataaftereachepoch.Mostofthetime,themodelconvergedafter
approximately 10 epochs, which took about 7h of ﬁne-tuning on a single Tesla
T4 GPU.
4.2 Datasets
We tested the proposed approach on Wikipedia datasets. We used individual
Wikipedia pages as documents (instances) and associated categories as labels
(classes). We chose 3 languages: Czech and Slovak, which are at our main focus,
and English to incorporate also one widely-studied dataset.
To keeptheproblem inthelimits ofaLSMC,which typically contains thou-
sands of labels (classes), we randomly selected an appropriate subset of each
dataset. This was especially important for English dataset, which has about
6 million of pages associated with more than 1.35 million diﬀerent categories
in total. Classifying documents into such a large collection of classes is a diﬀer-
ent task known as XMC (extreme multi-label text classiﬁcation), which cannot
2 https://storage.googleapis.com/bert models/2018 10 18/uncased L-12 H-768 A-
12.zip.
3 https://storage.googleapis.com/bert models/2018 11 23/multi cased L-12 H-
768 A-12.zip.
4 https://github.com/CyberZHG/keras-bert.Adjusting BERT’s Pooling Layer for LMTC 219
Table 1. Datasets statistics. For each dataset, we are showing the total number of
documents (split into train, development and test subsets) and a total number of
diﬀerentlabels.Sincewearedealingwiththemulti-labelproblem,wearealsoshowing
average per-document label count and average per-label document count.
Dataset #trainDocs #devDocs #testDocs #labels #labels/doc #docs/label
English 44651 14884 14884 1212 2.7 165.7
Czech 63631 21211 21211 2720 3.9 153.6
Slovak 75523 25175 25175 1268 1.5 147.2
be solved by standard pooling layer on top of BERT model due to extreme
computational complexity [3].
SincetheXMCtaskisnotinthefocusofthispaper,werestrictedWikipedia
dataset for each language by following these steps:
1. Extract all pages and associated categories from the latest Wikipedia dump.
2. Randomly select subcorpus containing 200 thousand pages.
3. To avoid rare labels, keep only categories with at least N = 50 pages in
selected subcorpus.
4. Remove pages without any associated label from selected subcorpus.
5. Split dataset into train-dev-test in ratio 60:20:20.
In this way, we created a dataset for each of three selected languages with
thenumberoflabelssuitableforLSMCtask.Thedetailsaboutcreateddatasets
are shown in Table1.
5 Results
Our achieved results are summarized in Table2. For each dataset, we ﬁne-tuned
the pre-trained BERT model with four diﬀerent output layers. We run each
experiment ﬁve times and report mean results with standard deviations.
Thebaselineusesonlyinformationencodedintheﬁnalhiddenstateof[CLS]
token,whichisthestandardapproachofhowtouseBERTmodelinclassiﬁcation
tasks. The other three tested pooling layers combine information from [CLS]
token with pooled sequence output as depicted in Fig.1(b). In all three variants
oftheproposedpoolinglayer,theﬁnalclassiﬁcationisbasedonasimpleelement-
wise vector addition of information from [CLS] token and pooled information.
To make the addition possible, vectors [CLS], AVG and MAX must have the
same size (one feature per label), which is ensured by setting a corresponding
number of neurons in the preceding feed forward layers.
As can be seen from the results, adding only the output from the average-
pooling ([CLS]+AVG rows) is not beneﬁcial. On the contrary, this pooling layer
adjustment harmed the classiﬁcation performance in all of our experiments. On
the other hand, for all tested datasets, adding information from max-pooling220 J. Leheˇcka et al.
Table 2. Results table. The abbreviations used for the pooling layer are following:
[CLS] stands for using information encoded in the ﬁnal hidden state of [CLS] token,
AVGisaverage-pooledinformationfromBERT’ssequenceoutput,MAXismax-pooled
informationfromBERT’ssequenceoutputandthe“+”symbolstandsforelement-wise
addition of vectors.
Dataset BERT-model Pooling layer F1[%]
English public English (uncased) [CLS] (baseline) 81.64 ± 0.13
[CLS]+AVG 81.05 ± 0.10
[CLS]+MAX 82.10 ± 0.12
[CLS]+MAX+AVG 82.26 ± 0.14
Czech public multi-lingual (cased) [CLS] (baseline) 77.41 ± 0.38
[CLS]+AVG 77.07 ± 0.31
[CLS]+MAX 77.92 ± 0.25
[CLS]+MAX+AVG 79.86 ± 0.17
Slovak public multi-lingual (cased) [CLS] (baseline) 88.70 ± 0.14
[CLS]+AVG 88.54 ± 0.03
[CLS]+MAX 89.07 ± 0.08
[CLS]+MAX+AVG 89.15 ± 0.12
([CLS]+MAX rows) improved F1 score and additional adding of AVG vector
leads to further improvement.
Tosummarizetheachievedresults,addingthemax-pooledinformationfrom
BERT’s output always improved F1 score while average-pooled information is
only advantageous in combination with max-pooling. A simple two-sided t-test
conﬁrmed that our improvements are statistically signiﬁcant with p<0.005 for
all tested datasets.
6 Conclusion
In this paper, we have presented our experiments with BERT models in the
task of Large-scale Multi-label Text Classiﬁcation (LMTC) in three languages
(English, Czech and Slovak). We proposed pooling layer adjustment leading to
animprovementintermsofF1score.Resultsofourexperimentsconﬁrmedthat
usingpooledinformationfromBERT’ssequenceoutputcanbeusedasadditional
informationinordertoenhancethequalityofmulti-labeltextclassiﬁcationwith
thousands of labels.
We obtained the best results when the document classiﬁcation was based on
thecombinedinformationoftheclassiﬁcationtokenwithbothmax-andaverage-
pooled information from BERT’s sequence output. The absolute improvement
of F1 varies between 0.3% for Slovak dataset and 3.3% for Czech dataset with
more than two times more labels.Adjusting BERT’s Pooling Layer for LMTC 221
Forfutureresearch,itwouldbeinterestingtoinvestigateindetailtherelation
between the total number of labels and improvement gained from the proposed
solution.
Acknowledgments. This research was supported by the Ministry of Culture of the
Czech Republic, project No. DG18P02OVV016.
References
1. Adhikari, A., Ram, A., Tang, R., Lin, J.: DocBERT: BERT for document classiﬁ-
cation. arXiv preprint arXiv:1904.08398 (2019)
2. Chalkidis, I., Fergadiotis, M., Malakasiotis, P., Androutsopoulos, I.: Large-scale
multi-label text classiﬁcation on EU legislation. arXiv preprint arXiv:1906.02192
(2019)
3. Chang,W.C.,Yu,H.F.,Zhong,K.,Yang,Y.,Dhillon,I.:X-BERT:extrememulti-
labeltextclassiﬁcationusingbidirectionalencoderrepresentationsfromtransform-
ers. arXiv preprint arXiv:1905.02331 (2019)
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)
5. Houlsby, N., et al.: Parameter-eﬃcient transfer learning for NLP. arXiv preprint
arXiv:1902.00751 (2019)
6. Liu, L., et al.: On the variance of the adaptive learning rate and beyond. arXiv
preprint arXiv:1908.03265 (2019)
7. Liu,Y.,Lapata,M.:Textsummarizationwithpretrainedencoders.arXivpreprint
arXiv:1908.08345 (2019)
8. Ma, X., Xu, P., Wang, Z., Nallapati, R., Xiang, B.: Universal text representation
from BERT: an empirical study. arXiv preprint arXiv:1910.07973 (2019)
9. Rietzler, A., Stabinger, S., Opitz, P., Engl, S.: Adapt or get left behind: domain
adaptation through BERT language model ﬁnetuning for aspect-target sentiment
classiﬁcation. arXiv preprint arXiv:1908.11860 (2019)
10. Song, Y., Wang, J., Liang, Z., Liu, Z., Jiang, T.: Utilizing BERT intermediate
layers for aspect based sentiment analysis and natural language inference. arXiv
preprint arXiv:2002.04815 (2020)
11. Sun,C.,Qiu,X.,Xu,Y.,Huang,X.:Howtoﬁne-tuneBERTfortextclassiﬁcation?
In: Sun, M., Huang, X., Ji, H., Liu, Z., Liu, Y. (eds.) CCL 2019. LNCS (LNAI),
vol. 11856, pp. 194–206. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-32381-3 16
12. Tsoumakas, G., et al.: WISE 2014 challenge: multi-label classiﬁcation of print
media articles to topics. In: Benatallah, B., Bestavros, A., Manolopoulos, Y.,
Vakali, A., Zhang, Y. (eds.) WISE 2014. LNCS, vol. 8787, pp. 541–548. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-11746-1 40
13. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, pp. 5998–6008 (2017)
14. Wang,A.,Singh,A.,Michael,J.,Hill,F.,Levy,O.,Bowman,S.R.:Glue:amulti-
task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 (2018)
15. Wang,C.,Li,M.,Smola,A.J.:Languagemodelswithtransformers.arXivpreprint
arXiv:1904.09408 (2019)Recognizing Preferred Grammatical Gender
in Russian Anonymous Online Confessions
B
AntonAlekseev1( ) andSergeyNikolenko1,2
1 Samsung-PDMIJointAICenter,SteklovMathematicalInstituteatSt.Petersburg,
St.Petersburg191023,Russia
anton.m.alexeyev@gmail.com
2 NeuromationOU,10111Tallinn,Estonia
Abstract. We present annotation results for a dataset of public anonymous
online confessions in Russian (“Overheard/Podslushano” group in VKontakte,
poststagged#family).Unlikemanyothercaseswithonlinesocialnetworkdata,
intentionallyanonymouspostsdonotcontainanyexplicitmetadatasuchasage
or gender. We consider the problem of predicting the author’s preferred gram-
maticalgenderforself-reference,aproblemthatprovedtobesurprisinglyhard
andnotreducibletosimplemorphologicalanalysis.Wedescribeanexpertlabel-
ing of a dataset for this problem, show the ﬁndings of predictive analysis, and
introducerule-basedandmachinelearningapproaches.
· ·
Keywords: Anonymousdata Userproﬁling RussianNLP
1 Introduction
TheWeb2.0erahasbroughtanabundanceofuser-generatedcontent(UGC)inpopular
social online services, both general-purpose such as Facebook, MySpace, VKontakte,
or Odnoklassniki, and specialized such as Instagram for images, Last.FM for music
etc. Some portals add a certain degree of anonymity, e.g., 4chan allows completely
anonymousresponses,ask.fmallowstoaskregisteredusersanonymousquestions,etc.,
and an important recent trend is the rise in the desirable level of anonymity; see also,
e.g., [5,7]. Anonymous texts lack important metadata usually available in social net-
workproﬁlessuchasage,gender,location,etc.Previousstudiesthathadaimedtopre-
dict, e.g., demographic information given user-generated texts [3,9,12], tried to mine
forcluesthatcomeupinananonymoustext:e.g.,amentionof“mywife”stronglysug-
geststhemaritalstatusoftheauthor.Or,ifauserreferstothemselvesinthefeminine
grammaticalgender,thisdoesnotleadtostrongconclusionsbutstillcanbeusedasa
featureforvariousauthorproﬁlingtasks(thoughactualusefulnessisyettobestudied)
orgeneratingcorrectautomaticresponsesfortheuser.
Inthiswork,wehavefocusedontheanalysisofanonymoustextswrittenbyVKon-
taktesocialnetworkusersintheOverheard (“Podslushano”)group1.Atypicalpostin
1https://vk.com/overhear,withmorethan3,987,000usersreadingthecommunityasofFebru-
ary20,2020.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.222–230,2020.
https://doi.org/10.1007/978-3-030-58323-1_24RecognizingPreferredGrammaticalGenderinRussian 223
this community is a short text; the editors claim that “people share their secrets, rev-
elationsandreal-lifesituationsanonymouslyinfrontofamassiveaudience”.Judging
bythehashtags(specialtokensusuallystartingwith#)providedbymoderators,oneof
the most popular topics in Overheard is family. Many posts touch upon serious cases
ofadversechildhoodexperiences,homeabuse,sex-relatedproblems,etc.Weconsider
the problem of detecting the grammatical gender of the author in short Russian texts;
e.g.,theauthorofthesentence
(“ButwhyamItoblame?”)prefersthemasculinegender.Russianspeakers
can easily recognize the gender of verbs, but there are many cases where the speaker
doesnotrefertothemselvesandthegrammaticalgenderremainsunclear.Notethatwe
aretalkingstrictlyaboutgrammaticalgender:sometimespeoplechangetheirgenderin
writing,andwearenotsuggestingaone-to-onecorrespondence.
The contribution of this work is twofold: ﬁrst, we introduce the dataset of 2603
Overheard postsinRussianannotatedwiththeauthors’preferredgrammaticalgender
(PGG); second, we develop and compare several models for PGG prediction. In the
paper,Sect.2reviewsrelatedwork,Sect.3describestheoriginaldataset,Sect.4intro-
ducesthebaselinemodels,inSect.5wereportanddiscusstheproblemandourresults,
andSect.6concludesthepaper.
2 RelatedWork
Anonymity on the Web has been studied and discussed in numerous research works,
e.g., [5,7]. We have found one study analyzing the Overheard community data: the
thesis[6]measuresthedynamicsoftopicsinthepostsdynamicsandreportsthatlarge
UGC-drivencommunitiesinevitablyshifttowardsfamily,healthissues,midlifecrises
etc.AlargenumberofworkshaveusedVKontaktedata;e.g.,thework[13]claimsto
have constructed the largest so far Russian-language sentiment analysis dataset based
onVKontakteposts.
Tothebestofourknowledge,thisistheﬁrststudyoftextauthor’spreferredgram-
maticalgenderpredictionforRussianoranyotherSlaviclanguages.
3 Data
We have collected all anonymous posts in the Overheard (Podslushano) VKontakte
community from January 4th, 2014 to November 27th, 2019 that were annotated
with #family hashtags by the moderators. The texts are usually free of mis-
spellingsandgrammaticalerrors.Wehavecollected6,803#familyposts(latestatthe
momentofcollection),removedthehashtags,andannotatedarandomsampleof2,631
ofthem.
The annotation task follows the proposed research question: each text is assigned
a label of “femn” (feminine grammatical gender), “masc” (masculine), or “unk”
(unknown).Wetookaformalapproach,instructingtheannotatorstouseonlygrammat-
ical clues, overlooking, e.g., the mention of a “wife” unless there is a masculine self-224 A.AlekseevandS.Nikolenko
reference2.Basedontheexperienceofseveralunsuccessfulattemptsatcrowdsourcing,
wehaveaskedannotatorstopayspecialattentiontoreportedspeechandquotations.Ifthe
quotewasrelatedtotheauthor(someoneaddressingthemusingsomegrammaticalgen-
der),thiswasavalidclue,butifthequotewasaboutadifferentpersononeobviouslycould
notuseittoderive
Basictextstatisticsoftheprepareddatasetarethefollowing:“femn”,1539(58.4%)
posts,11129sentences;“unk”,878(33.4%)posts,4743sentences;“masc”,214(8.1%)
posts,1574sentences.ToevaluatetheapproachesdescribedinSect.4,wehavesplitthe
dataintotraining(80%),development(10%),andtest(20%)sets,keepingthesharesof
classes(“femn”,“masc”,“unk”)approximatelythesameasinthewholedataset.
4 Prediction
Inthissectionweintroducethebaselinemodels.
Bag-of-Words. Inthebag-of-words (BoW)approach, thetextistreatedasamul-
tiset of its tokens. Clearly, BoW-based models will not be able to distinguish which
personagivengenderedverbrefersto.Still,weusethisasthesimplestbaselinepos-
sible,trainingalogisticregressionontokensandtokenbigramsasfeatures3,withand
withoutlemmatization(donewiththeRNNMorphanalyzer[4]).
Syntax-AwareRule-BasedApproach.InRussian,PGGpredictioncanbesolved
(uptoapoint)withafewsimplesyntax-basedrules,forexample:
2Wehavemadeseveralattemptstotackletheannotationtaskviacrowdsourcingplatforms,updat-
ingtheinstructionsandaddingmoreadvancedqualiﬁcationtests.However,mostannotatorsstill
derivedthegenderbasedonstereotypes,sowehadtoaskourownexpertstolabelthedata,which
explainsthemodestsizeofthecorpus.
3Wehaveusedtokensandbigramsavailableinthetrainingsetwiththeminimumdocument
frequencyof3;scikit-learn’s[11]defaultTF-IDFweightingschemewasemployed.RecognizingPreferredGrammaticalGenderinRussian 225
The algorithm is shown in Listing 1. The input of the algorithm is a dependency
grammar syntax tree with set of vertices V and set of arcs E. The recursive func-
tion IsGender checks whether the given subtree of a vertex vfrom allows to make
aconclusionthatauthorprefersgrammaticalgendergen.conjAllowisaparameter
thathelpstoworkwithverb“sequences” mentionedabove.ThefunctionIsGender
returnsMaybeNo,MaybeYesorCertainlyYes.Havingappliedthefunctiontoall
sentences in a text, we aggregate the outputs, ordering these values by “strength” for
eachgender:CertainlyYes(cid:2)MaybeYes(cid:2)MaybeNo.Asimilarprocedureisper-
formed in function AggChildren computer after the recursive calls of IsGender
onallsubtreesofvfrom.
To preprocess the data for syntax-based methods, we have used the UDPipe [14]
pretrainedmodelforRussian“russian-syntagrus-ud-2.5-191206.udpipe”4 [15].
“Bag-of-Arcs” Gradient Boosted Decision Trees Approach. The relative efﬁ-
ciency of the previous approach (at least compared to bag-of-words) leads to the idea
of allowing a machine learning model to construct syntax-based rules by itself. We
4As of June 11, 2020, the model is available at: https://lindat.mff.cuni.cz/repository/xmlui/
handle/11234/1-3131.226 A.AlekseevandS.Nikolenko
Table1.PGGpredictionresults(Pr—precision,Re—recall).
macro femn masc unk
Method F1 Pr Re Pr Re Pr Re
Bag-of-words,lemmatized 0.4923 0.682 0.695 0.278 0.233 0.534 0.537
Bag-of-words,tokenized 0.6148 0.762 0.750 0.526 0.465 0.578 0.611
Syntax-basedrules 0.7474 0.897 0.796 0.597 0.861 0.665 0.726
Bag-of-arcs,GBDT 0.7733 0.895 0.854 0.923 0.558 0.694 0.817
have again parsed all the sentences, and converted each text into a multiset (“counts-
preservingset”)oftheitemsusingthefollowingfeatureengineeringalgorithm:
1. foreverychildlessvertexv,generate(DepRel(v),Gender(v),(cid:3)no children(cid:3));
2. for every v without outgoing nsubj arcs, generate (DepRel(v),Gender(v),
Case(v),(cid:3)→no nsubj(cid:3));
Wecollectsuchitemsforeverysentenceandmergethemforeverytextintoasingle
multisetthatwecallbag-of-arcs(BoA).Theonlyweaknessofthisapproachcompared
tothepreviousoneisthatBoA-basedmodelscannotmakepredictionstakingsentence
splittingintoaccount.WehavetrainedaGradientBoostedDecisionTreesmodel(Light-
GBM[8,10]),ﬁndingthebesthyperparametersontrainingandvalidationsetsviathe
Optunaframework[2].Wehaverunahyperparameterssearchbasedontree-structured
Parzen estimators, maximizing the mean of on 10 last F1macro values in 4-fold vali-
dation. In a one-vs-all multiclass setting, one of the strategies leading to the achieved
score was to run 863 boosters, each of them being a decision tree of depth ≤ 2, with
minimal number of samples to create a “child” equal to 18 and feature sampling rate
equalto0.43.
5 ResultsandDiscussion
For evaluation, we have used the macro-averaged F1-measure. The results are pre-
sented in Table1. As expected, BoW-based approaches are relatively weak. Using
eli5 [1], we have built a table (see Fig.1) showing which logistic regression weights
“favor”/“disfavor” which classes. It clearly demonstrates that word-based representa-
tions make the models encode non-grammatical properties of the texts. E.g. for class
“femn”,thetopfeatureintermsofregressionmodelweightistheword (“hus-
band”)ratherthananygrammaticalfeature.Webelievethatamoresophisticatedapp-
roach could yield better results in terms of classiﬁcation performance. However, this
bias-encodingbehaviourisinescapableinword/n-grammodels,fromBoW-basedlin-
earmodelstothebest-performingtextclassiﬁcationonessuchase.g.[16].Thisiswhy
the main focus of this work are syntax-aware approaches that perform clearly better.RecognizingPreferredGrammaticalGenderinRussian 227
Fig.1.LargestlogisticregressionweightsintheBoW-basedsetting.
arerelationshiproles.Thisbias-encodingbehaviourisinescapableinword/n-grammodels,being
ourmotivationtoproposebias-freesyntax-basedmodelsdescribedinthispaper.
The rule-based method’s results are a challenging benchmark, at least for the bag-of-
arcsapproach,whichshowedsimilarorsuperiorperformancewhencomparedtoother
models,exceptfor“masculine”classrecall.Thelabelimbalanceinthedatasetshownin
Sect.3mightbethereasonwhyrule-basedapproachmightfavorthe“masculine”class
in a larger number of cases than an essentially statistical GBDT model. The natural
questioniswhyaretheresultssoweakonsosimpleatask?Itturnsoutthatthereare
several important problems that leave this problem open for further research, includ-
ingthemoreﬁne-grainederroranalysisbytestingtheimpactofeachofthechallenges
listedbelow.228 A.AlekseevandS.Nikolenko
6 Conclusion
Inthiswork,wehaveintroducedanewdatasetoftextsannotatedwithpreferredgram-
matical gender of their authors: “feminine”, “masculine”, and “unknown”. We have
established several baselines: a BoW-based model, a rule-based algorithm, and a fea-
tureengineered“bag-of-arcs”predictivemodel.Thelattershowedthebestperformance
intermsofmacro-averagedF-score:0.773.Theresultsshowthatinspiteoftheseem-
ingsimplicityofthetask,thereisstillalotofroomforimprovement,andwesuggest
thisasaninterestingproblemforfurtherresearch.
Acknowledgement. ThisworkwascarriedoutattheSamsung-PDMIJointAICenteratSteklov
MathematicalInstituteatSt.PetersburgandsupportedbySamsungResearch.Wewouldliketo
thankanonymousreviewersforinsightfulcommentsthathelpedustoimprovethepaper.
Appendix:instructionsforannotation
Whichgrammaticalgenderdoauthorsusewhentalkingaboutthemselves?5
Short description: We ask you to carefully read the short text and report in
what grammatical gender the authors refer to themselves, based on grammatical evi-
dence/clues.
It is usually clear which grammatical gender (feminine/masculine/etc.) the users
prefer when speaking about themselves in their posts. However, sometimes it may be
impossible.Notallcasesareobvious,pleasedoreadtheinstructions.
IMPORTANT:onlygrammaticalfeaturesandcluescanbeusedtodetermine
thegender.Thetaskisnottoguesswhetheramanorawomanwrotethetext.Thetask
is to determine with conﬁdence which grammatical gender they prefer when talking
aboutthemselves.
Samplecaseswithpossibleerrors.
5OriginallyinRussian,translatedintoEnglish.RecognizingPreferredGrammaticalGenderinRussian 229
References
1. ELI5:aPythonpackagetodebugmachinelearningclassiﬁersandexplaintheirpredictions
(2016).https://github.com/TeamHG-Memex/eli5/
2. Akiba,T.,Sano,S.,Yanase,T.,Ohta,T.,Koyama,M.:Optuna:anext-generationhyperpa-
rameteroptimizationframework.In:Proceedingsofthe25rdACMSIGKDDInternational
ConferenceonKnowledgeDiscoveryandDataMining(2019)
3. Alekseev,A.,Nikolenko,S.:Wordembeddingsforuserproﬁlinginonlinesocialnetworks.
Computacio´nySistemas21(2),203–226(2017)
4. Anastasyev, D., Gusev, I., Indenbom, E.: Improving part-of-speech tagging via multi-task
learningandcharacter-levelwordrepresentations.arXivpreprintarXiv:1807.00818(2018)
5. Christopherson,K.M.:Thepositiveandnegativeimplicationsofanonymityininternetsocial
interactions: “on the internet, nobody knows you’re a dog”. Comput. Hum. Behav. 23(6),
3038–3056(2007)
6.
7. Kang,R.,Brown,S.,Kiesler,S.:Whydopeopleseekanonymityontheinternet?informing
policyanddesign.In:ProceedingsoftheSIGCHIConferenceonHumanFactorsinCom-
putingSystems,pp.2657–2666(2013)
8. Ke,G.,etal.:Lightgbm:ahighlyefﬁcientgradientboostingdecisiontree.In:Advancesin
NeuralInformationProcessingSystems,pp.3146–3154(2017)
9. Kestemont,M.,etal.:Overviewoftheauthoridentiﬁcationtaskatpan-2018:cross-domain
authorship attribution and style change detection. In: Working Notes Papers of the CLEF
2018EvaluationLabs.Avignon,France,10–14September,2018/Cappellato,Linda[edit.]et
al,pp.1–25(2018)
10. Meng,Q.,etal.:Acommunication-efﬁcientparallelalgorithmfordecisiontree.In:Advances
inNeuralInformationProcessingSystems,pp.1279–1287(2016)
11. Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,Thirion,B.,Grisel,O.,Blondel,M.,
Prettenhofer,P.,Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cournapeau,D.,Brucher,
M.,Perrot,M.,Duchesnay,E.:Scikit-learn:machinelearninginPython.J.Mach.Learn.Res.
12,2825–2830(2011)230 A.AlekseevandS.Nikolenko
12. Rangel, F., Rosso, P., Potthast, M., Stein, B.: Overview of the 5th author proﬁling task at
pan2017:Genderandlanguagevarietyidentiﬁcationintwitter.WorkingNotesPapersofthe
CLEF,pp.1613–0073(2017)
13. Rogers,A.,Romanov,A.,Rumshisky,A.,Volkova,S.,Gronas,M.,Gribov,A.:Rusentiment:
anenrichedsentimentanalysisdatasetforsocialmediainRussian.In:Proceedingsofthe
27thInternationalConferenceonComputationalLinguistics,pp.755–763(2018)
14. Straka, M., Strakova´, J.: Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with
udpipe.In:ProceedingsoftheCoNLL2017SharedTask:MultilingualParsingfromRaw
TexttoUniversalDependencies,pp.88–99.AssociationforComputationalLinguistics,Van-
couver,Canada,August2017
15. Straka, M., Strakova´, J.: Universal dependencies 2.5 models for UDPipe (2019–12-06)
(2019), http://hdl.handle.net/11234/1-3131, LINDAT/CLARIAH-CZ digital library at the
InstituteofFormalandAppliedLinguistics(U´FAL),FacultyofMathematicsandPhysics,
CharlesUniversity
16. Yang,Z.,Dai,Z.,Yang,Y.,Carbonell,J.,Salakhutdinov,R.R.,Le,Q.V.:XLNET:generalized
autoregressivepretrainingforlanguageunderstanding.In:AdvancesinNeuralInformation
ProcessingSystems,pp.5754–5764(2019)Attention to Emotions: Detecting Mental
Disorders in Social Media
Mario Ezra Arago´n1 , A. Pastor Lo´pez-Monroy2, Luis C. Gonza´lez3,
B
and Manuel Montes-y-Go´mez1( )
1 Instituto Nacional de Astrof´ısica, O´ptica y Electr´onica (INAOE),
San Andr´es Cholula, Mexico
{mearagon,mmontesg}@inaoep.mx
2 Centro de Investigaci´on en Matem´aticas (CIMAT), Guanajuato, Mexico
pastor.lopez@cimat.mx
3 Facultad de Ingenier´ıa, Universidad Aut´onoma de Chihuahua, Chihuahua, Mexico
lcgonzalez@uach.mx
Abstract. Diﬀerent mental disorders aﬀect millions of people around
theworld,causingsigniﬁcantdistressandinterferencetotheirdailylife.
Currently, the increased usage of social media platforms, where people
sharepersonalinformationabouttheirdayandproblems,opensupnew
opportunities to actively detect these problems. We present a new app-
roachinspiredinthemodelingofﬁne-grainedemotionsexpressedbythe
usersanddeeplearningarchitectureswithattentionmechanismsforthe
detection of depression and anorexia. With this approach, we improved
the results over traditional and deep learning techniques. The use of
attention mechanisms helps to capture the important sequences of ﬁne-
grained emotions that represent users with mental disorders.
· ·
Keywords: Mental disorders Emotional patterns Deep learning
1 Introduction
Therearemanydiﬀerentmentaldisorderscharacterizedbyabnormalbehaviors,
thoughts, and perceptions that aﬀect the relationships and daily activities of
people [1]. Mental disorders continue to grow in all countries of the world and
provoke signiﬁcant impacts on health, social environments, and big expenses to
maintain hospitals. The causes of mental disorders are related to diﬀerent risk
factors like stressful environments, including abuse, neglect, traumatic events,
etc.[2].Mentaldisordersincreaseincountriesovertime,somestudiesrevealthat
one person in ﬁve suﬀers a mental disorder, and one in four will be damaged
with these problems at least once in their life.
Living in a modern world implies that social life is developed in the physical
worldandalsoinavirtualworldcreatedbysocialmediaplatforms.Around45%
oftheworldwidepopulationhasanactivesocialmediaaccount,wheretheyshare
personal information about their daily life [3]. This presents an opportunity to
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.231–239,2020.
https://doi.org/10.1007/978-3-030-58323-1_25232 M. E. Arag´on et al.
analyzethisinformationandactivelydetectpeoplethatpresentsignsofamental
disorder. With this in mind, the main focus of this research is to analyze social
media texts and detect people with depression or anorexia.
Severalstudiesforthedetectionofdepressionandanorexiahaveusedlinguis-
tic and sentiment analysis [4]. For example, [5] proposed the use of emotions as
features,suchas “anger”, “disgust” or “joy” instead of only positive or negative
sentiments.Makingastepforwardwithemotions,[6]introducedanewrepresen-
tation built using information extracted from emotion lexicons combined with
wordembeddings,creatingsub-groupsofemotionsforaﬂexiblerepresentationof
userswithdepression;thisapproachcreatesemotionsinaﬁne-grainedway,thus
beingcalled,sub-emotions.Inthisstudy,weextendthisprecedingworkbyeval-
uatingadeeplearningarchitecturethatincorporatesanattentionmechanismto
trytoexploitsub-emotionstocapturetheirrelevanceinthecontext.Totestthe
robustness of the approach we also add the analysis of another important men-
tal disorder, anorexia. Results suggest that the proposed approach outperforms
traditional and other deep learning methods.
2 Related Work
Depression is a common mental disorder that is characterized by recurrent sad-
ness, loss of interest in daily activities, feelings of guilt, low self-esteem, loss of
appetite, tiredness, among others symptoms. On the other hand, Anorexia Ner-
vosa is a mental disorder mainly characterized by weight loss. People that suﬀer
anorexia have diﬃculties maintaining an appropriate weight, as a consequence
of unusual habits of eating and a distorted body image. Recently, several works
have focused on detecting users with these mental disorders in social media
platforms.Oneofthemostcommonstrategiesconsistsinusingthefrequencyof
words and words sequences to create an encoded language of the users [4] and
build a rule-based classiﬁer. The results are still modest with this strategy; an
analysis showed an overlap in the language between healthy people and people
withamentaldisorder.Itisalsocommontoapplysentimentanalysistechniques
to look for the positive and negative charge in the users’ posts [4].
OtherstrategiesfocusontheusageofLIWC[9]toextractdiﬀerentcategories
ofwordsatapsychologicallevelandanalyzetheirrelationdepressionoranorexia
common topics [7]. Another approach proposed the creation of lexicons related
tomentaldisorders,forexample,in[8]theauthorsdesignedamethodtoexploit
a micro-blog platform for detecting psychological pressures from teenagers.
Finally,in[10]theauthorsemployedandcombinedmultiplestrategiesbased
on four machine learning models. They considered a wide range of features to
build their models, e.g., they used models based on LSTM neural networks
and convolutional neural networks for the extraction of local features, and then
extract readability features, LIWC features, user-level linguistic metadata, and
speciﬁc terms related to depression and anorexia. Although in that work it was
shown that neural models were useful to determine if a person suﬀers from
depression and anorexia, the interpretability of the models were limited; thisAttention to Emotions 233
presents an opportunity to create a model that extracts important information
using a neural model with attention mechanism to help ameliorate this issue.
3 Modeling Text by Sub-emotions
As previously mentioned, in [6] the authors proposed the use of sub-emotions
instead of words for representing the users’ posts. In this section, we brieﬂy
describe the creation of the sub-emotions and how to convert the posts’ content
into sub-emotions sequences.
Creation of Sub-emotions. The creation of sub-emotions, as proposed in [6],
is based on the lexical resource from [13]. This lexical resource consists of eight
recognizedemotions[15]andtwosentiments:Anger,Anticipation,Disgust,Fear,
Joy, Sadness, Surprise, Trust, Positive and Negative, respectively. Each of these
emotions is deﬁned by a set of words associated to it. The process to create the
sub-emotions consists of two steps. Given the set of words associated to each
emotion, ﬁrst we obtain a word vector for each word using pre-trained word
embeddingsfromFastText[14].Then,wegeneratesub-groupsofwordsusingthe
Aﬃnity Propagation clustering algorithm [16]. The centroids of each sub-group
represent the new sub-emotions. The main idea of this approach is to separate
the words of each emotion in diﬀerent topics. With the help of these topics,
we can capture more speciﬁc emotions that users expressed in their posts. For
example, if the lexicon associated to the emotion Surprise contains the words:
accident, art, crash, disaster, museum, magician, gallery and wizard, then, after
thisprocessitwillbegeneratedthreesubgroups,eachonerepresentingadiﬀerent
sub-emotion or type of surprise, such as Surprise1 = [accident, crash, disaster],
Surprise2 = [art, museum, gallery] and Surprise3 = [magician, wizard].
Text to Sub-emotions. Once generated the sub-emotions, we mask the users’
posts by replacing each word with the label of its closest sub-emotion. To do
this, we calculate for each word in the vocabulary their embedding vector using
FastText. Then, we use the set of sub-emotions to measure the cosine similarity
betweeneachwordvectorandallsub-emotions(i.e.,theircentroidvector).Once
we obtain these similarities, each word is replaced by the label of the closest
sub-emotion. For example, the text “The most important thing is to try and
inspire people.”, will be masked as “anticipation27 joy27 positive5 negative62
anticipation10 anticipation29 positive20 negative80 trust23 joy16”.
4 Deep Emotion Attention Model
Todetectusersthatsuﬀerfromamentalhealthdisorder,particularlydepression
or anorexia, our proposal consists on processing their posts masked with sub-
emotions using a deep learning model with an attention mechanism. Figure1
representsthewhole processstepbystep. Theintuition of usingattention mod-
els is that not all sub-emotions contribute equally to the representation of the
sentence in the post history. Using the attention mechanism, we can extract the234 M. E. Arag´on et al.
sub-emotionsthataremoreimportanttothemeaningofthesentenceandaggre-
gate this importance to the representation. In the following lines we described
the architecture in detail.
Fig.1. Diagram of the deep attention model.
Feature Extraction. Given an input text masked with its sub-emotions, ﬁrst,
we represent each sub-emotion with an embedding vector, which corresponds
to the centroid of the vectors from their associated words, and then, we use a
Convolutional Neural Network (CNN) architecture for feature extraction of the
sub-emotions [17]. The intuition about this network is to see the post history as
images, that is, we look for the convolution taking one or two sub-emotions at
once since our ﬁlters size are 1 and 2. We can think of ﬁlter sizes as unigrams
and bigrams. We obtain diﬀerent feature maps for each region and concatenate
together to form a single feature vector.
Sequence Learning. After we obtained the feature vectors with the CNN, we
capture the context provided for the sequence of sub-emotions using a BiDi-
rectional Gated Recurrent Unit (GRU). GRU helps us to remember previous
information learning the sequential structure of the sub-emotions, where every
sub-emotion is dependent on the previous one. Furthermore, the BiDirectional
GRU keeps the contextual information in both directions.
Attention Mechanism. With the CNN extracting the feature vectors and the
GRU taking care of the sequence structure, we add the ability to give higher
weight to more important sub-emotions using an attention mechanism. This
mechanismextractssub-emotionsthatareimportantinthesentenceandaddthe
information of those sub-emotions. Then, we multiplied each sub-emotion score
withtheirGRUoutputobtainingaweightaccordingtotheirimportance.Finally,
the summed outputs use dense layers with a softmax for the classiﬁcation.Attention to Emotions 235
5 Evaluation
5.1 Data Collections
We use the datasets from eRisk 2018 and 2019 evaluation tasks [11,12]. They
contain the post’s history and comments of several users from Reddit. For each
task, we have two categories of users, users that are aﬀected by depression or
anorexia, and control users that are not aﬀected by any mental disorder. The
positive class is composed of people that explicitly mentioned that they were
diagnosed by a medical specialist with depression or anorexia. Users with vague
expressions like “I think I have anorexia/depression” were discarded. The neg-
ative class of users is composed of random users from the Reddit platform, but
including users who often interact in the depression or anorexia threads to add
more realism to the data. Table1 shows some numbers from these data sets.
Table 1. Mental disorders datasets. Each dataset have two classes (No Control (have
mental disorder) = NC, Control (do not have mental disorder) = C).
Data set Training Test
NC C NC C
dep eRisk’18 135 752 79 741
anor eRisk’19 61 411 73 742
5.2 Experimental Settings
Preprocessing: For the experiments, we normalized all words with lowercase
andremovedspecialcharacters.Afterpreprocessing,wemaskedtheposts’words
with the created sub-emotions (refer to Sect.3 for details).
Classiﬁcation: Once the post history of each user is masked, we separate it
in parts of N sub-emotions1. We process each part of the post history as an
individual input and then we average the results from all their parts. If the
majority of the posts are positive the user is classiﬁed as showing a mental
disorder. The main idea is to detect consistent and major signs of depression or
anorexia through all the posts of the users. In the training part, we used the
weighted class parameter for the imbalance present in the collections.
Baselines: We considered methods using well-known representations based on
theoccurrencesofwords(BoW)andwordsequences(N-grams).Wealsoincluded
some deep learning approaches for text classiﬁcation: CNN and RNN networks
with Glove and word2vec word embeddings. Furthermore, RNN networks with
1 Weselect Nempirically, testing recommended sizesofsequences intheliterature of
25, 35, 50 and 100.236 M. E. Arag´on et al.
attention mechanisms that use words and general emotions are also considered
using FastText embeddings (we named these approaches as Deep Attention and
Deep Emotion Attention with emotions respectively).
Evaluation Measure: We expressed the results using the f1-measure over the
positive class, as done in the eRisk shared task [11].
5.3 Results of the Deep Emotion Attention Model
Table2presentstheevaluationresultsofourapproachandthediﬀerentbaselines
in the two tasks. The ﬁrst thing to notice is that our approach, using sub-
emotions as well as emotions as text representation, achieves the best results,
outperformingtraditionalanddeeplearningapproaches.Thisresultprovesthat
the attention mechanism helps to improve the detection of both depression and
anorexia.Theseresultsalsoindicatethattheuseofsub-emotionsallowobtaining
better performance than the use of broad emotions, demonstrating that sub-
emotionshelpstocapturemorespeciﬁctopicsofinterestandmoodsfromsocial
media users. On the other hand, it is surprising to observe that traditional
approachesbasedonaBOWrepresentationincombinationwithaSVMclassiﬁer
wereabletoachievebetterresultsthandeeplearningmodelsbasedonstandard
CNNandRNNarchitectures.Wepresumethattheseresultscouldbeduetothe
small size of the data collections as well as to the diversity of their vocabularies.
Table 2. F1 results over the positive class in the two collections.
Method Anorexia Depression
Baselines BoW 0.67 0.54
N-grams 0.66 0.54
RNN-Glove 0.65 0.46
RNN-word2vec 0.65 0.48
CNN-Glove 0.67 0.51
CNN-word2vec 0.66 0.48
Deep-Attention (with FastText) 0.66 0.50
eRisk results First place 0.71 0.64
Second place 0.68 0.60
Third place 0.68 0.58
Our methods Deep Emotion Attention (broad 0.71 0.46
emotions)
Deep Emotion Attention 0.79 0.58
(sub-emotions)Attention to Emotions 237
5.4 Analysis of the Results
For the analysis of what is captured by the attention model, we extracted the
weights given to each sub-emotion on diﬀerent sequences. For this, we selected
examples of sequences with a high probability of being positive cases of a men-
tal disorder and extracted the weights for each sub-emotion. Figure2 presents
some examples of these sequences, the shading represents the weight given to
the sub-emotion, a darker shade means a higher weight. In these examples, it
is possible to appreciate that the weight of sub-emotions depend on their sur-
rounding context. For example, take the sub-emotion “anticipation16” related
tolife,experience,andevents.Intheﬁrstsequence,theweightishighbecauseit
is close to sub-emotions related to worries, afraid, mistakes, and incidents. But,
its weight is lower in the second and third sequence because it is close to gain,
growth, home, and place. With the attention model, we can capture the impor-
tance of the sub-emotions in the post history taking into account the context
and the sub-emotions help the model to learn these patterns.
Fig.2. Examples of weighted sequences of sub-emotions, each sequence in this Figure
correspondstothelabelofsub-emotionassignedtoeachwordinasequenceofwords.
The lower part shows the topics related to each sub-emotion.
6 Conclusions
Inthispaperweconsideredtherepresentationofsocialmediapostsasasequence
of emotion and sub-emotion labels, and proposed using a deep neural architec-
ture with an attention mechanism to learn emotional patterns useful for the238 M. E. Arag´on et al.
automatic detection of users that suﬀer from mental health issues, particularly
depression and anorexia. Our experiments showed that this approach outper-
formed traditional and other deep learning methods, and conﬁrmed that repre-
senting the posts’ content by means of ﬁne-grained emotions is particularly per-
tinentforthesekindoftasks,sincetheyhelpcapturingspeciﬁctopicsofinterest
and moods from social media users. Because the attention model allowed learn-
ing the importance of the sub-emotions depending on their context in the post
history, its inclusion improved the interpretability and analysis of results.
Acknowledgments. ThisresearchwassupportedbyCONACyT-Mexico(Scholarship
654803 and Project CB-2015-01-257383).
References
1. Kessler,R.,Bromet,E.,Jonge,P.,Shahly,V.,Wilcox,M.:Theburdenofdepressive
illness. In: Public Health Perspectives on Depressive Disorders (2017)
2. Ocampo, M.: Salud mental en Mexico. NOTA-INCyTU NU´MERO 007 (2018)
3. Kemp, S.: (2019). https://wearesocial.com/blog/2019/01/digital-2019-global-
internet-use-accelerates
4. Schwartz, H.A., et al.: Towards assessing changes in degree of depression through
Facebook. In: Proceedings of the Workshop on Computational Linguistics and
Clinical Psychology: From Linguistic Signal to Clinical Reality (2014)
5. Xuetong, C., Martin, D., Thomas, W., Suzanne, E.: What about mood swings?
Identifying depression on Twitter with temporal measures of emotions. In: Com-
panion Proceedings of the The Web Conference 2018, International World Wide
Web Conferences Steering Committee (2018)
6. Arag´on,M.E.,L´opez-Monroy,A.P.,Gonz´alez-Gurrola,L.C.,Montes-y-G´omez,M.:
Detecting depression in social media using ﬁne-grained emotions. In: Proceedings
of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, vol. 1 (2019)
7. De Choudhury, M., Gamon, M., Counts, S., Horvitz, E.: Predicting depression
via social media. In: Proceedings of the 7th International AAAI Conference on
Weblogs and Social Media (2013)
8. Xue, Y., Li, Q., Jin, L., Feng, L., Clifton, D., Cliﬀord, G.: Detecting adolescent
psychological pressures from micro-blog. In: IJCNLP (2013)
9. Tausczik,Y.R.,Pennebaker,J.W.:Thepsychologicalmeaningofwords:LIWCand
computerized text analysis methods. J. Lang. Soc. Psychol. 29(1), 24–54 (2010)
10. Trotzek, M., Koitka, S., Friedrich, C.M.: Word embeddings and linguistic meta-
data at the CLEF 2018 tasks for early detection of depression and anorexia. In:
Proceedings of the 9th International Conference of the CLEF Association, CLEF
2018, Avignon, France (2018)
11. Losada,D.E.,Crestani,F.,Parapar,J.:OverviewofeRisk2018:earlyriskpredic-
tion on the internet (extended lab overview). In: Proceedings of the 9th Interna-
tional Conference of the CLEF Association, CLEF 2018, Avignon, France (2018)
12. Losada,D.E.,Crestani,F.,Parapar,J.:OverviewofeRisk2019earlyriskprediction
on the internet. In: Crestani, F., et al. (eds.) CLEF 2019. LNCS, vol. 11696, pp.
340–357. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-28577-7 27
13. Mohammad, S.M., Turney, P.D.: Crowdsourcing a word-emotion association lexi-
con. Comput. Intell. 29(3), 436–465 (2013)Attention to Emotions 239
14. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Trans. Assoc. Comput. Linguist. 5, 135–146 (2016)
15. Ekman, P.E., Davidson, R.J.: The Nature of Emotion: Fundamental Questions.
Oxford University Press, New York (1994)
16. Thavikulwat,P.:Aﬃnitypropagation:aclusteringalgorithmforcomputer-assisted
businesssimulationandexperimentalexercises.In:DevelopmentsinBusinessSim-
ulation and Experiential Learning (2008)
17. Kim,Y.:Convolutionalneuralnetworksforsentenceclassiﬁcation.In:Proceedings
of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP) (2014)Cross-Lingual Transfer for Hindi
Discourse Relation Identiﬁcation
B
Anirudh Dahiya( ), Manish Shrivastava, and Dipti Misra Sharma
Language Technologies Research Center, IIIT, Hyderabad, India
anirudh.dahiya@research.iiit.ac.in, {m.shrivastava,dipti}@iiit.ac.in
Abstract. Discourserelationsbetweentwotextualspansinadocument
attempt to capture the coherent structure which emerges in language
use.Automaticclassiﬁcationoftheserelationsremainsachallengingtask
especiallyincaseofimplicitdiscourserelations,wherethereisnoexplicit
textualcuewhichmarksthediscourserelation.Inlowresourcelanguages,
thismotivatestheexplorationoftransferlearningapproaches,morepar-
ticularly the cross-lingual techniques towards discourse relation classiﬁ-
cation.Inthiswork,weexplorevariouscross-lingualtransfertechniques
onHindiDiscourseRelationBank(HDRB),aPennDiscourseTreebank
styled dataset for discourse analysis in Hindi and observe performance
gains in both zero shot and ﬁnetuning settings on the Hindi Discourse
Relation Classiﬁcation task. This is the ﬁrst eﬀort towards exploring
transferlearningforHindiDiscourserelationclassiﬁcationtothebestof
our knowledge.
·
Keywords: Discourse relation classiﬁcation Cross-lingual transfer
learning
1 Introduction
Linguistic units such as clauses and sentences are stitched together to form a
widerandconsistentsemanticcontextthanjusttheindividualmeaningimmedi-
ately represented by them. Discourse Analysis forms the study of how such lin-
guisticunitscometogethertopresentacoherentsemanticstructureinlanguage.
Various frameworks such as Rhetorical Structure Theory [10] and Tree Adjoin-
ing Grammar for discourse [16] aim to understand and formalise the underlying
structure and semantics of these discourse units in language.
The Penn Discourse Treebank (PDTB) takes a theory neutral approach to
annotate the argument structure, semantics and attribution of discourse units
in text following [15]. Two adjacent spans of text (eg. clauses, sentences) called
the discourse arguments, may be related to each other by a discourse relation.
As shown in Example 1.1, this discourse relation may be explicitly established
bylexicalitems(calleddiscourseconnective)andthuscalledaexplicitdiscourse
relation,orimpliedbythediscourseargumentsandthuscalledimplicitdiscourse
relation.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.240–247,2020.
https://doi.org/10.1007/978-3-030-58323-1_26Cross-Lingual Transfer for Hindi Discourse Relation Identiﬁcation 241
Explicit Relation: The game was cancelled because it started to rain
heavily.
Implicit Relation: Encouragement is ﬁne, [Implicit but] compulsion is not.
Example 1.1: Examples for Explicit and Implicit Discourse relations. The
argument 1, connective and argument 2 have been formatted for clarity.
FollowingthelinesofPDTBannotations,discoursetreebanksweredeveloped
for several other languages like Hindi, Turkish, Chinese etc [11,18]. While the
developmentofdiscoursetreebanksinotherlanguageshasfosteredresearchinto
discourse parsers for these languages, the scarcity of annotated data has limited
theirperformanceandhinderedtheadoptionofrecentlyproposeddeeplearning
methods in low resource settings. With this purview, we explore various cross-
lingual transfer learning approaches for implicit discourse relation classiﬁcation
in Hindi.
ImplicitDiscourseRelationClassiﬁcationisoneofthemostchallengingparts
of a discourse parser owing to the absence of any explicit discourse connective
to mark the relation [17]. Infact during the development of PDTB, annotators
proceeded by ﬁrst inserting an implicit discourse connective along the argu-
ments, and then proceeding to mark the implicit discourse relation type. In
this work, we investigate both, the zero shot and the ﬁnetuning performance
of Implicit Discourse relation classiﬁcation for Hindi via cross-lingual transfer
learning from English. We evaluate the cross transferability across 3 diﬀerent
modeltypes,namelytheMUSEmultilingualembeddingsencoder,thepretrained
multilingual sentence encoder LASER, and the transformer based cross-lingual
Masked Language Model XLM. We also evaluate the performance on the rela-
tively easier task of explicit discourse relation classiﬁcation. Our results suggest
that discourse relation classiﬁcation can gain performance from both zero shot
andﬁnetuningonsmallamountofavailabletrainingdatafortheresourcescarce
discourse treebanks like Hindi.
2 Related Work
ImplicitDiscourserelationidentiﬁcationhasreceivedconsiderableinterestdueto
the challenging nature of the task. Traditional methods used linguistic feature
based approaches using dependency and syntactic parse, lexical and polarity
based features, often requiring extensive feature engineering to see performance
gains [12,13].
More recently, end to end neural network based approaches have led to
increase in performance, with word vector based feedforward network [3], latent
variable recurrent and recursive neural networks [6,7]. [14] perform a compre-
hensive set of experiments to show the eﬃcacy of simple bag of words feed-
forward networks at implicit discourse classiﬁcation, in comparison to various
recurrentandrecursivearchitectures.[2]encodemultiplelayersofinformationin
arguments, including character,subword, word,sentenceandsentencepairlevel
information to push the state of the art performance on the PDTB2 test set.242 A. Dahiya et al.
[8] for the ﬁrst time explore the cross-lingual transfer of implicit discourse
classiﬁcation in zero-shot setting from English to Turkish and the TED multi-
lingual discourse corpus. They employ a simple feedforward network proposed
in [14] to evaluate the cross lingual transferability of implicit discourse classiﬁ-
cation.
Table 1. Data distribution across relation types for PDTB2 and HDRB
PDTB2 HDRB
Sense Train Dev Test Train Dev Test
Comparison 1884 398 146 169 14 10
Contiguity 3263 622 276 262 13 9
Expansion 6756 1240 556 472 73 87
Temporal 659 93 68 62 14 8
3 Approach
AsseenintheexampleinExample1.1,theImplicitDiscourserelationsarechar-
acterised by an implied discourse connective which is absent in the surface form
in text. Owing to this lack of an explicit connective marking the relation, the
Implicit Discourse relation classiﬁcation is a challenging part of discourse pars-
ing,evenmoresoincomparisontoitsexplicitrelationclassiﬁcationcounterpart.
AsshowninTable1,theHindiDiscourseRelationTreebankisalowresource
dataset for the task in comparison to its English counterpart, the Penn Dis-
course Treebank. The datasets are annotated with textual spans marking the
discourse relation type (Implicit or Explicit),arg 1 span,arg 2 span,lexical con-
nective andvariouslevelsofdiscourse relation senses,asillustratedintheexam-
ples in Example1.1. The discourse relation senses are annotated with multiple
levels of granularity with the four senses at the top level, namely Comparison,
Contingency,ExpansionandTemporal.Sincethediscourserelationclassiﬁcation
is a semantic task and the annotations used in the two datasets largely follow
similar guidelines, we propose to investigate the cross-lingual transferability of
the task from English to Hindi.
We formulate the task as follows: Given the arg 1 and arg 2 for an implicit
discourse relation, we want to predict the relation sense for this discourse rela-
tion. For our study, since the amount of data was limited, we choose to predict
amongthe4toplevelrelationtypes.Weexplore3neuralnetworkmodelsforthe
relation classiﬁcation task, which take a pair of arguments as inputs to predict
the discourse relation among them. To work with the high class imbalance and
toallowacomparisonofresultswithpreviouswork[8],wetrainthemodelsina
one vs all manner, thus resulting in a model for each relation sense. The model
approaches are described below:Cross-Lingual Transfer for Hindi Discourse Relation Identiﬁcation 243
LASER Based Feedforward Neural Network: Following [8], we explore a
simple feedforward neural network for relation classiﬁcation. The input to the
modelisapairofmultilingualsentencerepresentationsgeneratedbytheLASER
encoder [1], which is multilingual sentence encoder trained on 93 languages for
the translation objective. The LASER encoder performs well on the zero-shot
cross-lingualNLI(XNLI),atasksimilartotheimplicitdiscourserelationclassi-
ﬁcationasbotharemulticlassclassiﬁcationtaskswhichexpectapairoftextsas
input.GivenV andV astheLASERrepresentationforarg 1 andarg 2 respec-
1 2
tively,theinputtothefeedforwardmodelis[V ;V ;(V +V )/2;V −V ;V ×V ],
1 2 1 2 1 2 1 2
where;representsconcatenation.Theresultingvectorisfollowedbya100dimen-
sionalhiddenlayerandReLUactivation,andﬁnallyto2dimensionsfollowedby
Softmaxfunctiontooutputthedistributionfortherelationsense.Sincethepre-
trainedLASERencodergeneratesthesemanticsentencerepresentationsaligned
across languages, we keep the incoming representations from the encoder ﬁxed.
Multilingual Word Embeddings Based Model: We also propose a mul-
tilingual word embedding based LSTM encoder pair model. We employ two
bidirectional LSTMs [5] to encode arg 1 and arg 2 using the multilingual word
embeddingsforHindiandEnglish(weuse100dimensionalglovevectorsaligned
inthesamespaceusingtheMUSElibrary[4]).Theterminalhiddenstatesofthe
bidirectional LSTM (128 dims in each direction) are concatenated to represent
the argument encoding, which are further concatenated to form the input to a
128 dimensional multilayer perceptron similar to the LASER based feedforward
neural network described above.
Cross-Lingual Language Models (XLM): Since pretrained masked lan-
guage models have demonstrated their eﬃciency at a variety of language under-
standing tasks, we propose to employ a pretrained cross lingual language model
(XLM) [9] for discourse relation classiﬁcation. It is a transformer based masked
languagemodelwhichischaracterisedbydeepbidirectionalmultiheadselfatten-
tiontogeneratetokenandsequencerepresentationstrainedonbothmaskedLM
and masked translation LM objective. The XLM has proven its eﬃcacy on the
similarcrosslingualnaturallanguageinferencetask(XNLI),andispretrainedon
both English and Hindi, further motivating its eﬃcacy for relation classiﬁcation
task. We segment the arguments using the BPE vocabulary for the pretrained
model, concatenate them as a single input sequence with a delimiter token, and
add[CLS]symbolastheﬁrsttokentorepresentthewholesequence.Theresult-
ing sequence is input to the pretrained XLM model and the resulting vector
representationcorrespondingtothe[CLS]tokenisfollowedbyalinearlayerand
Softmax function to predict the relation.
4 Experiments
The above described models are evaluated in both the zero-shot and ﬁnetuning
setup by training them ﬁrst on the Penn Discourse Treebank (PDTB2), and244 A. Dahiya et al.
then evaluating and ﬁnetuning them on the Hindi Discourse Relation Treebank
(HDRB)data.Tocompareresultswithpreviouswork,wesplitthePDTB2data
into the following respective splits: Training (Section 2–20), Validation (Section
0–1 and 23–24) and Test (Section 21–22). For HDRB, we split the Section 11
into validation and test portion, and Section 16 and Section 17 as the training
portion. To deal with the high class imbalance among the relation types, we
train each model in a one-vs-all manner for each relation type, and randomly
oversamplefromotherclassesateachepochtobalancethepositiveandnegative
samples.WetrainthemodelsusingCrossEntropyloss,anduseAdamoptimizer
for gradient updates. For ﬁnetuning, the learning rate was reduced by a factor
of 0.8. For XLM model, we also use a linear schedule with warmup for the ﬁrst
10% training steps. For LASER and MUSE model, we report the average of 25
runs to account for variance in performance.
Table 2. Results for implicit discourse classiﬁcation.
Training mode Test set
PDTB2 HDRB
LASER model
HDRB training only 39.31 29.60
Zero Shot 40.68 31.95
Finetuning 42.76 34.56
MUSE model
HDRB training only 35.90 30.55
Zero Shot 41.79 28.82
Finetuning 39.65 31.98
XLM model
HDRB training only 38.28 33.88
Zero Shot 43.15 37.06
Finetuning 43.05 36.75
In the zero-shot learning setup, we train our model on the PDTB2 training
set for upto 100 epochs with early stopping criteria on the validation f-score for
15 epochs. We then evaluate the best validation model on the test set of both
PDTB2 and HDRB. To evaluate the gain posed by further training the best
English model on the limited available Hindi data, we ﬁnetune the model on
HDRB training data with the same early stopping condition, and evaluate on
the HDRB test set.
To draw parallel with the easier explicit connective counterpart, we run the
same set of experiments for the explicit relation classiﬁcation. Since the explicit
connective is syntactically tied to arg 2, we append the discourse connective to
beginning or end of the arg 2 span corresponding to their relative position. TheCross-Lingual Transfer for Hindi Discourse Relation Identiﬁcation 245
rest of the preprocessing and modelling procedure for the explicit case follows
the implicit relation classiﬁcation setting.
Table 3. Results for explicit discourse classiﬁcation.
Training mode Test set
PDTB2 HDRB
LASER model
HDRB training only 44.04 41.33
PDTB pretraining 44.14 39.30
HDRB Finetuning 38.64 31.55
MUSE model
HDRB training only 57.68 59.60
PDTB pretraining 88.73 28.68
HDRB Finetuning 54.25 63.15
XLM model
HDRB training only 64.25 57.53
PDTB pretraining 88.25 59.30
HDRB Finetuning 73.85 58.23
5 Results and Discussion
Table2andTable3reporttheresultsofexperimentsonimplicitandexplicitdis-
courserelationclassiﬁcationrespectively.Allreportedresultsaremacro-averaged
F1 scores across the relation categories. We report the the performance on both
PDTB2 and HDRB test set in the columns of the tables. For each model class,
wereporttheresultoftrainingthemodelonlyontheHDRBtrainingset(HDRB
training only), which serves as a non-transfer learning baseline for that model
class, training the model only on PDBT2 training set (Zero Shot), and ﬁne-
tuning the PDTB2 pretrained models on the HDRB training set (Finetuning).
We primarily focus on the results corresponding to the HDRB test set to mea-
sure the eﬃcacy of our experiments towards zero shot and ﬁnetuning mode of
cross-lingual transfer learning.
Fortheimplicitcase,weobservethatpretrainingonthelargerPDTB2helps
in both zeroshot and ﬁnetuning settings for HDRB,when compared to training
onHDRBalone.Wealsoobservethatwhilezeroshotleadstoconsiderablegain
for both LASER and XLM model, it leads to a decrease in performance in case
of MUSE model. One explanation to this decrease is that while the encoders
for LASER and XLM model have already been pretrained on Hindi data in
theirencoderspeciﬁcpretrainingobjective(i.e.translationandmaskedlanguage
modelling), the MUSE model encoder needs ﬁnetuning on the Hindi sequences
to be able to perform well, and its eﬃcacy is observable when ﬁnetuned on the246 A. Dahiya et al.
limited HDRB training data. We also observe that in most cases, the models
perform better as the training data increases (due to both PDTB2 pretraining
and follow-up HDRB ﬁnetuning.) This is true even on the PDTB2 test data for
LASER model, where the sentence encodings are unperturbed by training on
discourse classiﬁcation.
For the explicit case, we observe that the model performances are consider-
ably higher compared to the implicit case. For the LASER model, we observe
that the HDRB deteriorates with increasing data. This could be explained by
the fact that the arg 2 when concatenated with the discourse connective forms
a clause with a connective, thus forming a half sentence which is not seen in the
encoder-speciﬁcpretraining,andthusleadstopoorlyalignedrepresentationsfor
such inputs. On the other hand, both MUSE and XLM perform well as they
are trained on HDRB data in both HDRB only as well as Finetuning settings.
The high performanceof zeroshot onPDTB2 testset indicates that themodels
learn the corresponding task well.
6 Conclusion
In this work, we present the ﬁrst set of experiments towards the eﬃcacy of
cross-lingual transfer learning for Hindi Discourse Relation Classiﬁcation. We
investigate both zero shot and ﬁnetuning transfer learning across 3 diﬀerent
modellingapproachesandshowconsistentgainsinperformanceforbothexplicit
and implicit discourse relation classiﬁcation. Our proposed methods can plug
into existing discourse parsers to improve relation identiﬁcation module, and
can also be further extended to develop end-to-end discourse parsers for low
resource languages.
References
1. Artetxe, M., Schwenk, H.: Massively multilingual sentence embeddings for zero-
shotcross-lingualtransferandbeyond.Trans.Assoc.Comput.Linguist.7,597–610
(2019)
2. Bai, H., Zhao, H.: Deep enhanced representation for implicit discourse relation
recognition. arXiv preprint arXiv:1807.05154 (2018)
3. Braud, C., Denis, P.: Comparing word representations for implicit discourse rela-
tion classiﬁcation (2015)
4. Conneau, A., Lample, G., Ranzato, M., Denoyer, L., J´egou, H.: Word translation
without parallel data. arXiv preprint arXiv:1710.04087 (2017)
5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
6. Ji, Y., Eisenstein, J.: One vector is not enough: entity-augmented distributed
semantics for discourse relations. Trans. Assoc. Comput. Linguist. 3, 329–344
(2015)
7. Ji, Y., Haﬀari, G., Eisenstein, J.: A latent variable recurrent neural network for
discourse relation language models. arXiv preprint arXiv:1603.01913 (2016)Cross-Lingual Transfer for Hindi Discourse Relation Identiﬁcation 247
8. Kurfalı, M., O¨stling, R.: Zero-shot transfer for implicit discourse relation classiﬁ-
cation. arXiv preprint arXiv:1907.12885 (2019)
9. Lample,G.,Conneau,A.:Cross-linguallanguagemodelpretraining.arXivpreprint
arXiv:1901.07291 (2019)
10. Mann,W.C.,Thompson,S.A.:Rhetoricalstructuretheory:Atheoryoftextorgani-
zation.UniversityofSouthernCalifornia,InformationSciencesInstituteLosAnge-
les (1987)
11. Oza, U., Prasad, R., Kolachina, S., Sharma, D.M., Joshi, A.: The hindi discourse
relationbank.In:ProceedingsoftheThirdLinguisticAnnotationWorkshop(LAW
III), pp. 158–161 (2009)
12. Park,J.,Cardie,C.:Improvingimplicitdiscourserelationrecognitionthroughfea-
ture set optimization. In: Proceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, pp. 108–112. Association for Compu-
tational Linguistics (2012)
13. Pitler,E.,Louis,A.,Nenkova,A.:Automaticsensepredictionforimplicitdiscourse
relationsintext.In:ProceedingsoftheJointConferenceofthe47thAnnualMeet-
ing of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP: volume 2, pp. 683–691. Association for Computational
Linguistics (2009)
14. Rutherford, A., Demberg, V., Xue, N.: A systematic study of neural discourse
models for implicit discourse relation. In: Proceedings of the 15th Conference of
the European Chapter of the Association for Computational Linguistics: Volume
1, Long Papers, pp. 281–291 (2017)
15. Webber, B.: D-ltag: extending lexicalized tag to discourse. Cogn. Sci. 28(5), 751–
779 (2004)
16. Webber, B.L., Joshi, A.K.: Anchoring a lexicalized tree-adjoining grammar for
discourse. arXiv preprint cmp-lg/9806017 (1998)
17. Xue,N.,Ng,H.T.,Pradhan,S.,Prasad,R.,Bryant,C.,Rutherford,A.:Theconll-
2015 shared task on shallow discourse parsing. In: Proceedings of the Nineteenth
Conference on Computational Natural Language Learning-Shared Task, pp. 1–16
(2015)
18. Zeyrek, D., Webber, B.: A discourse resource for Turkish: annotating discourse
connectives in the metu corpus. In: Proceedings of the 6th workshop on Asian
language resources (2008)Authorship Veriﬁcation with Personalized
Language Models
B
Milton King( ) and Paul Cook
Faculty of Computer Science, University of New Brunswick,
Fredericton, NB E3B 5A3, Canada
{milton.king,paul.cook}@unb.ca
Abstract. Malicious posts from a social media account by an unau-
thorized user could have severe eﬀects for the account holder, such as
the loss of a job or damage to their reputation. In this work, we con-
sider an authorship veriﬁcation task to detect unauthorized malicious
social media posts. We propose a novel approach for authorship veri-
ﬁcation based on personalized, i.e., user-tailored, language models. We
evaluate our proposed approach against a previous approach based on
word embeddings and a one-class SVM. A large amount of text might
not necessarily be available for an individual social media user. We
therefore demonstrate that our proposed approach out-performs previ-
ous approaches, while requiring orders of magnitude less user-speciﬁc
training text.
· ·
Keywords: Authorship veriﬁcation Language models Personalized
NLP
1 Introduction
Maliciouspostsfromasocialmediaaccountbyanunauthorizedusercouldhave
severe eﬀects for the account holder such as the loss of a job or a negative
impact on their reputation. Authorship veriﬁcation is the task of determining
if a given text is written by a particular author. One potential application of
authorship veriﬁcation is to determine if a social media post is indeed written
bytheaccountholder,orwhetheritisamaliciouspostbyanunauthorizeduser.
Inthiswork,weproposetheuseofpersonalized(user-tailored)languagemodels
forauthorshipveriﬁcation.Inourexperiments,wesimulatemaliciouspostsfrom
an unauthorized user with text from three sources that would typically not be
appropriateifpostedfromaprofessionalsocialmediaaccount,speciﬁcallyerotic
stories, hate speech, and email spam.
[5]approachedauthorshipveriﬁcationbyaveragingwordembeddingstorep-
resentadocumentandrecruitedaone-classSVMforclassiﬁcation.Theirmethod
outperformed the method used by [3], which was one of the best performing
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.248–256,2020.
https://doi.org/10.1007/978-3-030-58323-1_27Authorship Veriﬁcation with Personalized Language Models 249
models in the PAN-2013 authorship veriﬁcation task1 and used as the bench-
mark in the PAN-2014 authorship veriﬁcation task.2
Language models can be used to generate probability distributions over text
after training on a background corpus. Tuning language models to text from
peoplesharingdemographiccharacteristicswithanauthor,suchasageorgender,
hasoutperformeduntunedlanguagemodels[7].[6]foundthattuningalanguage
modeltoasingleauthorcanoutperformmodelsthatweretunedtotextswritten
by people sharing demographic characteristics with that author. They tuned an
LSTM language model using a priming technique, which involves exposing the
LSTM to text from the author to update its state without updating the hidden
layers(whichareusuallytunedduringtraining).Thisprimingapproachrequired
relatively little text from the author to outperform untuned models, as well as
models tuned on text from authors from the same demographic.
Weproposeandevaluatetwodiﬀerenttypesofmethodsforauthorshipveriﬁ-
cation.Theﬁrstmethodinvolvesrepresentingadocumentbyembeddingit,and
then using a one-class SVM for classiﬁcation. The second method involves per-
sonalizingalanguagemodelbytuningitontextfromanauthor,andcomparing
its probabilities to a non-personalized language model. We compare our models
across diﬀerent amounts of author-speciﬁc text available for model training and
tuning, and diﬀerent sized test documents. We show that our proposed person-
alized language model-based method outperforms embedding-based methods,
while requiring orders of magnitude fewer tokens from the user for tuning.
2 Dataset and Evaluation
Here we discuss the creation of our dataset, and the evaluation measures used.
2.1 Dataset
In this subsection, we describe the sources of positive and negative instances
used to create our dataset, and then the structure of the dataset. Here, positive
instancesaretextsfromaspeciﬁcauthor,whilenegativeinstancesaretextsfrom
one of the three malicious text types.
Sources of Text. The positive instances in our dataset consist of blog posts
from a corpus containing 19,320 authors [12]. We select all authors who have
at least 300 posts with at least 100 tokens in them, giving us 103 authors for
experiments. For each of these 103 authors, we ignore their 10% smallest and
10% largest documents to avoid outlier documents.
Thetextsbelongingtotheremainingauthorsareusedasabackgroundcorpus
fortrainingourlanguagemodels,whichconsistsofapproximately143M tokens.
The text from any one author is limited to 30k tokens to avoid text from any
1 http://pan.webis.de/clef13/pan13-web/author-identiﬁcation.html.
2 http://pan.webis.de/clef14/pan14-web/author-identiﬁcation.html.250 M. King and P. Cook
one author strongly biasing the corpus to represent their text. We replace each
word in the background corpus that has a frequency less than 10 with a special
token (UNK). We do this to reduce the cost of training language models.
Thenegativeinstancesweregatheredfromthreediﬀerentsources,speciﬁcally
erotic stories, hate speech, and email spam. We use several types of malicious
documents to avoid the task being framed as document classiﬁcation for a spe-
ciﬁc (malicious) text type. For example, if we only used hate speech, then the
task could be approached as identifying hate speech, as opposed to the broader
authorship veriﬁcation task that we consider.
All erotic stories are gathered from textﬁles.com/sex/EROTICA. We arbi-
trarily selected all plaintext documents with titles starting with “A” or “B”,
which resultedinarelatively large amount oftext. Weremoved lines containing
metadata from each document. We then selected all documents that contain at
least 200 tokens, giving us 1463 documents.
Wegatheredhatespeechdocumentsfromawhitesupremacyforumfromthe
dataset of [2]. The dataset originally contained individual sentences from the
documentswithinformationaboutwhichsentencesarefromthesamedocument
and the order in which they appear. We reconstructed the documents using this
information. We selected all documents that contain more than 50 tokens. This
resulted in 172 documents in total.
We gathered spam from the Enron-Spam dataset [9]. We removed lines con-
tainingthetext“subject:”andremovedparagraphboundaries.Wethenselected
all documents containing at least 200 tokens, resulting in 6230 documents.
All documents—positive and negative instances—were casefolded and tok-
enizedusingtheStanfordCoreNLPtoolkit[8],exceptforthehatespeechtexts,
which were pre-tokenized. All training and testing documents are prepended
with a start-of-sentence token and appended with an end-of-sentence token.
Dataset Structure. We split our dataset into a development set (DEV) and
a testing set (TEST). We randomly select 10 authors from the 103 blog post
authors for DEV for preliminary experiments, and we use the remaining 93
authors for TEST. The authors in TEST include 48 males and 45 females, with
an average age of 27 years, ranging from 14 to 48 years old.
The following design is used for both DEV and TEST. For each author, we
createuser-speciﬁctraining(DEV train andTEST train)andtesting(DEV test
and TEST test) datasets. For each author, we randomly select 45 of their doc-
uments for testing, which make up the positive instances in DEV test and
TEST test. The remaining documents for each author are put into DEV train
and TEST train. For each author, we then select 15 erotica, 15 hate speech,
and 15 spam documents as negative test instances, and add them to the user-
speciﬁctestingsets(i.e.,DEV test andTEST test).Agivenmaliciousdocument
is never included in both DEV and TEST. For each author, this gives 45 mali-
cious documents that do not belong to them. We repeat this 5 times, to create
5 diﬀerent test sets for each author. All test documents (positive and negativeAuthorship Veriﬁcation with Personalized Language Models 251
instances) are selected without replacement for an individual author. As such, a
given document will only occur in TEST test at most once for a given author.
We limit negative instances to approximately the ﬁrst 1000 tokens. This
reduces computational cost. Moreover, we control the number of tokens used
fromtestdocumentsinourexperiments,andnoneoftheauthorshipveriﬁcation
approaches considered use document length as a feature.
In TEST, the average amount of text from an author is approximately 200k
tokens, with a minimum and maximum of 97k and 526k, tokens respectively.
2.2 Evaluation
Weevaluateourmodelsonthe93authorsfromTEST over5iterations.Ineach
iteration, we select documents for training/tuning by concatenating documents
from TEST train until we have at least x tokens, where x is a parameter that
controls the amount of text available for training. We then give the model y
tokens of running text from a test document from TEST test for classiﬁcation,
where y is a parameter that controls the amount of text used from a test doc-
ument. Limiting the amount of text from training documents simulates having
only a small amount of text from the user available for building the model; lim-
iting the amount of text from test documents simulates documents of various
lengths, e.g., microblogs vs longer documents, such as blogs.
WeevaluateourmodelsusingaccuracyandF score.Ourtestsetscontaina
1
50/50 split of positive and negative instances, which makes accuracy an appro-
priate metric.
3 Methods
In this section, we describe our two methods for authorship veriﬁcation based
on embeddings and language models. We frame the authorship veriﬁcation task
as predicting whether an unknown document belongs to a given author or not,
while crucially only observing documents from that author during training, i.e.,
the model does not see negative instances during training. This resembles the
real-world scenario where we do not know in advance what kinds of malicious
documents an unauthorized user of a social media account would attempt to
post. The lack of negative instances during training means that standard super-
vised approaches to binary document classiﬁcation are not applicable.
3.1 Embeddings
In this method, we represent documents as embeddings, using either word2vec
[10] or DistilBERT [11], and then use a one-class SVM for classiﬁcation.
For word2vec, following [5], we train skipgram [10] on a snapshot of English
Wikipedia with an embedding size of 300 and a window size of ±8, and then
represent a document as the average of its word embeddings.252 M. King and P. Cook
DistilBERT [11] is a lighter-weight version of the BERT transformer model
with 6 layers, an embedding size of 768, and 12 attention heads. These are the
default parameters for the DistilBERT implementation from [13] which we use.
We embed a document by using the document as input to the model, which
generates an embedding for the document as part of its output layer.
We represent training documents using these approaches to document
representation—i.e.,eitherword2vecorDistilBERT—andthentrainaone-class
SVM. Given a test document, we use the same approach to representing it,
and then input it into the trained one-class SVM to classify the document as
belonging to the author or not. We use the one-class SVM implementation from
scikit-learn.3Weusethedefaultparametersforthismodel,whichincludearadial
basis function kernel.4 We refer to these approaches which use a one-class SVM
along with document representations based on either word2vec or DistilBERT
as W2V and BERT, respectively.
3.2 Language Models
In this method, we require two language models. The ﬁrst language model,
referredtoasthebackgroundlanguagemodel,istrainedonabackgroundcorpus
of blog posts (described in Sect.2.1). The second language model is a personal-
ized language model that is generated by copying the ﬁrst language model, and
thentuningitusingthetrainingtextfromasingleauthor.Ourlanguagemodels
consist of an LSTM with a single hidden layer with 1024 units, an embedding
size of 128, and are trained for 1 epoch with a batch size of 45.
We personalize language models—i.e., tune language models to a speciﬁc
author—using the priming technique from [6], which has performed relatively
well when only a small amount of author-speciﬁc text is available for tuning.
To prime our model, we expose the language model to text from the author’s
training documents to modify the LSTM’s state without altering the hidden
layer of the network that is tuned during standard training.
Given a test document, each language model—i.e., the background and per-
sonalized language model—outputs a probability for every token in the docu-
ment. We score each model by counting the number of tokens for which that
model assigns a higher probability than the other language model. We classify
the document as belonging to the author (i.e., a positive instance) if the per-
sonalized language model scored higher than the non-personalized background
language model, and classify the document as not belonging to the author (i.e.,
a negative instance) if the background model achieved a higher score.5 We refer
to this method as LM from hereon.
InpreliminaryexperimentsonDEV,wefoundLM performedwellonshorter
test documents, but performed poorly as the length of the test documents
3 https://scikit-learn.org.
4 These parameter settings could be tuned on DEV. We leave this for future work.
5 InpreliminaryexperimentsonDEV weconsideredalternativeapproaches,including
comparingtheperplexityofthetwolanguagemodels,andthresholdsfordiﬀerences
inprobability.Noneoftheseapproachesperformedaswellasourproposedapproach.Authorship Veriﬁcation with Personalized Language Models 253
increased.WebelievethiswasduetothefactthatthestateoftheLSTMchanges
fairly quickly as a test document is processed. As such, for longer documents,
the state of the personalized language model becomes similar to the state of the
background language model, and the eﬀect of personalization is lost. To address
this, we sentence tokenized the test documents,6 and reinitialized the person-
alized language model’s state to its original primed state at the beginning of
each sentence.7 We found that this sentence-level re-initialization always led to
improvements, and so only report results for this approach.
4 Experimental Results
In this section, we evaluate our proposed models using accuracy and F score.
1
We consider diﬀerent amounts of user-speciﬁc training text, to simulate having
varying amounts of user-speciﬁc data available. We further consider diﬀerent
cut-oﬀ lengths for test documents, to simulate diﬀerent test document sizes.
Fig.1.Accuracyforeachmethod,usingdiﬀerentamountsoftrainingtext,anddiﬀer-
ent test document sizes.
Figure1 shows the accuracy of our models when using diﬀerent amounts
of user-speciﬁc training data, and diﬀering cut-oﬀs for test document sizes;8
6 We used NLTK’s sentence tokenizer [1].
7 Each sentence had the beginning-of-sentence and end-of-sentence tokens appended
to the start and end of the sentence, respectively.
8 WedonotreportresultsforLM usingallavailableuser-speciﬁctrainingdata.These
experiments are computationally expensive, and preliminary experiments indicated
thatthisapproachperformedverywellwithonlymodestamountsoftrainingdata.254 M. King and P. Cook
the number appended to each model’s name in the legend indicates the cut-oﬀ
length for the number of tokens used from the test documents; i.e., BERT 100
indicates BERT with test documents cut-oﬀ after the ﬁrst 100 tokens. LM out-
performs both embedding-based models—i.e., W2V and BERT—regardless of
the amount of training text or test document size. Remarkably, LM achieves an
accuracy of 0.69 with only 100 tokens of user-speciﬁc training text when using
the full test documents. The embedding-based models perform relatively poorly
whenonlyasmallamountofuser-speciﬁctrainingdataisavailable,butperform
better as more training text is used. For every amount of training data, and
testdocumentlength,considered,BERT alwaysoutperformsW2V.Thehighest
accuracies achieved by BERT and W2V are 0.66 and 0.62, respectively. Unlike
the embedding-based models, LM does not always perform better when more
training text is available, which could be because the state of the LSTM does
not retain much information from tokens that are far away [4].
Figure1 also shows that the embedding-based models achieve close to their
highest values when using only 1000 tokens from the test documents, and do
not perform substantially better on longer test documents. This could be due
to the construction of the dataset, where malicious documents are truncated
to approximately 1000 tokens.9 LM generally performs better on larger test
documents, and achieves its best accuracy of 0.70 using full test documents and
10k tokens of user-speciﬁc training text.
Fig.2.F1scoreforeachmethod,usingdiﬀerentamountsoftrainingtext,anddiﬀerent
test document sizes.
Figure2 shows the F score of our models for diﬀerent amounts of training
1
data and test document sizes. The ﬁndings are overall similar to those in Fig.1,
9 We do not apply the same document size limitation to documents from the user.Authorship Veriﬁcation with Personalized Language Models 255
with LM outperforming the embedding-based models, and achieving a highest
F score of 0.74, and an F score of 0.73 when trained on only 100 tokens
1 1
of user-speciﬁc text. The best F score of an embedding-based model is 0.57.
1
Interestingly, here the performance of LM is not overly aﬀected by the test
documentlength.OnediﬀerencehereisthatBERT nolongeroutperformsW2V
for the same amount of training text and test document size.
5 Conclusions
Malicious posts from an unauthorized user on a social media account could be
damaging to the account holder. We proposed a novel approach to authorship
veriﬁcation using personalized language models, and evaluated it in an experi-
mental setup motivated by this scenario. We compared our proposed approach
against a previous embedding-based model, and showed that it outperformed
embedding-based models while requiring orders of magnitude less user-speciﬁc
training data, and shorter test documents. Our proposed language model-based
approach, trained on just 100 tokens of user-speciﬁc text, achieved an accuracy
and F score of 0.69 and 0.73, respectively, compared to the best embedding-
1
based model which achieved an accuracy and F score of 0.66 and 0.57, respec-
1
tively, while requiring much more user-speciﬁc training text. In future work, we
intend to consider language models based on transformers, and further sources
of malicious documents.
References
1. Bird,S.,Loper,E.,Klein,E.:NaturalLanguageProcessingwithPython.O’Reilly
Media Inc., Sebastopol (2009)
2. de Gibert, O., Perez, N., Garc´ıa-Pablos, A., Cuadros, M.: Hate speech dataset
from a white supremacy forum. In: Proceedings of the 2nd Workshop on Abusive
Language Online (ALW2), Brussels, Belgium, pp. 11–20 (2018)
3. Jankowska, M., Milios, E., Keselj, V.: Author veriﬁcation using common n-gram
proﬁlesoftextdocuments.In:ProceedingsofCOLING2014,Dublin,Ireland,pp.
387–397 (2014)
4. Khandelwal, U., He, H., Qi, P., Jurafsky, D.: Sharp nearby, fuzzy far away: How
neural language models use context. In: Proceedings of ACL 2018, Melbourne,
Australia, pp. 284–294 (2018)
5. King, M., Alhadidi, D., Cook, P.: Text-based detection of unauthorized users of
social media accounts. In: Bagheri, E., Cheung, J.C.K. (eds.) Canadian AI 2018.
LNCS (LNAI), vol. 10832, pp. 292–297. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-89656-4 29
6. King,M.,Cook,P.:Buildingpersonalizedlanguagemodelsthroughlanguagemodel
interpolation. In: CICLING 2019, La Rochelle, France (2019)
7. Lynn, V., Son, Y., Kulkarni, V., Balasubramanian, N., Schwartz, H.A.: Human
centered NLP with user-factor adaptation. In: Proceedings of EMNLP 2017,
Copenhagen, Denmark, pp. 1157–1166 (2017)256 M. King and P. Cook
8. Manning, C., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S., McClosky, D.: The
stanford CoreNLP natural language processing toolkit. In: Proceedings of ACL
2014, Baltimore, USA, pp. 55–60 (2014)
9. Metsis, V., Androutsopoulos, I., Paliouras, G.: Spam ﬁltering with Naive Bayes-
which Naive Bayes? In: Proceedings of the Third Conference on Email and Anti-
Spam 2006, vol. 17, pp. 28–69 (2006)
10. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word rep-
resentations in vector space. In: Proceedings of Workshop at the International
Conference on Learning Representations, 2013, Scottsdale, USA (2013)
11. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: DistilBERT, a distilled version of
BERT: smaller, faster, cheaper and lighter. In: NeurIPS EMC2 Workshop, Van-
couver, Canada (2019)
12. Schler,J.,Koppel,M.,Argamon,S.,Pennebaker,J.W.:Eﬀectsofageandgenderon
blogging. In: AAAI Spring Symposium: Computational Approaches to Analyzing
Weblogs, vol. 6, pp. 199–205 (2006)
13. Wolf,T.,etal.:Huggingface’stransformers:state-of-the-artnaturallanguagepro-
cessing. ArXiv abs/1910.03771 (2019)A Semantic Grammar for Augmentative
and Alternative Communication Systems
B B B
Jayr Pereira( ) , Nata´lia Franco( ) , and Robson Fidalgo( )
Center of Informatics, Federal University of Pernambuco, Recife, Brazil
{jap2,nmf,rdnf}@cin.ufpe.br
Abstract. The authoring of meaningful sentences is an essential
requirement for AAC systems aimed at the education of children with
complexcommunicationneeds.Somestudiesproposetheuseoflinguistic
knowledgedatabasestomeetthatrequirement.Inthispaper,wepropose
andpresentaSemanticGrammar(SG)forAACsystemsbasedonvisual
and semantic clues. The proposed SG was acquired using an automatic
processbasedonNaturalLanguageProcessing(NLP)techniquesforthe
extraction of semantic relations from text samples. We assessed the SG
precision on suggesting the correct words on reconstructing telegraphic
sentences and obtained a precision average of 90%.
·
Keywords: Semantic Grammar Augmentative and Alternative
·
Communication Ontology.
1 Introduction
Augmentative and Alternative Communication (AAC) [2] systems are essential
toolsforsupportingtheinclusionofchildrenwithcomplexcommunicationneeds
in the educational process. In this context, these systems must be intuitive and
may work with images (e.g., photos or pictograms) that represent an object,
person, place, or concept. Figure1 shows a telegraphic message created with a
set of pictograms. Notice that the telegraphic message is formed only by key-
words (i.e., nouns, verbs, adjectives, and adverbs), without connecting words
(i.e., conjunctions, prepositions, and articles) and verb conjugation. According
to [9], an AAC system should provide clues to help the authoring of telegraphic
phraseswithsyntacticandsemanticcorrectness.Thesecluescanbevisual(e.g.,
colorsandarrows)orsemantic(e.g.,questionsandsamplewords).Forthis,these
systems must rely on linguistic knowledge bases, which provide information on
how words relate in natural language.
Somestudies[12–14]proposetheuseofSemanticGrammars(SGs)asabasis
tosupporttheconstructionofunderstandablesentencesinAACsystems.Inthis
type of base, lexical semantics relations of hierarchy and predicate-argument
ThisresearchwassupportedbytheCoordinationofImprovementofHigherEducation
Personnel (CAPES) [88882.347547/2019-01 and 88887.481522/2020-00].
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.257–264,2020.
https://doi.org/10.1007/978-3-030-58323-1_28258 J. Pereira et al.
Fig.1. Example of a telegraphic sentence with pictograms.
connect the lexical concepts that make up a controlled vocabulary, facilitating
the text’s automatic analysis and construction [6]. However, the SGs used in
these works do not take into account visual and semantic cues for helping the
users on structuring sentences and founding words. Besides, the construction of
the proposed SGs does not take into account AAC vocabulary or grammar, or
even corpus evidence on how words are semantically related to each other in
natural language. To overcome these weaknesses, we propose an SG for AAC
systems based on three resources: (i) an AAC controlled vocabulary, (ii) text
samplesextractedfromanextensivecorpus,and(iii)agrammarextractedfrom
a therapeutic tool for teaching deaf children to construct and read well-formed
sentences.Besides,inthispaper,wereporthowtheproposedSGwasautomati-
cally generated by using NLP techniques for semantic role labeling, dependency
parsing, word sense disambiguation, and named entity recognition; and report
how it was evaluated.
2 Background and Related Work
A Semantic Grammar (SG) [6] is a linguistic knowledge base in which lexical-
semantic properties of hierarchy (e.g., cat isA animal) and predicate-argument
(e.g., cat hasDescriptor color) connect the words and concepts they denote. Its
theoretical foundation comes from the Frame Semantics theory [8], which states
that understanding the meaning of a word requires access to all the essential
knowledge related to that word. Thus, each word evokes a semantic frame that
representsthespeciﬁcconcepttowhichitrefers.Eachsemanticframeisasetof
statements that provide characteristics, attributes, and functions of a concept,
and their interactions with things necessary or typically associated with it [1].
Forexample, in thesentence“He ate the ﬁsh quickly”, the wordate denotes the
frame eat that carries its meaning and its attributes deﬁned by semantic roles:
the agent (He), the theme (the ﬁsh), and the manner (quickly).
In an SG, others semantic frames ﬁll the semantic roles of each semantic
framewithpredicativecharacteristics.Forexample,fortheframeeataframeof
persons may ﬁll the semantic role agent. According to [6], this allows the usage
of the lexical-semantic knowledge of hierarchy (e.g., man isA person) in the
process of analyzing and constructing sentences in natural language, avoiding
the grammatical ambiguity that can exist when using only grammatical classes
(e.g., noun).
Some AAC proposals use SGs, or similar linguistic bases, for supporting the
authoringofmeaningfulsentences.TheCOMPANSIONsystem[13],forexample,A Semantic Grammar for Augmentative 259
uses an ontology as a basis for expanding telegraphic sentences to complete and
correctness sentences. In this system, the users input keywords of a pretended
sentence, not necessarily well ordered, and the system ﬁnds the best order and
thewordrelationsthroughsemanticparsing,basedontheontology.Accordingto
[14],themainissueregardingthisapproachishoweﬃcientasemanticparsercan
be, given the existing gaps in the telegraphic text (e.g., absence of prepositions,
conjunctions, and verb conjugation). For this reason, the authors proposed a
AACsystembasedonacontrolledprocessforcreatingsentences.Inthissystem,
a set of speciﬁcations made by a surface realizer1, which uses an ontology as a
basis, controls the sentence construction by suggesting words. [12] adopted a
similar method on proposing the Simple Upper Ontology (SUpO), described as
a semantic grammar for beginning communicators. SUpO is an extension of a
subset of FrameNet [4] combined with grammar rules for text realisation.
The central gap of these solutions is the absence of the use of SGs based on
visual and semantic clues to support sentences authoring. Besides, these bases
constructionwasnotbasedonevidenceextractedfromtextsamples,and,except
for SUpO, they do not use vocabularies aimed at AAC users.
3 Semantic Grammar
3.1 Construction
Figure2showsanoverviewofthemethodusedtobuildtheproposedSG,which
is divided into three steps and takes three materials as input. The following
sub-sections present both, the inputs and the steps.
Fig.2. Overview of the method for constructing semantic grammar
Input Materials. The three materials taken as input are: a controlled vocab-
ulary, a text corpus, and a grammar. The controlled vocabulary consists of a
listofthe621wordsthathavethebestaccumulated-recall overchildren’sstate-
ments extracted from CHILDES corpora [11] and its corresponding WordNet
1 Amoduleofnaturallanguagegenerationsystems,thatconvertanabstractsemantic
representation into a linguistic utterance.260 J. Pereira et al.
synsets2 organized in a taxonomy. It is a result produced by a research project
started in [10]. The text corpus is the British National Corpus (BNC) [7], which
is extensive, has well-formed sentences, and is widely used in the ﬁeld of NLP.
The grammar is based on the Colourful Semantic (CS) [5], which is a therapeu-
tic tool used for teaching children with complex communication needs to create
and read meaningful well-formed sentences. As shown in Fig.3, this tool uses a
simple grammatical structure in which the slots are assigned by semantic roles
associatedwithcolorsandquestions.Accordingto[5],thisassociation(i)estab-
lishes a signiﬁcant relationship between the question and the semantic role, (ii)
associates each type of phrase with a visual sequence of colors, and (iii) serves
toalertchildrenwhentheyomitasemanticrole.Theinputgrammarconsistsof
a mapping of Colourful Semantics roles to PropBank-like [3] labels, and of rules
thatdetermineswhatclassofsynsetcanﬁlleachrole.Forexample,onlysynsets
of adverbs can ﬁll the manner role, and no verbs, adverbs, or adjectives synsets
can ﬁll the agent role.
Fig.3. Colourful semantics
Sentence Searching. The ﬁrst step of the method consists of searching in the
corpussentencesinwhicheachverbornounofthecontrolledvocabularyoccurs.
These are called reference sentences, and it is from there that we extracted the
predicate-argument relations. We established a limit maximum of 500 sentences
foreachword.However,itwasnotpossibletoreachthisnumberforsomewords
(e.g., breakfast, out, and lunch), due to the low frequency of its occurrence in
BNC.
Semantic Parsing. This step consists of extracting the semantic structures
(e.g., eat hasAgent person) from the reference sentences. For this, for each sen-
tence,twosub-tasksareperformed.Theﬁrstonehastheobjectiveofidentifying
the arguments (e.g, hasAgent, hasTheme) presents in the sentences. For doing
that, we use (i) Semantic Role Labeling (SRL) when the target word is a verb,
or(ii)DependencyParsing(DP)whenitisanoun.ForSRL,weusetheSLING
framework [17], which can identify verbal predicates in text and label its argu-
ments with ProbBank-like [3] labels. In this work, these labels are mapped to
VerbNet labels using SemLink [15] and then mapped to the roles used by CS.
2 A synset is a set of cognitive synonyms expressing a concept.A Semantic Grammar for Augmentative 261
For DP, we use spaCy dependency parser3 for identifying adjectival modiﬁers of
nouns and label it as Descriptor in CS labels. This process generates a semantic
structure for each sentence, in which the arguments are labeled with the CS
semantic roles, and its complements are words.
The second sub-task of semantic parsing consists of identifying the synsets
evokedbyeachofthesewords.Forthis,weuseNamedEntityRecognition(NER)
andWordSenseDisambiguation(WSD).ForNER,weuseSLING,whichlabels
namedentitieswithOntoNotesclasses(e.g.,PERSON,LOCATION,ORG)that
we mapped to WordNet synsets based on their meanings (e.g., PERSON is
mappedtoperson.n.01).ForWSD,weuseastructure-basedagentbasedonWu
& Palmer similarity [18], with labels ambiguous words with WordNet synsets.
However, the synsets identiﬁed by NER and WSD may not necessarily be part
of the set of synsets of the input controlled vocabulary. Therefore, we use Wu &
Palmer similarity to identify its most similar in the input set.
Theexecutionofthesetwosub-tasksgeneratesasemanticstructureinwhich
eachwordhastheargumentsthatwereidentiﬁedinthetextsamples,thesynsets
that ﬁll those arguments, and the frequencies with which each of these synsets
occurs. For example, the verb eat has the argument hasAgent that ﬁlled by the
synsetsperson.n.01andpronou.n.01withtheirrespectivefrequencies:10and8.
Removing Redundancies. There may be redundancy in synsets that ﬁll a
given argument of a given verb or noun. It happens when two or more synsets
of the same taxonomy branch (i.e., they have some level of inheritance relation)
ﬁll an argument. These redundancies can have adverse eﬀects when performing
queries on the SG, so they must be removed.
Forremovingtheredundancies,thesynsetsﬁllingagivenargumentareorga-
nizedinatreeaccordingtotheirhyperonymyrelationshipsfromWordNet.Then,
thetreeisanalyzedfromthelowesttothehighestnodes,removingthosethatare
less frequent than their nearest hyperonym. Next, a cut by importance is made
by establishing a cut-oﬀ threshold based on the frequency of occurrence of the
synsets.Thiscut-oﬀthresholdisestablishedempirically,usingZ-scorewhenthe
frequency distribution follows the normal distribution, and T-score when not.
Finally, the remaining synsets tree is analyzed, keeping only the highest nodes
of each branch. This way, the redundant and insigniﬁcant synsets are removed.
The remaining are inserted as complements to that argument in the SG.
3.2 Overview
TheproposedSGconsistsofanontologythatiscomposedbyastructureofcon-
cepts (synsets) with relations of hierarchy and predicate-argument. The hierar-
chy relations are inherited from WordNet, and the predicate-argument relations
are inserted by the procedure described in the previous section. The proposed
SG has a total of 4295 predicate-argument relationships, which can be used
3 https://spacy.io/.262 J. Pereira et al.
to perform searches in the prediction and suggestion of words during the cre-
ation of sentences. The properties used for these relations are based on CS (i.e.,
hasAgent, hasTheme, hasRecipient, hasManner, hasLocation, hasTime, and
hasDescriptor),andconnectsthepredicatestoitsarguments,asshowninFig.4.
Fig.4. Excerpt from the semantic grammar
4 Evaluation
AstheproposedSGconsistsofanontology,itisevaluatedfollowinganontology
assessment approach. It is a task-based assessment, which consists of assessing
how eﬃcient an ontology is in fulﬁlling the task for which it is directed [16].
In our case, SG’s task is to support the construction of meaningful telegraphic
sentences. For this reason, we evaluated the precision the SG has on suggesting
the correct words in the reconstruction of telegraphic phrases extracted from
CHILDES[11].Forthis,thesentencesofCHILDESwerepreprocessedtobecome
telegraphic, and its reconstruction was simulated.
Table1 shows the summary of evaluation results, with the total number of
sentences for each sentence type, the average precision, the number of sentences
that were wholly reconstructed (100%), and the number of those in which the
reconstruction precision was higher than or less than 50%. The results show
that the proposed SG can support the reconstruction of understandable tele-
graphic phrases with an average accuracy of 90%, considering a total of 1246
sentences.ThismeansthattheprobabilityofauserofanAACsystemusingthe
SGasadatabasetoﬁndthecorrectargumentsforverbalpredicatesduringsen-
tence construction is 90%. In addition, 71.9% of the sentences were fully recon-
structed.Thisnumberindicatesthelevelofcoveragethatthepredicate-argument
relationships present in the SG have over children’s statements extracted from
CHILDES. 26.6% of the sentences were not fully reconstructed but had a preci-
sion greater than or equal to 50%, and 1.4% (18) had precision less than 50%.
The low score of some sentences is caused by errors in the annotation of
CHILDES sentences (e.g., blackbirds lemmatized as black), or by semanticA Semantic Grammar for Augmentative 263
Table 1. Summary of evaluation results
Sentence type n Average 100% >50% <50%
precision
Agent + Verb + Theme 1162 0,91 864 285 13
Agent + Verb + Location 42 0,80 22 13 5
Agent + Verb + Theme + Location 23 0,86 10 13 0
Theme + Verb + Description 19 0,67 0 19 0
TOTAL 1246 0,90 896 332 18
errors, as in the telegraphic sentence “bed sit bed”. However, it is also the result
oftheabsenceofpredicate-argumentrelationshipsbetweensomeconceptsinthe
SG. For example, in the sentence “I sit in the chair”, no one of the arguments
of the verb sit was ﬁlled, given to the absence of relations between the concept
evoked by this verb and the concepts pronoun.n.01 (for I) and chair.n.01 (for
chair).
5 Conclusions
This paper proposes a Semantic Grammar (SG) as a basis for supporting well-
formedandsemanticcorrecttelegraphicsentencesauthoringinAACsystems.It
was acquired by an automatic process that extracted predicate-argument rela-
tions from text corpus using NLP techniques. For this, we used three main
resources:1)anAACdomaincontrolledvocabulary,2)acorpus,and3)agram-
mar based on visual and semantic clues. We evaluated the proposed SG using
a task-based ontology assessment approach. For this, we extracted a total of
1246 sentences from CHILDES corpora and simulated its reconstructions using
the knowledge represented in the SG. We compared the reconstructed sentences
with its references extracted from CHIELDS by assessing a modiﬁed precision
score and obtained a precision average of 90%. Besides, the SG supported the
full reconstruction of 71,9% of the sentences.
These results demonstrate that the proposed SG can provide the necessary
support for the construction of meaningful sentences. However, there are still
aspects that need to be addressed better. Besides, the proposed SG has not
been evaluated by humans or tested in real contexts of AAC use. As future
work, we intend to gather resources to replicate the SG construction method to
build a similar base for the Portuguese language.
References
1. Allan, K.: Natural language semantics (2001)
2. ASHA: Augmentative and alternative communication (2019). https://www.asha.
org/PRPSpeciﬁcTopic.aspx?folderid=8589942773xion264 J. Pereira et al.
3. Babko-Malaya,O.:Propbankannotationguidelines(2005).http://verbs.colorado.
edu
4. Baker, C.F., Fillmore, C.J., Lowe, J.B.: The Berkeley framenet project. In: Pro-
ceedings of the 17th International Conference on Computational Linguistics-
Volume 1, pp. 86–90. Association for Computational Linguistics (1998)
5. Bryan, A.: Colourful semantics: thematic role therapy. In: Language Disorders in
ChildrenandAdults:PsycholinguisticApproachestoTherapy,pp.143–161(1997)
6. Burton, R.R.: Semantic grammar: an engineering technique for constructing nat-
ural language understanding systems (1976)
7. Consortium, B., et al.: The British national corpus, version 3 (BNC xml edition).
DistributedbyOxfordUniversityComputingServicesonbehalfoftheBNCCon-
sortium 5(65), p. 6 (2007)
8. Fillmore, C.J.: Frame semantics. Linguistics in the Morning Calm, pp. 111–138
(1982)
9. Franco, N., Silva, E., Lima, R., Fidalgo, R.: Towards a reference architecture for
augmentativeandalternativecommunicationsystems.In:BrazilianSymposiumon
ComputersinEducation(Simp´osioBrasileirodeInform´aticanaEducac¸a˜o-SBIE),
vol. 29, p. 1073 (2018)
10. Franco,N.M., deLima, A.L., Lima,T.P.,daSilva, E.A., deLima,R.J., doNasci-
mento Fidalgo, R.: A recall analysis of core word lists over children’s utterances
for augmentative and alternative communication. In: 2017 IEEE 30th Interna-
tional Symposium on Computer-Based Medical Systems (CBMS), pp. 278–283.
IEEE (2017)
11. MacWhinney, B.: The CHILDES Project: Tools for Analyzing Talk, Volume II:
The Database. Psychology Press, New York (2014)
12. Mart´ınez-Santiago, F., D´ıaz-Galiano, M.C., Uren˜a-Lo´pez, L.A., Mitkov, R.: A
semanticgrammarforbeginningcommunicators.Knowl.-BasedSyst.86,158–172
(2015)
13. McCoy, K.F., Pennington, C.A., Badman, A.L.: Compansion: from research pro-
totype to practical integration. Nat. Lang. Eng. 4(1), 73–95 (1998)
14. Netzer,Y.,Elhadad,M.:Usingsemanticauthoringforblissymbolscommunication
boards. In: Proceedings of the Human Language Technology Conference of the
NAACL,CompanionVolume:ShortPapers,pp.105–108.AssociationforCompu-
tational Linguistics (2006)
15. Palmer, M.: Semlink: linking propbank, verbnet and framenet. In: Proceedings of
the Generative Lexicon Conference, GenLex 2009, Pisa, Italy, pp. 9–15 (2009)
16. Raad, J., Cruz, C.: A survey on ontology evaluation methods. In: International
ConferenceonKnowledgeEngineeringandOntologyDevelopment,partofthe7th
International Joint Conference on Knowledge Discovery, Knowledge Engineering
and Knowledge Management (2015)
17. Ringgaard, M., Gupta, R., Pereira, F.C.: Sling: a framework for frame semantic
parsing. arXiv preprint arXiv:1710.07032 (2017)
18. Wu, Z., Palmer, M.: Verbs semantics and lexical selection. In: Proceedings of the
32nd Annual Meeting on Association for Computational Linguistics, pp. 133–138.
Association for Computational Linguistics (1994)Assessing Unintended Memorization in
Neural Discriminative Sequence Models
B
Mossad Helali( ) , Thomas Kleinbauer, and Dietrich Klakow
Spoken Language Systems Group, Saarland Informatics Campus,
Saarland University, Saarbru¨cken, Germany
{mhelali,thomas.kleinbauer,dietrich.klakow}@lsv.uni-saarland.de
Abstract. Despite their success in a multitude of tasks, neural models
trained on natural language have been shown to memorize the intrica-
ciesoftheirtrainingdata,posingapotentialprivacythreat.Inthiswork,
weproposeametrictoquantifyunintendedmemorizationinneuraldis-
criminative sequence models. The proposed metric, named d-exposure
(discriminative exposure), utilizes language ambiguity and classiﬁcation
conﬁdence to elicit the model’s propensity to memorization. Through
experimental work on a named entity recognition task, we show the
validity of d-exposure to measure memorization. In addition, we show
that d-exposure is not a measure of overﬁtting as it does not increase
when the model overﬁts.
·
Keywords: Named entity recognition Natural language
·
understanding Privacy
1 Introduction
Neural networks have become prevalent in numerous machine learning tasks
in general and in natural language processing in particular. An issue that has
been identiﬁed with neural models, however, is that they tend to memorize
theirtrainingdata[2,7,10].Memorizationraisessevereprivacyconcernsincases
where such models are trained on datasets that contain sensitive information
such as credit card numbers, passwords, etc. If such models are deployed e.g. on
smartphones [5] or as a service [4], they give attackers access to the memorized
sensitive information.
The focus of this paper is on unintended memorization, which occurs when
modelsretaininformationthatareorthogonaltothelearningtask.Forexample,
forthetaskofnamedentityrecognition(NER)onadatasetofemails,memoriz-
ingpasswordsthatappearinthedatasetisunintended.Existingworkfocuseson
neuralgenerativesequencemodels,suchaslanguagemodelsandmachinetrans-
lation models, and uses model perplexity to quantify unintended memorization
ThisresearchhasreceivedfundingbytheEuropeanUnion’sHorizon2020researchand
innovation programme under grant agreement No. 3081705 – COMPRISE (https://
www.compriseh2020.eu/).
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.265–272,2020.
https://doi.org/10.1007/978-3-030-58323-1_29266 M. Helali et al.
[2]. In this paper, we propose a metric for discriminative models where perplex-
ity cannot be utilized. The idea is to give practitioners the means to assess the
degree of memorization in models intended for deployment, allowing them, e.g.,
to choose hyper-parameter settings that minimize privacy-threatening informa-
tion leakage.
Our main contributions are:
– A method for quantifying memorization in discriminative models. This
involves inserting speciﬁcally designed ambiguous phrases into the training
set of the model and analyzing the model’s conﬁdence with respect to the
created phrases. The proposed metric is named d-exposure (for discrimina-
tive exposure).
– An experimental validation of the proposed deﬁnition on a competitive neu-
ral NER model and benchmark dataset. As in previous work, we ﬁnd that
exposure increases with the number of repetitions of inserted phrases in the
trainingset.Inaddition,weconﬁrmthatd-exposureisnotameasureofover-
ﬁtting as unintended memorization does not increase when the model starts
to overﬁt.
2 Related Work
In one of the earliest studies on memorization in neural networks, Zhang
etal.[10]showthatneuralnetworkshavethecapabilitytoﬁtdatawithrandom
labels, meaning that state-of-the-art models are at risk of memorization. Song
et al. [7] present a method to create neural models that memorizes the training
data with no noticeable diﬀerence in utility. This raises concerns because utility
is often the main criterion for deciding which model to deploy. In their analysis
of memorization, Arpit et al. [1] show that memorization is not only dependent
on the model, but also on the dataset. While these works are important in the
analysisofmemorization,theydonotprovideaquantitativemethodforgauging
the depth of the problem.
The ﬁrst work on assessing unintended memorization in neural models on
languagetaskswasbyCarlinietal.[2].Toassessunintendedmemorization,they
deﬁneametric,namedexposure,thatisbasedoncomparingtheperplexityPx(s)
ofarandomphrasesinsertedintothetrainingsetwiththeperplexitiesofother
phrasesfromthesamerandomspace.Thebasictenetisthatasigniﬁcantlylower
perplexity of the inserted phrase vs. those of the other random phrases signals
that the neural model has unintentionally memorized that phrase. Speciﬁcally,
for a random phrase s inserted into the training set of a model θ, exposure is
deﬁned as:
exposureθ(s)=−log2Pr[Pxθ(r)≤Pxθ(s)] (1)
r∈R
where R is the random space of all such phrases. Note that high memoriza-
tion, i.e., low perplexity, is reﬂected by high exposure values. The authors test
theirdeﬁnitionempiricallyandconcludethatmemorizationisnotdirectlylinked
to overﬁtting but rather to the learning process itself, making memorizationAssessing Unintended Memorization 267
a prevalent issue in state-of-the-art neural models. However, the authors’ app-
roachislimitedtogenerativesequencemodelsbecausethedeﬁnitionofexposure
is based on perplexity. We show below how a similar line of reasoning can be
utilized for an exposure measure on discriminative models.
Anotherrelatednotiontotheproblemofmemorization ismembershipinfer-
ence attacks, where an attacker tries to infer whether a set of samples belong to
the dataset of a trained model. Truex et al. [9] have done an extensive analysis
on how such attacks can be carried out and on the vulnerability of the mod-
elsunderattack. Thoughmembershipinferenceisrelatedtoourwork,thereare
notablediﬀerencesbetweentheapproaches.First,thegoalofourworkistomea-
sure the model’s propensity to leaking information, not analyzing whether the
modelcanbeattacked.Forexample,foranoverﬁtmodel,membershipinference
probability increases [6], while memorization is not correlated with overﬁtting.
Moreover, calculating exposure is a simpler procedure that does not involve
building shadow datasets and attack models as in membership inference.
3 Approach
Theexistingdeﬁnitionofexposurein[2]isinapplicabletodiscriminativemodels
becauseitisbasedonperplexity,whichisnotsuppliedbydiscriminativemodels.
Instead, such models output for each class a level of conﬁdence that the input
wordbelongstothatclass.Thismotivatesadeﬁnitionofexposureper class asit
canbehavediﬀerentlyforeachclass.Whileexhaustiveenumerationofperplexity
isineﬃcient[2],itisfeasibletoenumeratethemodel’sconﬁdenceforallwordsin
eachclassbecausetheseareinthemagnitudeofonlyafewthousands,depending
on the dataset.
Intuitively, memorizing is the opposite of generalizing. A good model will
classify an unambiguous sentence with high conﬁdence. For example, in the
sentence“IpreferGermany”,thelastwordshouldclearlybelabeledasalocation
in an NER task. Polysemous words, however, may constitute diﬀerent named
entities depending on the context. For instance, the word “Jordan” could refer
toaperson(e.g.,MichaelJordan),alocation(e.g.thecountryofthesamename),
oranorganization(e.g.TheJordanCompany).Ifsomeofthesecasesappearwith
roughly the same frequency in the training data, an ambiguous test sentence,
such as “I prefer Jordan”, should thus be classiﬁed with low conﬁdence. Even
adding the same sentence to the training data should not change this – unless
the model tends to memorize sentences. In other words, an unexpected high
conﬁdenceintheclassiﬁcationofanambiguoussentencehintsatthepossibilityof
unintendedmemorizationinagivenmodel.Webaseourdeﬁnitionofd-exposure
on this notion and follow the general procedure given by Carlini et al. [2].
3.1 d-exposure for Discriminative Models
Given a ﬁxed phrase that has a word s with multiple possible class labels, we
insert the phrase in the training set with s labeled as Ci and train the model θ.268 M. Helali et al.
d-exposure for class Ci is then given by:
d-exposure (s)=−log Pr [conf(w)≥conf(s)] (2)
θ,Ci 2w∈Ci
whereconf(s)istheconﬁdencereturnedbythemodelwhenlabelings.Therefore,
d-exposure has a value ∈ [0,log2|Ci|] with |Ci| denoting the number of words
that are labeled only as Ci. Maximum d-exposure is obtained when s has the
highestconﬁdence(highmemorization)andviceversa.Notethatthisisthecase
if all words are assigned the correct class. If s is labeled incorrectly, however,
d-exposure is deﬁned to be zero. On the other hand, if other words in Ci are
incorrectly labeled, they are treated as having lower conﬁdence than s, because
the model classiﬁed the ambiguous phrase correctly while failing to correctly
classify the clear one. We apply the same process for other entity classes in the
dataset and calculate d-exposure of the model as:
(cid:2)
1
d-exposure (s)= d-exposure (s) (3)
θ N θ,Ci
Ci
whereNisthenumberofclasses.Thisdeﬁnitionallowsonetoignoreclassesthat
areconsideredirrelevantforthetaskathand.Forexample,ifoneisinterestedin
measuringthememorizationoftheirmodelonthenamesofpersonsandlocations
only,onecouldsimplycomputed-exposureforthesetwoclasses.Recallthatthe
purposeofthemetricistoguidethechoiceofmodelsettingsbeforedeployment.
Which phrases and classes to consider are choices made by the user.
4 Experimental Validation
In this section, we experimentally test the proposed deﬁnition of d-exposure
in order to: (1) show its validity as a measure of unintended memorization in
discriminative models, and (2) demonstrate that d-exposure is not linked to
overﬁtting.Weshowourresultsonanamedentityrecognitiontaskasanexample
of discriminative models.
4.1 Setup
We conduct our experiments on CoNLL-2003 [8], a popular NER dataset in
English. In our experiments, we focus on the tags: S-PER, S-ORG, and S-LOC.
We discard S-MISC to decrease the variability as including it would lead to
the inserted phrase having multiple correct labels (based on the deﬁnition of
S-MISC). Table1 shows statistics of these classes in CoNLL-2003 dataset. The
ﬁrst column is the number of unique entities that belong only to the respective
class (|Ci|); the second column is the number of unique entities that have more
than one possible label (i.e. candidates for s); the third column is the frequency
of each class in the training set.
For the inserted phrase, we choose the ambiguous format: “There are many
people who like ”, which allows entities of the three types to ﬁll the blank.Assessing Unintended Memorization 269
Table 1. Statistics of the cho- Table2.Numberofoccurrencesof
sen classes in the training set of the chosen entities in the training
CoNLL-2003. set of CoNLL-2003.
Label Exclusive Overlapped Frequency Word S-PERS-ORGS-LOC
S-ORG 1001 101 3836 Williams 7 8 0
S-PER 949 19 2316 Chelsea 5 6 0
S-LOC 937 97 6101 Melbourne0 4 5
Forthechosenentities,“Williams”wasinsertedasS-ORG,“Chelsea”asS-PER
and “Melbourne” as S-LOC. We chose these entities because their occurrences
in the training set are more balanced than others. Table2 shows the number
of occurrences of these entities as each class. That said, we found out that the
general behavior of d-exposure does not change based on the chosen entities, as
long as they are not highly imbalanced towards one class, nor does it change
based on the format, as long as it is ambiguous.
For the model, we use a BiLSTM with GloVe embeddings, SGD optimizer,
dropout (50%) and learning rate decay, implemented with Targer1, a neural
tagging library [3]. This model achieves an F1 of 90.0 on CoNLL-2003 dataset.
4.2 Repeated Occurrences in the Training Set
In this experiment, we test whether d-exposure increases with the number of
times the chosen phrase appears in the training set. The intuition is that the
more the model sees the sentence, the higher the incentive to memorize it. For
this matter, we insert the chosen sentences 4, 16, 64, 128 and 256 times and
observe d-exposure for each category. Figure1 shows the eﬀect of the number
of repetitions of the inserted sentence on d-exposure. As expected, d-exposure
generallyincreaseswiththenumberofrepetitions,implyingthatrepeatedoccur-
rence of a sentence in the training set tends to produce higher memorization.
Another observation is that d-exposure does not behave the same in all classes.
Rather,it ismuch lower for S-LOCthan the othertwo. This validates our claim
that exposure is to be measured per-class as diﬀerent classes occur in diﬀerent
contexts but the exact reasons for the diﬀering behavior require further inves-
tigation. In additional experiments with other model architectures not detailed
here, we found the same general trend in the curves but the behavior of S-PER
and S-LOC reversed. Table3 shows d-exposure evaluated at diﬀerent epochs
(columns) and number of repetitions (rows) for the three classes. The ﬁrst row
is the value of d-exposure when the phrase is not inserted in the training set.
1 https://github.com/achernodub/targer.270 M. Helali et al.
Fig.1. d-exposure vs. repetitions for individual classes and averaged on CoNLL-2003
at 150 epochs.
4.3 Overﬁtting
In this experiment, we observe the behavior of d-exposure against overﬁtting.
Weconductthisanalysistoconﬁrmthatexposureisnotameasureofoverﬁtting
butratherofmemorization.Ifitwasso,weexpectittoreachitsmaximumvalue
for all classes when overﬁtting begins or to keep increasing while the model is
overﬁtting. To make the model overﬁt, we train it only on 10% of the training
data, increase the number of epochs to 250 and disable learning rate decay and
dropout. Figure2 shows the results when the phrases are repeated 16 times. d-
exposureincreasesasthemodelislearningandstopsincreasingwhenoverﬁtting
begins. In addition, maximum d-exposure for S-LOC (8.0) or S-ORG (8.1) is
not reached at any point. Recall that the maximum d-exposure for a class Ci
is log2|Ci|, where |Ci| is the number of entities belonging only to that class.
ForS-PER,however,maximumd-exposure(7.5)isreachedonlyatstageswhere
the model has not yet overﬁt. Therefore, we conclude that d-exposure is not
correlated with overﬁtting and for the case of S-PER, the model has higher
memorization. Similar results were found for diﬀerent numbers of repetitions.
Table 3.d-exposurefortheclassesS-LOC,S-PER,andS-ORGfordiﬀerentnumbers
of repetitions (rows) and epochs (columns).
S-LOC S-PER S-ORG
25 50 75 100 125 150 25 50 75 100 125 150 25 50 75 100 125 150
0 1.76 1.42 1.51 1.33 1.55 1.46 0.00 0.00 0.00 0.31 0.34 0.00 0.00 0.00 0.00 0.00 0.00 0.00
4 1.87 1.68 1.47 1.71 1.57 1.46 0.00 0.43 0.48 0.59 0.72 0.78 0.00 0.00 0.00 0.82 0.00 0.00
16 1.87 1.74 1.72 2.00 1.78 1.51 0.49 0.58 0.82 0.98 0.79 1.10 0.00 1.12 0.67 1.28 1.28 1.26
64 2.68 2.27 2.30 2.47 2.68 2.52 2.71 1.40 3.57 2.47 4.47 3.64 2.38 3.14 2.96 2.89 4.16 4.16
128 3.27 3.00 3.50 2.51 2.09 2.09 4.61 4.50 3.94 4.76 4.80 5.19 2.24 4.11 5.01 4.24 4.68 5.51
256 4.52 3.03 3.43 3.57 3.83 3.29 8.89 8.31 8.31 9.89 7.89 7.89 3.46 4.21 6.16 5.21 6.06 6.16Assessing Unintended Memorization 271
Fig.2. d-exposure vs. overﬁtting for S-LOC, S-PER, and S-ORG on CoNLL-2003.
5 Conclusion and Future Work
Inthiswork,wepresentedameasureofunintendedmemorizationindiscrimina-
tiveneuralmodels.Itisinspiredbypreviousworkongenerativesequencemodels
but oﬀers an approach for tasks where measuring perplexity is not feasible. The
core idea is to identify the exposure of potentially private data with conﬁdence
assessments of model predictions. We show how ambiguous sentences can be
employed towards that goal in a named entity recognition task. One limitation
of this methodology is that it can only be applied to NER classes that share
some linguistic materials with at least one other class.
Weperformedanumberofin-depthexperimentstoillustratetheeﬀectiveness
ofournewmetricforassessingmodelmemorization.Whilewefocusononetask
here, with a reduced number of NE labels, we are nevertheless able to conﬁrm
the ﬁndings of the previous work on exposure for generative sequence models.
In particular, these are 1) higher d-exposure values for repeated insertions of
a test phrase into the training data; and 2) independence of d-exposure from
model overﬁtting. The ﬁrst ﬁnding conﬁrms that the number of occurrence of
a phrase in the training data, the expected memorization of that phrase in the
model, and the proposed metric all correlate positively. The second ﬁnding sets
our approach apart from methods such as membership inference attacks which
are prone to signiﬁcant performance drops for overﬁtted models.
In the future, we plan to perform similar validation experiments for other
natural language processing tasks as well. The deﬁnition of what constitutes an
“ambiguousphrase”foreachtaskposesachallengebutisanecessarystepinthe272 M. Helali et al.
proposedmethodology.FortheNERtaskaddressedhere,anumberofadditional
experiments are conceivable as well, e.g., going beyond single-word entities.
With a powerful metric now in place, an even more interesting future step
will be the exploration of principled ways in which counter-measures for model
memorization could be realized. Ultimately, assessing a potential information
leakage is only the ﬁrst step, supporting the prevention or conﬁnement of such
leakages must be the goal to aspire to.
References
1. Arpit, D., et al.: A closer look at memorization in deep networks. In: Proceedings
of the 34th International Conference on Machine Learning - Volume 70, ICML
2017, pp. 233–242. JMLR.org (2017)
2. Carlini,N.,Liu,C.,Kos,J.,Erlingsson,U´.,Song,D.:Thesecretsharer:measuring
unintended neural network memorization & extracting secrets. In: Proceedings of
the28thUSENIXSecuritySymposium,pp.267–284.USENIXAssociation,Santa
Clara, CA, USA, 14–16 August 2019 (2019)
3. Chernodub,A.,etal.:Targer:neuralargumentminingatyourﬁngertips.In:Pro-
ceedingsofthe57thAnnualMeetingoftheAssociationofComputationalLinguis-
tics (ACL’2019). Florence, Italy (2019)
4. Hesamifard,E.,Takabi,H.,Ghasemi,M.,Wright,R.:Privacy-preservingmachine
learningasaservice.Proc.PrivacyEnhancingTechnol.3,123–142(2018).https://
doi.org/10.1515/popets-2018-0024
5. Koneˇcny´, J., McMahan, H.B., Yu, F.X., Richt´arik, P., Suresh, A.T., Bacon, D.:
Federated learning: strategies for improving communication eﬃciency. In: NIPS
Workshop on Private Multi-Party Machine Learning (2016)
6. Long,Y.,Bindschaedler,V.,Gunter,C.A.:Towardsmeasuringmembershipprivacy
(2017). http://arxiv.org/abs/1712.09136
7. Song, C., Ristenpart, T., Shmatikov, V.: Machine learning models that remember
too much. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer
andCommunicationsSecurity,CCS2017,pp.587–601.AssociationforComputing
Machinery, New York (2017)
8. Tjong Kim Sang, E.F., De Meulder, F.: Introduction to the CoNLL-2003 shared
task: language-independent named entity recognition. In: Proceedings of the Sev-
enthConferenceonNaturalLanguageLearningatHLT-NAACL2003,pp.142–147
(2003)
9. Truex,S.,Liu,L.,Gursoy,M.E.,Yu,L.,Wei,W.:Demystifyingmembershipinfer-
ence attacks in machine learning as a service. IEEE Trans. Serv. Comput. (Early
Access), 05 February 2019
10. Zhang,C.,Bengio,S.,Hardt,M.,Recht,B.,Vinyals,O.:Understandingdeeplearn-
ingrequiresrethinkinggeneralization.In:5thInternationalConferenceonLearning
Representations,ICLR2017,Toulon,France,24–26April,2017,ConferenceTrack
Proceedings. OpenReview.net (2017)Investigating the Impact of Pre-trained
Word Embeddings on Memorization
in Neural Networks
B
Aleena Thomas( ) , David Ifeoluwa Adelani, Ali Davody, Aditya Mogadala,
and Dietrich Klakow
Spoken Language Systems Group, Saarland Informatics Campus,
Saarland University, Saarbru¨cken, Germany
{athomas,didelani,adavody,amogadala,dietrich.klakow}@lsv.uni-saarland.de
Abstract. Thesensitiveinformationpresentinthetrainingdata,poses
aprivacyconcernforapplicationsastheirunintendedmemorizationdur-
ing training can make models susceptible to membership inference and
attributeinferenceattacks.Inthispaper,weinvestigatethisproblemin
variouspre-trainedwordembeddings(GloVe,ELMoandBERT)withthe
helpoflanguagemodelsbuiltontopofit.Inparticular,ﬁrstlysequences
containing sensitive information like a single-word disease and 4-digit
PIN arerandomlyinsertedintothetrainingdata,thenalanguagemodel
istrainedusingwordvectorsasinputfeatures,andmemorizationismea-
sured with a metric termed as exposure. The embedding dimension, the
numberoftrainingepochs,andthelengthofthesecretinformationwere
observed to aﬀect memorization in pre-trained embeddings. Finally, to
addresstheproblem,diﬀerentiallyprivatelanguagemodelsweretrained
to reduce the exposure of sensitive information.
· ·
Keywords: Diﬀerential privacy Word representations Unintended
memorization
1 Introduction
Several advances were made in machine learning for addressing numerous tasks
of Computer Vision and Natural Language Processing (NLP). However, there
exist some practical hurdles when applying them in the industry, particularly
when it involves training the models from data containing sensitive information
such as users’ attributes, ﬁnancial information, and health records. It has been
recentlyshownthatanadversarycanrecoversensitiveinformation(asaresultof
memorization [3]) or observations used for training (a.k.a membership inference
attack[11–14])fromapubliclyavailablepre-trainedmodelinablack-boxattack,
i.e., without access to the training data.
Word embeddings [4,8] are often used as primary features for obtaining
state-of-the-art results in NLP tasks as they incorporate both syntactic and
semantic relationships between the words. However, when they are trained on
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.273–281,2020.
https://doi.org/10.1007/978-3-030-58323-1_30274 A. Thomas et al.
user-generated content like social media posts or clinical notes (containing con-
sultationnotes,patientdischargesummaries,andhistorynotes),therelationship
betweenusersandtheirattributese.g.,interests,disease,andhealthhistorycan
belearnedandre-identiﬁedusingmembershipinferenceattackmethods.Models
trained on sensitive information are publicly available like Twitter-Glove1 and
Clinical-BERT [2] as they help to improve the performance of downstream NLP
tasksinthedomainofinterest.Although,theextenttowhichthesemodelsleak
users’ information has not been quantiﬁed.
In this paper, we aim to quantify the leakage of sensitive information in pre-
trainedwordembeddings,namelyGloVe,ELMo,andBERTwhentheyareused
for downstream NLP tasks. Recently, the leakage of sensitive information has
been studied for text generation tasks [14] like machine translation, dialogue
generation and language modeling. This leakage can also be viewed as neural
networks memorizing secret information which was proven to be true [3]. A
simple attack on a language model is predicting sensitive information like credit
card number when given the context in which the secret information appears,
this is even more probable when we limit the space of most likely words, say
from all words in the vocabulary to only numbers. If indeed word embeddings
leadstobetterperformanceonmanyNLPtasksincludinglanguagemodel,does
it also make this attack easier?
It is not straightforward how to compute sensitive information captured by
word embeddings without using them to train an NLP task. So, we have made
use of a simple language model with these word vectors as input features. We
address the problem of quantifying the leakage of sensitive information in pre-
trainedembeddingsbyinvestigatingiftheyexacerbatetheproblemofmemoriza-
tion in a language model when used as input features. We quantify the amount
of information memorized by neural networks or exposed at inference using the
exposure metric proposed by [3]. Speciﬁcally, we compare the exposure of sensi-
tiveinformationonusingdiﬀerentkindsofembeddings:distributedembeddings
obtained by GloVe [8] and contextualized embeddings i.e., ELMo [9] and BERT
[4].Inourexperiments,weobservethatleakageinhigherdimensionalwordvec-
torsisgreaterthanorequaltotheleakageobservedinlower-dimensionalvectors
of the word representations. This is particularly concerning because oftentimes,
the higher dimensional embeddings have better performance when used as fea-
tures for downstream NLP tasks [4,10]. Training diﬀerentially private language
models [7] helps to drastically reduce the exposure of private information, thus
providing better privacy [3].
2 Memorization in Deep Learning Models
Recently,Carlinietal.[3]introducedtheexposure metrictomeasureunintended
memorization in neural networks. Given a model fθ, a secret format s, e.g.,
s = “My PIN number is ####”, and secret s[r] with a randomness r ∈ R
1 Glove trained on 2 billion tweets: https://nlp.stanford.edu/projects/glove/.Investigating Memorization in Pre-trained Word Embeddings 275
Algorithm 1. Precise Exposure
1: procedure CalculatePreciseExposure(D, s[r], EMBEDDING TYPE, R, K)
2: Additional Inputs: {s[r2],s[r3],...,s[rK]} for multiple insertions
3: s[r1]=s[r]
4: for k from 1 to K do
5: D←D∪s[rk]
6: end for
7: Z←getEmbeddings(D, embedding type)
8: θ←trainedLSTM(Z)
9: τ ←{}
10: for rˆ∈ R do
11: τrˆ ← log-perplexityθ(s[rˆ])
12: end for
13: τ(cid:2) ←sort(τ)
14: ρr ←getRank(s[r], τ(cid:2))
15: exposure(s[r])←log2|R|− log2ρr
16: end procedure
(randomness space), e.g., “My PIN number is 2467”; r =2467, the exposure of
the secret s[r] is deﬁned as:
exposureθ(s[r])=log2|R|−log2rankθ(s[r]) (1)
where, the rankθ(s[r]) is the rank of s[r] in the sorted list of log-perplexities
of s[rˆ] for all possible rˆ∈R. The sorting is in ascending order.
The exposure metric depends on the space of R and the implication is that
themaximumvalueofthemetricforlongersequencessuchascreditcardnumber
or4-digitPINishigherthanexposureforasinglewordpredictionlikeadisease.
Carlini et al [3] measured memorization by (1) adding a sequence (e.g., John’s
PIN number is 2467) containing secret information to the training data (2)
trainingalanguagemodelontheaugmenteddataset(3)computingtheexposure
based on the rank of the log-perplexity of the inserted secret, say 2467 from the
otherR=104 availablecombinationswhenthemodelisgivenacontext“John’s
PIN number is”. If the rank is very high especially if the domain of the training
dataset is very diﬀerent from the inserted sequence, this is an indication of
memorization.
3 Measuring and Preventing Unintended Memorization
3.1 Measuring Memorization of Secrets
Following the approach introduced by Carlini et al. [3], we analyze the eﬀect
of diﬀerent word representations on memorization. We make use of an LSTM
language model to compare the levels of exposure while using diﬀerent pre-
trained embeddings as input features.276 A. Thomas et al.
First,weaugmentthetrainingdatawithanadditionalsequencewithasecret
such as “my PIN number is 2467” i.e D ←D∪s[r] in Algorithm 1. For multiple
insertion of secrets, we insert multiple sequences, s[rk] with K diﬀerent PIN
numbers, where s[r ] = s[r]. Next, we obtain pre-trained embeddings for all
1
the training sequences. We represent all the input embeddings with Z which
is passed into the LSTM model. The trained weights, θ of the LSTM model
are learned after training and used to compute the log-perplexity τrˆ for all the
possible secret values, rˆ∈R.
Exposure is computed using the rank of the log-perplexity of the inserted
sequence containing secret s[r] from the list of all log-perplexities of diﬀerent
secretvalues(r ∈R).Thestep-by-stepprocedureforcomputingtheexposureof
asecretsentences[r]givenacorpusD,R,thenumberofsentencestobeadded,
K and the word embedding embedding type is in Algorithm 1.
3.2 Preventing Memorization with Diﬀerential Privacy
Diﬀerentialprivacyisastrongapproachtoresistattacksaimingtoextractsensi-
tiveinformationfromadatasetD.Themainideaisthatreleasinganaggregated
result should not disclose too much information about any individual record in
the dataset. More speciﬁcally, if we deﬁne a dataset D(cid:3) that diﬀers with D in
only one record, xn, when the attacker makes a query on both datasets, he/she
should get almost the same results.
Deﬁnition 1. (Diﬀerential Privacy [5]). A randomized algorithm M is (ε,δ)
diﬀerential private if for all sets of outputs S and for all neighboring datasets
D and D diﬀering on at most one data point
1 2
Pr[M(D )∈S]≤exp(ε)Pr[M(D )∈S]+δ (2)
1 2
Intuitivelya(ε,δ)diﬀerentialprivatealgorithmguaranteesthattheabsolute
valueofinformationleakagewillbeboundedbyεwithprobabilityatleast1−δ.
Therefore ε controls the level of privacy protection and so is called privacy loss.
Diﬀerential privacy can be integrated with deep learning to protects models
from diﬀerent kinds of attacks. However, directly applying random noise within
adeeplearningmodelyieldsinferiorperformancebecauseofthehighsensitivity
of the network’s output to the parameters. A solution to this challenge has
been proposed in [1]. The core idea is adding random noise to the stochastic
gradient descent (SGD) updates and make it private, leading to diﬀerentially
private SGD (DPSGD). Diﬀerentially private training can be used to prevent
unintended memorization and membership inference in deep neural models. In
particular,avariantofDPSGD[1]hasbeenusedin[3]totrainrecurrentneural
networks. It has been shown there that diﬀerential privacy fully eliminates the
memorization eﬀects and reduces exposure of secrets.
4 Experimental Setup
In this section, we present the experimental setups of two sets of experiments
on the Penn Treebank (PTB) dataset for diﬀerent word representations: GloVe,Investigating Memorization in Pre-trained Word Embeddings 277
ELMo and BERT embeddings. We train a 1-layer LSTM language model with
256 hidden units on 2,000 sentences from the PTB text data that consists of
35,000 sequences (assuming a minimum context size of one) and 12,921 vocab-
ulary words. The ﬁrst set of experiments is trained on the dataset augmented
withsecretsequence(s)usingAdamoptimizer [6],whilethesecondsetofexper-
iments helps to reduce memorization using the DPSGD training. We consider
pre-trained embeddings with various vector dimensions, d = 100,300,768,1024
i.e GloVe-{100d, 300d}, ELMo-1024d, BERT-{768d, 1024d}.
To study how the length of the secret aﬀects its memorization, we use two
typesofsecrets;namelysingle word - disease andfour digit - PIN.Disease type
ofsecretisinsertedasfollows-<name>is suﬀering from <disease>.Forall
the experiments with this type of secret augmented, we compute the exposure
value of the sequence, s[r]: john is suﬀering from alzheimers after training the
model on a dataset including this sentence. Since the number of diseases is too
smalltobeconsideredassamplespace,weassumethevocabularyasthesample
space from which the diseases are drawn from. In the PIN type of secret, the
inserted secret is of the form - < name > atm PIN number is < #### >.
Here, the sample space size for computing the exposure is 104.
The memorization could also be aﬀected by the presence of multiple secrets
in the dataset, which may confuse the model. In order to analyze the eﬀect of
having multiple secrets, we use two types of insertion of secrets:
– Single insertionwithapairofsecrets:inthiscase,weaugmentthedataset
with a single sentence that contains either disease or PIN type secret.
– Multiple insertions with unique pairs of secrets: in this case, we augment
the dataset with multiple sentences all of which contains either disease types
or PIN types. For example, we test the exposure of the sequence s[r] after
augmenting the dataset with M additional secrets like: {oliver is suﬀering
from inﬂuenza,laura is suﬀering from cholera,...}.Intheexperiments,M =
16 for disease type and M =10 for the PIN type secret.
(a) Secret-type: disease (b) Secret-type: 4-digit PIN
Fig.1. Exposure values for single insertion of a sequence with a pair of secrets and
multiple insertionofsequences withuniquepairsofsecrets. Thenumberofepochs for
the disease and the PIN type secrets are 5 and 40 respectively.278 A. Thomas et al.
5 Results
Inthissection,wepresenttheresultsoftheexperimentstoanalyzememorization
in word representations. Figure1 shows the exposure values for diﬀerent kinds
of embedding types for single and multiple insertions of secrets. From the plots,
we observe a pattern between the exposure value and embedding dimension
regardless of the secret type, or insertion type for GloVe and BERT embed-
dings. Higher exposure levels are observed for higher dimensions except when
the exposure values were already maximum. This indicates that for the same
embedding type, representations with higher dimensions may memorize more.
This is particularly concerning as a higher performance is generally observed
when the higher dimensions of an embedding type are used.
Wealsoobservethatmultipletypeofinsertionofsecretsdecreasestheexpo-
sure values except for GloVe embeddings with the disease secret type. This
suggests that the presence of multiple instances of the same type of secret could
confuse the model and helps in lowering the exposure levels.
(a) GloVe vs Random embeddings (b) GloVe vs BERT embeddings
Fig.2. Exposure values for random, GloVe and BERT embeddings at diﬀerent stages
of training for the disease secret type with single insertion type of secrets
The length of the secret was also found to aﬀect the memorization. One
interesting ﬁnding during the experiments was that the length of the secret
aﬀected the stage of training the exposure values reach the maximum. For the
disease type, the exposure values were already maximum at 40 epochs unlike in
the case of PIN. This is the reason for the lower number of epochs for disease
type of secret in Fig.1a.
Figure2 shows the exposure levels of random embeddings and pre-trained
embeddingstypes;GloVeandBERTatdiﬀerentstagesoftraining.Itisobserved
that the memorization in the case of GloVe saturates much earlier in training
compared to BERT as shown in Fig.2b. The memorization in BERT represen-
tations is seen to happen later in training, reaching the maximum exposure
only after the 36th epoch as compared to the 12th epoch in the case of GloVe.
The exposure values in the case of GloVe embeddings were observed to reach
its maximum value earlier than that of random embeddings. This shows thatInvestigating Memorization in Pre-trained Word Embeddings 279
Table 1. Exposure values of disease-type of secret for diﬀerent embedding-types with
diﬀerential privacy(DP) training vs non-DP training. The number of epochs for both
versions is 40.
Embedding type Single insertion Multiple insertion
Non-DP DP Non-DP DP
GloVe-100d 5.87 1.61 7.9 1.72
GloVe-300d 6.03 1.17 13.65 2.19
ELMo-1024d 13.39 2.04 9.33 0.65
BERT-768d 14.85 0.12 14.85 1.62
BERT-1024d 14.85 0.27 14.85 2.20
althoughpre-trainedembeddingsgiveanimprovementinperformanceoverran-
domembeddings,theformerareatahigherriskofexposingsensitiveinformation
in the training dataset.
Lastly, we performed experiments by training the LSTM models with
DPSGD (with parameters ε = 10, δ = 2e−5, resulting noise level σ = 0.44), we
observe a drastic reduction in exposure values as shown in Table1 especially for
models with maximum exposure (BERT-768d and BERT-1024d) from 14.85 to
less than 2.21 for both single insertion and multiple insertions of disease-type of
secrets.OurobservationconﬁrmswhatCarlinietal.[3]observedthatdiﬀerential
privacy helps in reducing memorization. But the DP versions run considerably
slower than non-DP versions, e.g., training the non-DP and DP version using
GloVe embeddings take on average, 12min and 14 hours respectively on GPU
(Nvidia Titan X). All the reported exposure values have a maximal standard
deviation of 1.09.
6 Conclusion
In this paper, we investigated memorization in word representations commonly
usedasfeaturesfortrainingthestate-of-the-artnaturallanguageunderstanding
tasks.Wecomparethedegreeofmemorizationofthreediﬀerentwordembedding
types(GloVe,ELMoandBERT).Alltheembeddingtypeswerefoundtoexpose
sensitive information up to a certain extent. This observation implies a possible
privacythreatwhentheyareusedinapplicationswithprivateandsensitivedata.
We observed an increase in the exposure levels (except when the exposure value
is already maximum) with the embedding dimension for GloVe and BERT, and
multiple instances of the sensitive information in the training dataset is seen
to lower memorization. Further, we observed that diﬀerent embedding types
start memorizing at diﬀerent stages of training. The GloVe embeddings were
found to reach maximum exposure level earlier in training compared to random
embeddings of the same dimension.
As future work, we plan to investigate membership inference attack on the
pre-trained embeddings and train diﬀerentially private variants of the embed-280 A. Thomas et al.
dings to prevent leakage of sensitive information in practical applications.
Recently, Vu [15] proposed dpUGC – a diﬀerentially private Word2Vec model
buttheutilityofthemodelwasnotinvestigatedonNLUtasks.Wehopetoinves-
tigatetheutilityofthediﬀerentiallyprivatevariantsofthewordembeddingson
standard NLU tasks like named entity recognition and text classiﬁcation.
Acknowledgments. The presented research has been funded by the European
Union’sHorizon2020researchandinnovationprogrammeprojectCOMPRISE(http://
www.compriseh2020.eu/) under grant agreement No. 3081705. We thank Emmanuel
Vincent and Thomas Kleinbauer for their feedback on the paper.
References
1. Abadi, M., et al.: Deep learning with diﬀerential privacy. In: Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security, pp.
308–318. ACM (2016)
2. Alsentzer,E.,etal.:PubliclyavailableclinicalBERTembeddings.In:Proceedings
of the 2nd Clinical NLP Workshop, NAACL, pp. 72–78 (2019)
3. Carlini,N.,Liu,C.,Erlingsson,U´.,Kos,J.,Song,D.X.:Thesecretsharer:evaluat-
ingandtestingunintendedmemorizationinneuralnetworks.In:USENIXSecurity
Symposium, pp. 267–284 (2018)
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep
bidirectionaltransformersforlanguageunderstanding.In:ProceedingsofNAACL,
pp. 4171–4186 (2019)
5. Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M.: Our data, our-
selves: privacy via distributed noise generation. In: Vaudenay, S. (ed.) EURO-
CRYPT2006.LNCS,vol.4004,pp.486–503.Springer,Heidelberg(2006).https://
doi.org/10.1007/11761679 29
6. Kingma,D.P.,Ba,J.:Adam:amethodforstochasticoptimization.In:Bengio,Y.,
LeCun,Y.(eds.)3rdInternationalConferenceonLearningRepresentations,ICLR
2015,SanDiego,CA,USA,7–9May,2015,ConferenceTrackProceedings(2015).
http://arxiv.org/abs/1412.6980
7. McMahan, B., Ramage, D., Talwar, K., Zhang, L.: Learning diﬀerentially private
recurrent language models. In: ICLR (2018)
8. Pennington, J., Socher, R., Manning, C.: Glove: global vectors for word represen-
tation. In: Proceedings of EMNLP (2014)
9. Peters, M., et al.: Deep contextualized word representations. In: Proceedings of
NAACL, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational
Linguistics, June 2018
10. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.:Languagemod-
els are unsupervised multitask learners (2018). https://d4mucfpksywv.cloudfront.
net/better-language-models/language-models.pdf
11. Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks
againstmachinelearningmodels.In:2017IEEESymposiumonSP,pp.3–18(2017)
12. Shokri,R.,Shmatikov,V.:Privacy-preservingdeeplearning.In:Proceedingsofthe
22nd ACM Conference on CCS, pp. 1310–1321 (2015)
13. Song, C., Ristenpart, T., Shmatikov, V.: Machine learning models that remember
too much. In: Proceedings of the ACM SIGSAC Conference on CCS, pp. 587–601
(2017)Investigating Memorization in Pre-trained Word Embeddings 281
14. Song,C.,Shmatikov,V.:Auditingdataprovenanceintext-generationmodels.In:
Proceedings of KDD, pp. 196–206. ACM, New York (2019)
15. Vu,X.-S.,Tran,S.N.,Jiang,L.:dpUGC:learndiﬀerentiallyprivaterepresentation
forusergeneratedcontents.In:ProceedingsofCICLing.LaRochelle,France(2019)SpeechInvestigating the Corpus Independence
of the Bag-of-Audio-Words Approach
B
Mercedes Vetr´ab1,2( ) and Ga´bor Gosztolya1,2
1 Institute of Informatics, University of Szeged, A´rpa´d t´er 2, Szeged, Hungary
{vetrabm,ggabor}@inf.u-szeged.hu
2 MTA-SZTE Research Group on Artiﬁcial Intelligence, Tisza Lajos k¨oru´t 103,
Szeged, Hungary
Abstract. In this paper, we analyze the general use of the Bag-of-
Audio-Words(BoAW)featureextractionmethod.Thistechniqueallows
us to handle the problem of varying length recordings. The ﬁrst step of
the BoAW method is to deﬁne cluster centers (called codewords) over
our feature set with an unsupervised training method (such as k-means
clustering or even random sampling). This step is normally performed
on the training set of the actual database, but this approach has its
own drawbacks: we have to create new codewords for each data set and
this increases the computing time and it can lead to over-ﬁtting. Here,
we analyse how much the codebook depends on the given corpus. In
our experiments, we work with three databases: a Hungarian emotion
database, a German emotion database and a general Hungarian speech
database.Weexperimentwithconstructingasetofcodewordsoneachof
thesedatabases,andexaminehowtheclassiﬁcationaccuracyscoresvary
on the Hungarian emotion database. According to our results, the clas-
siﬁcation performance was similar in each case, which suggests that the
Bag-of-Audio-Words codebook is practically corpus-independent. This
corpus-independence allows us to reuse codebooks created on diﬀerent
datasets,whichcanmakeiteasiertousetheBoAWmethodinpractice.
· · ·
Keywords: Emotion detection Bag-of-Audio-words Human voice
Sound processing
1 Introduction
Human speech is not only used for encoding the words uttered, but it also
includes some information about the speakers physical and mental state. One
of the latter attributes is the emotional state of the speaker. Nowadays emotion
detectionfromaudiodata(speechemotionrecognition,SER)isanactiveareaof
research with a wide range of possible applications, including human-computer
interfaces (monitoring human communication) [6], dialog systems [1] and call
centers [12]. In the future with good emotion recognition systems, we will be
able to create more human-oriented and friendlier systems.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.285–293,2020.
https://doi.org/10.1007/978-3-030-58323-1_31286 M. Vetr´ab and G. Gosztolya
Since the beginning of research in this area, many feature extraction and
classiﬁcation techniques have been used along with diﬀerent datasets to get the
best results. The basis of our study is a previous paper [11], where we investi-
gated the Bag-of-Audio-Words (BoAW [7]) technique and its eﬃciency. One of
themajorproblemusingtheBoAWtechniquewasthetimerequiredtogenerate
a codebook, which could be solved if we utilize a predeﬁned codebook instead
of generating a new one for each data set. In this paper, we discuss the conse-
quences of using a predeﬁned codebook. We address the question of whether a
codebook from another database can produce similar or better results than by
using a codebook from the original database. We perform our experiments on a
Hungarian emotion speech database; previous classiﬁcation accuracy scores on
this database were around 66–70%. We measured Unweighted Average Recall
(UAR, [9]) scores in the range 66–71%, so our view is that the BoAW method
with a predeﬁned codebook is a competitive technique for emotion recognition.
2 The Bag-of-Audio-Words Method
With the representation of emotional speech data, there are many open ques-
tions and problems. One of them is feature extraction from recordings. Often
the utterances we have to handle are of diﬀerent lengths, but most classiﬁca-
tion techniques require ﬁxed-sized feature vectors. The Bag-of-Audio-words is a
feature extraction method similar to the Bag-of-Words [7] technique. With the
BoAW feature representation, we can resolve the problem of varying length.
In the BoAW procedure, ﬁrst we have to extract the frame-level feature
vectors per recording; unfortunately, the number of vectors created depends on
the original length of the evaluated recording and the frame’s windowing size.
In the next step, we collect all the feature vectors from all the recordings of
the training set, put them into one big “bag” and perform clustering on it.
Cluster size (N) is one of the parameters of the BoAW method. The result
of the clustering step, the center vector of each calculated cluster, is called a
“codeword”. The group of codewords is then called the “codebook”.
After, inthevectorquantization step, weagain work with individual record-
ings and create a histogram for each recording (both for the training and test
sets). We calculate the closest codeword for each feature vector in the actual
recording and replace the original feature vectors by the index of the closest
codeword. We can also specify how many closest vectors we examine (this is
also a parameter of the BoAW method). As a result, the same sized (i.e. N)
histogram is produced for each recording. All of the codeword indices appear on
the histogram’s x axis. On the y axis, there are quantities which represent the
set of recording feature vectors that were mapped to a particular codeword.
In the last step, we normalize the histogram, so the given frequencies are
divided by the number of frames of the speech recording. These normalized
histograms will be our new feature vectors, that have an independent length
from the recording sizes (i.e. they will consist of N values) We will call this set
of histograms “Bag-of-Audio-Words” and use it as features for our classiﬁer.Investigating the Corpus Independence of the BoAW Approach 287
3 Data and Methods
3.1 Data Sets
In each experiment, we created and evaluated our classiﬁcation model on the
Hungarian emotion database training and test sets. The other two databases
were used to construct the codebook.
HungarianEmotionDatabase. Thisdatabasecontainsspeechfrom97native
Hungarianspeakers[10].Mostofthesegmentswererecordedfromacontinuous,
spontaneousspeakingtelevisionprogramwithactors,whiletheotherpartcame
from an improvisation show. In the ﬁrst case, the samples are vivid, and the
emotionsaremoreclearbecauseoftheactors.Thesamplesfromthesecondcase,
however, are closer to real-life emotions. The database contains 1111 sentences,
separated into an 831 sample training set (cca. 20min long) and a 280 sample
test set (cca. 7min long). We had four emotions: neutral, joy, anger and sad.
GermanEmotionDatabase. Thisdatabase(alsoknownasEmoDB)contains
speech from 10 native German speakers [2]. The recordings were made with
actorsagedbetween25and35.Eachparticipantproduced10Germanutterances
(5 short and 5 longer sentences), all of them with a diﬀerent emotion. The
classiﬁcation labels were: neutral, anger, boredom, disgust, fear, happiness and
sadness. The whole database contains approximately 25min of recordings.
Hungarian Speech Database. This database contains Hungarian television
news recordings taken from 8 diﬀerent TV channels [5]. The whole data set
consistsof28hofrecordings.Intermsofemotiondetection,allofthelabelscan
betreatedasneutralbecausenewsreadersarenotallowed toshowanyemotion.
3.2 Feature Set
Our frame-level feature set is based on the Interspeech ComParE Challenge [9].
This set contains 65 frame-level features (4 energy-related, 55 spectral and 6
voicingrelated).Weusedtheopen-sourceopenSMILEfeatureextractor[4]with
the IS13 ComParE conﬁg ﬁle. For each frame we calculated the derivatives (i.e.
Δ values) as well; these hold information about the dynamics of the samples.
3.3 Evaluation
ClassiﬁcationisperformedbytheLIBSVMlibrary[3].WeoptimizedtheSVMC
complexityparameterintherange10−5,10−4to100.Weappliedstandardization
ontheBoAWfeaturevectorsbeforeeachmodelwastrained.Intheoptimization
part of our experiments, we worked with the training set, based on speaker-
independent 10-fold cross-validation. In the test scenario, we trained one SVM
model on the whole training set with the optimal C parameter found above and
evaluated it on the test set.288 M. Vetr´ab and G. Gosztolya
Table 1. Baseline: best results got with normalization and standardization, when we
evaluate our technique with cross-validation and do it on the test set.
Feature-transformation UAR Codebook size
a CV Test
Normalization 5 58.08% 48.13% 512
10 57.48% 50.27% 512
Standardization 5 55.43% 53.54% 512
10 56.57% 64.32% 256
3.4 Parameters of the BoAW Method
TheBoAWmethodhasmanyadjustableparameters.Inourstudy,wetestedthe
eﬀect of the preprocessing method, the codebook size N, and the quantization
neighbour number parameters on the learning algorithm performance. For the
codebook building we used an open-source program called openXBOW [8].
Codebook size: In each experiment we tested the eﬀect of the following
lengths: 32, 64, 128, 256, 512, 1024, 2048.
Histogram neighbour number: Instead of looking for just the closest code-
word, each vector may also be assigned to a certain number of the closest code-
words. Previously [11] we found that using more neighbours leads to a more
precise description of the recordings besides the same feature vector size. This
is why we experimented with two diﬀerent settings (5 and 10).
Preprocessing techniques: If some of the features have an extremely high or
lowvaluecomparedtotheothers,itmaydominatetheEuclideandistanceduring
theBoAWvectorquantizationstep.Previously[11]wefoundthatpreprocessing
the frame-level vectors by standardizing or normalizing them can improve the
performance, so we tried both solutions.
Derivatives: In a previous study [11] we found that using the derivatives of
the frame-level attributes can improve the performance, so we also used them
in our experiments. The openXBOW tool also gives the opportunity to create
separate codebooks for the original frame-level values and another for the Δs;
because we opted for this technique, the codebook sizes provided have to be
multiplied by 2 to get the actual number of features.
4 Tests and Results
As the baseline, we create the codebook from the Hungarian emotion database
trainingset.OurresultsareshownonTable1.Thebestresultofcross-validation
(i.e. 58.08%) came with normalization, 5 neighbours, and a 512-sized codebook.
Thebestresultofthetest(i.e.64.32%)camewithstandardization,10neighbours
andN =512.Inaddition,itisclearthatin3outof4casestheresultsobtained
on the test database were lower than the results of cross-validation, which may
be due to overﬁtting to the training set during codebook creation.Investigating the Corpus Independence of the BoAW Approach 289
Fig.1.ResultsoftheBaseline andEmoDB generatedcodebookswithcross-validation
and evaluation on the test set.
Table 2. EMODB: best results with normalization and standardization, when we
evaluate our technique with cross-validation and do it on the test set.
Feature-transformation UAR Codebook size
a CV Test
Normalization 5 59.52% 70.07% 1024
10 60.13% 62.70% 256
Standardization 5 57.34% 66.59% 128
10 58.81% 70.70% 256
4.1 Codebook from the EmoDB Database
Next,wewantedtoknowwhetherworkingwithacodebookfromotherdatabases
couldproducesimilarorbetterresultsthanacodebookcreatedfromtheoriginal
database. In this part, the codebooks were created from EmoDB; then we built
the BoAW representation for the Hungarian emotion database and performed
classiﬁcation using these features.
Examining the results (see Fig.1 and Table2) we can see that there was a
signiﬁcant improvement over the baseline in all four test cases. In 2 cases out of
4, we also see a reduction in the size of the required codebook, which can also
reduce the time needed to produce a BoAW representation.
ThisimprovementandthefactthatallEmoDB testcaseshavemoreaccurate
scoresthanalltheEmoDB cross-validationscores,inouropinion,mightindicate
that a codebook made from the original database tends to lead to overﬁtting,290 M. Vetr´ab and G. Gosztolya
Fig.2. Cross-validation results got from the Baseline and from the News database
and a predeﬁned codebook (which is generated independently from the actual
training samples) can eliminate this problem.
4.2 Codebook from the News Database
Based on the previous tests, it is apparent that a codebook created from a dif-
ferent database led to signiﬁcant improvements. On the other hand, it is still
not clear whether the type of speech (e.g. rich of emotions or completely neu-
tral) present in the database used for codebook creation aﬀects the emotion
classiﬁcation performance. To examine this, next the codebooks were prepared
from subsets of the (non-emotion) Hungarian television recordings database [5].
Otherwise, all classiﬁcation steps were done similarly as before. To investigate
whetherthelengthofthedatabasealsoaﬀectsthisperformance,weusedan1-h,
2-h, 5-h, and 10-h long subset for codebook creation.
Basedontheresultsofthecross-validation(seeFig.2),wecouldnotcorrelate
the length of the database with the success of the classiﬁcation. The same can
besaidaboutthetypeofpreprocessingmethodandthenumberofclosestneigh-
bors: all the scores ranged from 55.75% to 60.74%. Most of the best-performing
codebook sizes were 1024 and 2048, which are relatively large feature sets. The
bestresultofcross-validationcamefroma1-hlengthdatabase,withstandardiza-
tion, taking 5 neighbors, using 1024 sized codebook, giving the score of 60.74%.
The results did not reveal signiﬁcant diﬀerences depending on the length of
the database, hence no general relationship could be found. In addition, we did
not get signiﬁcantly better or worse scores than using a codebook speciﬁcally
designed for emotion detection from a EmoDB database.Investigating the Corpus Independence of the BoAW Approach 291
Fig.3. Test result got from the Baseline and from the Hungarian speech database
Thebestscoreofthetestwas71.86%,withthe10-hdataset,standardization,
with 10 neighbours and a 1024-sized codebook. However, besides the required
increaseinthecodebooksize,noobviousinferencescouldbemadehere(Fig.3).
5 Conclusions
In this paper, the BoAW (Bag-of-Audio-Words) feature representation method
was simultaneously applied on multiple databases for emotion recognition. We
were interested in the possibility of creating BoAW codebooks from other
datasets; this would allow the re-using of codebooks for several corpora, there-
fore allowing to cut execution times signiﬁcantly. From this viewpoint, building
acodebookfromother,similarpurposedatabases givesbetterscoresthanthose
got using purpose-built database codebooks.
Based on our tests, it can be clearly stated that each predeﬁned codebook
can be successfully used to extract BoAW feature representations of any other
databases.ThebestscoreofthetestswiththeHungarianemotiondatabaseown
codebook was 64.32%. Compared to this, when we used other database code-
bookswegotbetterresults.ThebestscoreofthetestswiththeHungarianspeech
databasecodebookwas66–71.86%.ThebestscoreofthetestswiththeGerman
emotion database codebook was 66–70.70%. With these results, we could not
ﬁndaclearanswertowhetheritisadvisabletouseacodebookbetweenanytwo
databases created for similar purposes but a diﬀerent language or for a similar
language but diﬀerent purpose. In both cases, our results varied on a similar292 M. Vetr´ab and G. Gosztolya
scale, with no signiﬁcant diﬀerence. They just diﬀered in codebook size, so this
point requires deeper study.
Now we know that codebooks are portable, but there are several directions
wecanpursueinthefuture.Onegoodquestioniswhattypeofdatabasescanbe
most eﬀectively transferred from the viewpoint of codebook reusability. Is there
a close connection between certain types of databases? We could also test other
frame-level feature sets to see whether there are any beneﬁts in practice.
Acknowledgements. Thisstudywassupportedbytheproject“Integratedprogram
for training a new generation of scientists in the ﬁeld of computer science”, grant
no.EFOP-3.6.3-VEKOP-16-2017-0002.ThisstudywasalsosupportedbytheNational
Research, Development and Innovation Oﬃce of Hungary via contract NKFIH FK-
124413. This research was also supported by the grant TUDFO/47138-1/2019-ITM
of the Hungarian Ministry for Innovation and Technology. G´abor Gosztolya was also
funded by the J´anos Bolyai Scholarship of the Hungarian Academy of Sciences and
by the Hungarian Ministry of Innovation and Technology New National Excellence
Program U´NKP-19-4-SZTE-51.
References
1. Burkhardt, F., van Ballegooy, M., Engelbrecht, K.P., Polzehl, T., Stegmann, J.:
Emotion detection in dialog systems: applications, strategies and challenges. In:
Proceedings of ACII, Amsterdam, Netherlands, pp. 985–989 (2009)
2. Burkhardt,F.,Paeschke,A.,Rolfes,M.,Sendlmeier,W.,Weiss,B.:Adatabaseof
German emotional speech. In: Proceedings of Interspeech, pp. 1517–1520 (2005)
3. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines. ACM
Trans. Intell. Syst. Technol. 2, 1–27 (2011)
4. Eyben, F., W¨ollmer, M., Schuller, B.: Opensmile: the Munich versatile and fast
open-source audio feature extractor. In: Proceedings of ACM Multimedia, New
York, NY, USA, pp. 1459–1462 (2010)
5. T´oth, L., Gr´osz, T.: A comparison of deep neural network training methods for
large vocabulary speech recognition. In: Habernal, I., Matouˇsek, V. (eds.) TSD
2013. LNCS (LNAI), vol. 8082, pp. 36–43. Springer, Heidelberg (2013). https://
doi.org/10.1007/978-3-642-40585-3 6
6. James, J., Tian, L., Inez Watson, C.: An open source emotional speech corpusfor
human robot interaction applications. In: Proceedings of Interspeech, Hyderabad,
India, pp. 2768–2772 (2018)
7. Pancoast, S., Akbacak, M.: Bag-of-Audio-Words approach for multimedia event
classiﬁcation.In:ProceedingsofInterspeech,Portland,USA,pp.2105–2108(2012)
8. Schmitt,M.,Schuller,B.:openXBOW-IntroducingthePassauopen-sourcecross-
modalBag-of-Wordstoolkit.J.Mach.Learn.Res.18(96),1–5(2017).http://jmlr.
org/papers/v18/17-113.html
9. Schuller, B., et al.: The Interspeech 2013 computational paralinguistics challenge:
socialsignals,conﬂict,emotion,autism.In:ProceedingsofInterspeech,pp.148–152
(2013)Investigating the Corpus Independence of the BoAW Approach 293
10. Sztaho´,D.,Imre,V.,Vicsi,K.:Automaticclassiﬁcationofemotionsinspontaneous
speech.In:Esposito,A.,Vinciarelli,A.,Vicsi,K.,Pelachaud,C.,Nijholt,A.(eds.)
AnalysisofVerbalandNonverbalCommunicationandEnactment.TheProcessing
Issues.LNCS,vol.6800,pp.229–239.Springer,Heidelberg(2011).https://doi.org/
10.1007/978-3-642-25775-9 23
11. Vetra´b, M., Gosztolya, G.: ´erzelmek felismer´ese magyar nyelvu˝ hangfelv´etelekb˝ol
akusztikus sz´ozs´ak jellemz˝oreprezent´aci´o alkalmaz´as´aval. In: Proceedings of
MSZNY, Szeged, Hungary, pp. 265–274 (2019)
12. Vidrascu, L., Devillers, L.: Detection of real-life emotions in call centers. In: Pro-
ceedings of Interspeech, Lisbon, Portugal, pp. 1841–1844 (2005)Developing Resources for Te Reo Ma¯ori
Text To Speech Synthesis System
B
Jesin James1( ), Isabella Shields (Ng¯ati Porou)1,
Rebekah Berriman (Ng¯ai Tahu)1,
PeterJ.Keegan(Waikato-Maniapoto, Nga¯tiPorou)2,andCatherineI.Watson1
1 Department of Electrical, Computer, and Software Engineering,
University of Auckland, Auckland, New Zealand
{jesin.james,c.watson}@auckland.ac.nz
{ishi836,rber798}@aucklanduni.ac.nz
2 Te Puna W¯ananga, University of Auckland, Auckland, New Zealand
p.keegan@auckland.ac.nz
Abstract. Te reo Ma¯ori (the Ma¯ori language of New Zealand) is an
under-resourced language in terms of availability of speech corpora and
resourcesneededtodeveloprobustspeechtechnology.Ma¯oriisanendan-
geredindigenouslanguagewhichhasbeensubjecttorevitalisationeﬀorts
since the late 1970s, which are well known internationally. The M¯aori
community recognises the need for developing speech technology tools
for the language, which will improve its study and usage in wider and
more digital contexts. This paper describes the development of speech
resources in M¯aori to build one of the ﬁrst Text To Speech synthesis
system for the language. A speech corpus, extended dictionary and a
parametric speech synthesiser are the main contributions of the study.
Todeveloptheseresources,textprocessing,segmentationandalignment,
lettertosoundrulescreationwerealsodonewithexistingresourcesthat
weremodiﬁedtobeusedforMa¯ori.Theacousticsimilarityofsynthesised
speech vs natural speech was measured to evaluate the speech synthesis
system statistically. Future work required is described.
· ·
Keywords: Under-resourced language Tts system Te reo Ma¯ori
1 Introduction
Speech technology is a dominant research ﬁeld and technology that talks is
becoming the new norm. However, a majority of these studies are happening
only in a few privileged languages (1.4% of world languages) [10]. Languages
thatdonothavethetechnicalsupporttodevelopspeechprocessingtechnologies
aretermedasunder-resourcedlanguages.Tereo(meanslanguage)M¯aori1 isthe
indigenous language of New Zealand and has oﬃcial language status. However,
1 WethankTeHikuMediafortheirgeneroussupportinfundingthisproject(https://
tehiku.nz/). We thank the MAONZE research group [1] for their encouragement.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.294–302,2020.
https://doi.org/10.1007/978-3-030-58323-1_32Developing Resources for Te Reo Ma¯ori Text to Speech Synthesis System 295
New Zealand English is the dominant language in the country. The 2018 New
Zealand census reports2 that there are 185955 M¯aori speakers, which accounts
for only 4% of the total population (4.58 million). There is a lack of speech
and language resources in M¯aori, making it under-resourced. In 1998, Laws [12]
developed a diphone-based synthetic M¯aori voice. A Festival-based TTS system
wasplanned[13],butitsdevelopmentwasstoppedin2003.Thelexiconresources
developed by Laws are sadly lost, although a copy of the recorded diphones
remains. Since Laws’ pioneering work, a lot more tools have become available
to developing speech and language resources. But, no more work was done on
M¯aori. In this paper, we present the development of speech resources for M¯aori
(preliminary work reported in [18]), with the larger aim of developing a Text To
Speech synthesis (TTS) system.
2 Motivation : Te Reo M¯aori Revitalisation
M¯aorilanguagespokeninNewZealandderivesfromthelanguages/dialectsspo-
ken by arrivals from Eastern Polynesian region of the South Paciﬁc, 800years
ago. It was heavily inﬂuenced by English speakers who began arriving in the
early 1800s. Since the early 1900s, most M¯aori were taught in English and were
discouraged from speaking M¯aori in schools. Between 1950–1980 there was a
sharp decline in the number of ﬂuent M¯aori speakers [9,20]. From the 1980s
there have been strong M¯aori revitalisation eﬀorts (known internationally [2])
involving both M¯aori community initiatives, support from New Zealand Gov-
ernment in education, the media, oﬃcial contexts and other organisations. Te
Hiku Media is an example, who in addition to providing media services to the
local M¯aori, are involved in developing digital tools (like speech tools) for the
wider community. The M¯aori community is aware of sound change over time. It
is regarded as a result of the break in intergenerational transmission; thus, this
soundchangehasbeenregretted[20].TheMAONZEresearchgroup[1]hasbeen
doing extensive research into sound change in M¯aori [14,21]. A Ma¯ori pronun-
ciation aid (MPAi) is being developed [20], and research is focused on the ways
to provide feedback to people from the aid. In this context, a module that can
produce synthesised M¯aori speech can provide speech feedback to MPAi users.
Also,aTTSsystemcanbeusedbypeoplenotsoproﬁcientinM¯aoritolistento
how words/sentences are spoken. This has potential applications like e-readers,
human-computerinteractionsystemswherethetechnologyusedinNewZealand
currently is English-based. The new generation of M¯aori users are all exposed
to the latest technology. If M¯aori-based interfaces are available to them, te reo
M¯aori use can be boosted. Given all these applications, and the larger aim to
revitalise the language, speech resource development in M¯aori is essential.
M¯aori Phonology: (Details in [11,14]) Ma¯ori, as with other Polynesian
languages did not have an indigenous writing system. The Roman script was
used to write M¯aori since the early 1800s. Ma¯ori uses macrons to diﬀerentiate
2 https://www.stats.govt.nz/information-releases/2018-census-totals-by-topic-
national-highlights-updated.296 J. James et al.
between long and short vowels. The language has ten consonants <p, t, k, m,
n, N, f, w, R, h> and ﬁve short vowels <i, e, a, o, u>. Vowel length is phonemic,
i.e., there are ﬁve contrasting long vowels <¯ı, ¯e, ¯a, ¯o, u¯ >. Research points out
that the timing unit in M¯aori as the mora, a unit consisting of a short vowel
plus any preceding consonant. There are no consonant clusters.
3 Te Reo M¯aori Resources Development
As M¯aori is under-resourced, many resources needed for the TTS system had to
bedeveloped.KnowledgeofM¯aorilanguageisessentialtobuildtheseresources.
M¯aori Speech Corpus: To build a parametric speech model for M¯aori,
an appropriate speech corpus was developed. The transcript for the corpus was
sourced from a collection of M¯aori myths and legends called Ng¯a Mahi a Nga¯
tu¯puna [7]. These source ﬁles were processed, and text cues for recording were
produced. Lines were split according to the presence of [ . ? ! ; : ]. The source
ﬁlesuseoldM¯aorialphabet,wherelongvowelswererepresentedwithoutmacrons
(e.g.oldalphabet‘aa’,newalphabet‘¯a’).ConversiontothecurrentM¯aorialpha-
betwasmadebyreplacinglongvowels.Someoccurrencesofdoublevowelsshould
notbereplacedwithmacrosasthesequenceoccursacrossamorphemeboundary
(e.g. whakaaro) and they were hand-corrected. A basic phonetic transcription
was created via Python-based coding. This produced a new ﬁle which replaced
macronsymbols(e.g.‘a¯’)withasymbolandacolontoindicatelength(e.g.‘a:’),
and‘r’with‘R’,anddigrams‘ng’and’wh’with‘ŋ’and‘f’,respectively.Eachline
of the transcript was split into separate prompt ﬁles for recording.
Lexicon: An existing dictionary from the MAONZE project was used as
the starting point. All words which appeared in the transcript but not in the
originaldictionarywereaddedusingPythonscriptsthatperformedcomparison.
The created lexicon contains over 10000 words and 18000 names, along with a
phonetic transcription of words, syllable boundaries and stress mark up. The
latter two are determined using Bigg’s stress rules [3] and the division of words
into morae. The automated rules were checked on 959 hand-transcribed rules,
and the accuracy was 95%.
Corpus Recording: Recordings took place in a WhisperRoom Sound Iso-
lation chamber. The speaker was recorded using a Rode Lavalier Lapel micro-
phonekept15cmfromtheirmouth.ThemicrophonewasconnectedtoaRoland
OCTA-CAPTURE. Audacity(cid:2)R3 was used for audio capture at 44.1 kHz sam-
pling frequency. A computer monitor displaying the prompts was set up on a
deskinsidetherecordingenclosure.Thepre-ampliﬁergainwassetto37.5dB.A
middle-agedmaleM¯aorispeakerwasusedtorecordthecorpus.Thespeakerwas
giventimetoreadovereachphraseandfamiliarisewithitbeforerecording.Long
sentences were read as phrases by speaker, using his own judgement to deter-
mine ‘natural’ phrase boundaries within sentences. The audio playthrough was
enabled so that audio quality and correctness of the utterance could be assessed
during recording. While the recording was trimmed and saved by the recorder,
3 Audacity(cid:2)R software is copyright (cid:2)c1999–2019 Audacity Team.Developing Resources for Te Reo Ma¯ori Text to Speech Synthesis System 297
thespeakerreadthefollowingsentencetoberecorded.Eachrecordingwassaved
instereoWAVformat.Recordingsessionswerelimitedtoapproximately200sen-
tences to ensure speaker comfort. Each session took about 2–2.5h, accounting
for a break in between the session. In total, 1030 sentences were recorded. The
ﬁrst800ofthesecorrespondtotheﬁrst800linesofthetranscript.Theﬁnal230
were selected from the remaining transcript after a basic analysis of phone and
diphone coverage.
Diphone Coverage: A basic analysis of diphone coverage was undertaken
aftertheﬁrst800sentenceswererecorded.30occurrenceswerearbitrarilychosen
asthegoalforeachinsuﬃcientlycovereddiphone.Sentencesfromtheremaining
unrecorded transcript that had instances of these were selected. Comparison
of the frequency of occurrence of long and short vowels largely reﬂects rates
reported in [16]. Figure1 gives the diphone coverage of the corpus developed.
Diphone pairs such as /wu/ and /wo/ only appear in loanwords. Diphones /fu/
and /fo/, although not well covered in the corpus, are very rare. There are few
instances where the corpus insuﬃciently covers a diphone pair, such as ‘nge’
(/Ne/).
Alignment: Montreal Forced Aligner (MFA Version 1.0.0) [15] was used to
align text with the recordings. Recordings in WAV format, phonetic transcrip-
tions in TextGrid format and dictionary are needed for the alignment. Manual
checking of alignment and hand corrections were done. For converting TextGrid
to.labformat(neededforMaryTTSvoicecreation),thephonetierisextracted,
and each symbol and its corresponding end time are stored in the .lab format.
Fig.1. Occurrences of diphones in the Ma¯ori corpus298 J. James et al.
4 Te Reo M¯aori TTS System Development and Analysis
This project used MaryTTS [17] (Java-based) to generate a Hidden Markov
Model (HMM) - based (parametric speech synthesis) M¯aori voice. A deep-
learning approach was not taken as a suﬃciently large corpus is unavailable. An
open-source speech synthesiser with good speech quality and support for new
languages was needed for this project; which lead to the choice of MaryTTS.
Adding a New Language to MaryTTS: MaryTTS New Language Sup-
portwasusedtodeveloptheM¯aoriTTSsystem.ThelocalenameusedforM¯aori
was ‘mi’ following Windows locale codes. Requirements for new language addi-
tion are the allophone list, lexicon, letter to sound rules and language corpus.
The lexicon and language corpus were built, as described previously. An allo-
phone list was added specifying the features: vowel length, vowel height, vowel
frontness,liprounding,consonanttype,placeofarticulation,consonantvoicing.
39allophones(28forvowels,10forconsonants,oneforsilence)areidentiﬁedfor
M¯aori.
Creation of Letter to Sound Rules: MaryTTS transcription tool was
used to create letter to sound (LTS) rules using the dictionary and allophone
set. First, a manual speciﬁcation of all letters and corresponding phones that
can be used to render them are created. The complete lexicon that is used for
trainingisthenalignedtothecorrespondingphonesbasedonthemappingtable.
Then a classiﬁcation and regression tree was built using the training data for
LTS rules.
Voice Building: The voice building process in MaryTTS was followed
with the language resources. The linguistic features extracted are ToBI accents;
quinphones for each phone; part-of-speech of each word and features of each
phone. This is Text analysis and Linguistic analysis. HMM-based voice build-
ing was done based on these features. The pitch range was set to 50–300 Hz
(for male speaker). Mel-generalised cepstrum coeﬃcients, log of fundamental
frequency and strength were the speech features modelled. The ﬁnal step builds
thelanguage-basedspeechmodel.ThismodelwillbeusedforWaveform Gener-
ation.Anend-to-endM¯aoriTTSsystemwassetupasinFig.2,withM¯aoritext
input, and the output is synthesised speech. Native M¯aori speakers listened the
synthesisedspeechproduced,andtheycommentedthatthepronunciationswere
in alignment with those expected from M¯aori speakers. A client-server model-
based speech synthesiser for M¯aori was set up in the University of Auckland
robotspeech server and is available for the various research activities at the uni-
versity.WeareworkingonmakingtheonlineTTSsystemaccessibletothewider
public. The lexicon and speech corpora will not be made available publicly, as
we are guided by Te Hiku media’s data sovereignty stance4.
Signal-Based Quality Diagnosis: To evaluate the synthesised speech, a
simpliﬁed version of the signal-based quality diagnosis (mel-cepstral distortion -
MCD) described in [5] was implemented. MCD measures the diﬀerence between
4 https://tehiku.nz/te-hiku-radio/te-putahi/12707/dr-tahu-kukutai-keoni-mahelona-
data-sovereignty.Developing Resources for Te Reo Ma¯ori Text to Speech Synthesis System 299
Fig.2. The end-to-end Ma¯ori Text To Speech synthesis system
twosequencesofmelcepstraasshowninFig.3.Segmentationisdoneattheword
level(usingMFA),fortheoriginalandsynthesisedspeech.MFCCsareextracted
using [4]. Diﬀerence in timing of the two sequences is aligned by Dynamic Time
Warping (DTW) (based on [6]). Consider the synthesised speech and reference
original speech MFCCs as a time series X =(x1,x2...xN) and Y =(y1,y2...yM)
respectively. A correspondence between their elements is established by a warp-
ing curve Φ = (φt,ψt);t = 1,...T. (T depends on the lengths M and N).
The optimal warping curve is the one that minimises the distance between the
(cid:2)
two time series, represented by: Φˆ = (φˆt,ψˆt) = a(rφgtm,ψta)x = Tt=1 d(xφt,MyψΦt)mt,Φ
[19(cid:2)]. Here mt,φ: local weighting coeﬃcient, Mφ: path-dependent normalisation
= Tt=1mt,Φ, d(cid:2): local distance. The minimum cumulative distance is obtained
as:D(X,Y)= T d(xφˆt,yψˆt)mt,Φˆ.Thedistancemeasureobtainedistheaverage
t=1 M
Φˆ
per-step distance along the warping curve.
Fig.3. Acoustic similarity measure for synthesised speech
432 words from 30 sentences in the M¯aori corpus were tested. Example of
timeseriesalignmentobtainedafterwarpingisshowninFig.4.Thealignmentis
perfect when it is between the same words (a), and the alignment is poor when
theoriginalandsynthesisedversionsarefromdiﬀerentwords(c).Thealignment
is good for the original and synthesised version of the same word (b), which
is an indication of the performance of the TTS system. Normalised distance300 J. James et al.
Fig.4. Time series alignment using DTW of MFCCs. (a) Same words (b) natural vs
synthesised version of same word, (c) natural vs synthesised version of two words [6].
Fig.5.(a)ConfusionmatrixofdistancebetweenoriginalandsynthetisedMFCCs.(b)
The probability distribution of distance between original and synthesised MFCCs.
measure between the original and synthesised MFCC time series is then taken
as described in Fig.3 [8]. Figure5 (a) shows the confusion matrix of distance
measures for 6 example words. It can be seen that the distance is comparatively
lowerwhentheoriginalandsynthesisedMFCCsareofthesameword (seeFig.4).
Figure5 (b) shows the probability distribution of the distance measures for:
Comparison A: BetweenMFCCsoforiginalandsynthesisedversionsofdiﬀerent
words and Comparison B: Between MFCCs of original and synthesised versions
of same words for all words tested. It is clear that Comparison A results in
largerdistancecomparedtoComparison B.Thisstatisticallyshowstheacoustic
similarity between the words in the original and synthesised speech signals.
5 Conclusion and Future Work
This paper describes the development of te reo M¯aori TTS system. M¯aori is
under-resourced, and development of speech technology resources is critical for
itsrevitalisation.Aspeechcorpuswasdevelopedcontaining1030sentences.The
M¯aori lexicon was built with 10000 words and 18000 names. Existing tools (like
Montreal Forced Aligner) were customised to segment and align the text andDeveloping Resources for Te Reo Ma¯ori Text to Speech Synthesis System 301
speech signals in the corpus. These resources were then used to develop an
end-to-end M¯aori TTS system, where M¯aori text is entered, and the output
is synthesised speech. Acoustic similarity analysis of the synthesised speech was
done. Perceptual testing of the synthesised speech will be conducted. Future
work will focus on expanding the M¯aori lexicon and checking entries, especially
where orthography does not align with the current M¯aori phonetics. There is
also a need to incorporate modern M¯aori pronunciations (like the occurrence of
aﬀrication and semi-vowels not present in traditional M¯aori) into the lexicon.
Also, the eﬀect of phraseology on the assignment of stress/syllable structure
needs to be added to the linguistic analysis. In the real world, code-switching is
common in spoken M¯aori; therefore, any useful M¯aori TTS system will need to
accommodate New Zealand English. We hope to implement this by 2021 [18].
References
1. http://homepages.engineering.auckland.ac.nz/~cwat057/MAONZE/index.html2.
2. http://www.maoriMay12020language.info/mao_lang_abib.html
3. Biggs, B.: Let’s Learn M¯aori: A Guide to the Study of the Ma¯ori Language. A.H.
& A.W. Reed, Wellington (1969)
4. Ellis, D.P.W.: PLP and RASTA (and MFCC), and inversion) in Matlab. Online
web resource (2005). www.ee.columbia.edu/dpwe/resources/matlab/rastamat
5. Falk, T., Moller, S.: Towards signal-based instrumental quality diagnosis for text-
to-speech systems, vol. 15, pp. 781–784. IEEE (2008)
6. Giorgino,T.:Computing&visualizingdynamictimewarpingalignmentsinR:the
dtw package. J. Stat. Softw. 31(7), 1–24 (2009). Open Access Statistics
7. Grey, S.G.: Ng¯a Mahi a Nga¯ tu¯puna. New Plymouth, 3rd edition (1928)
8. Hall,K.C.,etal.:Phonologicalcorpustools,version1.3.[computerprogram](2017)
9. Harlow, R.: M¯aori: A Linguistic Introduction. Cambridge University Press, Cam-
bridge (2007)
10. James,J.,Watson,C.I.,Gopinath,D.P.:Exploringtexttospeechsynthesisinnon-
standard languages. In: International Conference on Speech Science Technology,
Australia, pp. 213–216 (2016)
11. Keegan,P.,Watson,C.,Maclagan,M.,King,J.,Harlow,R.:Theroleoftechnology
inmeasuringchangesinthepronunciationofMa¯oriovergenerations.In:Language
Endangermentinthe21stCentury:Globalisation,Technology&NewMedia,NZ,
pp. 65–71 (2012)
12. Laws,M.R.:AbilingualspeechinterfaceforNewZealandEnglishtoM¯aori.Ph.D.
thesis, Doctoral dissertation, University of Otago, New Zealand (1998)
13. Laws,M.R.:SpeechdataanalysisfordiphoneconstructionofaMa¯orionlineText-
to-speech Synthesizer. In: IASTED International Conference, USA, pp. 103–108
(2003)
14. Maclagan, M., Harlow, R., King, J., Keegan, P., Watson, C.: Acoustic analysis of
Ma¯ori:historicaldata.In:Conference:AustralianLinguisticSociety,p.104(2005)
15. McAuliﬀe, M., Socolof, M., Mihuc, S., Wagner, M., Sonderegger, M.: Montreal
forced aligner: trainable text-speech alignment using Kaldi. In: Interspeech, USA,
pp. 498–502 (2017)
16. Rácz,P.,Hay,J.,Needle,J.,King,J.,Pierrehumbert,J.B.:GradientM¯aoriphono-
tactics. In: Te Reo, vol. 59, pp. 3–21 (2016)302 J. James et al.
17. Schroder, M., Trouvain, J.: The German text-to-speech synthesis system MARY:
a tool for research, development and teaching. Int. J. of Speech Tech. 6, 365–77
(2003). https://doi.org/10.1023/A:1025708916924
18. Shields, I., Watson, C., Keegan, P., Berriman, R., James, J.: Creating a synthetic
te reo mo¯ri voice. In: International Conference on Language Technology for All,
Paris (2019). https://lt4all.org/media/papers/P1/136.pdf
19. Tormene, P., Giorgino, T., Quaglini, S., Stefanelli, M.: Matching incomplete time
serieswithdynamictimewarping:analgorithmandanapplicationtopost-stroke
rehabilitation. In: Artiﬁcial Intelligence in Medicine, vol. 45, pp. 11–34. Elseivier
(2008)
20. Watson, C., Keegan, P., Maclagan, M., Harlow, R., King, J.: The motivation
and development of MPAi, a M¯aori pronunciation aid. In: Annual Conference of
the International Speech Communication Association, Stockholm, pp. 2063–2067
(2017)
21. Watson, C., Maclagan, M., King, J., Harlow, R., Keegan, P.: Sound change in
Ma¯ori and the inﬂuence of New Zealand English. J. Int. Phonetic Assoc. 46(2),
185–218 (2016). Cambridge University PressAcoustic Characteristics of VOT
in Plosive Consonants Produced
by Parkinson’s Patients
B
Patricia Argu¨ello-V´elez1,5 , Tomas Arias-Vergara2,3,4( ) , Mar´ıa Claudia
Gonza´lez-R´ativa1 , Juan Rafael Orozco-Arroyave2,3 , Elmar No¨th3 ,
and Maria Elke Schuster4
1 Faculty of Communications, Universidad de Antioquia UdeA, Calle 70 N,
52-21 Medell´ın, Colombia
2 Faculty of Engineering, Universidad de Antioquia UdeA, Calle 70 N,
52-21 Medell´ın, Colombia
tomas.arias@udea.edu.co
3 Pattern Recognition Lab., Friedrich-Alexander University,
Erlangen-Nu¨rnberg, Germany
4 Department of Otorhinolaryngology, Head and Neck Surgery,
Ludwig-Maximilians University, Munich, Germany
5 Facultad de Salud, Universidad Santiago de Cali, Cali, Colombia
Abstract. VoiceOnsetTime(VOT)hasbeenusedasanacousticmea-
sure for a better understanding of the impact of diﬀerent motor speech
disorders in speech production. The purpose of our paper is to present
a methodology for the manual measuring of VOT in voiceless plosive
soundsandtoanalyzeitssuitabilitytodetectspeciﬁcarticulationprob-
lems in Parkinson’s disease (PD) patients. The experiments are per-
formedwithrecordingsofthediadochokinetic evaluation whichconsists
in the rapid repetition of the syllables /pa-ta-ka/. A total of 50 PD
patients and 50 healthy speakers (HC) participated in this study. Man-
ual measurements include VOT values and also duration of the closure
phase,durationoftheconsonant,andthemaximumspectralenergydur-
ingtheburstphase.Resultsindicatethatthemethodologyisconsistent
andallowstheautomaticclassiﬁcationbetweenPDpatientsandhealthy
speakers with accuracies of up to 77%.
· · ·
Keywords: Voice onset time Acoustic analysis Speech processing
Diadochokinesis
1 Introduction
Parkinson’s disease (PD) is a neurodegenerative disorder characterized by the
progressivelossofneuronsinthemid-brain[6].Primarymotorsymptomsinclude
Supported by Antioquia University.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.303–311,2020.
https://doi.org/10.1007/978-3-030-58323-1_33304 P. Argu¨ello-V´elez et al.
tremor,rigidity,freezingofgait,andposturalinstability.PDalsoaﬀectsmuscles
involved in the speech production process, resulting in hypokinectic dysarthria,
whichisasetofmotorspeechdisordersincludingbradylalia,lackofarticulation
accuracy, and dysphonia [4]. Many of the symptoms are controlled with medi-
cation, however there is no clear evidence indicating the positive eﬀects of those
treatments to reduce motor speech disorders. Proper speech therapy combined
with the pharmacological treatment improve the communication ability of PD
patients [16]. Thus, it makes sense to develop methodologies based on acoustic
analysistoevaluatespeechimpairmentsinPDpatients.Theresourcesofinstru-
mentalphoneticsallowacousticanalysisofsegmentalandsupra-segmentalchar-
acteristics ranging from isolated sounds to spontaneous speech analysis. These
tools allow a linguistic and physiological understanding of atypical phenomena
inspeechandtheirrelationshipwiththepresenceofsymptomsrelatedwithsub-
glottic, glottic or supra-glottic nature. In the case of PD, the diadochokinetic
exercises (DDK) are used to study the production of voiceless plosive conso-
nants (VPC) and vocal segments, which allows the analysis of coordination in
supra-glottic and glottic components [2].
The pronunciation of the VPCs /p/,/t/, and /k/ involves the production
of VOT which is deﬁned as the time between the burst and the beginning of
the emission of the next vowel [9]. In hypokinetic dysarthria VOT is useful to
assess speech impairments by identifying the increase or decrease in duration.
Thesepatternsmayberelatedtoglottaladduction,degreeofvocalcordtension,
and quality of intra-oral pressures [5]. Preliminary results describe a signiﬁcant
VOT reduction in some VPCs, which can be explained by the loss of neuro-
muscular control during speech production. There is also another hypothesis
where increase in VOT is related with loss of laryngeal and supra-laryngeal
coordination [10].
VOT is measured by manual syllable-by-syllable labeling. The wide-band
spectrogramshowsthefrequencyanditsrelationwiththedurationofthesignal.
Abruptchangesassociatedtophysiologicalphenomenacanalsobecaptured.The
burst is visually identiﬁed as a short explosion bar with energy distributed over
the entire frequency spectrum. The onset of the vowel is identiﬁed as the high
energy values with the corresponding formant structure in the spectrogram [8].
Manual labeling requires to consider the relationship among these results and
the signal represented in the oscillogram to have a better view of the signal’s
disturbances including abnormal changes in the consonant to vowel segment [1].
There are other measurement parameters to deﬁne acoustic integrity of VPCs
such as the duration of the closure and the point of maximum spectral energy
in the burst. These characteristics are considered in this paper through manual
labeling and represent physiological correlations with supra-glottic pressure and
articulatory accuracy of the sound.
From the automation/engineering point of view, VOT has been considered
in DDK exercises to extract relevant information such as VOT duration and
VOT ratio between VOT and vowel length (CV ratio). These parameters were
used in [11] to classify between PD patients and healthy speakers. The authorsAcoustic Characteristics of VOT in Parkinson’s Speech 305
reported accuracies of 92.2%. In [12] the authors measured VOT and obtained
articulatorycharacteristicsinrelationtophysiologicalcorrelatesofvocalquality,
articulatory accuracy, occlusion quality, and glottal and supra-glottal coordina-
tion. In general, automated methods help in reducing costs and time of clinical
screenings including those that required to evaluate and monitor motor speech
disorderinPDpatients[14].Theautomaticcomputationalmethodsshouldcon-
sider the existence of approximate or incomplete productions in DDK tasks.
Note that the alternating and rapid repetition of the plosives /p/, /t/, and
/k/ generate variations that reveal patterns like debilitation of the burst, pres-
ence of voicing and loss of the articulatory tension. Fusion of automatic and
manual labeling methods may improve the accuracy in detecting the aforemen-
tioned variations and allow the description of the acoustic “correctness” of each
consonant segment. The purpose of our paper is to present a methodology for
measuringVOTofVPCsin/pa-ta-ka/usingphonetic-acousticmanualmethods
and the automatic measurement method to evaluate and analyze the accuracy
of the detection of speech disorders in PD patients.
2 Methodology
Detailsoftheproposedmethodologyareprovidedbelow.Itincludesthedescrip-
tion of the database and the steps followed in the manual labeling process along
with its automatic evaluation.
2.1 Data
Speech recordings of the PC-GITA database are considered [13]. This corpus
includes 50 PD patients and 50 healthy speakers. The participants were asked
to perform the rapid repetition of /pa-ta-ka/ for at least 3 seconds. The speech
signals were captured in a sound-proof booth using a professional audio setting.
All of the patients were evaluated by a neurologist expert following the MDS-
UPDRS-III scale [7]. Table1 summarizes demographic and clinical information
of the speakers.
Table 1. Clinical and demographic information of the speakers. Values in terms of
(Mean±Standard deviation).
PD patients HC speakers
Male Female Male Female
Number of speakers 25 25 25 25
Age [Years] 61±11 60±7 60±11 61±7
Years diagnosed 13±11 9±5 – –
MDS-UPDRS-III 37.4±21.7 37.6±14.0 – –306 P. Argu¨ello-V´elez et al.
2.2 Acoustic Phonetic Analysis of VPCs
Voiceless plosive consonants are characterized by three stages in Spanish: app-
roach, closure, and release. During the approach phase, the articulators move
towards each other, creating an obstruction of airﬂow during the closure phase.
Finally,thearticulatorsmoveawayfromeachotherduringthereleasephasepro-
ducinganexplosiveburstofairwithenergyspreadacrosstheaudiblespectrum.
Figure1showstimeandspectralrepresentationoftheVPC/p/,followedbythe
vowel /a/. The shaded regions represent the closure and the release phases. The
closure phase is characterized by the absence of speech, which can be observed
in the time signal (Fig.1A) and in the spectrogram (Fig.1B). The release stage
is typically observed by looking at the spectral representation. Precision to pro-
duce voiceless plosive sounds can be reduced due to the presence of a motor
problem. Loss of pressure in the lips changes /p/ sounds; impaired movement
of the tongue aﬀects /t/ sounds, and loss of contact with the soft palate and
no burst deviates /k/ sounds. Altered versions of these sounds are perceived
as /β/, /δ/, and /γ/, instead of /p/, /t/, and /k/, respectively. These weak-
ened consonants are characterized by the loss of tension, increased voicing and
incomplete contact of the articulators. As a result, there is no silence nor burst
during the closure and release phases, respectively. Figure2 shows an example
of a weak consonant /β/ (/p/), produced in the transition from one utterance
of /pa-ta-ka/ to another.
Fig.1.Time(FigureA)andspectral(FigureB)representationsofthevoicelessplosive
sound /p/. The shaded regions represent the closure and release phases.
2.3 Manual Labeling
Manual labels are found using the software Praat [3], as follows:
1. Determine the start and total duration of the consonant in a syllable by
syllable fashion. The total duration of the VPC is measured as the total
duration of the approach, closure, and release phases.
2. Identify the closure phase as the time prior to the burst indicated by the
point of minimum intensity relative to the surrounding sounds, i.e., tension
phase with increased supra-glottic pressure.Acoustic Characteristics of VOT in Parkinson’s Speech 307
Fig.2. Weak consonant /β/ (/p/) produced in the syllables /pa-ta-ka/.
3. Measure the maximum spectral energy during the release stage. A spectral
slice is extracted in the ﬁrst energy burst produced in a VPC sound. Then,
the point with highest spectral energy is extracted.
4. Measure the VOT by placing labels at the initial burst of the consonant and
vowel onset. The time of the initial burst is detected by computing the zero
crossing points. The vowel onset is set at the beginning of a periodic-like
signal. Formant frequencies and the presence of pitch are used to mark the
beginning of voicing in a stop-vowel transition.
5. Identify weak consonants /β/, /δ/, and /γ/, in order to measure negative
VOT. In this case, VOT is measured as the time between the end of the pre-
viousvowelandthebeginningofthenextone.Theenergybetweensyllables,
e.g. /ka/ to /βa/, /pa/ to /δa/, and /ta/ to /γa/, is computed to detect the
beginning of the weakened consonant.
These steps are followed to extract the acoustic measures: VOT value, dura-
tionoftheclosurephase,totaldurationoftheconsonant,andthefrequencypoint
with the maximum spectral energy measured in the burst phase. Mean value,
standarddeviation,kurtosis,andskewnessarecomputedfromtheacousticmea-
sures to create 48-dimensional feature vectors (4 measurements * 3 plosives * 4
functionals) per speaker.
2.4 Automatic Classiﬁcation Between PD Patients and HC
Speakers
A radial basis function – Support Vector Machine (rbf–SVM) with margin
parameter C and kernel bandwidth γ is considered. Parameters are optimized
through a grid search with 10−4 < C <104 and 10−6 < γ <103. The selection
criterion is based on the performance obtained in the training set following a
10-fold cross validation strategy. The performance of the system is evaluated by
means of accuracy (Acc), sensitivity (Sen), speciﬁcity (Spe), and the F1-score.
Additionally, the Area Under the ROC Curve (AUC) is considered to present
results more compactly. The values of the AUC range from 0.0 to 1.0, were 1.0
means perfect classiﬁcation.308 P. Argu¨ello-V´elez et al.
3 Results and Discussion
3.1 Preliminary Observations with Manual Labels
To evaluate the suitability of the proposed method to detect abnormal produc-
tion of VPCs based on manual labels, the following measures are considered:
the VOT, duration of the closure phase, total duration of the consonant, and
the frequency point with the maximum spectral energy measured in the burst
phase.Figure3showsboxplotswiththefourmanualacousticfeatures.Kruskal-
Wallistestswereappliedandsigniﬁcantdiﬀerencesarefoundinalmostallofthe
acoustic features except for the duration of the consonants /p/ and /t/ and the
frequency of burst in /t/ and /k/. Figure4A shows the number of positive and
negative VOTs measured for the PD and HC groups. In general, the number of
positive VOT values is higher than the negative ones. Also, the number of neg-
ative VOTs is higher in PD compared with respect to the HC group. Figures4B
and 4C show the number of negative VOT per consonant measured in the HC
andPDgroups,respectively.Thepresenceoftheweakconsonantsindicatesloss
of acoustic integrity of VPC, this phenomenon was observed in both HC and
PD speakers. As shown in Figs.4B and 4C, /p/ is more sensible to turn into its
voicedversion/β/.Finally,itisrelativelycommontoobserveanincreaseinthe
approximationsin/β/and/γ/withintheHCgroup.ThisisbecauseinSpanish
the DDK /pa-ta-ka/ has the energy of the accent in /ta´/, which maintains the
closure and the explosion in this dental sound.
Fig.3.Box-plotsofacousticmeasuresextractedfromeachgroup.Kruskal-Wallistests
wereappliedwiththefollowingsigniﬁcancecriteria:p-values:***p<0.001;**p<0.01;
*p<0.05 and n.s. (non-signiﬁcant). Light grey diamonds represent outliers.Acoustic Characteristics of VOT in Parkinson’s Speech 309
Fig.4. Number of positive and negative VOT measurements found in our data.
Figure4A shows the number of positive and negative VOTs measured for the PD
and HC groups. Figure4B shows the number of negative VOTs measured in the HC
groupforeachconsonant.Figure4CshowsthenumberofnegativeVOTsmeasuredin
the PD group for each consonant.
3.2 Automatic Classiﬁcation of PD vs. HC Subjects
Table 2 shows the performance of the rbf–SVM classiﬁer. Four scenarios were
considered: feature vectors from consonants /p/, /t/, /k/, and the combina-
tion of all. The highest accuracies were obtained with the SVMs trained with
features from the consonant /p/ and with features from the three consonants
(Acc=77%).Furthermore,thelowestaccuracywasobtainedwhentherbf–SVM
is trained only with features of the consonant /k/ (Acc=68%).
Table 2. Classiﬁcation results (PD vs. HC) using manually extracted acoustic mea-
sures. Acc: Accuracy. Sen: Sensitivity. Spe: Speciﬁcity. AUC: Area under the ROC
curve
Feature set Acc (%) Sen (%) Spe (%) F1-score AUC
Consonant /p/ 77 80 74 0.77 0.82
Consonant /t/ 71 70 72 0.71 0.76
Consonant /k/ 68 84 52 0.67 0.74
All consonants 77 76 78 0.77 0.83
Theresultsobtainedhereconﬁrmthatarticulatoryimprecisionisacommon
characteristic in PD speech and it is exhibited as a slowing down in the transi-
tion towards the beginning of the vowel. The decrease in supra-glottic tension,
which debilitates the frequency burst in the consonants, is also conﬁrmed by
experiments. VOT allows accurate classiﬁcation of PD vs. HC people. As indi-
cated in [17] there is an increas in VOT of PD patients which is associated with310 P. Argu¨ello-V´elez et al.
voicing and aspiration. The method proposed in this paper shows accuracies of
upto77%intheconsonant/p/,whichisapproximatedasβ inseveralcasesdue
to the absence of tension before the burst as a decrease in the closure in bilabial
sounds [15].
4 Conclusions
Acoustic phonetic analysis with manual labeling allows validation of the acous-
tic characteristics of consonants and their variations depending on the linguis-
tic context while automatic methods are established as rapid detection tools
that together determine the accuracy, sensitivity and speciﬁcity of cases. In this
way, the fusion of both methodologies makes possible the classiﬁcation between
healthy people and people with PD from the measurement of VOT and closure.
In the future, we plan to speed up the labeling process and to automatize the
feature extraction.
Acknowledgments. TheauthorsacknowledgetotheTrainingNetworkonAutomatic
Processing of PAthological Speech (TAPAS) funded by the Horizon 2020 programme
of the European Commission. Tom´as Arias-Vergara is under grants of Convocatoria
DoctoradoNacional-785ﬁnancedbyCOLCIENCIAS.TheauthorsalsothankstoCODI
from University of Antioquia (grant Numbers 2018-23541 and 2017-15530).
References
1. Abramson, A.S., Whalen, D.H.: Voice Onset Time (VOT) at 50: theoretical and
practical issues in measuring voicing distinctions. J. Phonetics 63, 75–86 (2017)
2. Ackermann, H., Hertrich, I., Hehr, T.: Oral Diadochokinesis in Neurological
Dysarthrias. Folia Phoniatrica et Logopaedica 47(1), 15–23 (1995). https://doi.
org/10.1159/000266338
3. Boersma,P.,etal.:Praat,asystemfordoingphoneticsbycomputer.Glot.Int.5,
341–345 (2002)
4. Darley, F., Aronson, A., Brown, J.: Clusters of deviant speech dimensions inthe
dysarthrias.J.SpeechHear.Res.12(3),462–496(1969).https://doi.org/10.1044/
jshr.1203.462
5. Forrest,K.,etal.:Kinematic,acoustic,andperceptualanalysesofconnectedspeech
producedbyParkinsonianandnormalgeriatricadults.J.Acoust.Soc.Am.85(6),
2608–2622 (1989). https://doi.org/10.1121/1.397755
6. Gelb,D.J.,Oliver,E.,Gilman,S.:DiagnosticcriteriaforParkinsondisease.Arch.
Neurol. 56(1), 33–39 (1999)
7. Goetz, C.G., et al.: Movement Disorder Society-sponsored revision of the Uniﬁed
Parkinson’sDiseaseRatingScale(MDS-UPDRS):scalepresentationandclinimet-
ric testing results. Move. Disord. 23(15), 2129–2170 (2008). https://doi.org/10.
1002/mds.22340
8. Klatt, D.H.: Voice onset time, frication, and aspiration in word-initial consonant
clusters. J. Speech Hear. Res. 18(4), 686–706 (1975), http://www.ncbi.nlm.nih.
gov/pubmed/1207100Acoustic Characteristics of VOT in Parkinson’s Speech 311
9. Lisker, L., Abramsson, A.: A cross-language study of voicing in initial stops:
acousticalmeasurements.WORD20(3),384–422(1964).https://doi.org/10.1080/
00437956.1964.11659830
10. Mart´ınez-Ferna´ndez, R., Gasca-Salas, C., Sanch´ez -Ferro, A., A´ngel Obeso, J.:
Actualizaci´on en la enfermedad de Parkinson. Parkinson’s disease: a review.
Revista M´edica Cl´ınica Las Condes 27(3), 364–376 (2016). https://doi.org/10.
1016/j.rmclc.2016.06.010
11. Montan˜a,D.,Campos-Roca,Y.,P´erez,C.J.:ADiadochokinesis-basedexpertsys-
tem considering articulatory features of plosive consonants for early detection
of Parkinson’s disease. Comput. Methods Programs Biomed. 154, 89–97 (2018).
https://doi.org/10.1016/J.CMPB.2017.11.010
12. Novotny, M., Rusz, J., Cmejla, R., Ruzicka, E.: Automatic evaluation of articu-
latory disorders in Parkinson’s Disease. IEEE/ACM Trans. Audio Speech Lang.
Process. 22(9), 1366–1378 (2014). https://doi.org/10.1109/TASLP.2014.2329734,
http://ieeexplore.ieee.org/document/6827910/
13. Orozco-Arroyave,J.R.,etal.:NewSpanishspeechcorpusdatabasefortheanalysis
of people suﬀering from Parkinson’s disease. In: Language Resources and Evalua-
tion Conference, (LREC), pp. 342–347 (2014)
14. Orozco-Arroyave, J.R., et al.: NeuroSpeech: an open-source software for Parkin-
son’s speech analysis. Dig. Signal Process. 77, 207–221 (2018)
15. Parveen,S.,Goberman,A.M.:Presenceofstopburstsandmultipleburstsinindi-
vidualswithParkinsondisease.Int.J.Speech-Lang.Pathol.16(5),456–63(2014).
https://doi.org/10.3109/17549507.2013.808702
16. Schulz, G.M., Grant, M.K.: Eﬀects of speech therapy and pharmacologic and sur-
gical treatments on voice and speech in Parkinson’s disease: a review of the lit-
erature.J.Commun.Disord. 33(1),59–88(2000).https://doi.org/10.1016/s0021-
9924(99)00025-8
17. Tykalova, T., et al.: Distinct patterns of imprecise consonant articulation among
Parkinson’s disease, progressive supranuclear palsy and multiple system atrophy.
Brain Lang. 165, 1–9 (2017). https://doi.org/10.1016/J.BANDL.2016.11.005A Systematic Study of Open Source
and Commercial Text-to-Speech
(TTS) Engines
B
Jordan Hosier1, Jordan Kalfen2, Nikhita Sharma1, and Vijay K. Gurbani1,2( )
1 Vail Systems, Inc., Chicago, USA
{jhosier,nsharma,vkg}@vailsys.com
2 Illinois Institute of Technology, Chicago, USA
jkalfen@hawk.iit.edu, vkg@iit.edu
Abstract. The widespread availability of open source and commer-
cial text-to-speech (TTS) engines allows for the rapid creation of tele-
phony services that require a TTS component. However, there exists
neither a standard corpus nor common metrics to objectively evaluate
TTS engines. Listening tests are a prominent method of evaluation in
the domain where the primary goal is to produce speech targeted at
human listeners. Nonetheless, subjective evaluation can be problematic
andexpensive.Objectiveevaluationmetrics,suchaswordaccuracyand
contextualdisambiguation(is“Dr.”renderedasDoctororDrive?),have
the beneﬁt of being both inexpensive and unbiased. In this paper, we
studysevenTTSengines,fouropensourceenginesandthreecommercial
ones.WesystematicallyevaluateeachTTSengineontwoaxes:(1)con-
textual word accuracy (includes support for numbers, homographs, for-
eignwords,acronyms,anddirectionalabbreviations);and(2)naturalness
(how natural the TTS sounds to human listeners). Our results indicate
thatcommercialenginesmayhaveanedgeoveropensourceTTSengines.
1 Introduction
As voice enabled devices gain prominence in our daily lives, it is increasingly
important that such technologies possess human-like speech capabilities. The
perceptualqualityofTTSspeechsynthesistechnologyimpactstheacceptability
of such systems. For this reason, there is a push among TTS researchers to
make synthetic speech more naturalistic. The applications for such technologies
are vast, including solutions for the visually impaired, hands-free technology,
customer-service centers, etc. In the market today, there are many TTS engines
with varied capabilities. The goal of this study is to propose a set of evaluation
metrics which can be used to evaluate TTS engines. The proposed evaluation
is based on a measure we call contextual word accuracy (formally deﬁned in
Sect.3), and the necessary, though subjective measure of naturalness, i.e., how
do human listeners rank the TTS engines?
SevenTTSengineswereconsidered:fouropensourceenginesandthreecom-
mercial engines. The open source engines were:
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.312–320,2020.
https://doi.org/10.1007/978-3-030-58323-1_34A Systematic Study of TTS Engines 313
– Mimic1: Mimic is the light-weight TTS component based on Carnegie Mel-
lon’s FLITE software (see below).
– CMUFLITE2:FLITEisasmallTTSsynthesisenginedevelopedatCarnegie
Mellon University (CMU) and is designed for small, embedded machines as
well as large servers.
– MaryTTS 3: MaryTTS is a Java-based multilingual TTS synthesis platform
using a Hidden Markov Model (HMM-) model.
– DeepVoice34 [2]: DeepVoice3 is a fully convolutional attention-based neural
TTS system.
Thefollowingcommercialengineswereevaluatedusingtheirrespectivecloud-
based interfaces:
– Voicery5: Voicery is a commercial start-up oﬀering a deep neural network.
– Acapela6: Acapela is a European company specializing in personalized digi-
tized voices.
– Selvy7: Selvy a TTS synthesis engine from a South Korean company.
There are additional commercial engines such as Amazon Polly8, Google
Tacotron [3], and IBM Watson Text to Speech9. While we are not aware of
any scientiﬁc study comparing these engines in a formal manner, it is widely
assumed by practitioners that these engines are the state-of-art in TTS. Given
this assumption, we use Amazon Polly as a control variable and benchmark on
which to evaluate the seven TTS engines under consideration.
The remainder of the paper is organized as follows: Sect.2 motivates the
work, Sect.3 presents our evaluation corpus of 21 test utterances, Sect.4 details
the evaluation methodology, and Sect.5 presents results and discusses ﬁndings.
Beyond Sect.6 are several appendices that provide the raw data to elaborate on
results.
2 Related Work and Contribution
Despite recent advancement in speech synthesis, the evaluation of such tech-
nology has seen little advancement and lacks an established gold standard of
evaluation metrics. The classic approach for TTS evaluation is to synthesize a
set of samples, present the samples to listeners, and to draw conclusions about
the systems based on listener evaluation.
1 https://mycroft.ai/documentation/mimic (last visit: April 23, 2020).
2 http://www.festvox.org/ﬂite/ (last visit: April 23, 2020).
3 http://mary.dfki.de (last visit: April 23, 2020).
4 https://github.com/r9y9/deepvoice3 pytorch (last visit: April 23, 2020).
5 https://www.voicery.com (last visit: March 2020).
6 https://www.acapela-group.com/ (last visit: April 23, 2020).
7 http://speech.diotek.com/en/text-to-speech-demonstration.php(lastvisit:April23,
2020).
8 https://aws.amazon.com/polly/ (last visit: February 2020).
9 https://www.ibm.com/Watson/services/text-to-speech/ (last visit: May 2019).314 J. Hosier et al.
Objective measures have been developed for speech quality evaluation in
telecommunication systems, such as measuring mel cepstral distortion [4,5].
While these serve as a proxy for how well the TTS model represents natu-
ral speech, automating this process is challenging and often requires a bench-
mark natural speech signal [6]. While some measures do not require a refer-
ence speech signal [8], subjective listening tests remain the gold standard in
the literature. The most common listening tests are Mean Opinion Score tests
(MOS, ITU-T Rec. P.10, 2006), MUltiple Stimuli with Hidden Reference and
Anchor(MUSHRA,ITU-TRec.BS.1543,2015),preferencetests,andtranscrip-
tion tasks. The attributes measured by such tests include measures of natural-
ness, intelligibility, similarity, etc.
The Blizzard Challenge was developed to better understand and compare
research techniques in building corpus-based speech synthesizers on the same
data [7]. Competitors present the results from a standard listening test and
describetheirsystems.Thesetestsincludedlisteningtoaﬁxednumberofutter-
ancesandsubsequentlyassigningadomain-speciﬁcMOSscorebasedonthetest
set.Whilethischallengehasawelldevelopedlisteningtest,itisalsosubjective.
PrimaryContributions: TTSenginesareusedinavarietyofapplicationsand
itisimportantthatsuchtechnologiesareﬂexibleenoughtoadapttotheproper-
tiesofnovelenvironments.HoweverTTSsystemscanbefragile,andoftenbreak
downwithminorchangesinthelexicon.Thisworkproposesacorpus(Sect.3)of
diversesetofEnglishphonologicalandmorphologicalartifacts(homographs,for-
eignloanwords,acronyms,directionalabbreviations,etc.)thatpresentpotential
challengestoTTSengines.Weseektoestablishthiscorpusasacanonicalcorpus
for evaluating TTS engines. Furthermore, we propose two evaluation method-
ologies(Sect.4):anobjectivemetricthatallowsforimpartialevaluationofTTS
response to complex input, and while the second metric is a subjective listen-
ing test, we attempt to control for subjectivity in evaluating it through using
multiple advanced voting techniques.
3 Evaluation Corpus
The set of 21 vectors used to evaluate the TTS engines is shown in Table1.
These sentences represent a diverse set of English phonological and morpholog-
ical grammatical constructs that present potential challenges to TTS engines.
Whilethesesentenceswouldbeeasilyproducedandunderstoodbyhumans,they
includeambiguitiesandhomographsthatcouldpresentchallengestoaTTSsys-
tem - challenges which potentially indicate inadequate training of the system.
Thus, we test if the system can render these vectors with the accuracy that a
human reader could easily achieve.
These stimuli included sentences with homographs (Test cases 10–12) and
foreign words (Test cases 18 and 21). We also evaluate forms of abbreviations,
including context dependent abbreviations (i.e. “Dr.” as a preﬁx to a name will
beexpandedas“Doctor”,while“Dr.”asasuﬃxtoanaddressisexpectedtobeA Systematic Study of TTS Engines 315
expanded as “Drive”), abbreviations in addresses (i.e. “Apt.”, “Pl.”, “Pkwy.”),
and abbreviations of names (i.e. “Chas.” for “Charles”). Finally, we evaluate
numbers(i.e. roman numerals, numbers instreetaddresses,and numbersoccur-
ring in a string denoting times or dates) and symbols (“&”).
Table 1. Corpus for evaluating the TTS engines
Testcase Sentence
1 AmericanCommunications&Engineering,Inc.islocatedat123NW.MainSt.,Apt.
1A,St.Paul,MN60655
2 NatomaProfessionalCtr.555OakdalePkwy.,islocatedat123S2ndPkwy.,Ste.
700,Ft.Lauderdale,FL.
3 ValorTelecomLtd.1910E.KimberlyPl.P.O.B93425,OldVillageSq.,CA
4 Sec.ofStateHillaryClintonandSen.LisaMurkowskispokewithPres.Mahmaud
AbbastodiscussFASB
5 Ex-Gov.SarahPalinandEx-HPCEOCarlyFiorinametwithIsraeliEx-Prime
MinisterArielSharontodiscussRBOC
6 TreasurySec.TimothyF.GeithnerusedtobetheCOOatJPMorganandearned
$4.5-million-a-yearandearnedanMBAfromHarvard
7 Rep.Chas.RangelPh.DwascensuredbyPETA
8 Mr.JohnSmithSr.andMrs.JaneSmithworkedatLeviStrauss&Co.withtheir
son,JohnSmithJr.,anddaughterMs.JudySmith
9 Gen.DouglasMacArtherwastiredofreceivingSPAMfromtheNYSE
10 Theyweretooclosetothedoortocloseit
11 Thedovedoveintothewater
12 Theteamleadhadleadustovictory
13 AfterIreadabookIaddittomylistofbooksthatI’veread.
14 Thefarmwasusedtoproduceproduce
15 Peoplewhouseareofnouse
16 Prof.Robt.B.Reichisabonaﬁderocketscientist
17 Dr.AlbertEinstein,Phdhadalotofchutzpahturningdownthepresidency.
18 Jas.A.BaroneIIIsaidbonvoyagetoCapt.Wm.O.Barnettbeforethecoupd’etat
19 IwasbornMon.,Sept.25,1989at12:30AM
20 Lt.Cmd.Jas.W.MarkswasbornWed.the3rd.OfMar.at2:30p.m
21 IliveinLaCrossecounty,Wisconsin.ThisisclosetoEauClaireandPrarieduChien
In summary, these 21 cases present non-trivial challenges to TTS engines to
unambiguouslypronouncethesentenceinamannerconsistentwithexpectations.
4 Evaluation Methodology
Wepresenttwometricsofevaluation.Theﬁrstofthesemetricsisφ,orcontextual
word accuracy. To evaluate φ, a sentence is considered as a bag of words. With
that assumption, φ is deﬁned as:
(cid:2)n
1
φ= n I(xi), (1)
i=1316 J. Hosier et al.
where n is the total number of words in the bag, xi enumerates over all the
words in the bag, and I(xi) is the word pronunciation identity function deﬁned
as: (cid:3)
1:xis pronounced as expected
I(x)= (2)
0: otherwise
The range of φ is [0,1], and we seek to maximize φ. If all of the words in
the bag are rendered in the expected manner, φ will be 1.0. Thus, contextual
word accuracy measures both the phonological and morphological eﬀects of a
TTS engine producing all words in the sentence. Scoring word-level accuracy
was done manually, and was a rather straighforward process. When determing
accuracy, we were tracking word stress and phonetic realization to determine
whether a word was rendered correctly or not.
The second metric is naturalness. We evaluated the TTS engines on natu-
ralness by synthesizing the 21 sentences and presenting them to listeners. The
listeningtestaskedparticipantstorateenginesbyplacingtheminrankedorder
from most to least human-like.
We recruited 14 participants, each of whom ranked, in descending order of
preference, the seven TTS engines according to how natural they deemed the
rendering to be. The participants ranged in age from 16years to 64years, with
a median age of 28. They were asked to listen to a portion of a passage called
“The Rainbow Passage” rather than the 21 test vectors used in the previous
evaluations in an eﬀort to make the grammatical artifacts that were the target
of the accuracy evaluation less salient to participants. “The Rainbow Passage”
is a standard reading passage, commonly used in speech evaluations, reading
comprehensiontests,andfortestinglanguagerecognitionsoftware10.Theresult
was an audio ﬁle rendered by each TTS engine. (Appendix C contains a link
to these ﬁles.) In addition, the participants were asked an open-ended question:
“What cues in the speech made you ﬁnd it more (or less) robotic?” (Results in
AppendixB.)Tominimizeselectionbias,weexplicitlychoseindividualswhoare
notintheﬁeldoflinguistics,andexcludedcolleaguesatourrespectiveacademic
or industrial institutions. Instead, we chose participants who were not involved
in any area related to speech technologies. To eliminate conﬁrmation bias, each
subject was presented the recordings in isolation from other participants.
We score the resulting TTS engine rankings in two ways; Condorcet voting
andtheBordaCountmethod[1].Thesemethodsarepreferredoverothers(e.g.,
averagingthevotesacrossallparticipants)astheyarerobustandlessinﬂuenced
by presence of outliers. The Condorcet method selects the best candidate (i.e.
TTS engine) by considering pairwise head-to-head elections among the candi-
dates, and selects the candidate that would win the majority of the votes in
all such pairwise contests. Under certain circumstances (presence of cycles in
voting, e.g., A is preferred over B, B is preferred over C, C is preferred over
10 “When the sunlight strikes raindrops in the air, they act like a prism and form a
rainbow. The rainbow is a division of white light into many beautiful colors. These
take the shape of a long round arch, with its path high above, and its two ends
apparently beyond the horizon”.A Systematic Study of TTS Engines 317
A), the Condorcet method may not elect an authoritative winner, however, this
turned out not to be the case with our voting. The Borda Count method asks
participants to rank candidates in order of preference. Then, each engine, for
each ballot, is given a certain number of points corresponding to the number of
engines ranked lower. After counting all the votes, the candidate with the most
points is the winner. The advantage of this method is that it selects a broadly
acceptable candidate instead of those preferred by a majority.
5 Results and Discussion
5.1 Contextual Word Accuracy (φ)
The results for contextual word accuracy are presented in Fig.1. In tabulating
these results, we included Amazon Polly as our control variable as we discussed
in Sect.1. Appendix A shows in detail how each TTS engine fared against each
test case, resulting the speciﬁc value of φ.
Results demonstrate that φ is high among commercial TTS engines, with
Acapela reaching a word accuracy rate of 0.975 with minimal variance across
the accuracy rate for each of the 21 sentences. Amazon Polly is a close second
with an average word accuracy rate of 0.967, with some dispersion around the
1st and 3rd quartiles with respect to the median.
The open source
engines are less accu-
rate; the best accu-
racy is seen by FLITE
(0.844) and the low-
est accuracy by Deep-
Voice3 (0.761). This is
surprising given that
DeepVoice3usesconvo-
lutionalsequencelearn-
ing and is considered
a state-of-art neural
speech synthesis sys-
tem. Fig.1.Contextualwordaccuracy(%)acrosstheevaluation
corpus.
5.2 Naturalness
Amethodofrankedcomparisonwasusedtoevaluatenaturalness.Asmentioned
in Sect.4, we produced an audio ﬁle containing the rendering of “The Rainbow
Passage”fromeachengine.(AppendixCcontainsalinktoaZIParchiveofthese
ﬁles;DeepVoice3onlyrendered9swithwhatappearstobeanabrupt,premature
termination, and Acapela also terminates prematurely after 12s.) The partici-
pantswereaskedtoranktheaudioﬁlesandansweranopenendedquestion,i.e.,
“What cues in the speech made you ﬁnd it more (or less) robotic?” (Answers to
the question provided by the participants are in the link in Appendix B).318 J. Hosier et al.
The identity of each TTS engine was hidden from the participants. Instead,
an opaque name (“Engine 1”, ..., “Engine 7”) was provided for ranking. Partic-
ipants were told to rank each engine from 1 (most natural sounding) to 7 (least
natural sounding), and were permitted to rank more than one TTS engine at
the same level. Results of the ranking are in the table in Table2.
Table 2.Rawrankingsof14participants(EnimpliesTTSEngineN;a-impliesthat
the participant did not vote for any TTS engine at that rank.)
Participants
Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 E7 E6,E7 E7 E7 E7 E7 E6,E7 E6 E2 E7 E7 E7 E6 E7
2 E6 E1 E4 E1,E6 E2 E2 E2 E2,E7 E7 E1 E6 E6 E2 E6
3 E1 E4 E3 E2,E4 E4 E3 E3,E5 E5 E6 E6 E1 E1 E1 E2
4 E2 E5 E2 E3,E5 E6,E3,E5 E6 E4 E1 E1 E4 E4 E5 E7 E1
5 E4 E3 E5 – E1 E5 E1 E4 E4,E5 E2 E5 E2 E4 E4
6 E3 E2 E6 – – E4 – E3 E3 E5 E3 E4 E5 E5
7 E5 – E1 – – E1 – - - E3 E2 E3 E3 E3
The result of Condorcet voting and the Borda Count indicated a unanimous
winner, Voicery. Condorcet declares as winner the candidate that wins every
comparison against all other candidates. Thus, for 7 candidates, Condorcet per-
forms 21 pairwise comparisons and chose the winner to be the candidate who
wins every comparison with all other candidates. That candidate is Voicery.
The Borda Count method assigns points
to each candidate in the ranked lists corre-
sponding to the number of candidates that
were ranked lower. After counting all the Table 3. Borda Count of TTS
engines based on votes received by
votes, the candidate with the most points is
each engine
thewinner.AnadvantageoftheBordaCount
TTS engine Points
is that, in addition to declaring an absolute
Engine 7 (Voicery) 53
winner, it provides a ranking of the remain-
Engine 6 (Selvy) 42
ing candidates. As Table3 shows, Voicery
Engine 1 (Acapela) 32
received the highest score with Selvy receiv-
Engine 2 (DeepVoice3) 32
ingthesecondhighest.Thecontrastbetween
Fig.1 and Table3 is instructive. The φ value Engine 4 (MaryTTS) 27
for Voicery is not the strongest as is evi- Engine 5 (Mimic) 20
dent from Fig.1, however, it is deemed the Engine 3 (FLITE) 18
mostnaturalistic.DeepVoice3didnotreceive
a high score in the φ metric, but tied for third place in the naturalness metric.
This discrepancy demonstrates a need for further research in new metrics for
evaluating TTS engines.A Systematic Study of TTS Engines 319
6 Conclusion
In this paper, we study evaluating open source and commercial TTS engines
on both subjective and objective measures. The two metrics used—aggregate
accuracy (objective), and naturalness (subjective)—demonstrate their viability
foruseinbusinessandacademiccontexts.Wehaveattemptedtocontrolforthe
subjectivity in naturalness by using robust voting techniques such as Condorcet
andBordaCountthathaveadvantagesoversimpletechniqueslikemajorityvote.
Our results indicate that the commercial TTS engines are superior to their
open source counterparts. While some open source engines receive high marks
for aggregate accuracy, they fall short on measures of naturalness. From the
sevenTTSenginesevaluated,noneemergeasaclearwinneracrossbothmetrics.
Acapela is the winner among commercial TTS engines with respect to φ (c.f.,
Fig.1), while FLITE gets the nod in the open source category. The naturalness
metric clearly points to Voicery as the winner, but Voicery is not the preferred
enginewithrespecttoφ.Assumingeachmetricisweighedevenly,Acapelawould
be declared the winner, but clearly, naturalness is an important metric where
Acapela does not perform as expected.
In summary, although open source TTS engines do not reach the level of
naturalness of the commercial engines, they demonstrate aggregate accuracy
thatshowpromisefordeploymentinbusinessandacademicsettings.Futurework
should explore ranking TTS engines with weighted contributions of subjective
andobjectivemetrics.ItcouldalsoproveinterestingtoevaluatetheTTSengines
using an automated speech recognition (ASR) system, however such a method
would only evaluate accuracy as it would not be able to evaluate naturalness.
Finally,futureworkshouldconsiderexpandingthecorpusweproposeinSect.3,
perhapsincludingnon-Englishlangauges,andexploreevaluatingthenaturalness
ofTTSrenderingsthroughmoreobjectivemeasures,includingfeatureextraction
for similarity comparisions to human speech.
A Appendix A: Evaluation of TTS Engines on Our
Corpus
URL: http://www.cs.iit.edu/∼vgurbani/tsd2020/appendix-a.pdf
SHA-1 Hash: b14f7632306c2c9aa4154882d97c1c829ee48224
B Appendix B: Survey Answers by Participants
URL: http://www.cs.iit.edu/∼vgurbani/tsd2020/appendix-b.pdf
SHA-1 Hash: f92c24fd84c35ee0be210801122deccf17ab0818320 J. Hosier et al.
C Appendix C: Rendering of “The Rainbow Passage”
URL: http://www.cs.iit.edu/∼vgurbani/tsd2020/tsd-paper1023.zip
SHA-1 Hash: 8ef25f33b2f95300abb1e3200d0d7cc9ead856e8
References
1. Eric,P.:Votingmethods.TheStanfordEncyclopediaofPhilosophy(2012).http://
plato.stanford.edu/entries/voting-methods/
2. Wei, P., et al.: Deep voice 3: 2000-speaker neural text-to-speech (2017). arXiv
preprint arXiv:1710.07654
3. Wang,Y.,etal.:Tacotron:afullyend-to-endtext-to-speechsynthesismodel(2017).
arXiv preprint arXiv:1703.10135
4. Yamagishi, J., et al.: Analysis of speaker adaptation algorithms for HMM-based
speech synthesis and a constrained SMAPLR adaptation algorithm. IEEE Trans.
Audio Speech Lang. Process. 17(1), 66–83 (2009)
5. Tribolet,J.M.,etal.:Astudyofcomplexityandqualityofspeechwaveformcoders.
In: IEEE International Conference on Acoustics, Speech, and Signal Processing,
ICASSP 1978, vol. 3. IEEE (1978)
6. M¨oller, S., Falk, T.H.: Quality prediction for synthesized speech: comparison of
approaches. In: International Conference on Acoustics (2009)
7. Black, A.W., Tokuda, K.: The blizzard challenge-2005: evaluating corpus-based
speech synthesis on common datasets. In: Ninth European Conference on Speech
Communication and Technology (2005)
8. Stoll, G., Kozamernik, F.: A method for subjective listening tests of intermediate
audio quality. ITU Working Party (2001)Automatic Correction of i/y Spelling
in Czech ASR Output
Jan Sˇvec1(B) , Jan Leheˇcka1 , Luboˇs Sˇm´ıdl2 , and Pavel Ircing2
1 NTIS, University of West Bohemia Pilsen, Pilsen, Czech Republic
{honzas,jlehecka}@ntis.zcu.cz
2 Department of Cybernetics, University of West Bohemia Pilsen,
Pilsen, Czech Republic
{smidl,ircing}@kky.zcu.cz
Abstract. Thispaperconcentratesonthedesignandevaluationofthe
method that would be able to automatically correct the spelling of i/y
in the Czech words at the output of the ASR decoder. After analysis of
boththeCzechgrammarrulesandthedata,wehavedecidedtodealonly
with the endings consisting of consonants b/f/l/m/p/s/v/z followed by
i/yinbothshortandlongforms.Thecorrectionisframedastheclassi-
ﬁcation task where the word could belong to the “i” class, the “y” class
or the “empty” class. Using the state-of-the-art Bidirectional Encoder
Representations from Transformers (BERT) architecture, we were able
to substantially improve the correctness of the i/y spelling both on the
simulated and the real ASR output. Since the misspelling of i/y in the
CzechtextsisseenbythemajorityofnativeCzechspeakersasablatant
error,thecorrectedoutputgreatlyimprovestheperceivedqualityofthe
ASR system.
· ·
Keywords: Grammatical error correction ASR BERT
1 Introduction
The correct spelling of the homophones y/i is being taught already in primary
school and therefore any misspelling of those letters is perceived as a blatant
error. There is a range of situations (see Sec. 2) where this spelling depends on
the context of the surrounding words. Those words guiding the spelling could
appear both before and after the word in question and could be rather distant
from it. It means that the n-gram language models that are still prevalent in
real-time ASR engines are not able to capture the necessary word dependencies
and often select the incorrect variant.
We propose a method that post-processes the ASR output using techniques
that are mostly employed in the task called Grammatical Error Correction
(GEC)1. This task has recently received a signiﬁcant attention [1,8] and is usu-
1 Weareawareofthefactthatthephenomenonthatwearedealingwithfallslinguis-
tically into the domain of orthography, not grammar. However, the context depen-
dency described above simply makes GEC methods more suitable.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.321–330,2020.
https://doi.org/10.1007/978-3-030-58323-1_35322 J. Sˇvec et al.
ally addressed by the cutting-edge NLP techniques. In fact, two-thirds of the
participants of the BEA-2019 Shared Task on Grammatical Error Correction,
employed an approach based on the Transformer architecture [11], including its
recentvariantcalledBERT[4].Webasedoursolutiononthesamearchitecture.
The paper is structured as follows: Sect.2 oﬀers a brief sketch of the lin-
guistic phenomena that we are trying to address. Section3 explains the details
of our models, Sect.4 introduces data set for training and evaluation, Sect.5
describes the experimental setup including a non-trivial baseline model used for
comparison and ﬁnally Sect.5.3 discusses the achieved results.
2 Description of Czech y/i Related Grammar
In Czech, the phoneme i could be written both as i and y. The actual spelling
depends on the context. The context could be very short, most often it is just
the preceding consonant – i is written after consonants ˇz/ˇs/ˇc/ˇr/c/j/dˇ/ˇt/nˇ
(althoughthebigramsdˇi/ˇti/nˇiareveryrareandusedinsomeexceptionalcases),
andyafterconsonantsh/ch/k/r/d/t/n.Thisrulehasanexceptioniftheword
is a foreign word, in which case the spelling depends on the original word. After
consonants b/f/l/m/p/s/v/z both i and y could be written. The same rules
apply for long vowel´ı which could be written both as´ı and y´. The choice of
i or y depends on many factors:
1. If the word is or is derived from a so-called listed word (a word belonging
to a speciﬁed set that is mentioned in Czech language reference books), it is
usually written with y (ml´yn, lit. a mill, mlyna´ˇr, lit. a miller).
2. If the word ends with a vowel i preceded by consonant b/f/l/m/p/s/v/z,
the choice of i/´ı or y/y´ depends on the intra- or inter-sentence context, e.g.:
(a) If the word in question is a past tense or conditional, then the choice
dependsonthegrammatical gender ofthesubject(Chlapci jedli.,lit.The
boys ate./d´ıky jedly., lit. The girls ate.). This is complicated by the fact
that the subject in Czech could be unexpressed and its gender must be
”transferred” from the previous sentence or sentences. Also, if there is
multiple subject in the sentence composed of words with diﬀerent gram-
matical gender (The boy and the girl ate.), the masculine gender “domi-
nates” the feminine and the i is used (Chlapec a d´ıvka jedli).
(b) If the word in question is an adjective, the choice depends on the gram-
matical number of the corresponding noun (hloup´y chlapec, lit. stupid
boy/hloup´ı chlapci, lit. stupid boys).
(c) Ifthewordinquestionisanoun,thechoicedependsonthemorphologyof
the corresponding inﬂectional paradigm (chlupy, lit. hair is written with
y because its pattern hrady, lit. castles is written with y since it follows
consonant d).
(d) There is also an exception, for example word brzy (lit. soon) is a listed
word and the grammar of i/y does not depend on the context.Automatic Correction of i/y Spelling in Czech ASR Output 323
This simpliﬁed description is only to illustrate the relative complicated
orthographyof i/yinCzech.Theseruleshavemanyexceptionsandthechildren
arelearningtherulesthroughalmostthewholetimetheyareattendingelemen-
taryschools.Theknowledgewheretowritei/y issupposedtobeanelementary
knowledge and it is not tolerated if there is an error in writing of i/y in written
materials (e.g. newspapers, school works, subtitles, personal letters).
2.1 The Role of Grammar Errors in ASR Output
As we stated in the previous section, the errors in writing i/y in Czech are
not tolerated. At the same time, the rules are very complicated and there is a
necessity to understand not only the meaning of the given word but also the
meaning of the whole sentence (more sentences). If the ASR output is used to
automatically generate subtitles for the audio or to produce a transcript, the
errors in i/y are usually ﬂagrant. As stated above, some word forms are not
ambiguous because they are always written with i or y regardless of context. In
thiscases,thei/yoccurusuallyinthemiddleoftheword.Themostambiguities
occur in the word endings due to reasons described in Sect.2.
The speech recognizer usually uses the correct form if the context needed to
disambiguatei/y ﬁtsintothen-gramhistoryofthelanguagemodel.Otherwise,
the recognizer selects the more probable variant. The errors are most frequent
in the past tense of verbs. This motivates the work described in this paper - to
design, train and evaluate a model which automatically corrects the word form
of the ambiguous words.
3 Proposed Solution
In this section, we will describe a novel method for i/y spelling disambiguation
based on the surrounding word context. The method is not based on a classical
NLP approach using syntactic parsing, it employs machine learning with much
simpler features instead. We have observed that the majority of i/y errors is
caused by wrong word endings consisting of consonant b/f/l/m/p/s/v/z fol-
lowed with i/´ı/y/y´ (see Table1 and related comments for concrete numbers).
Wethereforefocusonlyoncorrectingsuchcases.Thenthetaskcouldbesimpli-
ﬁed to a classiﬁcation task, where for each word we assign the following classes:
i (the word should end with i/´ı), y (the word should end with y/´y) and ∅ (the
word is not ending with i/´ı/y/´y).
Thetrainingdataforthisclassiﬁcationtaskcouldbeeasilyobtainedbymin-
ingwebtextdata,forexamplefromnewsportalsetc.Weusethestate-of-the-art
approach based on the Transformer architecture and BERT pre-training [4] to
be able to generate context-dependent vector representations of input tokens.
Although the input to BERT could be the whole word tokens, more common
is the use of sub-word tokens. The pre-trained models supplied by Google2 use
2 Avaialable for download from https://github.com/google-research/bert.324 J. Sˇvec et al.
WordPiece tokenization. Unfortunately, the code for generating WordPieces is
Google’sinternalC++codeandonlytheresultingWordPiecevocabularyissup-
plied together with pre-trained models. Therefore we decided to use a similar
method SentencePiece authored also by Google [6] which allows to estimate the
sub-word lexicon and tokenization model from unlabeled textual data3.
3.1 SentencePiece Model
The SentencePiece method provides a lossless tokenization, i.e. the tokenized
text could be transformed into a sequence of tokens and back without any loss
ofcharactersorpunctuationandspacing.Theonlyparameterneededtotrainthe
SentencePiecemodelisthesizeofthevocabulary.Inourexperimentsweuse100k
SentencePiecesaswehaveahugeamountoftextualdataforCzechlanguage,so
thehighernumberoftokensisoutweighedbytheprecisemodellingofthewords
(highernumberofwordsisdirectlyincludedintothevocabularyinsteadofbeing
composed from sub-word tokens). A great advantage of SentencePiece models is
the production of self-contained models, which are easily usable in subsequent
tasks and provide reproducible results. We can use the SentencePiece algorithm
for processing the raw input text, but in the context of ASR (which already
produces word-level tokens) we use it to split word tokens into a sequence of
sub-wordunits.Thiswaythemodelisabletoprocessthewordsnotseenduring
training.
3.2 BERT Pre-training
For pre-training the BERT Transformer, we used the collection of web data
processed in our web mining tool [9]. Our motivation was to train an ASR-
friendly BERT Transformer for Czech, so we removed all punctuation marks
and casing information from the BERT training data. The architecture of the
BERT Transformer is the same as for the Google’s BERT-base model.
Our text corpus for pre-training consists of more than 8 million documents
harvested during the last decade from Czech news servers. With a total word
count exceeding 2.75 billion words and vocabulary size 6.4 million words, this
corpus provides rich data source for pre-training Czech BERT models.
From the text corpus, we prepared two variants of datasets: (1) sequences
with maximum length of 128 tokens and 20 predictions per sequence, and
(2) sequences with maximum length of 512 tokens and 80 predictions per
sequence. Since pre-training with longer sequences is disproportionately expen-
sive,dataset(1)wasusedmostofthetimeduringthepre-training,whiledataset
(2) was used only at the ﬁnal stage to tune positional embeddings. For both
datasets, we used whole word masking and duplication factor of 2. The total
counts of training examples were 82.6 million and 38.4 million respectively.
Wepre-trainedBERTmodelintwophases.Intheﬁrstphase,wetrainedfor
2 million gradient steps with dataset (1), batch size 256 and the learning rate
3 SentencePiece code available from https://github.com/google/sentencepiece.Automatic Correction of i/y Spelling in Czech ASR Output 325
warmed up over the ﬁrst 10000 steps to a peak value at 1·10−4, followed by
250 thousand steps with dataset (2) and batch size 64. After that we evaluated
themodelanddecidedtotrainmorestepsastheperformancewasstillimproving.
In the second phase, we decreased the learning rate to 2·10−5 and trained for
2.55 million more steps with dataset (1) followed by 200 thousand steps with
dataset (2). After the total of 5 million steps, we evaluated the model again
andstoppedthepre-trainingasthemodelhadalreadyconvergedtomaskedLM
accuracy equal to 0.5 and next sentence accuracy 0.98. The whole pre-training
took approximately 4 weeks on one 8-core TPU with 128GB of memory.
3.3 Prediction Model and Fine-Tuning
The pre-trained BERT model could be used to build a task-speciﬁc classiﬁer. In
thiscase,theBERTmodelisembeddedintotheclassiﬁcationneuralnetworkand
subsequently optimized with most of the BERT parameters ﬁxed. This transfer
learning is called a ﬁne-tuning of the Transformer. We used the keras-bert
library4 together with the Keras [3] framework.
The classiﬁcation layers are stacked on the top of the BERT pre-trained
Transformer. Those layers are time-distributed dense layers, i.e. the layers out-
put the prediction for each input token (SentencePiece) based on the feature
vector generated by BERT Transformer without taking any contextual feature
vectors in account. In other words, the classiﬁcation output for a given token is
dependentonlyonthecurrentBERTfeaturevector.Thecontextualdependency
is fully modelled using the multi-head self-attention mechanism included in the
BERT Transformer. The classiﬁcation layers consist of two dense ReLU layers
(256and64units)andonesoftmaxlayer(outputprobabilitiesofclasses∅,i,y).
Thetrainingdataforpredictingi/yclassesareautomaticallygeneratedfrom
the mined web text mainly from the news portals (only the texts of articles
without discussion). We suppose that the news are written by journalists who
are able to write grammatically correct sentences with i/y used correctly. For
training we use the sequence length of 128 SentencePiece tokens and the input
text is processed as a single stream without any overlaps. For each word ending
withconsonantb/f/l/m/p/s/v/zfollowedwithi/´ı/y/y´,therespectivetarget
class is set for the last SentencePiece of the word, all other SentencePieces have
an ∅ class assigned. To prevent the transformer from learning an identity map,
we randomly ﬂip (with equal probability) the i/y at the end of the input word,
so that the classiﬁer could not rely on the correctness of the input assignment
and is forced to classify based on the word root and the word’s context.
During ﬁne-tuning we update the parameters of the following Transformer
layers: self-attention normalization layers, feed forward normalization layers,
multi-headed self-attention of the last layer and the adapter layers [5]. The clas-
siﬁcation layers are initialized and fully trained.
In the prediction phase, the target classes are assigned not to the words but
to the input SentencePieces. Some kind of decision strategy must be employed
4 Available from https://github.com/CyberZHG/keras-bert.326 J. Sˇvec et al.
tocreateaword-levelpredictionsfromtheSentencePiecepredictions.Weexper-
imented with many schemas like average pooling or voting but the best results
were achieved with the following schema: if the model predicts y class for at
least one SentencePiece, then the predicted class for the word is y, otherwise if
themodel predicts iclass forat least oneSentencePiece, thepredictedclass is i.
Otherwise the ∅ class is used. Having the word-level predictions, the ending of
the word is changed according to the predicted class. If the word does not end
with i/´ı/y/´ı or the predicted class is ∅, the word is kept untouched.
4 Data Description
We used the large corpus of web text mined from news portals [9]. The down-
loaded web pages were automatically cleaned into the form of plain-text, the
metadata were extracted. The tokenization was performed so that the text con-
sists of a sequence of space separated tokens. The availability of the metadata
(esp. the date of publication) allow us to easily split the data into diﬀerent par-
titions (See Table1). We pre-trained the BERT model in Feb 2020, so we used
the data that we have never seen (month 02/2020) as the test data (further
denotedasweb data).Whilethepre-trainingprocesswasrunning,wedeveloped
the proposed method and we used the year 2018 as development data.
Table 1. Train/development/test partitions.
Trainingphase Traindata Dev.data Testdata
BERTpre-training Years2000-01/2020 – –
2.75billiontokens – –
BERTﬁne-tuning Years2011–2017,2019 Year2018 Month02/2020
2.1billiontokens 213milliontokens 15.7milliontokens
Baselinemodel Years2016,2017,2019 Year2018 Month02/2020
717milliontokens 213milliontokens 15.7milliontokens
For evaluating the proposed model on an ASR output, we used two diﬀerent
tasks.Intheﬁrsttask,weused10kparagraphsnotcontainingnumeralswritten
as digits5 from the BERT ﬁne-tuning test data and we synthesized them using
our in-house high-quality TTS system [7,10]. The synthetic voice was randomly
chosen from the set of 6 voices. The resulting audio was recognized using the
UWebASR ASR service [12]. We denote such data as synth. ASR. The second
task is a real data from the Czech MALACH archive [13] containing natural
spontaneous interviews with Holocaust survivors.
The Table2 shows the number of ambiguous cases of i/y at the end of the
word. The column i/y incorr. shows the number of cases which are incorrectly
5 Numeralssuch1000aresynthesizedandrecognizedasonethousand andarecausing
errors during ASR evaluation, since the reference is normalised this way.Automatic Correction of i/y Spelling in Czech ASR Output 327
assigned using the most probable variant (row Web data) or using the ASR
(Synth. ASR / MALACH data). The automatic correction method should focus
especiallyonsuchcases.TheClsf. acc.columnshowstheclassiﬁcationaccuracy
when considering the most probable variant or ASR output only, i.e. without
usinganycorrectionmethod.ThelastcolumnASR acc.displaystherecognition
accuracywhenASRisinuse.Thevaluesimposealowerboundontherespective
metrics.Notethatthemostprobable/ASRoutputvarianthavebydeﬁnitionthe
value of F1-metric equal to 0% (see Sect.5.2 for description of these metrics).
Table 2. Table summarizing the number of ambiguous cases (more in the text). Just
for the comparison - the number of ambiguous tokens with i/y in the middle is only
6856 for the web data.
Testdata Tokens Ambig.endingsi/y i/yincorr Clsf.acc.% ASRacc%
Webdata 15.7M 239k 62k 74.0 –
Synth.ASR 401k 6246 910 85.1 91.9
MALACHdata 63k 495 115 83.3 80.9
5 Experiments and Results
5.1 Baseline Method
We compare the results achieved by the proposed method with a strong base-
line.ThebaselineisdesignedasalogisticregressionwithTF-IDFfeaturevectors
computed from left- and right- context of the word for which the target class
(∅, i, y) is predicted. The TF-IDF features were computed as L2-normalized
n-gram (unigram, bigram and trigram) features and maximum number of fea-
tures was limited to 500k. The feature vector has a doubled dimensionality, i.e.
1M features (500k features for left context and 500k for right context. We used
sklearn implementation of logistics regression with the SAGA solver [2]. In the
experiments we determined the optimum length of left context to be 10 words
and for the right context 5 words.
5.2 Evaluation Metrics
To evaluate the performance of i/y disambiguation we use at ﬁrst the classiﬁca-
tionaccuracy evaluatedonthewordsendingwithconsonantb/f/l/m/p/s/v/z
followedbyi/´ı/y/y´.Becausenotallwordswithsuchendingsareambiguous(one
of such example is the pronoun si which is an invalid word with the y ending),
we extract the set of ambiguous pairs of words from the training, development
and test data and we evaluate only on such words.
The classiﬁcation accuracy is easily understandable since it express the por-
tion of words correctly classiﬁed using the automatic grammar error correction.
Thedrawbackofthismethodisthatmanyofthewordsformscouldbecorrectly328 J. Sˇvec et al.
guessed only by assigning the most probable label for a given ambiguous pair.
We therefore use the input tokens which were replaced with the most probable
word-formasareferencepointandwecomputetheF1 metricusingthefollowing
numbers:
– TP – number of true positives, i.e. the number of cases, where the word form
was successfully predicted and the most probable word-form is diﬀerent from
the correct word form.
– FP – number of false positives denotes the number of words, which were
changed from the correct most probable form into an incorrect one.
– FN–numberoffalse negatives countingthenumberofcases,wherethemost
probable word form is incorrect and it wasn’t corrected by the model.
Then the F1-metric is computed using a formula F1 = 2·TP . Such
(2∗TP+FP+FN)
deﬁnition of metric allows us to compensate the eﬀect of the prior distribution
of the classes i and y for diﬀerent words. Together with F1-metric, we are also
able to compute precision P = TP and recall R= TP .
TP+FP TP+FN
ForanexperimentwiththeASRresult,weusetheclassicalrecognitionaccu-
racydeﬁnedasAcc= H−I whereH isthenumberofcorrectlyrecognizedwords,
N
I is the number of insertions and N is the number of words in the reference.
5.3 Results
In the experimental evaluation, we evaluated the baseline method and the ﬁne-
tunedBERTmodelonthetextualwebdataandtheASRoutputsontwodiﬀer-
entdatasets.TheresultsareshowninTable3.Theclassiﬁcationaccuracy(Clsf.
acc.) and ASR accuracy (ASR acc.) could be directly compared with the lower-
bounds presented in Table2. The ﬁne-tuned BERT model clearly outperforms
thebaselineandalsothelower-boundonallthreetasksinboththeclassiﬁcation
accuracyandtheF1-metric.TheperformanceisdegradingwhentheASRerrors
areintroduced.ItisinterestingthattheRecallisdeterioratingmorerapidlywith
loweringASRaccuracythanthePrecisionforboththebaselineandBERT-based
models. The eﬀect of i/y spelling disambiguation on the ASR accuracy is not
signiﬁcant due to the low number of i/y incorrectly predicted by an ASR (910
and 115 words, see Table2), but the i/y disambiguation greatly improves the
grammatical correctness and readability of the ASR output.
Table 3. Experimental results
Dataset Model Clsf.acc.% Precision% Recall% F1-metric% ASRacc.%
Webdata Baseline 83.1 73.9 53.8 62.3 –
BERTﬁne-tuning 96.7 93.6 93.4 93.5 –
Synth.ASR Baseline 86.9 58.2 44.6 50.5 91.9
BERTﬁne-tuning 96.2 90.5 84.9 87.6 92.0
MALACH Baseline 81.3 39.6 25.2 30.8 80.8
BERTﬁne-tuning 88.8 77.2 50.3 60.9 81.1Automatic Correction of i/y Spelling in Czech ASR Output 329
6 Conclusion
We presented a simple and powerful method for correcting the ASR output.
Themethodwasdesignedandevaluatedoni/yspellingdisambiguation taskfor
Czech language, but it could be easily modiﬁed to other languages and similar
grammatical phenomenons. The method uses the Transformer pre-trained using
theBERTapproachwithtrainingdataspeciallydesignedtobecompatiblewith
ASR output. In the future work we would like to use the same Transformer in
other tasks improving the readability of ASR output.
Acknowledgments. This research was supported by the Technology Agency of the
Czech Republic, project No. TN01000024. Computational resources were supplied by
theproject“e-InfrastrukturaCZ”(e-INFRALM2018140)providedwithintheprogram
Projects of Large Research, Development and Innovations Infrastructures.
References
1. Bryant,C.,Felice,M.,Andersen,Ø.E.,Briscoe,T.:TheBEA-2019sharedtaskon
grammaticalerrorcorrection.In:ProceedingsoftheFourteenthWorkshoponInno-
vative Use of NLP for Building Educational Applications, pp. 52–75. Association
for Computational Linguistics, Florence (2019)
2. Buitinck,L.,etal.:APIdesignformachinelearningsoftware:experiencesfromthe
scikit-learnproject.In:ECMLPKDDWorkshop:LanguagesforDataMiningand
Machine Learning, pp. 108–122 (2013)
3. Chollet, F., et al.: Keras (2015). https://keras.io
4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep
bidirectionaltransformersforlanguageunderstanding.In:Proceedingsofthe2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, vol. 1 (Long and Short Papers), pp.
4171–4186. Association for Computational Linguistics, Minneapolis (2019)
5. Houlsby, N., et al.: Parameter-eﬃcient transfer learning for NLP. In: Chaudhuri,
K., Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference
on Machine Learning. Proceedings of Machine Learning Research, PMLR, Long
Beach, California, USA, 09–15 June 2019, vol. 97, pp. 2790–2799 (2019)
6. Kudo,T.,Richardson,J.:SentencePiece:Asimpleandlanguageindependentsub-
word tokenizer and detokenizer for neural text processing. In: Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations. pp. 66–71. Association for Computational Linguistics, Brussels
(2018)
7. Matouˇsek,J.,Tihelka,D.:AnnotationerrorsdetectioninTTScorpora.In:INTER-
SPEECH, Lyon, France, pp. 1511–1515 (2013)
8. Ng, H.T., Wu, S.M., Briscoe, T., Hadiwinoto, C., Susanto, R.H., Bryant, C.:
The CoNLL-2014 shared task on grammatical error correction. In: Proceedings of
theEighteenthConferenceonComputationalNaturalLanguageLearning:Shared
Task, pp. 1–14. Association for Computational Linguistics, Baltimore (2014)
9. Sˇvec,J.,Leheˇcka,J.,Ircing,P.,Skorkovsk´a,L.,Praˇza´k,A.,Vavruˇska,J.,Stanislav,
P., Hoidekr, J.: General framework for mining, processing and storing large
amountsofelectronictextsforlanguagemodelingpurposes.Lang.Res.Eval.48(2),
227–248 (2013). https://doi.org/10.1007/s10579-013-9246-z330 J. Sˇvec et al.
10. Tihelka,D.,Hanzl´ıˇcek,Z.,J˚uzov´a,M.,V´ıt,J.,Matouˇsek,J.,Gr˚uber,M.:Current
state of text-to-speech system ARTIC: a decade of research on the ﬁeld of speech
technologies.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2018.LNCS
(LNAI),vol.11107,pp.369–378.Springer,Cham(2018).https://doi.org/10.1007/
978-3-030-00794-2 40
11. Vaswani,A., etal.: Attentionisall youneed. In:Guyon,I.,et al.(eds.) Advances
in Neural Information Processing Systems 30, pp. 5998–6008. Curran Associates,
Inc. (2017)
12. Sˇvec,J.,Bul´ın,M.,Praˇza´k,A.,Ircing,P.:UWebASR-web-basedASRenginefor
Czech and Slovak (2018)
13. Sˇvec, J., Psutka, J.V., Trmal, J., Sˇm´ıdl, L., Ircing, P., Sedmidubsky, J.: On the
use of grapheme models for searching in large spoken archives. In: 2018 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 6259–6263 (2018)Transfer Learning to Detect Parkinson’s
Disease from Speech In Diﬀerent
Languages Using Convolutional Neural
Networks with Layer Freezing
B
Cristian David Rios-Urrego1( ) , Juan Camilo Va´squez-Correa1,2 ,
Juan Rafael Orozco-Arroyave1,2 , and Elmar No¨th2
1 Faculty of Engineering, University of Antioquia UdeA, Medell´ın, Colombia
cdavid.rios@udea.edu.co
2 Pattern Recognition Lab, Friedrich-Alexander-Universit¨at Erlangen-Nu¨rnberg,
Erlangen, Germany
Abstract. Parkinson’s Disease is a neurodegenerative disorder charac-
terized by motor symptoms such as resting tremor, bradykinesia, rigid-
ity and freezing of gait. The most common symptom in speech is called
hypokineticdysarthria,wherespeechischaracterizedbymonotoneinten-
sity,lowpitchvariabilityandpoorprosodythattendstofadeattheend
of the utterance. This study proposes the classiﬁcation of patients with
Parkinson’s Disease and healthy controls in three diﬀerent languages
(Spanish, German, and Czech) using a transfer learning strategy. The
processisfurtherimprovedbyfreezingconsecutivediﬀerentlayersofthe
architecture. We hypothesize that some convolutional layers character-
ize the disease and others the language. Therefore, when a ﬁne-tuning
in the transfer learning is performed, it is possible to ﬁnd the topology
thatbestadaptstothetargetlanguageandallowsanaccuratedetection
of Parkinson’s Disease. The proposed methodology uses Convolutional
Neural Networks trained with Mel-scale spectrograms. Results indicate
that the ﬁne-tuning of the neural network does not provide good per-
formanceinalllanguageswhileﬁne-tuningofindividuallayersimproves
the accuracy by up to 7%. In addition, the results show that Transfer
Learningamonglanguagesimprovestheperformanceinupto18%when
compared to a base model used to initialize the weights of the network.
· ·
Keywords: Parkinson’s disease Speech processing Transfer
·
Learning Convolutional neural networks
1 Introduction
Parkinson’s Disease (PD) is a neurological disorder characterized by progressive
lossofdopaminergicneuronsinthesubstantianigraofthemidbrain[3].PDpro-
duces motor and non-motor deﬁcits in patients such as resting tremor, bradyki-
nesia,rigidityandfreezingofgait,whichcontributesigniﬁcantlytodecreasethe
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.331–339,2020.
https://doi.org/10.1007/978-3-030-58323-1_36332 C. D. Rios-Urrego et al.
quality of life of the patients [6,7]. Most PD patients develop several speech
deﬁcits, which are grouped and called hypokinetic dysarthria [18]. This type of
dysarthria is characterized by a low voice volume, reduced voice quality, reduc-
tionofprosodicpitch,imprecisepronunciationofconsonantsandvowels,lackof
ﬂuency, voice tremor, and others. These symptoms often have adverse eﬀects in
the speech intelligibility and the quality of life [20].
Diﬀerent studies in the literature about the automatic evaluation of PD
speecharebasedondeeplearningtechniques,especiallyinConvolutionalNeural
Networks (CNN). For instance, in [4], the authors implemented diﬀerent CNNs
todiscriminate41PDpatientsand40healthycontrols(HC)usingutterancesof
the sustained vowel /ah/. Each audio was transformed into spectrograms. The
authors implemented data augmentation techniques. The best performing net-
work was composed of 2 convolutional layers, 2 max pooling layers and a fully
connected layer. This achieved accuracies of up to 75.7%. The authors from [14]
modeledthearticulatorydeﬁcitsinPDpatientswithCNNs.Initially,thetransi-
tions between voiced and unvoiced segments were detected to model diﬃculties
ofpatientstostart/stopthevibrationofthevocalfolds.Then,atime-frequency
representationforeachtransitionwascomputedtotraintheCNNs.Theauthors
consideredspeechrecordingstoclassifyPDpatientsandHCsubjectsin3diﬀer-
ent languages (Spanish, German, and Czech). They obtained accuracies ranging
from 70% to 89%, depending on the language. The authors in [17] presented
an approach to PD detection using a ResNet architecture dedicated originally
to image classiﬁcation. Initially, the authors trained a base model using the
ImageNet and Saarbruecken Voice Database (SVD) databases. Then, the model
was re-trained and evaluated using spectrograms calculated on the sustained
vowel /ah/ of 50 PD patients and 50 HC subjects from the PC-GITA database.
Theauthorsuseda10-foldspeakerindependentstratiﬁedcross-validationstrat-
egy. The accuracy obtained in the validation set was 91%. A similar work was
performed in [9] where 2 transfer learning strategies were implemented (layer
freezing and ﬁne-tuning) for the classiﬁcation of the PD patients using hand-
writing signals. The authors showed that the proposed strategies improved the
accuracy up to 92.3%. Finally, in [15], the authors proposed to use a transfer
learningstrategyamonglanguagestodiscriminatebetweenPDpatientsandHC
subjects. The authors considered recordings in 3 diﬀerent languages (Spanish,
German and Czech). Base models were created for each language and after-
wards the parameters were transferred to the other languages. According to
their results, the transfer learning strategy improves the accuracy by up to 8%.
The main objective of the present study is to discriminate between PD
patients and HC speakers in diﬀerent languages using a layer freezing strat-
egy in a transfer learning scenario. The aim is to have robust models designed
foreachlanguagebasedonpreviousknowledgefromadiﬀerentcorpus.In[5],the
eﬃciency of partial layer freezing was demonstrated for image recognition using
CNNs when transfer learning is applied to a small target database. Therefore,
we believe that by performing a ﬁne-tuning in the transfer learning, only the
layers that perform the disease characterization are transferred from the baseFreezing Layer in CNN to Classify PD from Speech 333
languagetothetargetone.Weshowtheeﬀectofcopyingalllayersandﬁne-tune
asequential number of layers, by freezing somelayers of the basemodels. CNNs
were trained with utterances in Spanish, German and Czech using Mel-scale
spectrograms. The results indicate that layer freezing improves the performance
ofthemodelsinupto7%ofaccuracywithrespecttothemodelsusingatransfer
learning of all parameters (without layer freezing).
2 Materials and Methods
2.1 Data
Three databases in diﬀerent languages are considered: Spanish, German and
Czech, each database containing PD patients and HC subjects. All recordings
were recorded in controlled acoustic conditions and down-sampled to 16 kHz.
Demographic information of the participants is shown in Table1. Labels of the
neurological state of the patients, according to the Movement Disorder Society
- Uniﬁed Parkinson’s Disease Rating Scale (MDS-UPDRS-III) are included [2].
Table 1. Clinical and demographic information of the speakers in the three datasets.
G.: gender (M. male or F. female). Values are reported in terms of mean ± standard
deviation.
G Spanish German Czech
PD HC PD HC PD HC
#ofSubjects M 25 25 47 44 30 30
F 25 25 41 44 20 19
Age[years] M 61.3±11.4 60.5±11.6 66.7±8.7 63.8±12.7 65.3±9.6 60.3±11.5
F 60.7± 7.3 61.4± 7.0 66.2±9.7 62.6±15.2 60.1±8.7 63.5±11.1
Yearsdiagnosed M 8.7±5.9 – 7.0±5.5 – 6.7±4.5 –
F 12.6±11.6 – 7.1±6.2 – 6.8±5.2 –
MDS-UPDRS-III M 37.8±22.1 – 22.1±9.9 – 21.4±11.5 –
F 37.6±14.1 – 23.3±12.0 – 18.1±9.7 –
Subjects from the three corpora were requested to perform diﬀerent speech
tasks including the rapid repetition of /pa-ta-ka/, reading isolated sentences,
readingatext,andamonologue.Additionalinformationaboutthespeechtasks
for each database is available in [12] for Spanish, [1] for German, and [13] for
Czech.
2.2 Segmentation
Weconsidertheonsetandoﬀsetsegmentstomodelthecapabilityofthepatients
to start/stop the vocal fold vibration [11]. The change between a voiced and
unvoiced segment is detected based on the presence of fundamental frequency334 C. D. Rios-Urrego et al.
values.Oncethebordersaredetected,wetake80msofthesignaltotheleftand
to the right, forming segments with 160ms length. The segmented transitions
aretransformedintoaMel-scalespectrogramwith80Melﬁltersandatimeshift
of 4ms to get a 80×41 time-frequency representation to feed the CNN.
2.3 CNN Model
The CNN implemented here consists of four convolutional layers of size 4, 8, 16,
and 32 respectively, each one followed by a max-pooling layer of size 2×2. In
addition, 3 fully connected layers of size 128, 64, and 2. ReLu activations are
considered in the hidden layers, and a softmax activation function is considered
intheoutputtomaketheﬁnaldecision.Forthetrainingofthenetworkweused
Pytorchwithacross-entropylossfunctionandanAdamoptimizer.Dropoutand
L2-regularization techniques are also used [15].
2.4 Transfer Learning
The main objective of transfer learning is to take the knowledge (weights, and
biases) of previously trained models to improve the performance of a target
model [16]. We perform transfer learning among languages for the classiﬁcation
of PD patients and HC subjects. Therefore, we take a base model, that is, a
modelinaspeciﬁclanguage,andweusethismodeltore-trainthenetworkwith
the remaining two languages.
Frozen Layers. Layer freezing means that the parameters of some layers in
a model are not updated when performing transfer learning. In this work, in
addition to performing a total transfer of parameters among languages, we also
perform a freezing of the 4 convolutional layers in a sequential way. This allows
the system to perform a high level characterization from the base model and
adjusttothetargetmodelwiththeremaininglayers(withoutlayerfreezing)[10].
3 Experiments and Results
The experiments are divided as follows: (1) CNNs are trained to classify PD
vs. HC speakers in each language individually. Thus they can be used as a base
model in the transfer learning. For this case, we use a random parameter ini-
tialization. The results are the same as those obtained in [15] and are used as
the baseline of this work. (2) CNNs from each language are retrained with data
from the remaining two languages using a transfer learning strategy with layer
freezing, i.e., we sequentially freeze the convolutional layers in order to keep a
constantpartofthelearnedweightsfromthebasemodels,Fig.1summarizesthis
procedure. All experiments were performed through a 10-fold speaker indepen-
dent stratiﬁed cross-validation strategy. In addition, the McNemar’s test was
performed between the baseline and the best performing model after transfer
learning with layer freezing. This test checks if the disagreements between two
casesmatch.Intermsofcomparingtwobinaryclassiﬁcationalgorithms,thetest
evaluates whether the two models make errors in the same proportion [8].Freezing Layer in CNN to Classify PD from Speech 335
Fig.1. Transfer learning strategy proposed in this study to classify PD patients vs.
HC subjects from speech with utterances from diﬀerent languages. Fx, x∈{1,2,3,4}
indicates the number of sequentially frozen convolutional layers.
3.1 Transfer Learning with Layer Freezing
CNNs were trained for each language using a transfer learning strategy with
layerfreezing,denotedasFx,wherexindicatesthenumberofsequentiallyfrozen
convolutionallayers.Forexample,F meansafreezingoftheﬁrst3convolutional
3
layers. Table2 shows results when base models in Spanish and German are re-
trained to classify Czech speakers. In this case, the best results are obtained
in F for both base models, i.e., transfer learning without layer freezing. The
0
resultsimproveby4%withrespecttothebaselinewhentheSpanishbasemodel
is re-trained (from 68.5% to 72.5%). The McNemar’s test was performed to
compare these two models (p > 0.05). This results implies that both models
make errors in the same proportion, i.e., transfer learning had no signiﬁcant
eﬀectontheresults.Thisisbecausethelayerfreezingincreasesthesensitivityof
themodels,butreducingthespeciﬁcity,similartothebaseline(94%sensitivity).
This behavior generates over-ﬁtting in the system and also low accuracy.
The results to test the German data are shown in Table3. On the one hand,
when we used the Spanish base model, the best result is obtained with F with
2
an accuracy of 78.3%, mainly because the sensitivity improves with respect to
F . This is because the Spanish base model has higher sensitivity (74.0%) than
0
speciﬁcity(68.0%)(seeTable4).Ontheotherhand,whenweusedtheCzechbase
model, CNN with F improved the accuracy by 5.3% compared to F (without
4 0
layerfreezing).Inaddition,thisresultimprovestheaccuracyby18.9%compared
to the German base model (without using transfer learning). For this case, the336 C. D. Rios-Urrego et al.
Table 2. Classiﬁcation for the transfer learning using the Czech target model. T.
lang.: Target language. F. l.: Frozen layers. Acc: Accuracy. Sen: Sensitivity. Spe:
Speciﬁcity. Values are reported in terms of mean ± standard deviation.
T.lang. L.F. Spanishbasemodel Germanbasemodel
Acc(%) Sen(%) Spe(%) Acc(%) Sen(%) Spe(%)
Czech F0 72.5±13.9 82.0±14.7 62.0±28.9 70.6±14.6 80.0±16.3 62.5±26.4
F1 65.7±12.6 86.0±18.9 45.0±22.7 65.9±11.5 84.0±12.6 48.0±31.5
F2 60.4±17.1 76.0±30.9 44.0±30.9 65.4±16.5 86.0±25.0 45.5±30.7
F3 65.9±19.4 78.0±17.5 54.0±35.3 66.5±17.3 92.0±13.9 40.0±32.7
F4 60.6±15.1 94.0± 9.6 27.0±34.6 58.5±14.5 92.0±13.9 25.0±34.4
Czechbaseline 68.5±14.1 94.0±13.5 42.0±33.2 68.5±14.1 94.0±13.5 42.0±33.2
statistical test produced a p (cid:2) 0.05, which implies that transfer learning has a
signiﬁcant eﬀect on the performance of the model.
Table 3. Classiﬁcation for the transfer learning using the German target model. T.
lang.: Target language. F. l.: Frozen layers. Acc: Accuracy. Sen: Sensitivity. Spe:
Speciﬁcity. Values are reported in terms of mean ± standard deviation.
T.lang. L.F. Spanishbasemodel Czechbasemodel
Acc(%) Sen(%) Spe(%) Acc(%) Sen(%) Spe(%)
German F0 77.3±11.3 86.3±13.8 68.3±14.2 76.7± 7.9 87.5±11.0 66.0±15.6
F1 75.5± 6.7 86.0±13.6 65.0±14.1 77.9±11.9 82.1±18.1 73.8±19.8
F2 78.3±11.3 90.0±11.0 68.6±13.5 75.0±12.6 78.4±21.8 71.5±13.2
F3 75.7± 8.5 78.8±21.8 72.6±17.7 74.7±10.6 80.7±14.4 68.8±17.0
F4 75.0± 9.8 71.4±18.7 78.6±16.7 82.0±11.0 93.3±7.7 70.7±18.4
Germanbaseline 63.1±11.7 43.1±38.0 83.1±17.7 63.1±11.7 43.1±38.0 83.1±17.7
Finally, Table 4 shows the results of classifying the speakers of the Spanish
corpus using base models in German and Czech. For this case, we obtained the
bestresultwhentheﬁrstlayerwasfrozen(F ),improvingtheaccuracyinupto
1
7%comparedtotheCNNwithoutlayerfreezing.Thisimprovementissupported
by McNemar’s test with a p(cid:2)0.05 which implies a signiﬁcant change in model
performance because of layer freezing. These results conﬁrm that the transfer
learning strategy with layer freezing can improve the classiﬁcation accuracy of
systems when ﬁne-tuning some layers from the base model.Freezing Layer in CNN to Classify PD from Speech 337
Table 4. Classiﬁcation for the transfer learning using the Spanish target model. T.
lang.: Target language. F. l.: Frozen layers. Acc: Accuracy. Sen: Sensitivity. Spe:
Speciﬁcity. Values are reported in terms of mean ± standard deviation.
T.lang. L.F. Czechbasemodel Germanbasemodel
Acc(%) Sen(%) Spe(%) Acc(%) Sen(%) Spe(%)
Spanish F0 72.0±13.1 67.0±11.6 78.0±23.9 70.0±12.5 62.0±19.9 78.0±29.0
F1 78.0±16.8 76.0±20.6 80.0±18.8 77.0±11.6 64.0±18.4 90.0±14.1
F2 74.0±10.7 72.0±14.0 76.0±24.6 71.0±20.8 54.0±26.7 88.0±19.3
F3 74.0±18.4 66.0±25.0 82.0±17.5 76.0±13.5 60.0±23.1 92.0±14.0
F4 73.0± 6.7 60.0±13.3 86.0± 9.7 68.0±14.7 48.0±30.1 88.0±10.3
Spanishbaseline 71.0±15.9 74.0±25.0 68.0±28.6 71.0±15.9 74.0±25.0 68.0±28.6
4 Conclusion
We proposed a methodology based on transfer learning with layer freezing to
classify between PD patients and HC subjects from speech in three diﬀerent
languages:Spanish,German,andCzech.Theobjectiveistoimprovetheperfor-
manceofthenetworkwithrespecttoarandominitializationorafulltransferof
theparameters.Firstly,weobtainedthebasemodelsforeachlanguage(without
transfer learning) using CNNs trained with Mel-scale spectrograms extracted
from the transitions between voiced and unvoiced segments. Secondly, the pro-
posed methodology was implemented using 1 language as a target model and
the remaining 2 languages as base models.
The results show that the proposed strategy improves the accuracy of CNNs
in up to 7% compared to a full ﬁne-tune of the CNN (F ). These result open a
0
gaptoinvestigatethehypothesisthatsomelayersofthebasemodelarefocused
onthe characterization of the disease, therefore, theselayers should befrozenin
the transfer learning. While the remaining layers (unfrozen) were in charge of
characterizingandadjustingtheneuralnetworktothetargetlanguage.Inaddi-
tion,itwasobservedthatwiththeproposedmethodtheaccuracyofthemodels
improves in a range from 4% to 18% compared to the base models without any
transfer learning. In future research, it is necessary to create more robust base
models to identify which layers are responsible for characterizing the pathology
and the language. A ﬁrst approach for such understanding could be the use of
saliency maps and class-speciﬁc image generation [19].
Infurtherexperiments,wewilltraintheCNNswithdatafromotherdiseases
in the same language and with data from diﬀerent languages, with the aim
to accurately identify the layers focused on characterizing the presence of the
disease and to observe the level of abstraction of each layer. In addition, we
will also address experiments for the detection of PD using transfer learning
with CNNs trained for emotion recognition. We hypothesize that as the disease
progresses, patients suﬀer disorders that can aﬀect their emotional life, which
can be reﬂected in changes of emotions that are perceived from the speech.338 C. D. Rios-Urrego et al.
Acknowledgments. TheworkreportedherewasﬁnancedbyCODIfromUniversity
ofAntioquiabygrantNumber2017–15530.Thisprojecthasreceivedfundingfromthe
European Unions Horizon 2020 research and innovation programme under the Marie
Sklodowska-Curie Grant Agreement No. 766287.
References
1. Bocklet, T., et al.: Automatic evaluation of parkinson’s speech-acoustic, prosodic
and voice related cues. In: Proceedings of INTERSPEECH, pp. 1149–1153 (2013)
2. Goetz, C., et al.: Movement disorder society-sponsored revision of the uniﬁed
parkinson’s disease rating scale (mds-updrs): scale presentation and clinimetric
testingresults.Mov.Disord.OﬃcialJ.Mov.Disord.Soc.23(15),2129–2170(2008)
3. Hornykiewicz, O.: Biochemical aspects of parkinson’s disease. Neurology 51(2
Suppl 2), S2–S9 (1998)
4. Khojasteh, P., et al.: Parkinson’s disease diagnosis based on multivariate deep
features of speech signal. In: Proceedings of LSC, pp. 187–190. IEEE (2018)
5. Kruithof, M., et al.: Object recognition using deep convolutional neural networks
withcompletetransferandpartialfrozenlayers.In:ProceedingsofSPIE,vol.9995,
p. 99950K. International Society for Optics and Photonics (2016)
6. Logemann, J.A., et al.: Frequency and cooccurrence of vocal tract dysfunctions
in the speech of a large sample of parkinson patients. J. Speech Lang. Hear. Res.
43(1), 47–57 (1978)
7. McKinlay, A., et al.: A proﬁle of neuropsychiatric problems and their relationship
to quality of life for parkinson’s disease patients without dementia. Parkinsonism
Relat. Disord. 14(1), 37–42 (2008)
8. McNemar, Q.: Note on the sampling error of the diﬀerence between correlated
proportions or percentages. Psychometrika 12(2), 153–157 (1947)
9. Naseer,A.,etal.:Reﬁningparkinson’sneurologicaldisorderidentiﬁcationthrough
deep transfer learning. Neural Comput. Appl. 32(3), 839–854 (2020)
10. Oquab,M.,etal.:Learningandtransferringmid-levelimagerepresentationsusing
convolutional neural networks. In: Proceedings of CVPR, pp. 1717–1724 (2014)
11. Orozco-Arroyave, J.R.: Analysis of speech of people with Parkinson’s disease, vol.
41. Logos Verlag Berlin GmbH (2016)
12. Orozco-Arroyave,J.R.,etal.:NewSpanishspeechcorpusdatabasefortheanalysis
ofpeoplesuﬀeringfromParkinson’sdisease.In:ProceedingsofLREC,pp.342–347
(2014)
13. Rusz,J.:DetectingspeechdisordersinearlyParkinson’sdiseasebyacousticanal-
ysis (2018)
14. Va´squez-Correa,etal.:Convolutionalneuralnetworktomodelarticulationimpair-
ments in patients with Parkinson’s disease. In: Proceedings of INTERSPEECH,
pp. 314–318 (2017)
15. Va´squez-Correa,J.C.,etal.:Convolutionalneuralnetworksandatransferlearning
strategytoclassifyparkinson’sdiseasefromspeechinthreediﬀerentlanguages.In:
Nystro¨m,I.,Hern´andezHeredia,Y.,Mili´anNu´n˜ez,V.(eds.)CIARP2019.LNCS,
vol. 11896, pp. 697–706. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-33904-3 66
16. Wang, D., Zheng, T.F.: Transfer learning for speech and language processing. In:
Proceedings of APSIPA, pp. 1225–1237. IEEE (2015)Freezing Layer in CNN to Classify PD from Speech 339
17. Wodzinski, M., et al.: Deep learning approach to Parkinson’s disease detection
using voice recordings and convolutional neural network dedicated to image clas-
siﬁcation. In: Proceedings of EMBC, pp. 717–720. IEEE (2019)
18. Yorkston,K.M.,etal.:Theeﬀectofratecontrolontheintelligibilityandnatural-
ness of dysarthric speech. J. Speech Hear. Disord. 55(3), 550–560 (1990)
19. Yosinski, J., et al.: Understanding neural networks through deep visualization
(2015). ArXiv Preprint arXiv:1506.06579
20. Yunusova,Y.,Weismer,G.G.,Lindstrom,M.J.:Classiﬁcationsofvocalicsegments
from articulatory kinematics: healthy controls and speakers with dysarthria. J.
Speech Lang. Hear. Res. (2011)Speaker-Dependent BiLSTM-Based
Phrasing
B
Mark´eta J˚uzova´1,2( ) and Daniel Tihelka2
1 Department of Cybernetics, Faculty of Applied Sciences,
University of West Bohemia, Pilsen, Czech Republic
2 New Technology for Information Society, Faculty of Applied Sciences,
University of West Bohemia, Pilsen, Czech Republic
juzova@kky.zcu.cz
Abstract. Phrase boundary detection is an important part of text-to-
speech systems since it ensures more natural speech synthesis outputs.
However, the problem of phrasing is ambiguous, especially per speaker
and per style. This is the reason why this paper focuses on speaker-
dependent phrasing for the purposes of speech synthesis, using a neural
networkmodelwithaspeakercode.Wealsodescriberesultsofalistening
test focused on incorrectly detected breaks because it turned out that
some mistakes could be actually ﬁne, not wrong.
· ·
Keywords: Phrase boundary detection Text-to-speech system
·
Neural network Speaker-dependent phrasing
1 Introduction
Aphraseisagroupofwordscarryingaspecialmeaning.Forhumans,itisnatural
tosplitsentencesintophraseswhichmakesthespeechmorecomprehensibleand
easy to follow by listeners. And it also has one very important physiological
reason – a man needs pauses in speech to breathe. Although text-to-speech
(TTS) systems do not need to take a breath, the phrase breaks detection is
an important part of the text processing before synthesizing the prompt [15].
AppropriatephrasingmakestheTTSoutputsmorenaturalandunderstandable.
More technically, the phrasing problem could be deﬁned as a sequence-to-
sequenceproblem.Theauthorof[15]usesthefollowing:Thephrasingisalooking
for asequenceofjuncturetypesj0,j1,... jn for aninputsequenceoftokenst0,
t1,... tn,whereji =1ifaphrasebreakfollowsatokenti,andji =0otherwise.
There are many diﬀerent approaches to this task. One of the most used is
also the simplest one – phrase boundaries are put in the input sentence accord-
ing to punctuation, mostly commas. This approach has also been used in our
TTS system [17] for years. However, modern stochastic methods, supporting
sequence-to-sequencetraining,provedtobeagoodtoolsforthephraseboundary
detectionproblem[2,16],outperformingthe“classical”classiﬁcation-basedtech-
niqueswiththedecision-makingabouteachjuncturetypeseparately[4,7,12,14].
This paper presents a neural network (NN) phrasing model.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.340–347,2020.
https://doi.org/10.1007/978-3-030-58323-1_37Speaker-Dependent BiLSTM-Based Phrasing 341
The main problem of the task of dividing an input sentence with phrase
breaks in smaller units is that there are usually more correct phrase boundary
positioning – which very complicate the evaluation. Contrary to many natural
language processing (NLP) tasks with clear correct and wrong predictions, in
the phrasing issue some of false positives and false negatives are not wrong, in
fact. Nevertheless, for this paper, we decided to evaluate the proposed phrasing
model on the labeled data in a common way in Sect.3, i.e. using default mea-
suresaccuracy,precision,recall andF1-score.Butwealsoinspectedsomeofthe
“faults” and prepared a small listening test to estimate the percentage of really
wrong predicted phrase breaks and no-breaks (see Sect.3.1).
2 Training NN-based Phrasing Model with Speaker Code
Ingeneral,thephrasingproblemisveryvaguesincethereareusuallymorepossi-
ble(correct)sentencesplittingsintophrases.Anditdependsonmanyaspects(a
speaker,situation,audience,etc.)whichexactrepresentationisusedbyhumans.
Therefore, it seems to be a good idea to focus more on training a speaker-
dependentmodel(ase.g.[8,9,13])sincethespeechsynthesisshouldsoundmore
natural when adopting various characteristic of the speaker who had recorded
the speech corpus used. So we decided to train one model for phrase bound-
ary detection using data from more speakers and just mark each input sentence
with a speciﬁc code to allow the network to distinguish one speaker from the
others–thesimilarapproachissometimesusedforthespeechsynthesisitselfto
improve the performance of the conventional speaker-dependent neural network
method [6].
2.1 Training Data
Asourtrainingdataforthisissue,weusedourproprietarylarge-scalespeechcor-
porarecordedforthepurposesofspeechsynthesis,6Czechvoices(bothmaleand
female) and 2 English ones; all of them were recorded in neutral, “newspaper”
style, and they are used as commercial voices in our TTS system ARTIC[17].
These corpora were automatically segmented [3,10,11] and the prosodic breaks
andbreathswerelabeled.Afterwards,thisinformationaboutthebreaks/breaths
positions in the signal were “copied” to the text representation of the particular
sentences. The corpora were also labelled with part-of-speech (POS) tags – we
used our proprietary NN-based tagger for Czech and default NLTK tagger for
English [1].
Duetothat,weobtainedtextcorporawiththeinformationofcorrectphrase
boundaries – speciﬁc for each speaker. Naturally, we got some sentences with
diﬀerentphrasing(dependingonthespeaker),seetheexamplebelow(EN:There
is nothing worse than having a sports watch under a good suit):
(cid:129) speaker1:
Nen´ı nic horˇs´ıho | neˇz m´ıt pod dobry´m oblekem | sportovn´ı hodinky.342 M. J˚uzov´a and D. Tihelka
(cid:129) speaker2:
Nen´ı nic horˇs´ıho | neˇz m´ıt pod dobry´m oblekem sportovn´ı hodinky.
The Table1 contains the basic statistics of all corpora used. 90% of all the
data were used for the training phase and 10% for the evaluation in the Sect.3.
Thetableshowsthatsomecorporahavesigniﬁcantnumberofbreaks(pausesin
speech) not corresponding any comma in the sentence text representation.
Table 1. Text corpora statistics.
Language Speaker No.ofsentences No.ofjunctures No.ofbreaks No-commabreaks
Czech Speaker1 9,619 104,387 16,202 4,060
Speaker2 8,189 91,065 12,487 443
Speaker3 12,151 119,115 12,851 776
Speaker4 9,484 102,106 15,133 2,969
Speaker5 7,662 85,595 12,590 2,451
Speaker6 9,288 93,542 20,137 10,025
English Speaker1 19,909 132,556 9,856 2,241
Speaker2 19,909 133,035 10,229 2,612
2.2 Neural Network Architecture
Fortheexperiment,wedesignedaNN-basedmodelshowninFig.1.Astheinput
ofthenetwork,weusedPOStagsti (correspondingtothewordswi intheinput
sentence which are not used in the model) with the punctuation marks pi and
the speaker code s. The ﬁrst layer of our model is the embedding layer which
transforms each POS tag to a vector representation. These are, altogether with
the punctuation of the sentence and the speaker code, put to the bidirectional
Long-Short-Term-Memory [5] (biLSTM) layer. The output of biLSTM layer is
thenputtotheDense layer,whichoutputsthesequenceofbreaks/no-breaks for
the given sentence (or more precisely for the frame of words at the input).
The proposed model was trained independently for 2 diﬀerent languages,
Czech and English, however, together for all voices per language presented in
Sect.2.1. The Table2 shows the best settings for the NN architecture.
Table 2. Best settings for the BiLSTM phrasing model.
Language Bestaccuracy Bestloss Framecount Tagsembeddings UnitsinbiLSTM
Czech 97.7% 0.066 10 64 64
English 98.4% 0.073 10 32 64Speaker-Dependent BiLSTM-Based Phrasing 343
3 Results
For the evaluation we used the usual measures: Accuracy (Acc), Precision (P),
Recall (R) and F1-score (F1), deﬁned as below:
tp+tn
Acc= (1)
tp+tn+fp+fn
Fig.1. The structure of our BiLSTM phrasing model344 M. J˚uzov´a and D. Tihelka
tp
P = (2)
tp+fp
tp
R= (3)
tp+fn
P ·R
F1=2· (4)
P +R
where
(cid:129) tp = true positives – correctly predicted phrase boundaries
(cid:129) tn = true negatives – correctly predicted junctures with no-breaks
(cid:129) fp = false positives – incorrectly predicted breaks at no-breaks junctures
(cid:129) fn = false negatives – missed phrase boundaries.
The results are shown in the Table3 and Table4. We compare the proposed
BiLSTMmodelwithspeaker-code(NN Speaker)tothebaseline phrasingmodel
usingonlycommas(OnlyComma)andalsotothesameBiLSTMmodelwithout
speaker-code (NN General), i.e. a general phrasing model for all the voices,
training per language.
The results show that the proposed model which uses the speaker code out-
performsthesamemodeltrainedwithoutinformation aboutthespeakerifeval-
uated per speaker. This means we can follow the particular speaker’s phrasing
style more precisely. And let us also add that both NN-based models mostly
outperform thebaseline approach for phrasing–thephrasing basedonsentence
punctuation (mostly commas).
3.1 Listening Tests
As mentioned in Sect.1, the phrasing suﬀers from the problem of ambiguous
results which makes a clear evaluation more diﬃcult. The in-depth inspecting
of false positives (fp; the proposed model said there was a break but there
was no break in the data at that position) revealed that some of them are not
wrong–thesejustmatchedanotherpossiblephrasingoftheparticularsentence.
And,ontheotherhand,somefalsenegatives (fn;i.e.missedphraseboundaries)
seemed to be strange in the particular sentence. Therefore, we decided to pre-
pare a short listening test with 30 randomly selected sentences across (Czech)
speakers with 25 false positives and 25 false negatives. These sentences (with
predicted and “correct” phrase breaks) were synthesized with our LSTM-based
TTS[18] and the listeners were asked to mark for every pause whether it was
ﬁneorstrange/disturbing,regardingthesentencemeaning.NotethattheLSTM-
based TTS was used to prevent the situation when the listener’s evaluation was
aﬀectbyapossibleunnaturalspeechartefactwhichcouldoccasionallyappearin
unit selection synthesis, or by an unnatural intonation which could occasionally
appear in DNN-based synthesis.
We had 15 listeners, 7 of them being speech synthesis experts. For some
phrasebreakstheanswersdiﬀeralot,sowecountedtheanswersforeachphrase
break and divide the breaks into 3 groups:Speaker-Dependent BiLSTM-Based Phrasing 345
(cid:129) “break is OK” – if more than 70% of listeners vote for that
(cid:129) “break is strange” – if more than 70% of listeners vote for that
(cid:129) “tie” – otherwise (the answers were almost balanced)
ThelisteningtestclearlyprovedthattheresultsofBiLSTM-basedmodelare,
in fact, higher than the results shown in the Table3 and Table4 – see Table5.
Morethanahalfofrandomlyselectedfalsepositives areconsideredtobecorrect
bymostofthelistenersand,similarly,aboutahalfofmissedphrasebreaks(false
negatives) were marked as strange by the majority.
Table 3. The comparison of results for Czech voices
Speaker Phrasing model Acc P R F1
Speaker1 NN Speaker 97.4% 100.0% 88.9% 94.1%
NN General 97.0% 100.0% 87.3% 93.2%
OnlyComma 96.7% 99.8% 86.2% 92.5%
Speaker2 NN Speaker 99.6% 99.8% 98.5% 99.1%
NN General 99.5% 100.0% 97.8% 98.9%
OnlyComma 99.4% 99.8% 97.6% 98.7%
Speaker3 NN Speaker 99.3% 99.5% 97.0% 98.2%
NN General 99.3% 99.7% 96.8% 98.2%
OnlyComma 99.3% 100.0% 96.5% 98.2%
Speaker4 NN Speaker 97.5% 100.0% 89.4% 94.4%
NN General 97.2% 100.0% 88.1% 93.6%
OnlyComma 96.9% 100.0% 87.1% 93.1%
Speaker5 NN Speaker 97.4% 99.5% 89.1% 94.0%
NN General 97.2% 99.8% 88.2% 93.6%
OnlyComma 97.1% 99.8% 87.4% 93.8%
Speaker6 NN Speaker 91.8% 98.8% 72.6% 83.7%
NN General 91.1% 100.0% 70.3% 82.6%
OnlyComma 89.6% 100.0% 66.5% 79.9%
Table 4. The comparison of results for English voices
Speaker Phrasing model Acc P R F1
Speaker1 NN Speaker 98.8% 100.0% 94.6% 97.2%
NN General 98.6% 100.0% 93.6% 96.7%
OnlyComma 98.5% 99.3% 93.8% 96.4%
Speaker2 NN Speaker 98.5% 99.8% 93.6% 96.6%
NN General 98.3% 100.0% 92.8% 96.3%
OnlyComma 97.6% 96.4% 93.0% 94.7%346 M. J˚uzov´a and D. Tihelka
Table 5. Listening test results.
Break is OK Tie Break is strange
fp 14 5 6
fn 5 7 13
4 Conclusion
The paper focused on training a speaker-dependent neural-network model for
phraseboundarydetectionandcomparedtheresultstotheoutputsofageneral
phrasing model (trained on data from several speakers) and to the results of
the baseline system. The results presented in Sect.3 show the advantages of
speaker-dependent phrasing.
The false positives and false negatives were further examined during the
listening test. The answers proved that not all of them are real mistakes so the
ability of the proposed model to correctly detect appropriate phrase boundaries
is very high.
The future work includes the training on diﬀerent languages and testing the
general and speaker-dependent models on a new, unseen speaker.
Acknowledgements. This research was supported by the Czech Science Founda-
tion (GACR), project No. GA19-19324S, and by the grant of the University of West
Bohemia, project No. SGS-2019-027.
References
1. Bird,S.,Klein,E.,Loper,E.:NaturalLanguageProcessingwithPython:Analyzing
Text with the Natural Language Toolkit. O’Reilly Media Inc., Newton (2009)
2. Fernandez, R., Rendel, A., Ramabhadran, B., Hoory, R.: Prosody contour predic-
tion with long short-term memory, bi-directional, deep recurrent neural networks.
In: INTERSPEECH 2014, pp. 2268–2272. ISCA (2014)
3. Hanzl´ıˇcek,Z.,V´ıt,J.,Tihelka,D.:LSTM-basedspeechsegmentationforTTSsyn-
thesis. In: Ekˇstein, K. (ed.) TSD 2019. LNCS (LNAI), vol. 11697, pp. 361–372.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-27947-9 31
4. Hirschberg, J., Prieto, P.: Training intonational phrasing rules automatically for
English and Spanish text-to-speech. Speech Commun. 18(3), 281–290 (1996)
5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural comput. 9(8),
1735–1780 (1997)
6. Hojo,N.,Ijima,Y.,Mizuno,H.:DNN-basedspeechsynthesisusingspeakercodes.
IEICE Trans. Inf. Syst. 101(2), 462–472 (2018)
7. J˚uzov´a, M.: Prosodic phrase boundary classiﬁcation based on Czech speech cor-
pora.In:Ekˇstein,K.,Matouˇsek,V.(eds.)TSD2017.LNCS(LNAI),vol.10415,pp.
165–173. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-64206-2 19
8. J˚uzov´a,M.:Onthecomparisonofdiﬀerentphraseboundarydetectionapproaches
trained on Czech TTS speech corpora. In: Karpov, A., Jokisch, O., Potapova, R.
(eds.) SPECOM 2018. LNCS (LNAI), vol. 11096, pp. 255–263. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-99579-3 27Speaker-Dependent BiLSTM-Based Phrasing 347
9. Louw,J.A.,Moodley,A.:Speakerspeciﬁcphrasebreakmodelingwithconditional
randomﬁeldsfortext-to-speech.In:2016PatternRecognitionAssociationofSouth
Africa and Robotics and Mechatronics International Conference, pp. 1–6 (2016)
10. Matouˇsek, J., Romportl, J.: Automatic pitch-synchronous phonetic segmentation.
In: INTERSPEECH 2008, pp. 1626–1629. ISCA, Brisbane, Australia (2008)
11. Matouˇsek, J., Tihelka, D., Psutka, J.: Experiments with automatic segmentation
forCzechspeechsynthesis.In:Matouˇsek,V.,Mautner,P.(eds.)TSD2003.LNCS
(LNAI), vol. 2807, pp. 287–294. Springer, Heidelberg (2003). https://doi.org/10.
1007/978-3-540-39398-6 41
12. Mishra,T.,Kim,Y.J.,Bangalore,S.:Intonationalphrasebreakpredictionfortext-
to-speech synthesis using dependency relations. In: ICASSP 2015, pp. 4919–4923
(2015)
13. Prahallad, K., Raghavendra, E.V., Black, A.W.: Learning speaker-speciﬁc phrase
breaks for text-to-speech systems. In: SSW (2010)
14. Read,I.,Cox,S.:Stochasticandsyntactictechniquesforpredictingphrasebreaks.
Comput. Speech Lang. 21, 3233–3236 (2005)
15. Taylor, P.: Text-to-Speech Synthesis, 1st edn. Cambridge University Press, New
York (2009)
16. Taylor,P.,Black,A.:Assigningphrasebreaksfrompart-of-speechsequences.Com-
put. Speech Lang. 12, 99–117 (1998)
17. Tihelka,D.,Hanzl´ıˇcek,Z.,J˚uzov´a,M.,V´ıt,J.,Matouˇsek,J.,Gr˚uber,M.:Current
state of text-to-speech system ARTIC: a decade of research on the ﬁeld of speech
technologies.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2018.LNCS
(LNAI),vol.11107,pp.369–378.Springer,Cham(2018).https://doi.org/10.1007/
978-3-030-00794-2 40
18. V´ıt,J.,Hanzl´ıˇcek,Z.,Matouˇsek,J.:Czechspeechsynthesiswithgenerativeneural
vocoder. In: Ekˇstein, K. (ed.) TSD 2019. LNCS (LNAI), vol. 11697, pp. 307–315.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-27947-9 26Phonetic Attrition in Vowels’ Quality
in L1 Speech of Late Czech-French
Bilinguals
B
Marie H´evrova´1,2( ), Tom´aˇs Boˇril1 , and Barbara Ko¨pke2
1 Faculty of Arts, Institute of Phonetics, Charles University,
Na´mˇest´ı Jana Palacha 2, 116 38 Praha 1, Czech Republic
marie.hevrova@univ-tlse2.com
2 URI Octogone-Lordat, Universit´e Toulouse II – Jean-Jaur`es,
5 all´ee Antonio Machado, 31058 Toulouse, France
Abstract. This study examines phonetic attrition of the ﬁrst lan-
guage (L1) aﬀected by second language (L2) in Czech speakers living
in Toulouse (late Czech-French bilinguals – CF). We compared the pro-
duction of vowels by 13 CF and 13 Czech monolinguals living in the
CentralBohemianRegion(C).CFhadbeenlivinginFranceforatleast
one year and started to learn French when they were more than 6years
old.BothCandCFwerespeakersofCommonCzech.Werecordedtheir
production in reading task and semi-spontaneous speech and performed
measurements of vowel formants.Results show a statistically signiﬁcant
diﬀerence between F1 of CF [a:] and F1 of C [a:], and between F3 of
CF [i:] and F3 of C [i:]. These ﬁndings are discussed in relation to the
perceptual approach suggesting that several vowels can be perceived as
diﬀerent in C and CF production.
· ·
Keywords: Phonetic attrition Vowels’ quality Late Czech-French
bilinguals
1 Introduction
Intensive use of an L2 can inﬂuence the speaker’s L1 at the phonetic level [14],
a phenomenon often branded as ﬁrst language phonetic attrition or phonetic
cross-linguistic inﬂuence. The former, ﬁrst language attrition, refers to the non-
pathologicaldeclineofpreviousL1languageskills[13],whichhappensasa“natu-
ralconsequenceofdecreaseinthe[L1]use”[12]andconsistsoflong-termchanges
due to extensive, and not necessarily recent, L2 contact [6]. The latter, cross-
linguistic inﬂuence (CLI), introduced by [24], refers to any kind of eﬀect that
Supported by a doctoral grant of the French Research Ministry, a Charles University
PhDscholarship,theCharlesUniversityprojectSVV–2019,Jazykana´strojeprojeho
zkouma´n´ı,andbytheCharlesUniversityprojectProgresQ10Languageintheshiftings
of time, space, and culture.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.348–355,2020.
https://doi.org/10.1007/978-3-030-58323-1_38Phonetic Attrition in Vowels’ Quality 349
one language may have on another. For [21], L1 attrition is one among these
possible kinds of eﬀect, a position we will adopt here.
For now, only a small part of studies in the area of phonetic attrition and
CLI examined vowels by acoustic measurement (see, e.g., [4,17]). In addition,
there is no study on the inﬂuence of L2 French on L1 Czech at the phonetic
level, although several interesting diﬀerences exist in the vowel systems of both
languages (see [16]). The present paper proposes to ﬁll this gap with a study
investigating phonetic attrition in vowels’ quality in the L1 speech of Czechs
who have been living in France for more than one year and started to learn
French after the age of six (henceforth CF, late Czech-French bilinguals).
1.1 Comparison of Czech and French Vowels
Without [@], Czech comprises 10 monophthongal not nasalized vowels [25] and
French 11 monophthongal not nasalized vowels [18]. Czech distinguishes short
and long vowels contrary to French, where vowel’s length is not a phonological
feature.ThearticulatoryfeaturesoftheseCzechandFrenchvowelsaredescribed
in Table 1 showing that these languages do not attribute the same articulatory
properties to [E], [a] and [o]. Some inconsistencies exist among authors in the
IPA symbols used for certain vowels (see [16]). We use the symbol [u], and not
[U], for Czech /u/ for the reason of simplicity and the symbol [E] for Czech /e/
becausethissoundisacousticallyslightlynearertoFrench[E]thantoFrench[e].
Table 1. Articulatory properties of Czech and French not nasalized monophthongal
vowels,(whitecolumn=Czechvowels,graycolumn=Frenchvowels).Vowelswiththe
same IPA symbol, but diﬀerent articulatory properties are in bold. Source [16,18]
Anteriority Front Central Back
Lip shape unrounded rounded unrounded rounded
Close I, i: i y u, u: u
Degree Close-mid e ø o
of Mid E, E: o, o:
aperture Open-mid E œ O
Open a a, a: A
Regarding the link between articulatory and acoustic properties of vowels,
the F1 is traditionally determined by degree of aperture and F2 by anteriority
andliparticulation[18,25].TheF3canalsobedeterminedbylipshape[18].[29]
also suggests to include F3 and F4 in acoustic studies of French vowels because
F4 with F3 makes a prominent energy packet in the high frequencies (F3/F4).
Therefore, in our study, we will analyse F1, F2, F3 and F4.
Based on the results of [10,20,26,27], Table 2 compares F1 and F2 means of
Czech and French not nasalized vowels. In the present study, we focus on the350 M. H´evrov´a et al.
productionofCF,allfemalespeakers,inareadingtask(hereafterRT)andsemi-
spontaneous speech (hereafter SS). Therefore, the formants obtained from the
productionofonlyfemalespeakersinRTsandSSsarecomparedinTable2.[26]
studiedCzechvowelsof48womenagedfrom20to30yearsreadingacontinuous
text. [27] analysed the production of 9 Frenchwomen reading themonosyllables
formed by either /pV1/, where V1 was /e/, /o/, /u/, /y/, or /ø/, or /pV2R/
where V2 was /i/, /E/, /a/, /O/, or /œ/. In [20], 10 Czech women aged 25–
34years commented spontaneously on 20 objects. [10] analysed a speech of 15
French women mainly extracted from broadcast news. We are conscious that
the F1 and F2 means in Table 2 cannot be considered as reference values for
any female speaker because each study used for the creation of Table 2 has its
limitations.Forexample,[28]reproaches[27]thatthe/R/usedincodaposition
could lengthen the previous vowel and consequently increase the F1 value and
decrease the F2 value.
Thefrequencydiﬀerencelimen(DLF)referstothediﬀerenceinthefrequency
values perceptible by the human ear [16]. The DLF for F1 is 10–30Hz and 20–
100Hz for F2 according to [9]. In Table 2, the F1 values of vowels that diﬀer in
Czech and French from 30–60Hz are in slight gray, and in dark gray when the
diﬀerence is higher than 60Hz. F2 values are in slight gray when the diﬀerence
between Czech and French is 100–200Hz, and in dark gray when the diﬀerence
is more than 200Hz. Table 2 does not contain the values of Czech [o:], as this
vowel, infrequent in Czech speech, will not be analyzed in our study. Regarding
F3 and F4, to the best of our knowledge, there is no study comparing these
formants of Czech and French vowels produced in RTs and SSs. Only studies of
formant values of vowels in isolation in Czech and French give means of F3 and
F4 for certain vowels [16,19].
Inourstudy,allCFwerelivingintheToulousearea.[8]supportsthatFrench
spokeninToulousediﬀersfromstandardFrenchalthoughmorethanonevariety
of Toulouse French exists [7,8]. For a majority of speakers from Toulouse, the
phonological diﬀerences between French [e] and [E], [œ] and [ø], [a] and [A], and
[O] and [o] are absent in minimal pairs [8], while other speakers from Toulouse
may respect these diﬀerences according to the position rule [7,8]. Thus, from
a phonological point of view, vowels in Toulouse French can diﬀer from vowels
of standard French. However, as far as we know, no study focused entirely on
acoustic properties of Toulouse French vowels. Hence, we can only suppose that
the Czech vowels of CF may be more inﬂuenced by vowels of Toulouse French
than standard French. However, no prediction about this can be made as an
acoustic study of Toulouse French vowels is lacking.
Taking into account all these considerations, we made the hypothesis that
thephoneticCLIismorelikelytooccurinvowelswhichareacousticallyslightly
dissimilar in French and in Czech and in vowels which exist only in one of both
languages.Phonetic Attrition in Vowels’ Quality 351
Table 2. F1 and F2 of Czech and French vowels for female speakers in RTs and SSs
according to [10,20,26,27]. (CZ=Czech, FR=French).
Readingtask Semi-spontaneousspeech
Formant F1 F2 F1 F2
Language CZ FR CZ FR CZ FR CZ FR
i NA 350 NA 2400 NA 348 NA 2365
i: 328.5 NA 2603 NA 287 NA 2504 NA
I 492.1 NA 2251.2 NA 411 NA 2177 NA
y NA 350 2050 NA NA 371 2063 NA
e NA 450 NA 2300 NA 423 NA 2176
E 686.3 650 1823 2000 650 526 1726 2016
E: 709.5 NA 1904.3 NA 671 NA 1825 NA
a 780.9 750 1480.2 1550 733 685 1322 1677
a: 801.2 NA 1417.6 NA 784 NA 1436 NA
ø NA 450 NA 1650 NA 420 NA 1693
œ NA 550 NA 1650 NA 436 NA 1643
u 415.3 350 1003.6 850 330 404 1221 1153
u: 343.6 NA 757 NA 341 NA 851 NA
o 528 450 1166.2 950 474 438 1161 1140
O NA 600 NA 1200 NA 528 NA 1347
2 Method
We recorded the Czech production in RT and in SS of 13 female native Com-
mon Czech speakers (mean=35.1years) living in the Central Bohemian region
of the Czech Republic (hereafter C) and 13 CF speakers of Common Czech
(mean=34.2years). All CF have not never lived in any region where some vari-
ety of Czech diﬀerent from Common Czech is spoken. They all declared not to
think to speak Czech with some speciﬁc accent as for example Moravian accent
in socio-linguistic form ﬁlled after recording. The average of their length of resi-
denceinFrancewas9.9years(min=1.42year,max=28.25years).AllCandCF
speakers were aged 20–50years, hence the stability of their f was assured [11].
0
In the RT, the speakers read a short text chosen from [5]. In the SS, they
talked for one minute and a half about one or more proposed topics such as
plans for holidays or the next weekend, describing a typical day, job, studies,
family, hobbies, etc. CF were recorded in a quiet recording studio (PETRA) at
University of Toulouse using a Neumann TLM 49 microphone and sound card
MOTUULmk3.Theyreceivedasmallrewardforparticipation.Cwererecorded
in a quiet, comfortably furnished oﬃce with a low level of ambient noise and
short natural reverberation in Prague. A head-mounted condenser microphone
(Bayerdynamic Opus 55) was plugged directly into a pocket recorder set to
uncompressed 48 kHz 16-bit mode.
Allrecordingswereorthographicallytranscribed.Theirsemi-automaticalseg-
mentation and labeling in Praat [2] were corrected manually. Vowels’ boundary
placement was guided by the presence of full formant structure. Initial glottal352 M. H´evrov´a et al.
stops and ﬁnal voice decay time were not considered to be part of the vowel.
Vowelsendingbytheschwaofhesitation,vowelsinforeignwordssuchasEnglish
names of movies or names of French cities, unpronounced and semi-pronounced
vowels in the recordings of SS were excluded from the analysis. Vowels preceded
orfollowedbynasalconsonantsinRTandinSSwereexcludedfromtheanalysis
too, since nasal context coarticulation may lead to uncontrolled extra formants.
The Czech conjunction /a/, meaning “and” in English, longer than 150ms was
considered as a hesitation and excluded from analysis (cf. [23]). The conjunc-
tion /a/ with duration lower than 150ms was labelled as a short Czech [a] and
included in analysis. Formants were measured automatically using Praat script
computing the mean of formant value from the second third of the vowel dura-
tion.Thisway,weresolvedtheissueoftheeﬀectofcoarticulationontheformant
value. In total, the analysis involved 10 147 vowels.
The data were analyzed in RStudio [22] using the packages lme4 [1], dplyr
[31], rPraat [3], and ggplot2 [30]. We computed the mean value of each formant
of each vowel for each task and each group separately. The signiﬁcance level
was set at α=0.05. In order to examine diﬀerences between C and CF vowels’
formant values, we performed linear mixed-eﬀects models for each formant of
each vowel. We analyzed the relationship between group and formants’ values.
We had intercepts for speakers and words of the vowel’s occurrence as random
eﬀects.Asﬁxedeﬀects,we enteredgroup andtask. Visual inspection of residual
plots did not reveal any obvious deviations from homoscedasticity or normality.
P-values were obtained by likelihood ratio tests of the full model with the eﬀect
in question against the model without the eﬀect in question.
3 Results
The analysis showed that the group aﬀected F1 of [a:] (χ2(1) = 5.6428,p =
0.01753) increasing the F1 value of CF by 51.17Hz ± 21.18 (standard errors).
This result is also visible on the Fig.1 showing F1 and F2 ﬁelds with values in
Hertz of C and CF obtained in RT and in SS. The group aﬀected also F3 of [i:]
(χ2(1)=7.5502,p=0.006) increasing the F3 value of CF by 114.41Hz ± 40.13
(standard errors). There were no other signiﬁcant results.
4 Discussion and Conclusions
Our study showed a signiﬁcant diﬀerence between the groups in F1 value of [a:]
and in F3 value of [i:]. By comparison of [a:] F1 value of our C in Table 3 with
its values in Table 2, we suppose that the C [a:] F1 corresponds to the standard
pronunciation of this vowel in Common Czech. Similarly, comparing the [i:] F3
value of our C with [20] results, we assume that the [i:] F3 of C corresponds to
the standard pronunciation of this vowel in Common Czech.
As shown by [14], studies of phonetic L1 attrition and CLI support two
possible explanations for sound changes: ‘assimilation’ and ‘dissimilation’. In
the former case, L1 sounds shift towards L2 sound’s norms. In the latter case,Phonetic Attrition in Vowels’ Quality 353
Fig.1. Czech vowels in RT (left) and in SS (right) plotted in the F1-F2 plane. The
ellipses indicate 50% of the formant values, shown in Hz
Table3.FormantvaluesofCandCFvowelswhicharesupposedtobeperceivedasdif-
ferent. (v=vowel, m=mean, CI=conﬁdence interval, light gray=C, dark gray=CF,
RT=reading task, SS=semi-spontaneous speech)
v F1m F1CI v F2m F2CI v F2m F2CI v F3m F3CI
RT SS SS RT
a: 791 767,815 i: 2421 2377,2465 E: 1746 1701,1792 i: 3236 3189,
3282
a: 881 855,907 i: 2515 2490,2541 E: 1912 1838,1986 i: 3368 3313,
3424
SS I 2023 2000,2046 a 1481 1465,1496 SS
I 439 434,444 I 2140 2115,2166 a 1589 1570,1608 i: 3115 3075,
3154
I 417 411,423 E 1831 1814,1848 i: 3320 3278,
3361
a: 778 765,791 E 1943 1924,1962
a: 861 844,879
thespeakertriestomaintainadiﬀerencebetweenL1andL2sound,whichleads
to deepening of the acoustic distance between these two sounds. In the light of
this suggestion, the signiﬁcant diﬀerence in [a:] F1 value between groups may
be considered as the result of dissimilation: the acoustic distance between CF
[a:] F1 and French [a] F1 is bigger than the diﬀerence between C [a:] F1 and
French [a] F1 (see Table2 and 3). For the CF [i:] F3 value, we can speak about
assimilation. According to the study of vowels in isolation [16], the F3 of French
[i] is signiﬁcantly higher than the F3 of Czech [i:]. Therefore, the F3 of CF [i:] is
probably inﬂuenced by French [i].
Usingaperceptualapproach,wesupposethatDLFcanpredictiftwosounds
will be perceived as the same or diﬀerent. Hence, Table 3 presents formants’
mean values for vowels which are expected to be perceived diﬀerently in C and354 M. H´evrov´a et al.
CF production: conﬁdence intervals of formants are not overlapping between C
and CF and the diﬀerence in formants’ mean value between C and CF is equal
to or higher than DLF. [a:] F1 and [i:] F3 have been already discussed above.
For the others, we suppose that, due to the assimilation, F1 and F2 of CF’ [I] is
probablyinﬂuencedbyFrench[i]aswellasF2ofCF’[E]and[E:]isbyFrench[E]
andF2ofCF’[a]byFrench[a].F2of[i:]ishigherinCFthaninCprobablydue
to a small dissimilation. Table 3 shows also that the diﬀerences between groups
in formant values, which can be perceived by the human ear, are more frequent
in SS than in RT. This observation is in agreement with the ﬁndings of [15].
Takentogether,thispapershowedtendenciesofphoneticattritiononvowels
in L1 production of CF, which are statistically signiﬁcant or perceptually pre-
dictable. The study of inter-speaker variation in the results should allow us to
better understand the results.
References
1. Bates,D.,M¨achler,M.,Bolker,B.,Walker,S.:Fittinglinearmixed-eﬀectsmodels
using lme4. J. Stat. Softw. 67(1) (2015)
2. Boersma, P., Weenink, D.: PRAAT: doing phonetics by computer (2019)
3. Boˇril,T.,Skarnitzl,R.:ToolsrPraatandmPraat.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,
I.,Pala,K.(eds.)TSD2016.LNCS(LNAI),vol.9924,pp.367–374.Springer,Cham
(2016). https://doi.org/10.1007/978-3-319-45510-5 42
4. Bullock,B.E.,Dalola,A.,Gerfen,C.:Mappingthepatternsofmaintenanceversus
mergerinbilingualphonology:thepreservationof[a]vs.[a]inFrenchvilleFrench.
In: Montreuil, J.P.Y. (ed.) New Perspectives on Romance Linguistics: Phonetics,
PhonologyandDialectology,pp.15–30.JohnBenjamins,Amsterdam/Philadelphia
(2006)
5. Cˇapek, K.: Jak se co dˇel´a. O lidech. Cˇeskoslovensky´ spisovatel, Praha (1960)
6. Chang, C.: Phonetic drift. In: Schmid, M.S., K¨opke, B. (eds.) The Oxford Hand-
book of Language Attrition, pp. 191–203. Oxford University Press, Oxford (2019)
7. Courd`es-Murphy, L.: Nivellement et sociophonologie de deux grands centres
urbains:Le syst`eme vocalique de Toulouseet de Marseille. Ph.D. thesis, Toulouse
2 (2018)
8. Durand, J.: Essai de panorama phonologique: Les accents du midi. In: Baronian,
L., Martineau, F. (eds.) Le fran¸cais d’un continent `a l’autre. M´elanges oﬀerts `a
Yves-Charles Morin, pp. 123–170. Presse de l’Universit´e Laval, Qu´ebec (2009)
9. Flanagan, J.: Speech Analysis; Synthesis and Perception. Springer-Verlag, Berlin,
New York (1972). https://doi.org/10.1007/978-3-662-01562-9
10. Gendrot, C., Adda-Decker, M.: Impact of duration on F1/F2 formant values of
oral vowels: an automatic analysis of large broadcast news corpora in French and
German. In: Interspeech 2005, pp. 2453–2456. Lisbon, Portugal (2005)
11. Hollien, H., Shipp, T.: Speaking fundamental frequency and chronologic age in
males. J. Speech Hear. Res. 15(1), 155–159 (1972)
12. K¨opke,B.:Firstlanguageattrition:frombilingualtomonolingualproﬁciency?In:
DeHouwer,A.,Ortega,L.(eds.)TheCambridgeHandbookofBilingualism.CUP,
Cambridge (2019)
13. K¨opke, B., Schmid, M.: Language attrition: the next phase. In: First Language
Attrition: Interdisciplinary Perspectives on Methodological Issues, pp. 1–43. John
Benjamins, Amsterdam (2004)Phonetic Attrition in Vowels’ Quality 355
14. de Leeuw, E.: Phonetic attrition. In: Schmid, M.S., K¨opke, B. (eds.) The Oxford
Handbook of Language Attrition, pp. 204–217. Oxford University Press, Oxford
(2019)
15. Major, R.C.: Losing English as a ﬁrst language. Mod. Lang. J. 76(2), 190–208
(1992)
16. Maurov´a Paillereau, N.: Perception et production des voyelles orales du fran¸cais
pardesfuturesenseignantestch`equesdeFran¸caisLangueEtrang`ere(FLE).Ph.D.
thesis, Sorbonne Paris Cit´e (2015)
17. Mayr, R., Price, S., Mennen, I.: First language attrition in the speech of Dutch-
English bilinguals: the case of monozygotic twin sisters. Bilingualism Lan. Cogn.
15(4), 687–700 (2012)
18. Meunier, C.: Phon´etique acoustique. In: Auzou, P. (ed.) Les dysarthries, pp. 164–
173. Solal (2007)
19. Paillereau, N., Skarnitzl, R.: An acoustic-perceptual study on Czech monoph-
thongs. In: Radeva-Bork, T., Kosta, P. (eds.) Current Developments in Slavic
Linguistics. Twenty Years After, pp. 453–465. Peter Lang, Berlin (2020)
20. Paillereau,N.,Chl´adkov´a,K.:SpectralandtemporalcharacteristicsofCzechvow-
els in spontaneous speech. AUC PHILOLOGICA 2019(2), 77–95 (2019)
21. Pavlenko,A.:L2inﬂuenceonL1inlatebilingualism.IssuesAppl.Linguist. 11(2)
(2000)
22. RCoreTeam:R:ALanguageandEnvironmentforStatisticalComputing.RFoun-
dation for Statistical Computing, Vienna, Austria (2019)
23. Ruboviˇcov´a, C.: Tempo ˇreˇci a realizace pauz pˇri konsekutivn´ım tlumoˇcen´ı do
ˇceˇstiny ve srovn´an´ı s p˚uvodn´ımi ˇcesky´mi projevy. Magistersk´a pra´ce. Univerzita
Karlova, Filozoﬁck´a fakulta, Foneticky´ u´stav, Praha (2014)
24. SharwoodSmith,M.:Cross-linguisticaspectsofsecondlanguageacquisition.Appl.
Linguist. 4(3), 192–199 (1983)
25. Skarnitzl, R., Sˇturm, P., Vol´ın, J.: Zvukov´a ba´zeˇreˇcov´e komunikace: Foneticky´ a
fonologicky´ popisˇreˇci. Karolinum, Praha (2016)
26. Skarnitzl,R.,Vol´ın,J.:Referenˇcn´ıhodnotyvokalicky´chformant˚upromlad´edospˇel´e
mluvˇc´ı standardn´ıˇceˇstiny. Akustick´e listy 18, 7–11 (2012)
27. Tubach, J.P.: La Parole et son traitement automatique. Paris Milan Barcelone,
Masson (1989)
28. Vaissi`ere, J.: Area functions and articulatory modeling as a tool for investigating
thearticulatory,acousticandperceptualpropertiesofsoundsacrosslanguages.In:
Sol´e,M.,Beddor,P.S.,Ohala,M.(eds.)ExperimentalApproachesPhonology,pp.
54–71. OUP, Oxford (2007)
29. Vaissi`ere, J.: On the acoustic and perceptual characterization of reference vowels
in a cross-language perspective. In: The 17th International Congress of Phonetic
Sciences (ICPhS XVII), pp. 52–59, China (2011)
30. Wickham, H.: ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag, New
York (2016). https://doi.org/10.1007/978-0-387-98141-3
31. Wickham,H.,Fran¸cois,R.,Henry,L.,Mu¨ller,K.:dplyr:Agrammarofdatamanip-
ulation, R package version 0.8.4 (2020)Assessing the Dysarthria Level
of Parkinson’s Disease Patients
with GMM-UBM Supervectors Using
Phonological Posteriors
and Diadochokinetic Exercises
B
Gabriel F. Miller1( ) , Juan Camilo Va´squez-Correa2,3 , and Elmar No¨th2
1 Multimedia and Signal Processing Lab, Friedrich-Alexander-Universit¨at,
Erlangen-Nu¨rnberg, Causerstr. 7, 91058 Erlangen, Germany
gabriel.f.miller@fau.de
2 Pattern Recognition Lab, Friedrich-Alexander-Universit¨at, Erlangen-Nu¨rnberg,
Martensstr 3, 91058 Erlangen, Germany
{juan.vasquez,elmar.noth}@fau.de
3 Faculty of Engineering, University of Antioquia UdeA, Medell´ın, Colombia
Abstract. Parkinson’s disease (PD) is a neuro-degenerative disorder
that produces symptoms such as tremor, slowed movement, and a lack
ofcoordination.Oneoftheearliestindicatorsisacombinationofdiﬀer-
ent speech impairments called hypokinetic dysarthria. Some indicators
that are prevalent in the speech of Parkinson’s patients include, impre-
cise production of stop consonants, vowel articulation impairment and
reducedloudness.Inthispaper,weexaminethosefeaturesusingphono-
logical posterior probabilities obtained via parallel bidirectional recur-
rent neural networks. We also utilize information such as the velocity
and acceleration curve of the signal envelope, and the peak amplitude
slope and variance to model the quality of pronunciation for a given
speaker. With our feature set, we train Gaussian Mixture Model based
UniversalBackgroundModelsforasetoftrainingspeakersandadapta
model for each individual speaker using a form of Bayesian adaptation.
With the parameters describing each speaker model, we train SVM and
RandomForestclassiﬁerstodiscriminatePDpatientsandHealthyCon-
trols (HC), and to determine the severity of dysarthria for each speaker
compared with ratings assessed by expert phoneticians.
1 Introduction
Parkinson’s disease (PD) is a neuro-degenerative disorder that produces symp-
toms such as tremor, slowed movement, and lack of coordination. It is believed
that the underlying cause for these physical symptoms is due to the immod-
erate spread of the protein α-synuclein throughout the peripheral and central
nervous systems (PNS, CNS). The general function of α-synuclein is to help
regulate the release of dopamine, a type of neurotransmitter that is critical for
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.356–365,2020.
https://doi.org/10.1007/978-3-030-58323-1_39Assessing the Dysarthria Level of PD Patients 357
controllingthestartandstopofvoluntaryandinvoluntarymovements[8].Inthe
case of PD patients, excessive α-synuclein begins to accumulate along the PNS
and CNS, and is toxic to aﬀected cells. This ultimately leads to a loss of neu-
ronal populations, in particular, dopaminergic neurons in the substantia nigra
and contralateral striatum, both being structures in the basal ganglia largely
responsible for regulating both our reward network and motor systems [15,18].
Currently, there is no treatment to halt or slow the progression of PD, though
there are several pharmacotherapeutic and neurosurgical options available that
oﬀeranalleviationofcertainsymptoms.Alargecontributingfactorfortherenot
being any stronger intervening methods, stems from the fact that the disease is
often diagnosed after roughly 50% of neurons in the substantia nigra have been
irrevocably damaged and over 80% of striatal dopamine has been depleted [17].
This highlights the importance of being able to identify the earliest symptoms
ofPDinordertobeproactiveinaddressingthediseaseintheprodromalphase.
This of course requires the ability to discern how severe symptoms of a given
patient are, and quantify it accurately and consistently.
Speech is well known to be one of the more complex motor skills that we
perform requiring precision from over 100 diﬀerent muscles. This in turn makes
ourvocalsystemoneofthemoresensitivemotorrelatedsystemstotheeﬀectsof
PD [4]. One of the earliest indicators of PD is a combination of diﬀerent speech
impairments called hypokinetic dysarthria that aﬀects roughly 90% of all those
diagnosedwithPD[3,9,10,17].Manystudieshavefocusedonidentifyingspeech
speciﬁc bio-markers that characterize these impairments and distinguish PD
patients from healthy controls (HC). Some of the more prevalent speech symp-
toms include: a hoarse sounding voice, imprecise production of stop consonants
(e.g. ‘p’, ‘t’, ‘k’), vowel articulation impairments, and reduced loudness [3,5].
With the aim to reduce the subjectivity of the clinical evaluation process
when diagnosing potential PD patients, researchers have pushed to develop
many diﬀerent signal processing and machine learning techniques to identify
PD. In[19], the authors consideredphonological posteriorfeatures (a setof vec-
tors that are used to express the pronunciation of a given speaker), recurrent
neural networks based on gated recurrent units, as well as Mel frequency cep-
strumcoeﬃcients(MFCCs)toassessspeechimpairmentsofPDpatients.Results
for assessing the diﬀerent levels of dysarthria were best when all features were
used. In [21], the authors proposed a series of diﬀerent features that were used
to predict the dysarthria level of both PDs and HCs, most notably articulation
features (i.e. formant frequencies, MFCCs and their derivatives) and i-vectors.
The scores were correlated with modiﬁed Frenchay-Dysarthria-Assessment (m-
FDA) scores (a modiﬁed version of the common rating system used to evaluate
dysarthria which is detailed in Sect2.1), and Spearman-correlation results of
up to 0.63 were achieved when using articulation features and 0.69 with the
i-vector approach.
In this paper, we consider several novel ways of characterizing PD, the ﬁrst
one being with phonological posterior features. We believe these features are
intuitive in nature; more so than features such as MFCCs or embeddings from358 G. F. Miller et al.
neural networks as they are much more interpretable for clinicians [19]. The use
ofphonologicalposteriorshasbeenconsideredforseveralapplicationsrelatedto
pathologicalspeech.In[20],theauthorsdevelopedatooltoextractphonological
posteriors directly from speech signals utilizing a bank of parallel bidirectional
recurrent neural networks. These networks estimate the posterior probabilities
oftheoccurrenceofdiﬀerentphonologicalclasses,withareportedaccuracyover
90%. Another set of features we consider are statistics derived from the ampli-
tudeenvelopeofspeechsignalsandmodeledasakinematicsystem.Suchsystems
are said to capture the smoothed amplitude ﬂuctuation pattern over time, illus-
trating how energy is distributed across a given signal [7]. These statistics have
been shown to hold a correspondence to the kinematic statistics of the lower lip
(i.e. lip velocity and acceleration), which in turn has been noted to be greater
in speakers with PD relative to normal geriatrics [5].
The rest of the paper is organized as follows: in Sect.2.1, we consider the
materials and methods used in the evaluation process such as the data set and
evaluation methods. In Sect.2.2, we go more in-depth into the features that
were considered. In Sect.2.3, we discuss the classiﬁcation method used to eval-
uate the aforementioned features; in particular their ability to classify PD and
HC patients. We also detail how our classiﬁer was used to identify the severity
of the dysarthria of all patients. In short, this was done by training Gaussian
Mixture Model based Universal Background Models (GMM-UBMs) for a set of
training speakers, GMM-UBMs being one of the more dominant techniques for
text-independentspeakerrecognition[14].Wethenadaptamodelforeachindi-
vidual speaker using a maximum a-posterior adaption process [14], and build
GMM-UBM Mean Interval (GUMI) supervectors. We then use these supervec-
tors to derive a kernel that measures the statistical dissimilarity between the
UBMandagivenadaptedspeakerdistributionbasedontheBhattacharyyadis-
tance [23]. Finally, we train SVM and Random Forest Classiﬁers based on the
GUMI supervectors, to classify both PD and HC patients and to assess their
respective severity of dysarthria. In Sect.3 we look at the correlation between
m-FDAscoresandthediﬀerentfeaturesconsidered,aswellasdiscusstheresults
of our classiﬁer in making the binary distinction between PD and HC patients,
as well as predicting the dysarthria level via the m-FDA. It was found that the
best Spearman correlation score when correlating the predicted m-FDA scores
with the actual scores was roughly 0.6. These results are competitive to what
has been achieved in studies that used more complex features.
2 Procedural Overview
2.1 Dataset and Evaluation Methods
The speech quality of patients is commonly addressed using diadochokinetic
(DDK) exercises, which consist of the rapid repetition of syllables like /pa-
ta-ka/ [22]. The exercise is particularly helpful in evaluating PD speakers, as
it requires the continuous movement of diﬀerent articulators such as the lips,
tongue and velum. The performance of DDK tasks are variable with the timingAssessing the Dysarthria Level of PD Patients 359
in which patients perform the exercises, e.g. if a patient has just taken their
medication, they generally tend to perform better. Though this was in a sense
controlled for, it still is worth mentioning.
In this paper we consider a set of 100 speakers from the PC-GITA cor-
pus [12] (50 PD patients and 50 HC subjects), all of them Colombian Spanish
native speakers. All participants were recorded pronouncing the phrase /pa-
ta-ka/ repeatedly and were evaluated remotely by clinicians with the m-FDA
scale to measure their dysarthria severity [19]. Though the Movement Disor-
der Society–Uniﬁed Parkinson’s Disease Rating Scale (MDS-UPDRS-III) is the
commonly used scale to evaluate potential PD patients, it only utilizes one item
out of 33 to evaluate speech. The FDA scale on the other hand covers a wide
rangeofspeech-relatedmotoractionsincludingreﬂex,respiration,lipmovement,
palate movement, laryngeal capacity, tongue posture/movement, intelligibility,
and swallowing. However, in many cases travelling from home to a clinic is not
possible, e.g. for those who live in more rural areas, or for many PD patients
in intermediate or advanced stages who typically have a reduced mobility. To
overcome some of these issues, the m-FDA scale was introduced. The scale was
designed speciﬁcally such that it could be administered remotely and thus only
considers speech recordings of a given patient and ignores tests such as swal-
lowing which requires a clinician to be present in order to make an evaluation.
Some of the diﬀerent categories evaluated include respiration, lip movement,
palate/velum movement, laryngeal movement, intelligibility, and monotonicity.
In total, the scale considers 13 items, each of which are assigned scores from
0 (completely healthy) to 4 (very impaired). Thus the scale ranges from 0 to
52 [21].
It’s worth noting that the evaluation of HC patients we considered included
smokers,whoexhibitsomespeechqualitiescommonlyfoundinPDpatients(e.g.
hoarseness, or shortness of breath).
2.2 Features
Phonological Features. Forpathological speechprocessing, only asmall sub-
setofbasicfeaturesarecommonlyused,e.g.fundamentalfrequency,jitter,shim-
mer, or formant frequencies. More complex feature sets like MFCCs, Perceptual
Linear Predictors, or embeddings from neural networks are sometimes avoided
in this context due to their lack of interpretability [21]. Phonological features
are believed to be a viable feature candidate when it comes to speech pathol-
ogy, as they are commonly understood by clinicians as features describing the
movements of the articulators in the vocal tract and also are robust in terms of
relaying information about the dysarthria level of a patient. Phonological fea-
tures are generally represented by phonological posteriors, the probabilities of
phonological classes inferred from a given speech signal [1].
In our study, we consider classes that correspond to both plosive and voiced
segments of speech. In particular we consider the following phonological classes:
‘stop’ and ‘consonantal’ (classes that include phonemes /p/, /t/, and /k/),
‘back’, and ‘open’ classes (classes that include /a/), and also the ‘anterior’ and360 G. F. Miller et al.
‘close’ classes (classes containing vowels /e/ and /i/ and /i/ and /u/ respec-
tively). The ‘stop’, ‘consonantal’, ‘back’ and ‘open’ phoneme classes were all
chosen as they contain phonemes that makeup the utterance, /pa-ta-ka/. Thus,
the expectation for the class posteriors was that they would be higher for less
dysarthric patients who are able to fully articulate each syllable. The ‘close’,
and‘anterior’classesontheotherhandwereusedtoidentifyspeakersthathave
diﬃculty in fully articulating the voiced segments of /pa-ta-ka/. Impairment of
vowel articulation, is said to occur as a consequence of a reduced articulatory
range of motion, i.e. “undershooting” of articulatory gestures, which is said to
be a consequence of PD [9]. The phonological features were extracted using the
Phonet toolkit [20], which is freely available and based on recurrent neural net-
works with gated recurrent units.
Kinematic Features. One of the more important aspects for qualitatively
assessing dysarthria is rhythm [3,9,10]. It is well known that the pulmonic air
pressure is the primary energy source of speech, and the amplitude modulation
describing the pressure and rhythm for a given speaker is largely determined by
articulatorybehaviors,especiallymandibleandlipmovements[7].Furthermore,
itisnotedin[7]thatthekinematicparametersoftheamplitudeenvelopecapture
the smooth amplitude ﬂuctuation pattern over time and indicate how energy is
distributed across a given signal.
Inpractice,thekinematicparametersoftheamplitudeenvelopeareobtained
by applying the Hilbert transform to the signal, taking the complex-modulus,
applying a low pass ﬁlter, and then normalizing to consider any potential gain
factor [7]. The result is known as a displacement curve. We then take the ﬁrst
and second order diﬀerence to obtain the envelope velocity and acceleration.
Itwasshownin[2,7]howtheseparameterscorrespondtoarticulatorymove-
ments during speech. Both papers assert that speaker-speciﬁc articulatory kine-
matics, including velocity, acceleration and spatial displacement reﬂect speaker
individualitybecauseofanatomicalidiosyncrasiesofthearticulatorsandtheway
speakers acquired control over them. In particular, the authors in [2] noted that
the amplitude envelope co-varied with the area of a speaker’s mouth opening
(i.e. larger mouth opening areas correlated with peaks in the amplitude enve-
lopedisplacementcurve),andtheauthorsin[7]notedacorrespondencebetween
the amplitude envelope and lower lip kinematics. This was done by comparing
the peaks and troughs of the displacement curve to the signal measurements of
a speaker’s lip and mandible movement measured by x-ray microbeams. It was
shown that the two signals displayed a strong inverse correspondence, meaning
peaks in the ﬁrst and second order diﬀerences of the amplitude envelope typi-
cally matched with troughs in the lower lip displacement curve and vice-versa.
Thoughtheauthorsheredidnotconsiderspeakerswithdysarthriainparticular,
ithasbeenshowninpriorresearchthatthelowerlipclosingvelocitiesexpressed
as a function of movement amplitude are greater for PD speakers than for nor-
mal geriatrics [5]. It was also noted in this study that the increased velocity ofAssessing the Dysarthria Level of PD Patients 361
lower lip movement may reﬂect a diﬀerence in control of lip elevation for PD
speakers, an eﬀect that increases with the severity of dysarthria.
Spectrogram Slope and Peak Standard Deviation. One characteristic
common in Parkinson’s disease speakers is an inconsistency of speech pattern
in the subject speaker [3]. In order to model this, we apply a peak detection
algorithm to the spectrogram of each speaker, and take the slope of a ﬁtted
line through these peaks. We also utilize the standard deviation of the peak
amplitudes in order to evaluate the stability in the energy distribution for the
diﬀerentsyllablesintheDDKtask.Asnotedin[3],previousﬁndingsofacoustic
and kinematic studies report a reduced amplitude and velocity of articulators
(lips, tongue, jaw) for PD patients, suggesting that articulation deﬁcits reﬂect
hypokinesia and rigidity of the vocal tract. We hypothesized the slope and peak
amplitudeinformationwouldconveythis,withspeechoutputvolumedecreasing
over time (corresponding to a more negative slope), and a higher variation in
peakamplitudes(correspondingtoahighervariabilityinspeechenergyoutput).
2.3 Classiﬁcation Method
GMM-GUMI Supervectors. One of the most common and eﬀective tools
usedfortext-independentspeakerrecognition,hasbeenGMM-UBMs[13].These
modelsarewellknownfortheireﬀectivenessandscalabilityinmodelingthespec-
tral distribution of speech [23]. In this approach, speaker models are obtained
fromtheadaptationofaGMM-UBMthroughthemaximuma-posteriori(MAP)
criterion. The GMM-UBM is usually trained by means of the expectation-
maximization (EM) algorithm from a background data set, which includes a
range of diﬀerent speakers, and produces a set of parameters, namely mean vec-
tors, covariance matrices and mixture weights, that characterize a speaker set.
Forourconsiderations,webuildthreediﬀerenttypesofUBMs,oneconsisting
of only PD patients, one with HC, and one using a combination of both. We
train each UBM based on the aforementioned features, with an optimal number
ofGaussiancomponentschosenvia10foldcrossvalidation.FromtheUBMs,we
adapt speaker models for all patients in a leave two out manner (one PD and
one HC patient left out per iteration), and build a GUMI supervector (stacked
meanintervalsoutputfromtheBayesianadaptationprocessasdetailedin[23]).
We then classify PD patients and HC subjects using the GUMI supervectors
and two diﬀerent classiﬁcation strategies: an SVM with a Bhattacharyya based
kernel,andarandomforest.Wealsoconsiderasupportvectorregression(SVR)
approach and a random forest-based regression to predict the m-FDA score
assigned to the speakers by expert phoneticians.
3 Results
All extracted features were assigned a Spearman correlation score (with respect
tothem-FDAsofallpatients).Themeanandstandarddeviationforallfeatures362 G. F. Miller et al.
Fig.1.Correlationbetweenallfeaturesconsideredandm-FDAscoresforthetotalset
ofspeakers(left),andofthe90thpercentileofextremecases(m-FDAscoreslessthan
or equal to 2 or greater than or equal to 40) (right).
wereestimatedoverconsecutiveutterancesofthephrase/pa-ta-ka/.Resultscan
be seen in Fig.1. The average and standard deviation of the posterior proba-
bility output for the ‘close’ class (containing vowels /e/ and /i/) were among
the highest absolute correlative scores with the m-FDA (0.49 for the mean and
0.45 for the standard deviation). The positive correlation is a result of the fact
that patients that had a higher average posterior probability of uttering vowels
/e/or/i/ratherthan/a/arelikelymoredysarthric,meaningtheywouldhypo-
thetically have had a higher m-FDA score assigned to them. When we zoom
in and look at the more extreme cases, (i.e. speakers with m-FDA scores less
than or equal to 2 and greater than or equal to 40 which represents 10% of the
overall set), correlative scores are higher. The ‘anterior’ class, which is said to
be characterized by an obstruction located in front of the palato-alveolar region
of the mouth [19] which also characterizes a speaker’s inability to formulate the
/a/soundconsistently,similarlyperformedwell(correlationof0.44,0.43forthe
mean, and standard deviation respectively for all cases and 0.57 and 0.65 in the
moreextremecases).Kinematicfeaturesalsoexhibitedhighabsolutecorrelative
scores(-0.6 inthemostextremecases).This canbeattributed tothelinknoted
in [7] that the ﬁrst and second order diﬀerences in the amplitude envelope for a
given speaker inversely correspond to the velocity and acceleration of lower lip
movement which as noted are typically higher in PD patients [5].
The results for binary classiﬁcation (PD vs. HC) and for predicting the m-
FDA scores of all speakers are shown in Table 1. Results are observed for each
typeofUBM,andforeachclassiﬁer.Themostaccuratepredictorwithrespectto
binaryclassiﬁcationwasachievedwithanHCbasedUBMandanSVMclassiﬁer
(accuracy of 89%). This same model and classiﬁer achieved the highest correla-
tion predicting the m-FDA score (0.6). The performance of the HC based UBMAssessing the Dysarthria Level of PD Patients 363
can likely be attributed to the fact that range of m-FDA scores for PD patients
was much more variable compared to HC patients. Thus, when training on only
HC patients, there is a much clearer distinction for the classiﬁer to recognize.
Table 1.ResultsofPD,HCandPD+HCbasedUBMstoclassifypatientsusingSVM
andRandomForest(RF)classiﬁers.Resultsincludetheaccuracy(ACC)classifyingPD
and HC subjects, and the Spearman’s correlation coeﬃcient ρ predicting the m-FDA
score of the participants.
ACC. ρ ACC. ρ
RF RF SVM SVM
PD-based UBM 78.0 0.43 75.0 0.33
HC-based UBM 63.0 0.42 89.0 0.60
PD+HC-based UBM 63.0 0.26 72.0 0.24
4 Conclusions
In this paper we considered a set of diﬀerent features that are useful in distin-
guishing PD and HC patients, and in assessing the severity of a given patient’s
speech dysarthria. The features were chosen such that they were both com-
prehensive and easily interpretable. It was found that the average back, open,
anterior and close phoneme class posterior probabilities, as well as the average
andstandarddeviationoftheﬁrstandsecondorderenvelopekinematicparame-
terswereamongthestrongestindicators,asseenbythecorrelationwithpatient
m-FDA scores. Those features were also shown to be strongly correlated when
considering the most extreme cases (i.e. patients with m-FDA scores above and
belowcertainthresholds).Diﬀerentmethodsforclassiﬁcationwerealsodiscussed
andusedtoshowthattheconsideredfeatureswereinfactusefulforclassiﬁcation
purposes, as well as dysarthria assessment. The best results were found to be
competitive with those obtained in studies that utilized more complex features.
The results obtained with our proposed method are similar and comparable
to others reported in the literature when the same data was used, and which
considered diﬀerent sets of features. For instance, features based on phonation,
articulation, and prosody [12], features based on Gaussian mixture model rep-
resentations [11], features based on nonlinear dynamics [6], and empirical mode
decomposition [16], among others.
Moving forward, studies considering hybrid models that utilize features such
as MFCCs, or neural network embeddings as well as more easily interpretable
features, such as those we considered in this paper, should be done. In addition,
it would be of use to consider a wider variety of languages to ensure the results
obtained are consistent in diﬀerent contexts.364 G. F. Miller et al.
References
1. Cernak,M.,Orozco-Arroyave,J.,Rudzicz,F.,Christensen,H.,V´asquez-Correa,J.,
No¨th,E.:CharacterisationofvoicequalityofParkinson’sdiseaseusingdiﬀerential
phonological posterior features. Comput. Speech Lang. 46, 196–208 (2017)
2. Chandrasekaran,C.,Trubabnova,A.,S´ebastien,S.,Caplier,A.,Ghazanfar,A.:The
natural statistics of audiovisual speech. PLoS Comput. Biol. 5, e1000436 (2009)
3. Chenausky, K., MacAuslan, J., Goldhor, R.: Acoustic analysis of PD speech.
Parkinson’s Dis. (2011)
4. Duﬀy, J.: Motor Speech Disorders: Substrates, Diﬀerential Diagnosis, and Man-
agement. Elsevier Health Sciences, Amsterdam (2013)
5. Forrest,K.,Weismer,G.,Turner,G.:Kinematic,acoustic,andperceptualanalyses
of connected speech produced by Parkinsonian and normal geriatric adults. J.
Acoust. Soc. Am. 85, 2608 (1989)
6. Godino-Llorente,J.,Shattuck-Hufnagel,S.,Choi,S.,Moro-Velazquez,L.,Gomez-
Garcia, J.: Towards the identiﬁcation of idiopathic Parkinson’s disease from the
speech. New articulatory kinetic biomarkers. PloS one 12, e0189583 (2017)
7. He, L., Dellwo, V.: Amplitude envelope kinematics of speech signal: parameter
extraction and applications. In: 28. Konferenz Elektronische Sprachsignalverar-
beitung 2017, Saarbru¨cken (2017)
8. Hornykiewicz,O.:BiochemicalaspectsofParkinson’sdisease.Neurology51,S2–S9
(1998)
9. Lai,B.,Joseph,K.:EpidemiologyofParkinson’sdisease.BCMed.J. 43,133–137
(2001)
10. Lansford, K., Liss, J., Caviness, J., Utianski, R.: A cognitive-perceptual approach
toconceptualizingspeechintelligibilitydeﬁcitsandremediationpracticeinhypoki-
neticdysarthria.In:CommunicationImpairmentsinParkinson’sDisease,vol.2011
(2011)
11. Moro-Velazquez, L., et al.: A forced gaussians based methodology for the diﬀer-
ential evaluation of Parkinson’s disease by means of speech processing. Biomed.
Signal Process. Control 48, 205–220 (2019)
12. Orozco-Arroyave, J.R., Arias-London˜o, J.D., Vargas-Bonilla, J.F., Gonzalez-
R´ativa, M.C., N¨oth, E.: New spanish speech corpus database for the analysis of
people suﬀering from Parkinson’s disease. In: Proceedings of the Ninth Interna-
tional Conference on Language Resources and Evaluation (LREC 2014), pp. 342–
347 (2014)
13. Reynolds, D.: Comparison of background normalization methods for text-
independent speaker veriﬁcation. In: Proceedings of the European Conference on
Speech Communication and Technology, pp. 963–966 (1997)
14. Reynolds, D., Quatieri, T., Dunn, R.: Gaussian Mixture Models, pp. 659–663.
Springer, Boston (2009). https://doi.org/10.1007/978-0-387-73003-5
15. Rodriguez-Oroz, M., et al.: Initial clinical manifestations of Parkinson’s disease:
featuresandpathophysiologicalmechanisms.In:TheLancetNeurology,vol.8,pp.
1128–1139. Lippincott-Raven (2009)
16. Rueda,A.,V´asquez-Correa,J.,Rios-Urrego,C.,Orozco-Arroyave,J.,Krishnan,S.,
No¨th,E.:Featurerepresentationofpathophysiologyofparkinsoniandysarthria.In:
Proceedings of INTERSPEECH, pp. 3048–3052 (2019)
17. Rusz,J.:DetectingspeechdisordersinearlyParkinson’sdiseasebyacousticanal-
ysis. J. Acoust. Soc. Am. (2018)Assessing the Dysarthria Level of PD Patients 365
18. Su¨dhof,T.:Basicneurochemistry:molecular,cellularandmedicalaspects.In:Neu-
rology 1998. Lippincott-Raven (1999)
19. Va´squez-Correa, J.C., Garcia-Ospina, N., Orozco-Arroyave, J.R., Cernak, M.,
No¨th,E.:PhonologicalposteriorsandGRUrecurrentunitstoassessspeechimpair-
ments of patients with Parkinson’s disease. In: Sojka, P., Hor´ak, A., Kopeˇcek, I.,
Pala,K.(eds.)TSD2018.LNCS(LNAI),vol.11107,pp.453–461.Springer,Cham
(2018). https://doi.org/10.1007/978-3-030-00794-2 49
20. Va´squez-Correa, J., Klumpp, P., Orozco-Arroyave, J., N¨oth, E.: Phonet: a tool
based on gated recurrent neural networks to extract phonological posteriors from
speech. Proc. Interspeech 2019, 549–553 (2019)
21. Va´squez-Correa, J., Orozco-Arroyave,J., Bocklet, T., N¨oth, E.: Towardsan auto-
matic evaluation of the dysarthria level of patients with Parkinson’s disease. J.
Commun. Disord. 76, 21–36 (2018)
22. Va´squez-Correa,J.C.,Rios-Urrego,C.D.,Rueda,A.,Orozco-Arroyave,J.R.,Krish-
nan,S.,N¨oth,E.:Articulationandempiricalmodedecompositionfeaturesindiado-
chokinetic exercises for the speech assessment of Parkinson’s disease patients. In:
Nystro¨m,I.,Hern´andezHeredia,Y.,Mili´anNu´n˜ez,V.(eds.)CIARP2019.LNCS,
vol. 11896, pp. 688–696. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-33904-3 65
23. You,C.,Lee,K.A.,Li,H.:GMM-SVMkernelwithaBhattacharyya-baseddistance
for speaker recognition. IEEE Trans. Audio Speech Lang. Process. 18, 1300–1312
(2010)Voice-Activity and Overlapped Speech
Detection Using x-Vectors
Jiˇr´ı Ma´lek(B) and Jindˇrich Zˇdˇa´nsky´
Institute of Information Technologies and Electronics,
Technical University of Liberec, Studentsk´a 2, 46010 Liberec, Czech Republic
{jiri.malek,jindrich.zdansky}@tul.cz
Abstract. The x-vectors are features extracted from speech signals
using pretrained deep neural networks, such that they discriminate well
amongdiﬀerentspeakers.Theirmainapplicationliesinspeakeridentiﬁ-
cation and veriﬁcation. This manuscript studies, which other properties
areencodedinx-vectors.Thefocusliesondistinguishingbetweenspeech
signals/noiseandutterancesofasinglespeakerversusoverlapped-speech.
We attempt to show that the x-vector network is capable to extract
multi-purpose features, which can be used by several simple back-end
classiﬁers. This means a common feature extracting front-end for the
tasksofvoice-activity/overlapped speechdetectionandspeakeridentiﬁ-
cation.Comparedtothealternativestrategy,thatistrainingofindepen-
dent classiﬁers including feature extracting layers for each of the tasks,
the common front-end saves computational time during both training
and test phase.
· ·
Keywords: Voice activity detection Overlapped speech detection
·
x-vectors Time-delayed deep neural networks
1 Introduction
The goal of speaker embeddings is to map utterances to ﬁxed-dimensional
vectors which encode characteristics of the given speaker. The concept of
speaker embeddings has been introduced for tasks such as speaker recogni-
tion[23]anddiarization[7].Severalembeddingvariantsexistsuchasi-vectors[4]
stemming from Gaussian-Mixture-Model-based Universal-Background-Model
(GMM-UBM, [20]) or embeddings derived from Deep Neural Networks (DNN).
TheDNN-basedfeaturesdiﬀermostlybytopologyofthenetworkusedtoextract
them. The embeddings derived from fully-connected DNN (sometimes abbre-
viated as d-vectors) were proposed in [24], whereas utterance-based embed-
dings for analysis of sequences via long-short term memory (LSTM) networks
were presented in [2,11]. The x-vectors studied in this work were introduced
in [7,23]. These attempt to alleviate complicated training of sequence-based
approachesandyetutilizethecontextcontainedintheutterance.Thex-vectors
are extracted using the time-delayed DNN (TDNN) topology proposed in [18].
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.366–376,2020.
https://doi.org/10.1007/978-3-030-58323-1_40Voice-Activity and Overlapped Speech Detection Using x-Vectors 367
Although the embeddings/x-vectors are primarily trained to contain infor-
mation about the speakers, other properties are encoded within as well and are
subject of studies. The authors in [26] study i-vectors and d-vectors from [2,24]
to ﬁnd suitability of the embeddings to classify qualities like utterance length,
channel information, speaker gender or speaking rate. Recent work in [19] ana-
lyzes directly the x-vectors and next to to the already mentioned properties
studies also environmental eﬀects. This includes session identiﬁcation (diﬀerent
instances of single speaker occurrence) and classiﬁcation of background noise
used for augmentation.
Thespeakercharacteristicsencodedintheembeddingcanbeutilizedtosolve
several problems besides already mentioned speaker identiﬁcation and diariza-
tion. Fully supervised extraction of target speaker via pretrained DNN-based
beamformer was presented in [30,31]. Blind source separation-based extraction
of the desired speaker using x-vectors was proposed in [13]. In the context of
speech recognition and voice assistant design, the adaptation of general multi-
condition acoustic model for speciﬁc speaker using embeddings was presented
in [29]. Application to speaker adaptive speech synthesis was presented using
i-vectors, d-vectors and LSTM-based embeddings in [5,6,27], respectively.
This manuscript investigates, whether the x-vectors encode information
aboutotherspeech-relatedproperties,speciﬁcallyabsenceofspeech(non-speech)
andpresenceofmultipleactivespeakers(overlappedspeech/cross-talk).Conven-
tional approach to detection of these phenomenons lies in training of Gaussian
mixture model (GMM,[15]) using pre-designed features. To this end, kurtosis,
spectral ﬂatness measure and mel frequency cepstral coeﬃcients (MFCC) were
utilized in [22,28]. Recently, DNN-based approaches were introduced in [14,21].
Ourmotivationforutilizationofx-vectorsforthesetaskscomparedtotrain-
ingofadedicatedclassiﬁerliesinpossibilityofsharingcommonfeatureextract-
ing front-end for several classiﬁcation tasks, such as speaker identiﬁcation and
cross-talk/non-speech detection. This conﬁguration lowers the computational
demandsofclassiﬁcationduringbothtrainingandtestphase.Theextensivefea-
tureextractoristrainedonceandtheratherundemandingback-endisretrained
formultiplepurposes.Suchjointclassiﬁerisbeneﬁcialforspeakerdiarization[7]
or when utilizing blind extraction methods for speech enhancement [13]. With-
out additional information, blind methods extract arbitrary speech source. It
is thus beneﬁcial to have prior information, whether to apply the extraction
(i.e., detect speech segments with background noise and cross-talk segments)
and whether the target speaker is active at all (i.e., perform identiﬁcation of an
active speaker).
We investigate an utterance-wise classiﬁcation, where the whole utterance is
assigned to one of three classes (non-speech, speech, cross-talk) and frame-wise
classiﬁcation,whereeachframeoftheaudio-signalisassignedindependently.We
compare several variants of augmentation for the training datasets of x-vector
DNNandinvestigatefunctionalityoftheresultingnetworkswithrespecttopres-
ence of distortions (reverberation and background noise) in the test utterances.368 J. M´alek and J. Zˇdˇ´ansky´
Moreover, several conﬁgurations of the back-end classiﬁers are discussed, with
respect to accuracy and complexity of the classiﬁer.
2 Datasets and Methods
2.1 The x-Vector DNN
Ourimplementationofthex-vectorDNN,describedinTable1,comesfrom[23].
Its input consists of a single-channel audio signal, i.e., no spatial information is
used. The input features are 40 ﬁlter bank coeﬃcients computed from frames
of length of 25 ms and frame-shift of 12.5 ms. The TDNN (time-delayed DNN)
layers introduced in [18] operate on frames with a temporal context centered on
the current frame (cid:2). The TDNN layers build on top of the context of the earlier
layers, thus the ﬁnal context is a sum of the partial ones.
In contrast to [23], we introduced four diﬀerences in the DNN: 1) We use
all the frames in the context (and usually longer context) without any sub-
sampling in order to exploit the time-dependencies in the signal. 2) To reduce
the number of trainable parameters arising from the longer context, we weight
all the frames in the context by a trainable matrix at the input of each TDNN
layer and perform mean time-pooling. 3) We replaced the rectiﬁed linear units
at the output of TDNN and fully-connected layers by exponential linear units
(ELU),whichspeedsupconvergenceinourcase.4)Thepoolinglayercomputes
onlymeansofframes(variancesareomitted)inthecontext,whichisduringthe
training phase set to Lc =101.
The DNN was trained to classify N speakers and possibly non-speech class.
Thetrainingexamplesconsistedof201framesoffeaturesandthespeakerlabel.
Table 1. Description of the DNN producing the x-vectors. The input size for the
TDNN layers is stated after the mean pooling operation.
Layer Layer context Total context Input×output
TDNN 1 (cid:2)±80 161 40×1024
TDNN 2 (cid:2)±4 169 1024×768
TDNN 3 (cid:2)±4 177 768×512
TDNN 4 (cid:2)±4 185 512×384
TDNN 5 (cid:2)±4 193 384×256
TDNN 6 (cid:2)±4 201 256×128
Fully-conn. 1 (cid:2) 201 128×128
Pooling (cid:2)± Lc−1 max(201,Lc) (Lc·128)×128
2
Fully-conn. 2 (cid:2) max(201,Lc) 128×128
Softmax − max(201,Lc) 128×NVoice-Activity and Overlapped Speech Detection Using x-Vectors 369
2.2 Back-End Classiﬁers
To process the x-vector embeddings, we utilize three classiﬁers, which were pre-
viously used either directly for cross-talk detection or speaker identiﬁcation;
namely GMM [15], probabilistic linear discriminant analysis (PLDA, [12]) and
fully-connected DNN [9]. We assign into three classes: 1) non-speech, which
includes silence and noise-only audio, 2) speech, corresponding to utterances
of a single person and 3) overlapped speech/cross-talk, for simultaneous talk-
ing of two people. The x-vectors are length-normalized prior back-end classiﬁer
training. For speciﬁc information on training dataset, see Sect.2.3.
We train the GMM via the maximum-likelihood approach using the
expectation-maximizationalgorithm.Themodelsemployfullcovariancematrix.
The number of components for each class is selected in interval 1−25, via mini-
mization of the Akaike information criterion (AIC). The GMMclassiﬁer trained
in this manner is rather large, featuring from 490k−655k of free parameters.
The precise number varies for each of the x-vector variants. In order to reduce
the number of model parameters, we employ optional feature-vector dimension-
alityreductionviaLinearDiscriminantAnalysis(LDA,eﬃcientimplementation
from[3]).Sincethreeclassesareclassiﬁed,thereducedfeature-vectordimension
is 2 and the classiﬁer consists of 15 free parameters.
We utilize the PLDA classiﬁer in a similar form, which is widely applied
in speaker identiﬁcation and diarization scenarios. Here, a hypothesis is tested
whethertheembeddingofanunknowntestexampleisproducedbyanyofknown
classes(speakers).Theknownspeakersarerepresentedbyembeddingscomputed
from short clean utterances called enrollments. In this form, the PLDA has
advantage that it allows classiﬁcation of a class (speaker) unseen in the training
phase (open class-set). This advantage is not exploited for the given task, since
hereweworkwithknownclasses/closedclass-set.Wedonotperformanyfeature
dimensionality reduction prior the PLDA modeling, which results into models
with about 65k parameters. The selection of the enrollment signals is discussed
in Sect.2.3.
Our DNN classiﬁer has fully-connected topology. The input layer of size 128
accepts one x-vector without any context, in order to be comparable to the
other back-end classiﬁers. Based on best results in preliminary experiments, we
selected the hyper-parameters such that the DNN contains one hidden layer
of size 128 and ReLU nonlinearity. Training proceeds through minimization of
cross-entropy loss via Adam optimizer. The classiﬁer contains about 17k train-
able parameters.
2.3 Training Datasets and Their Augmentations
The training data for the x-vector DNN originate from the development part
of the Voxceleb database [16] and the training part of the LibriSpeech cor-
pus [17]. The data from training part of the TIMIT dataset [8] are used to train
the back-end classiﬁers. VoxCeleb is an audio-visual dataset consisting of short
clips of human speech, extracted from interview videos uploaded to YouTube.370 J. M´alek and J. Zˇdˇ´ansky´
The provided utterances are recorded in various environments and may contain
reverberation and background noise. The part of the Librispeech dataset desig-
nated “train-360-clean” is utilized in our training. It contains excerpts of read
audio-books with total duration 360 hours. The data should be rather free of
distortions thus we subject it to additional augmentations. The TIMIT corpus
for automatic speech recognition consists of 6300 English phonetically rich sen-
tences read by 630 speakers. The dataset is recorded in anechoic and practically
noiseless environment, thus we apply additional augmentations to it as well.
The train set of the x-vector network, as summarized in Table2, is
compiled in three variants, in order to study the inﬂuence of various augmenta-
tions on the classiﬁcation. Each training set contains one instance of Voxceleb
dataset without any additional augmentation. Further, one or more instances of
Librispeech dataset are added, each with one of the following augmentations:
1. None: The original Librispeech dataset.
2. Reverberation: Theutterancesareconvolvedwithartiﬁcialimpulseresponses
generated by [10]. The artiﬁcial RIRs originate from a shoe-box room of size
8×7×3. We generate RIRs corresponding to four diﬀerent rooms with T
60
ranging from 175−650 ms. The source-microphone distance is 1−2 m.
3. Noise: The background noise with was added to the original Librispeech
dataset.
4. Reverberation+noise: The background noise with was added to the reverber-
ated Librispeech dataset.
Noises for augmentations by background noise are taken from the train-
ing part of the CHiME-4 simulated dataset ([25], we use channel 1 of six-
channel recordings) and the development dataset available in the Task 1 of the
DCASE2018 challenge [1]. This data were also added to the training set with-
out speech, creating a non-speech class besides the speakers classiﬁed by the
network.
The noise in the background is ampliﬁed, such that the signal-to-noise ratio
(SNR) measured in intervals with active speech is 10 dB. This means that the
noiseisloudercomparedtoglobalSNRcase,whenthewholesignalsareconsid-
ered without respect to speech activity.
The Training Set of the Back-End Classiﬁers requires smaller amounts of
data,sincetheclassiﬁerscontainsmallernumberoffreeparameters.Itisderived
fromthetrainingpartoftheTIMITcorpus[8].Theoriginaltrainingdataconsist
of 4620 sentences uttered by 462 speakers. A subset of 4400 utterances is used
to train the classiﬁers, 220 sentences are reserved for the enrollment of PLDA.
Since the TIMIT dataset does not contain any cross-talk or environmental
distortions, we introduce the overlap and signal augmentations artiﬁcially. This
is done in a manner similar to [21], in order to make the results approximately
comparable to this work. We leave one instance of a sentence as it is (speech
class). Another instance containing cross-talk is created, such that an utterance
of a diﬀerent speaker is selected and summed together. A random shift is intro-
ducedatthebeginningofthesummedutterance,suchthataminimumlengthofVoice-Activity and Overlapped Speech Detection Using x-Vectors 371
overlap is one second. The frame-wise true class-labels are created using energy
thresholding of the original TIMIT ﬁles. Reverberation and noise are added in
combinations as described for the x-vector DNN training set. The noise data
originate from the train part of the CHiME-4 dataset [25]. Because the x-vector
network requires context of frames, the classiﬁcation of short sentences is bur-
dened with higher error rate. To mitigate, we add 100 frames of non-speech to
the beginning and end of the augmented signals.
The described procedure results in 4400 (utterances)×2 (speech classes)×5
(reverberation conditions including anechoic)×2 (background noise presence)
+ 6900 (noise-only signals) = 94900 training signals and 220 × 2 × 5 × 2 +
238 = 4638 enrollment sentences for PLDA. From each such signal one feature
vectorfortrainingofback-endclassiﬁersiscreated.Itisanaverageofallvectors
corresponding to the most represented class in the signal.
Table 2. X-vector network variants
X-vector variant Augmentations
X-vec:Rev None, reverberation
X-vec:NoiBg None, reverberation, noise
X-vec:NoiComp None, reverberation, noise, reverberation+noise, non-speech class
2.4 Test Dataset
OuttestdatasetisderivedfromthetestpartoftheTIMITcorpus[8].Thecross-
talkisintroducedinasamemannerasdescribedinSect.2.3forthetrainingsetof
the back-end classiﬁers. The original test data consist of 1680 sentences uttered
by 168 speakers, sampled at 16 kHz. The noise data originate from the test
part of the CHiME-4 dataset [25]. This results in 1680 (utterances)×2 (speech
classes)×5 (reverberation conditions including anechoic)×2 (background noise
presence)+2960(noise-onlysignals)=36560testsignals.Inotherwords,there
is 16800 signals of both speech and cross-talk in various acoustic conditions and
2960 examples of noise-only signals.
Consideringtheframe-wiseoccurrenceofclasses,about58%offramescorre-
sponds to speech, 21% to cross-talk and 21% to non-speech. Non-speech frames
are located in the noise-only signals and also at the boundaries of speech in all
other ﬁles.
3 Experiments
We report results of two types of experiments in this section.
1. The utterance-wise experiments are designed to assign a single class label to
eachofthetestﬁles.Thesearemeanttoverify,whetherthex-vectorsencode372 J. M´alek and J. Zˇdˇ´ansky´
the information about non-speech/cross-talk. The reference for each ﬁle is
selectedsuchthat:1)Non-speechisassignedtoﬁleswithnospeech,2)Speech
is assigned to ﬁles with speech and without cross-talk and 3) Cross-talk is
assignedtoﬁleswithoverlappingutterances.Theclassiﬁcationproceedsonly
on frames, which correspond to the true ﬁle reference. An average feature
vector is computed using these frames and assigned to the class using one
of the back-end classiﬁers. The utterance-wise experiments tend to be overly
optimistic, because the decision of the classiﬁer is supported by information
obtained from multiple frames.
2. The frame-wise experiments correspond to practical utilization of the classi-
ﬁersinthereal-world,classifyingeachoftheframeswithinasignalseparately.
The result for each test ﬁle is a set of time-aligned estimated class-labels.
Theresultsarereportedseparatelyforeachoftheacousticconditionswithinthe
test set. There are 3360 anechoic noiseless signals, 16800 noiseless signals (both
reverberatedandanechoic),2960noise-onlysignalsand16800noisysignals.The
experiments are evaluated using accuracy [%], either per-ﬁle for utterance-wise
experiments or per-frame for frame-wise scenarios.
The comparison of x-vector network conﬁgurations is performed with the
GMMclassiﬁerwithoutanydimensionalityreductioninSects.3.1and3.2.Using
the best conﬁguration, we compare the various discussed back-end classiﬁers in
Sect.3.3.
3.1 Utterance-Wise Experiments
The results summarized in Table3 indicate that the x-vectors encode the non-
speech/cross-talk information. The classiﬁcation is highly accurate (more than
95%) for both anechoic and reverberant noiseless conditions. The features also
well discriminate the non-speech class (more than 99%).
The classiﬁcation seems to be signiﬁcantly less accurate in the presence
of background noise. The accuracy drops to 65.9% for X-vec:Rev, which does
not have any noisy data within its training set. The augmentations applied to
the training data of the x-vector network compensate partly for this deterio-
ration. The best accuracy for the noisy data rises to 80.6%, achieved by the
X-vec:NoiComp network.
Table 3. Accuracy [%] achieved in the utterance-wise experiments using GMM back-
end classiﬁer.
X-vector variant Anechoic Noiseless Noise-only Noisy Total
X-vec:Rev 99.38 95.30 99.93 65.93 82.18
X-vec:NoiBg 98.69 95.96 99.90 73.98 86.18
X-vec:NoiComp 98.78 96.07 99.90 80.61 89.27Voice-Activity and Overlapped Speech Detection Using x-Vectors 373
3.2 Frame-Wise Experiments
The accuracy is lower compared to the utterance-wise case; the best total accu-
racyis76.7%achievedbytheX-vec:NoiCompvariant.Weconjecturethatthisis
caused: 1) by the fact that the back-end classiﬁers analyze each frame indepen-
dently (compared to frame-average over the whole signal) and 2) by the design
oftheback-endtraining,whichcorrespondstoutterance-wiseclassiﬁcation.The
trainingexamplesareaveragesofx-vectorsintrainingsignals,whichmaybetoo
approximate for the frame-wise scenario.
The eﬀect of augmentation is also much less signiﬁcant for the frame-wise
experiment.TheX-vec:NoiCompnetworkoutperformstheX-vec:Revvariantby
1.2%intotalresults,comparedtomorethan7%forutterance-wiseclassiﬁcation
(Table4).
Table 4. Accuracy [%] achieved in the frame-wise experiments using GMM back-end
classiﬁer.
X-vector variant Anechoic Noiseless Noise-only Noisy Total
X-vec:Rev 82.70 76.18 99.73 67.30 75.48
X-vec:NoiBg 81.56 75.98 99.51 68.11 75.72
X-vec:NoiComp 82.49 77.82 99.55 68.48 76.68
3.3 Comparison of Back-End Classiﬁers
The comparison of the back-end classiﬁers is presented in Table5. The highest
accuracy(76.7%)isachievedusingtheGMMclassiﬁerwithoutanydimensional-
ityreduction.TheDNNclassiﬁeryieldstotalaccuracylowerby1.6%,however,it
hastheadvantage ofsigniﬁcantly lower numberoffreeparameters(528k forthe
GMM and 17k for the DNN). Compared to GMM, the GMM:LDA and PLDA
achieve total accuracy lower by about 3%. PLDA achieves the lowest accuracy
forthenon-speechclass.Weconjecture,thisispartlyduetosub-optimalenroll-
ment set, which is diﬃcult to compile for the heterogeneous non-speech class.
Allclassiﬁers(i.e., x-vectorsingeneral)arehighlyaccuratefordetectionoflong
intervalsofnon-speech(columnNoise-onlyinTable5),howeverencountererrors
at the boundaries of speech.
Thisexperimentcanbepartiallycomparedtoinvestigationperformedin[21],
where similar (but not identical) test set was created by adding augmentation
via reverberation and background noise to TIMIT. In [21], the baseline clas-
siﬁer (GMM model trained on MFCC, spectral-ﬂatness measure and kurtosis)
achieved 64.5% accuracy on the noisy data. Thus, the x-vectors appear to be
better suited to the discussed classiﬁcation then the conventional pre-designed
features. However, the DNN-based feature extractors/classiﬁers dedicated to
classiﬁcation of cross-talk achieved accuracy 71.8 −79.9% on the noisy data,
thus outperforming the x-vectors.374 J. M´alek and J. Zˇdˇ´ansky´
Table5.Comparisonofback-endclassiﬁersviaaccuracy[%]obtainedintheframe-wise
experimentsusingx-vectorsettingX-vec:NoiComp.Lastthreecolumnsstateaccuracy
for each class achieved on the whole test dataset.
Classiﬁer Anechoic Noiseless Noise-only Noisy Total Total Total Total
speech cross-talk non-speech
GMM(noreduction) 82.49 77.82 99.55 68.48 76.68 77.78 77.64 73.77
GMM:LDA 80.16 76.81 99.80 62.99 73.90 65.75 86.91 83.19
PLDA 80.13 76.35 92.46 65.22 73.68 73.33 83.59 64.51
DNN 82.23 75.55 99.42 67.12 75.09 71.29 86.48 73.97
4 Conclusions
This manuscript investigated to what extend the speech presence and cross-
talk are encoded within the x-vector features. The following conclusions were
drawn: 1) The x-vectors can be used for detection of cross-talk and non-speech
in a frame-wise manner, especially for environments with low activity of back-
ground noise. 2) The detection using x-vectors outperforms the conventional
pre-designed features, such as kurtosis or MFCC, but it achieves lower accuracy
compared to DNN-based feature extractors/classiﬁers specialized for this task.
3) The negative eﬀects of reverberation can be mitigated to high degree using
the augmentation of training data for the x-vector network. 4) Signiﬁcant accu-
racydeteriorationisobservedondatawithbackgroundnoise;theaugmentation
does have limited eﬀect here. 5) The GMM classiﬁer without any dimensional-
ity reduction achieves the highest accuracy from the back-end classiﬁers. The
accuracy of DNN classiﬁer is slightly lower, but it consists of considerably less
trainable parameters.
For the future work: 1) In the current form, the frame-wise classiﬁcation
suﬀersfromshortspuriouschangesbetweenclasses,especiallyfornoisyenviron-
ments. This behavior can be mitigated by inclusion of smoothing of the output
results,e.g.,intheformofweightedﬁnite-statetransducers.2)Frame-wiseclassi-
ﬁcationcanbeimprovedbytrainingaback-endclassiﬁer,whichtakesframecon-
text into consideration, such as TDNN or convolutional neural network. 3) The
classiﬁcationmaybeimprovedbyinclusionofthecross-talkdirectlyinthetrain-
ing of the x-vector network. However, direct addition of a cross-talk class to the
training dataset violates the idea behind the current targets of the x-vector net-
work, i.e., one speaker is active in one training utterance at the most. More
plausible variant lies in a change of the x-vector network cost-function/targets.
One variant is to allow multi-label classiﬁcation, i.e., the training utterance can
contain more than one speaker.
Acknowledgments. ThisworkwassupportedbytheTechnologyAgencyoftheCzech
Republic (Project No. TH03010018).Voice-Activity and Overlapped Speech Detection Using x-Vectors 375
References
1. DCASE 2018 challenge. http://dcase.community/challenge2018/index. Accessed
27 Mar 2020
2. Bhattacharya, G., Alam, J., Stafylakis, T., Kenny, P.: Deep neural network based
text-dependent speaker recognition: preliminary results. In: Proceedings of the
Odyssey, pp. 2–15 (2016)
3. Cai,D.,He,X.,Han,J.:SRDA:aneﬃcientalgorithmforlarge-scalediscriminant
analysis. IEEE Trans. Knowl. Data Eng. 20(1), 1–12 (2007)
4. Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P., Ouellet, P.: Front-end factor
analysisforspeakerveriﬁcation.IEEETrans.AudioSpeechLang.Process. 19(4),
788–798 (2010)
5. Doddipatla, R., Braunschweiler, N., Maia, R.: Speaker adaptation in DNN-based
speech synthesis using d-vectors. In: INTERSPEECH, pp. 3404–3408 (2017)
6. Fu, R., Tao, J., Wen, Z., Zheng, Y.: Phoneme dependent speaker embedding and
modelfactorizationformulti-speakerspeechsynthesisandadaptation.In:ICASSP
2019–2019 IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), pp. 6930–6934. IEEE (2019)
7. Garcia-Romero,D.,Snyder,D.,Sell,G.,Povey,D.,McCree,A.:Speakerdiarization
usingdeepneuralnetworkembeddings.In:2017IEEEInternationalConferenceon
Acoustics, Speech and Signal Processing (ICASSP), pp. 4930–4934. IEEE (2017)
8. Garofolo,J.S.,etal.:TIMITacoustic-phoneticcontinuousspeechcorpus.Linguist.
Data Consortium 10(5) (1993)
9. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, Cambridge
(2016). http://www.deeplearningbook.org
10. Habets, E.A.: Room impulse response generator. Technische Universiteit Eind-
hoven, Technical report, vol. 2(2.4), p. 1 (2006)
11. Heigold,G.,Moreno,I.,Bengio,S.,Shazeer,N.:End-to-endtext-dependentspeaker
veriﬁcation. In: 2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 5115–5119. IEEE (2016)
12. Ioﬀe, S.: Probabilistic linear discriminant analysis. In: Leonardis, A., Bischof, H.,
Pinz, A. (eds.) ECCV 2006. LNCS, vol. 3954, pp. 531–542. Springer, Heidelberg
(2006). https://doi.org/10.1007/11744085 41
13. Jansky´, J., M´alek, J., Cˇmejla, J., Kounovsky´, T., Koldovsky´, Z., Zˇd’´ansky´, J.:
Adaptive blind audio source extraction supervised by dominant speaker identiﬁ-
cation using x-vectors. In: ICASSP 2020–2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 676–680 (2020)
14. Kuneˇsov´a, M., Hru´z, M., Zaj´ıc, Z., Radov´a, V.: Detection of overlapping speech
forthepurposesofspeaker diarization. In:Salah, A.A., Karpov,A., Potapova,R.
(eds.) SPECOM 2019. LNCS (LNAI), vol. 11658, pp. 247–257. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-26061-3 26
15. McLachlan, G.J., Peel, D.: Finite Mixture Models. Wiley, Hoboken (2004)
16. Nagrani,A.,Chung,J.S.,Zisserman,A.:Voxceleb:alarge-scalespeakeridentiﬁca-
tion dataset. arXiv preprint arXiv:1706.08612 (2017)
17. Panayotov, V., Chen, G., Povey, D., Khudanpur, S.: Librispeech: an ASR corpus
based on public domain audio books. In: 2015 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210. IEEE (2015)
18. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. In: Sixteenth Annual Conference
of the ISCA (2015)376 J. M´alek and J. Zˇdˇ´ansky´
19. Raj, D., Snyder, D., Povey, D., Khudanpur, S.: Probing the information encoded
in x-vectors. arXiv preprint arXiv:1909.06351 (2019)
20. Reynolds, D.A., Quatieri, T.F., Dunn, R.B.: Speaker veriﬁcation using adapted
gaussian mixture models. Digit. Signal Process. 10(1–3), 19–41 (2000)
21. Sajjan, N., Ganesh, S., Sharma, N., Ganapathy, S., Ryant, N.: Leveraging LSTM
modelsforoverlapdetectioninmulti-partymeetings.In:2018IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.5249–5253.
IEEE (2018)
22. Shokouhi,N.,Sathyanarayana,A.,Sadjadi,S.O.,Hansen,J.H.:Overlapped-speech
detectionwithapplicationstodriverassessmentforin-vehicleactivesafetysystems.
In: 2013IEEEInternational Conference on Acoustics, Speech andSignal Process-
ing, pp. 2834–2838. IEEE (2013)
23. Snyder, D., Garcia-Romero, D., Sell, G., Povey, D., Khudanpur, S.: X-vectors:
robustDNNembeddingsforspeakerrecognition.In:2018IEEEInternationalCon-
ference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5329–5333.
IEEE (2018)
24. Variani, E., Lei, X., McDermott, E., Moreno, I.L., Gonzalez-Dominguez, J.:
Deep neural networks for small footprint text-dependent speaker veriﬁcation. In:
2014 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 4052–4056. IEEE (2014)
25. Vincent,E.,Watanabe,S.,Nugraha,A.A.,Barker,J.,Marxer,R.:The4thCHiME
speech separation and recognition challenge. http://spandh.dcs.shef.ac.uk/chime
challenge/chime2016/. Accessed 27 Mar 2020
26. Wang, S., Qian, Y., Yu, K.: What does the speaker embedding encode? In: Inter-
speech, pp. 1497–1501 (2017)
27. Wu,Z.,Swietojanski,P.,Veaux,C.,Renals,S.,King,S.:Astudyofspeakeradap-
tation for DNN-based speech synthesis. In: Sixteenth Annual Conference of the
International Speech Communication Association (2015)
28. Yella, S.H., Bourlard, H.: Overlapping speech detection using long-term conversa-
tionalfeaturesforspeakerdiarizationinmeetingroomconversations.IEEE/ACM
Trans. Audio Speech Lang. Process. 22(12), 1688–1700 (2014)
29. Zhao,Y.,Li,J.,Zhang,S.,Chen,L.,Gong,Y.:Domainandspeakeradaptationfor
cortanaspeechrecognition.In:2018IEEEInternationalConferenceonAcoustics,
Speech and Signal Processing (ICASSP), pp. 5984–5988. IEEE (2018)
30. Zmolikova,K.,Delcroix,M.,Kinoshita,K.,Higuchi,T.,Ogawa,A.,Nakatani,T.:
Speaker-aware neural network based beamformer for speaker extraction in speech
mixtures. In: Interspeech, pp. 2655–2659 (2017)
31. Zmolikova, K., et al.: Speakerbeam: speaker aware neural network for target
speaker extraction in speech mixtures. IEEE J. Sel. Topics Signal Process. 13(4),
800–814 (2019)Introduction of Semantic Model to Help
Speech Recognition
B
Stephane Level, Irina Illina( ) , and Dominique Fohr
Universit´e de Lorraine, CNRS, Inria, 54000 Nancy, France
{irina.illina,dominique.fohr}@loria.fr
Abstract. Current Automatic Speech Recognition (ASR) systems
mainlytakeintoaccountacoustic,lexicalandlocalsyntacticinformation.
Long term semantic relations are not used. ASR systems signiﬁcantly
decrease performance when the training conditions and the testing con-
ditionsdiﬀerduetothenoise,etc..Inthiscasetheacousticinformation
can be less reliable. To help noisy ASR system, we propose to supple-
ment ASR system with a semantic module. This module re-evaluates
the N-best speech recognition hypothesis list and can be seen as a form
of adaptation in the context of noise. For the words in the processed
sentence that could have been poorly recognized, this module chooses
words that correspond better to the semantic context of the sentence.
To achieve this, we introduced the notions of a context part and possi-
bility zones thatmeasurethesimilaritybetweenthesemanticcontextof
the document and the corresponding possible hypothesis. The proposed
methodologyusestwocontinuousrepresentationsofwords:word2vecand
FastText.WeconductexperimentsonthepubliclyavailableTEDconfer-
encesdataset(TED-LIUM)mixedwithrealnoise.Theproposedmethod
achieves a signiﬁcant improvement of the word error rate (WER) over
the ASR system without semantic information.
· ·
Keywords: Automatic Speech Recognition Semantic context
Embeddings
1 Introduction
Despite constant eﬀorts and some spectacular advances, the ability of a com-
putertorecognizespeechisstillfarfromequalingthatofhumans.CurrentASR
systemssigniﬁcantlydeteriorateperformancewhentheconditionsinwhichthey
are trained and those in which they are used diﬀer. The causes of variability
between these conditions can be the acoustic environment and/or the acquisi-
tion of the signal. Even if many approaches to compensate this variability have
been proposed [18], the performance of an ASR system on a given word always
depends on the distortion at the precise moment when this word was spoken.
Current ASR systems mainly take into account only acoustic (acoustic
model), lexical and syntactic information (local n-gram language models). We
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.377–385,2020.
https://doi.org/10.1007/978-3-030-58323-1_41378 S. Level et al.
suggest moving towards a contextualization of the ASR system. Indeed, lexi-
cal and semantic information is important for an ASR system to be eﬃcient.
Recently, several researchers have proposed to use semantic information to
improve the ASR performance. For example, exploring the topic and semantic
context to enable the recovery of proper names [14], using a semantic language
model based on the theory of frame semantics [2], assigning semantic category
labels to entire utterances and re-ranking the N-best list of ASR [11]. [7] learns
semanticgrammarfortheASRsystem.In[5]authorscombineinformationfrom
the semantic parser and ASR’s language model for re-ranking. In [4], a method
for re-ranking black-box ASR hypotheses using an in-domain language model
and semantic parser trained for a particular task is investigated.
In this article, we propose to complete the noisy ASR step by adding the
semanticinformationinordertodetectthewordsintheprocessedsentencethat
couldhavebeenpoorlyrecognizedandtoinvestigatewordsofsimilarpronuncia-
tions that correspond better to the context. This semantic analysis re-evaluates
(rescores) the N-best transcription hypotheses (N-best) and can be seen as a
form of dynamic adaptation in the speciﬁc context of noisy data. Reevaluation
is performed through a deﬁnition of context part and possibility zones. Seman-
tic information is introduced using predictive continuous representations [3,9].
These representations have proven to be eﬀective for a series of natural lan-
guage processing tasks [1]. The eﬃciency and the semantic properties of these
representations motivate us to explore them for our task of ASR in mismatched
conditions.Wehopethatinverynoisyparts,thelanguagemodelandtheseman-
ticmodelcouldremovetheacousticambiguitiesinordertoﬁndthewordsspoken
bythespeaker.Allourmodelsarebasedonhigh-performanceDNNtechnologies.
Compared to the previous works using the rescoring of N-best list [12,15,16],
we don’t use several features, and we only rely on semantic information. Fur-
thermore, the speciﬁcity of our approach is the use of the context part and
the possibility zones of N-hypotheses list: semantic part represents the semantic
information of the topic context of the document to recognize and possibility
zone corresponds to the area where we want to ﬁnd the words to be corrected.
Thisallows ustogivelessimportancetothewordsinthepossibilityzonewhich
do not correspond to the context of the document, and to give low semantic
score to the corresponding hypothesis.
2 Proposed Methodology
2.1 Semantic Model
An eﬀective way to take into account semantic information is to re-evaluate
(rescore)thebesthypothesesoftheASRsystem.Thissystemprovidesanacous-
tic score Pac(w) and a linguistic score Plm(w) for each word of the hypothesis
sentence. The best sentence is the one that maximizes the probability of the
word sequence: (cid:2)
Wˆ =argmax Pac(w)α·Plm(w)β (1)
hi∈Hw∈hiIntroduction of Semantic Model to Help Speech Recognition 379
Wˆ is the recognized sentence (the end result); H is the set of N-best sentence
hypotheses;hi isthei-thsentencehypothesis;w isahypotheticalword.αandβ
representtheweightsoftheacousticandthelanguagemodels.Theseweightsare
essential because acoustic scores and linguistic scores are not always normalized
(they are often likelihoods and not probabilities).
We want to add semantic information to guide the recognition process. The
most natural approach to integrating this information is to modify the calcula-
tion of the probability of the sequence of words in the following way:
(cid:2)
Wˆ =argmax Pac(w)α·Plm(w)β ·Psem(w)γ (2)
hi∈Hw∈hi
We added the semantic probability of each word: Psem(w). To have a good
balance between the diﬀerent models, we introduce a third weight γ to weigh
the semantic information. It will be adjusted on a development corpus.
2.2 Deﬁnition of Context Part and Possibility Zones
To estimate the semantic probability, we propose to introduce the concepts of
context part and possibility zone. A context part consists of words which are
common to all the N-best hypotheses generated by the ASR. We assume that
they are correct. This context part allows to extract semantic information of
the topic context of the document or of the current part of the document to be
recognized. The context part can contain several parts. A zone of possibilities is
an area between the context parts. It is in this area that we want to ﬁnd the
words to be corrected. From the N-best hypotheses of a sentence, we extract
only one context part and one or more possibility zones. Each zone can contain
severalwords.Figure1illustratestheseconceptsonanexample.Here,the2-best
hypotheses list is the following:
H1: the cat eats the big fat mouse
H2: the cat bits the bigfoot mouse
In this example, the context part Zcont is composed of four words: Zcont =
{the,cat,the,mouse}.Thesearethewordswhicharecommontoallthehypothe-
ses and we assume that they are correct. Between these words, we deﬁne
two possibility zones: the ﬁrst is made up of two alternatives, eats and bits:
Zpos,1 = {eats,bits}. The second is also made up of two alternatives: Zpos,2 =
{bigfat,bigfoot}.Onealternativecorrespondstoachoiceinthepossibilityzone.
We assume that the possibility zones correspond to the zones where the ASR
hesitates between diﬀerent solutions.
Toobtainthecontextpart,weuseadynamicprogrammingalgorithmwhich
allows us to pair the hypotheses two by two in order to determine the words
common to all the hypotheses. If the context part is empty, we don’t study this
sentence.380 S. Level et al.
Fig.1. Illustration of the context part and the possibility zones, as an example.
2.3 Semantic Representation of the Context Part
and the Possibility Zones
To take into account the semantics of the document, we propose to represent
each word of the N-best hypotheses by an embedding vector. In our approach, we
used word2vec [9] and FastText [3]. We compute an average embedding Econt
for the context part which is equal to the average of the embedding vectors of
all the words in the context part. In the same way, we calculate an average
embedding Epos(i,ah) for i-th possibility zone of alternative ah of hypothesis
h as the average of the embedding vectors of all the words in this alternative
of possibility zone. We use the angular similarity to estimate a semantic score
between each possibility zone and the context part:
Ssem(Econt,Epos(i,ah))=1− cos−1cos(Econt,Epos(i,ah)) (3)
π
From the semantic representations of the context part and the possibility
zones,wecomputeasemanticprobabilityofahypothesish.A semantic prob-
ability of a hypothesis h Psem(h) is computed as follows:
(cid:2)Np
Psem(h)= Ssem(Econt,Epos(i,ah)) (4)
i=1
where Np is the number of possibility zones. We assume that the Eq.(2) can be
approximated as follow:
Hˆ =argmaxPac(h)α·Plm(h)β ·Psem(h)γ (5)
h∈H
whereHˆ istheN-bestlist.TheEq.(5)isusedtore-ranktheN-besthypothesis
list. For each hypothesis we compute the semantic score and associate it with
acousticandlinguisticscoresaccordingto(5).Thehypothesisobtainingthebest
score is considered as the recognized sentence.Introduction of Semantic Model to Help Speech Recognition 381
3 Experiments
3.1 Corpus Description
We used the publicly available TED-LIUM corpus [6], containing the recordings
of the TED conferences. This corpus is well suited to our study because each
conference is focused on a particular subject. We want to add the semantic
module to improve the performance of our recognition system.
We used the partition of the TED corpus into a train, a development and
a test corpus proposed in the TED-LIUM distribution: 452h for training, 8
conferences (496 sentences, 17926 words) for development and 11 conferences
(1091 sentences, 27021 words) for testing.
3.2 Recognition System
Our recognition system is based on the Kaldi voice recognition toolbox [13].
We used TDNN triphone acoustic models, trained on the training part of TED-
LIUM. The lexicon and language model was provided in the TED-LIUM distri-
bution. The lexicon contains 150k words and the language model has 2 million
4-g, learned from a textual corpus of 250 million words. We also performed the
recognitionusingtheRNNLMmodel[10].Wewanttoseeifusingmorepowerful
language model (LM), the proposed semantic module can improve the ASR. As
usual, we used the development set to choose the best parameter conﬁguration
and the test set to evaluate the proposed methods with this best conﬁguration.
We used the word error rate (WER) to measure the ASR performance.
The performance of our ASR system on TED-LIUM using n-gram LM is
around 8% of WER. We are not interested in noise-free conditions because in
this case the acoustics allow to properly guide the recognition. This research
work was carried out as part of an industrial project. This project concerns the
recognition of speech in noisy condition, more precisely, in a ﬁghter aircraft.
To get closer to actual conditions, we added noise to the development and test
sets: additive noise at 10dB and 5dB SNR (noise of F16 from the NOISEX-92
corpus [17]).
3.3 Embeddings
We trained word2vec model on a text corpus of a billion words extracted from
theOpenWebText corpus.Thegeneratedmodelshavethesizeof300andmodel
700K words. As FastText model, we used the same embedding dimension. The
advantage of FastText compared to word2vec is the taking into account of all
possible words.
4 Experimental Results
4.1 Overall Results
Before performing the speech recognition evaluation, we wanted to investigate
the impact of the semantic module alone on the search for the best sentence,382 S. Level et al.
withoutusingtheacousticandlinguisticscores.Forthis,forareferencesentence
text, we simulated the recognition errors by replacing a random word (or two
successive words) of the reference sentence by one (or two) acoustically close
word(s). This can be easily performed using a phonetic dictionary. In this way,
wegeneratedN-besthypothesesforthegivensentence(N =10).Weperformed
this generation for every 496 sentences of the development set.
After N-best hypotheses generation, we used our semantic module to rank
the11hypotheses(the10generatedsentencesplusthecorrectsentence)andwe
evaluatedthenumberoferrorscorrectedonthetophypothesis.Here,wedidnot
use the acoustic and the language scores. For 496 sentences of the development
set,theword2vec-basedsemanticmodulecorrectsabout67%ofsimulatederrors
andtheFastTextsemanticmodulecorrectsabout61%oferrors.Weseethatthe
longcontextembeddingsalonesucceedtocorrectthelargenumberoferrors.This
showsthattheproposedsemanticmodulecaptureswellthesemanticinformation
of a sentence.
Table1presentstheWERforthedevelopmentandthetestsetsfortwonoise
condition(10dBand5dB)andtwolanguagemodels(n-gramandRNNLM).The
ﬁrst line of results (method Random), corresponds to the random selection of
the recognition result from the N-best hypotheses without using the semantic
module. The second line, Baseline, corresponds to the speech recognition sys-
tem without using the semantic module (standard ASR). The last line, Oracle,
represents the maximum performance that can be obtained by searching in the
N-best hypotheses: selection of the hypothesis which minimizes the WER for
each sentence. The other lines of the table give the performance of the proposed
approaches.Ateachcaseofthetable,valuebetweentheparenthesescorresponds
to the recognition result using the RNNLM. From this table we can make the
following observations.
Table 1. Recognition results in terms of WER (%). N-best hypotheses list of 50
hypotheses.TED-LIUMdevelopmentandtestsets,SNRof10dBandof5dB.n-gram
LM and RNNLM (between the parentheses).
Method SNR 10dB SNR 5dB
Dev Test Dev Test
Random 17.9 (14.8) 24.1 (21.5) 34.2 (29.8) 42.1 (39.3)
Baseline system 15.7 (12.3) 21.1 (17.7) 32.7 (28.2) 40.3 (37.1)
word2vec embedding 15.3 (12.0) 20.7 (17.6) 31.9 (27.4) 39.4 (36.4)
FastText 15.2 (11.8) 20.5 (17.5) 31.8 (27.4) 39.2 (36.1)
Oracle 9.6 (6.9) 12.8 (10.3) 25.4 (21.1) 30.5 (27.6)
The proposed semantic module outperforms the baseline system for all con-
ditionsandallevaluatedembeddings.Forexample,onthetestset,thesemantic
module with the FastText obtained an absolute improvement of 0.6% for 10dBIntroduction of Semantic Model to Help Speech Recognition 383
andn-gramLM(21.1%WERversus20.5%WER)and1.1%for5dBandn-gram
LM (40.3% versus 39.2%) compared to the baseline system. This represents 8%
of relative improvement for 10dB and about 11% for 5dB in the reduction of
the gap between the baseline and the oracle systems. For all datasets, noise
levels and two language models the obtained improvements are signiﬁcant (con-
ﬁdence interval is computed according to matched-pairs test [8]). This shows
thattheproposedsemanticmoduleisabletocaptureasigniﬁcantproportionof
the semantic information in the data.
Theproposedembeddingsgivesimilarperformanceswithaslightsuperiority
oftheFastText embedding.Alltheseobservationarevalidfortwoexperimented
language models: n-gram and RNNLM.
4.2 Impact of Hyperparameters
Figure2(left)showstheevolutionoftheWERaccordingtotheparameterγ (cf.
Eq.(2)) for the development set, SNR of 5dB and n-gram LM. We observe that
this parameter plays an important role. For too large values of γ (bigger than
300),thesemanticinformationbecomesdominantcomparedtotheacousticand
linguistic information and the WER begins to increase. Therefore, the value of
γ between 100 and 300 seems to be optimal. Figure2 (right) reports the WER
as a function of the N-best list size. We can see that 5 or 10 hypotheses are not
enough. Using more than 25 hypotheses shows no further improvement.
Fig.2.Semanticmodulewithword2vecembedding,TED-LIUMdevelopmentset,SNR
of 5dB. WER as a function of the semantic weight γ (left ﬁgure) and the N-best
hypothesis number (right ﬁgure). The dotted line corresponds to the baseline result.
n-gram LM.
5 Conclusion and Discussion
In this article, we proposed a new approach of introducing semantic informa-
tion for the performance improvement of a noisy ASR system. We investigated
a new methodology for taking into account semantics through predictive repre-
sentations that capture the semantic characteristics of words and their context.384 S. Level et al.
The eﬃciency and the semantic properties of these representations motivate us
to explore these representations for our task of speech recognition. We used
word2vec and FastText embeddings. The semantic information is taken into
account through the rescoring module of the N-best hypotheses of the recog-
nitionsystem.Semanticrepresentationsareappliedtothecontextpartandpos-
sibility zones. We evaluated our methodology on the corpus of TED-LIUM con-
ferenceswithaddedrealnoise.TheproposedmethodologyshowsabetterWER
comparedtothebaselinesystem.Thisrepresents8%ofrelativeim-provementfor
10dB and about 10% for 5dB in the reduction of the gap between the baseline
and the oracle systems. These improvements are statistically signiﬁcant. This
observation is valid for the ASR with n-gram and with RNNLM.
It is important to note that in word2vec and FastText the word embedding
is static and the words with multiple meanings are conﬂated into a single rep-
resentation. In future work, we would like to investigate the dynamic BERT
embedding. We will conduct a deep analysis of the performance of semantic
module as a function of the noise characteristics (e.g., nonstationarity) and the
uncertainty propagation in noisy environment to guide the rescoring.
Acknowledgments. The authors thank the DGA (Direction G´en´erale de
l’Armement, part of the French Ministry of Defence), Thales AVS and Dassault Avi-
ation who are supporting the funding of this study and the “Man-Machine Teaming”
scientiﬁc program in which this research project is taking place.
References
1. Baroni,M.,Dinu,G.,Kruszewski,G.:Don’tcount,predict!Asystematiccompar-
ison of context-counting vs. context-predicting semantic vectors. In: Proceedings
ofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics,pp.
238–247 (2014)
2. Bayer, A., Riccardi, G.: Semantic language models for automatic speech recogni-
tion. In: Proceedings of the IEEE Spoken Language Technology Workshop (SLT)
(2014)
3. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. In: Transactions of the Association for Computational Lin-
guistics, pp. 135–146 (2017)
4. Corona, R., Thomason, J., Mooney, R.: Improving black-box speech recognition
usingsemanticparsing.In:ProceedingsoftheThe8thInternationalJointConfer-
ence on Natural Language Processing, pp. 122–127 (2017)
5. Erdogan,H.,Sarikaya,R.,Chen,S.,Gao,Y.,Picheny,M.:Usingsemanticanalysis
to improve speech recognition performance. Comput. Speech Lang. 19, 321–343
(2005)
6. Hernandez,F.,Nguyen,V.,Ghannay,S.,Tomashenko,N.,Est`eve,Y.:TED-LIUM
3: twice as much data and corpus repartition for experiments on speaker adap-
tation. In: Karpov, A., Jokisch, O., Potapova, R. (eds.) SPECOM 2018. LNCS
(LNAI),vol.11096,pp.198–208.Springer,Cham(2018).https://doi.org/10.1007/
978-3-319-99579-3 21Introduction of Semantic Model to Help Speech Recognition 385
7. Gaspers, J., Cimiano, P., Wrede, B.: Semantic parsing of speech using grammars
learned with weak supervision. In: Proceedings of the HLT-NAACL, pp. 872–881
(2015)
8. Gillick,L.,Cox,S.:Somestatisticalissuesinthecomparisonofspeechrecognition
algorithms. In: Proceedings of ICASSP, vol. 1, pp. 532–535 (1989)
9. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-
sentationsofwordsandphrasesandtheircompositionality.In:AdvancesinNeural
Information Processing Systems, vol. 26, pp. 3111–3119 (2013)
10. Mikolov T., Kombrink S., Burget L., Cernocky J.-H., Khudanpur S.: Extensions
of recurrent neural network language model. In: Proceedings of the ICASSP, pp.
5528–5531 (2011)
11. Morbini,F.,etal.:Arerankingapproachforrecognitionandclassiﬁcationofspeech
inputinconversationaldialoguesystems.In:ProceedingsoftheSpokenLanguage
Technology Workshop (SLT), pp. 49–54. IEEE (2012)
12. Ogawa,A.,Delcroix,M.,Karita,S.,Nakatani,T.:RescoringN-bestspeechrecog-
nition list based on one-on-one hypothesis comparaison using encoder-classiﬁer
model. In: Proceedings of the ICASSP (2018)
13. Povey, D., et al.: The Kaldi speech recognition toolkit. In: Proceedings of IEEE
Workshop on Automatic Speech Recognition and Understanding (2011)
14. Sheikh, I., Fohr, D., Illina, I., Linar`es, G.: Modelling semantic context of OOV
wordsinlargevocabularycontinuousspeechrecognition.IEEE/ACMTrans.Audio
Speech Lang. Process. 25(3), 598–610 (2017)
15. Shin, J., Lee, Y., Jung, K.: Eﬀective sentence scoring method using BERT for
speech recognition. In: Proceedings of Machine Learning Research, vol. 101, pp.
1081–1093 (2019)
16. Song,Y.,etal.:L2RS:alearning-to-rescoremechanismforautomaticspeechrecog-
nition. arXiv:1910.11496v1 (2019)
17. Varga, A., Steeneken, H.: Assessment for automatic speech recognition: II.
NOISEX-92:adatabaseandanexperimenttostudytheeﬀectofadditivenoiseon
speech recognition systems. Speech Commun. 12(3), 247–251 (1993)
18. Zhang,Z.,Geiger,J.,Pohjalainen,J.,Mousa,A.,Jin,W.,Schuller,B.:Deeplearn-
ing for environmentally robust speech recognition: an overview of recent develop-
ments. ACM Trans. Intell. Syst. Technol. 9(5), 1–28 (2018)Towards Automated Assessment
of Stuttering and Stuttering Therapy
B
Sebastian P. Bayerl1( ) , Florian Ho¨nig2 , Jo¨elle Reister2,
and Korbinian Riedhammer1
1 Technische Hochschule Nu¨rnberg Georg Simon Ohm, Nuremberg, Germany
{sebastian.bayerl,korbinian}@ieee.org
2 Institut der Kasseler Stottertherapie, Bad Emstal, Germany
Abstract. Stuttering is a complex speech disorder that can be identi-
ﬁedbyrepetitions,prolongationsofsounds,syllablesorwordsandblocks
whilespeaking.Severityassessmentisusuallydonebyaspeechtherapist.
While attempts at automated assessment were made, it is rarely used
in therapy. Common methods for the assessment of stuttering sever-
ity include percent stuttered syllables (%SS), the average of the three
longest stuttering symptoms during a speech task or the recently intro-
duced Speech Eﬃciency Score (SES). This paper introduces the Speech
ControlIndex(SCI),anewmethodtoevaluatetheseverityofstuttering.
UnlikeSES,itcanalsobeusedtoassesstherapysuccessforﬂuencyshap-
ing. We evaluate both SES and SCI on a new comprehensively labeled
dataset containing stuttered German speech of clients prior to, during
and after undergoing stuttering therapy. Phone alignments of an auto-
maticspeechrecognitionsystemarestatisticallyevaluatedinrelationto
their relative position to labeled stuttering events. The results indicate
that phone length distributions diﬀer in respect to their position in and
around labeled stuttering events.
· ·
Keywords: Speech and voice disorders Pathological speech
Language
1 Introduction
Stuttering is a speech disorder with a prevalence of 1% of the population [4]. It
is a complex disorder of nerve coordination between both brain hemispheres. It
canbeidentiﬁedbyrepetitions,prolongationsofsounds,syllablesorwords,and
blocks while speaking.
In addition to these so-called core symptoms, a wide variety of linguistic,
physical, behavioral and emotional accompanying symptoms can occur, some of
them overlapping the core symptoms. Stuttered disﬂuencies are usually accom-
panied by physical tension [12]. The frequency of occurrence and the dura-
tion of the symptoms vary considerably depending on individual severity and
can seriously impair the communication of the person who stutters (PWS) [3].
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.386–396,2020.
https://doi.org/10.1007/978-3-030-58323-1_42Towards Automated Assessment of Stuttering and Stuttering Therapy 387
The individual appearance of the symptoms of each PWS also depends on the
respective communication situation, the linguistic complexity of the utterance
and the typical phased progress of the speech disorder [6,19]. Since PWS know
exactly what they want to say, the cause of the stuttered disﬂuency does not lie
inplanningorformulating speech,butinexecutingtheplanofarticulation [12].
The condition is treatable but not curable.
One possible technique to overcome stuttering is a technique called ﬂuency
shaping[9,26].Goodresultscouldbeachievedbyadaptingittostutteringther-
apy [13]. PWS learn a method to overcome blocks which is characterized by
“easy” voice onset [10]. A German adaption of this technique is the Kasseler
Stottertherapie which has also been proven to work well [7,8]. To assess the
severityofstutteringandstutteringtherapysuccessinsomeway,itisimportant
to measure stuttering in a reliable way. This is important both for therapeutic
practice and research. A stuttering diagnosis consists of the objective and sub-
jective evaluation of the stuttering symptoms as well as the evaluation of the
impairment of everyday life caused by the disorder. It should provide a reliable
picture of the individual severity of stuttering.
The objective evaluation of linguistic symptoms typically measures the fre-
quency of stuttering events in percent of stuttered syllables (%SS), whereby the
numberofstutteredsyllablesisrelatedtooverallspokensyllables.However,this
measure has only little agreement among diﬀerent observers [6] and does not
take into account the type of stuttering symptom, e.g. one-time syllable repeti-
tion vs. several-second tense block, nor its duration, which signiﬁcantly reduces
the signiﬁcance of %SS regarding the severity of stuttering [6,24]. Additionally
to%SS,thedurationofstutteringeventscanbedeterminedinordertoincrease
the reliability of the results. However, commonly only a small part of the dura-
tion of stuttering events is taken into account, e.g.; in SSI-4 only the average of
the three longest stuttering symptoms is used [23]. These methods also do not
record atypical stuttering disﬂuencies, which however can occur as accompany-
ing linguistic symptoms and can signiﬁcantly inﬂuence the impression of the
severity of stuttering. Subjective stuttering severity rating scales are a widely
used measure for assessing the severity of stuttering. These are commonly used
both in speech therapy [18] and in clinical research [27]. For clinical purpose,
severity rating scales are more reliable than %SS, to provide a statement about
individual stuttering severity [11].
Methods for the automated assessment of stuttering and stuttering severity
have been proposed in the past. No¨th, Niemann, Haderlein, et al. use a stan-
dard speech recognition system and evaluated vowel and fricative durations on
a standardized reading task to discriminate between PWS and normal speakers
[16]. To classify prolongations and repetitions, Chee et al. extracted Mel Fre-
quency Cepstral Coeﬃcients (MFCC) and used them to train k-NN and LDA
classiﬁers on a very small sample taken from the University College London
Archive of Stuttered Speech [5]1. Mundada et al. use the K-Means clustering
algorithm to separate normal speakers from PWS. They also use MFCC feature
1 Available at https://www.uclass.psychol.ucl.ac.uk/uclassfsf.htm.388 S. P. Bayerl et al.
extractionandDynamicTimeWarping(DTW)forclassiﬁcation[15].S´wietlicka
et al. use artiﬁcial neural networks to discriminate between syllable repetitions,
blocks before words that start with a plosive, and phone prolongations [25].
Alharbi et al. recognize the need to develop customized ASR that can produce
full verbatim transcripts including pseudo words and word parts without mean-
ing. Their approach is mainly focused on the detection of repetitions [1]. Ochi
et al. investigated the automatic evaluation of soft articulatory contact, as it is
taught in stuttering therapy. Detecting modiﬁed speech is necessary to account
for it in automatic evaluation of PWS that went through speech therapy [17].
Our Contributions. In this work, we introduce the Speech Control Index
(SCI), a new method to evaluate the severity of stuttering which can also be
used to assess therapy success for ﬂuency shaping. We evaluate both SES and
SCI on a new comprehensively labeled dataset acquired at the Institut der Kas-
seler Stottertherapie(KST) containing stuttered German speech of clients prior
to, during and after undergoing stuttering therapy. Based on phone alignments
of an automatic speech recognition system, we perform a statistical evaluation
of phone length distributions in relation to stuttering events.
2 Data
The data used in this paper was speciﬁcally created and labeled with stuttering
andstutteringtherapyinmind.Inthefuture,datagatheredforthisworkwillbe
usedtocreatemeanstoprovideunobstrusivemonitoringofstuttering.Thus,the
datasetwascreatedtorepresentrealityasgoodaspossible.Nospecialrecording
equipment was used and the dataset was recorded with consumer hardware. All
recordingswerecreatedbefore,duringandaftertherapyattheKST.Thetherapy
containsanumberofdiﬀerenttaskssuchasreading,callingunacquaintedpeople
for inquiry purposes or talking to strangers in the street.
The labeling was done by two clinical linguists familiar with stuttering ther-
apy at the KST. The data is labeled in great detail diﬀerentiating twelve states
of ﬂuent or disﬂuent speech as well as prosodic pauses and blocks. The focus
is to comprehensively label stuttering behavior such as interrupted or repeated
wordsorsentencesinwholeorparts.Thedatasetalsolabelsinterjections,which
can be a typical stuttering related behavior, even though it is also common in
regular speakers. Another unique feature of the dataset is the labeling of mod-
iﬁed speech: speech as it is produced when applying the ﬂuency shaping tech-
nique taught and trained at the KST. Additionally to the labeling of stuttering
behavior,atranscriptisprovidedinwhichwordabortionsaremarkedandtran-
scribed in a verbatim way. During preprocessing, the recordings were resampled
to 16kHz where necessary and in case of stereo recordings only one channel is
used. The dataset contains 214 recordings by 37 speakers of which 28 were male
and 9 were female. The dataset amounts to about 207min of labeled speech.
To the best of our knowledge, these features make it one of the largest and
most comprehensively labeled datasets containing stuttered speech. One of its
most important features is the existence of stuttered and modiﬁed speech priorTowards Automated Assessment of Stuttering and Stuttering Therapy 389
to, during and after therapy, enabling extensive research and the creation of
practical applications that can be used in a therapeutic context.
3 Method
To assess the severity of stuttering or disﬂuency of speakers using ﬂuency shap-
ing, common evaluation methods such as %SS or SSI-4 are insuﬃcient, as these
methodsdonotaccountfortherapyartefactsandaccompanyinglinguisticsymp-
toms. Since a purely subjective measure of stuttering severity has many draw-
backs in clinical practice, we chose to calculate the SES based on classifying
speech as either eﬃcient, ineﬃcient or silence.
3.1 Speech Eﬃciency Score
The Speech Eﬃciency Score (SES) is a recent method for the evaluation of
(dis)ﬂuent speech that was proposed by Amir et al. [2]. This method puts the
fractionofﬂuentspeechinrelationtothefractionofdisﬂuentspeech.Thus,SES
determines the communicative eﬃciency of a speaker by focusing on the time
domain.
Eﬃcient time
SES = ·100% (1)
Total time−Silence
With this method, all kinds of disﬂuencies, both typical and atypical to
stuttering, are taken into account, as well as the duration of the ﬂuent and
disﬂuentspeechcomponents,whichmakesitsuperiortopreviousmethods.Amir
et al. concluded that, due to the high correlation they found between SES and
subjectiveseverityratingscales,SESalsoprovidesreliableinformationaboutthe
severityofstuttering.SinceSESconsidersprolongations,whichareperceivedas
abnormal,tobeineﬃcient,itmustbeassumedthattheSESfailstotakeadapted
speaking behaviors into account. This in turn implies that for the calculation of
SES,speechfractionsthat contain modiﬁedspeech,suchasﬂuencyshaping, are
counted as ineﬃcient.
Fluency shaping focuses on restructuring the way of speaking, aiming at
modifying speech in a way that little or no stuttering symptoms occur. The
technique includes gentle voice onsets as well as syllable and word bindings, in
which the vibration of the vocal cords is not supposed to stop. It allows PWS
to regain a high degree of control over their own speech and speak much more
ﬂuently. However, applying this technique, especially at the beginning of the
therapy,soundsquiteunnaturalduetotheprolongationsthatarenotpresentin
anormalﬂowofspeech[20].SincecalculatingSESincludesspeechfractionsthat
have been modiﬁed by ﬂuency shaping as ineﬃcient, the measure does not give
a reliable picture of the severity of stuttering in PWS who apply this technique.390 S. P. Bayerl et al.
3.2 Speech Control Index
To address the shortcomings of SES in the context of speech therapy using
ﬂuencyshaping,weproposeanewmethodthatcanbeusedtoassesstheseverity
ofstutteringbutstillisabletoaccountforandmeasuretherapeuticsuccess.The
Speech Control Index (SCI) was developed at the KST and accounts for speech
modiﬁcations which relate to ﬂuency shaping. By adding modiﬁed speech to
the controlled speech, the SCI not only provides a measure for the individual
severity of stuttering, but also whether or not PWS are able to control their
speech by using the speaking technique. The SCI quantiﬁes the proportion of
time between controlled speech components, which means ﬂuent and modiﬁed
speech, and uncontrolled speech components such as disﬂuencies and blocks.
Thus SCI, similar to SES, considers speaking over time.
To achieve this, speech fractions are grouped in one of three categories:
1. Controlledtime-allpartsofspeechproducedthatcanbeconsideredﬂuent
or modiﬁed, which means a PWS uses a speaking technique to overcome
stuttering. Additionally prosodic pauses are added to this category.
2. Disﬂuent time - all parts of a sample that can be identiﬁed as stuttered
disﬂuencies are being counted to disﬂuent speech, i.e. repetitions of sounds,
syllables, words, prolongations, blocks and silent blocks. In addition, speech
fractions containing atypical stuttering disﬂuencies such as the repetition of
phrases, interjections, revisions including incomplete words and phrases are
being added to disﬂuent time.
3. Silence - long pauses in which the PWS is not speaking and not trying to
speak as well as interruptions by the dialogue partner, etc.
Accordingly, “Total time” in Eq.2 is the sum of the three aforementioned
categories.
Based on the correlation between subjective severity rating scales and the
SES, Amir et al. concluded that SES also provides reliable information about
the severity of stuttering [2]. As calculation of SCI is similar to SES beside the
attributionofmodiﬁedspeechfractions,thesameisexpectedtoholdfortheSCI.
In cases where PWS do not use the speaking technique, which can be assumed
for recordings done prior to therapy, both measures are equal. The same is true
for cases in which only little speaking technique is applied, which is conﬁrmed
by Fig.1.
Controlled time
SCI = ·100% (2)
Total time−Silence
3.3 Phone Durations
One of the core symptoms of stuttering is the prolongation of sounds. This
should be directly observable in the time alignment outputs produced by an
automatic speech recognition (ASR) system. Such information can be used to
diﬀerentiatebetweenaPWSandanormalspeakingperson.AmajordiﬃcultyisTowards Automated Assessment of Stuttering and Stuttering Therapy 391
that phone lengths are unique speech properties characteristic of every speaker
and may vary depending on various factors. To generalize such an assessment, a
sample of multiple speakers is necessary. It can be assumed that especially close
to and during a stuttering event, phone durations should on average be longer
than during ﬂuent speech portions. To verify these assumptions, phone align-
ments were produced and categorized with respect to their relative position to
stuttering events: Phones inside labeled disﬂuencies, phones within 0.25s before
a disﬂuency, 0.25s before and after a disﬂuency, 0.25s before, after and inside
a disﬂuency. To have a set that is free from modiﬁcations, which also prolong
phone lengths, a set of phones was chosen which where within speech fractions
labeled as ﬂuent. The sets were then reﬁned by the phone classes vowels, frica-
tives,sonorantsandplosives.Altogether,44setsofphonedurationdistributions
were created, but the individual sets became to small to make generalizable
conclusions.
To obtain the alignments for calculating phone lengths, an ASR system
trainedbasedonthesystemdescribedin[14]wasused.Fortraining,theGerman
part of the Spoken Wikipedia Corpora, the German subset of the m-ailabs read
speech corpus as well as the Tuda-De corpus were used2. Only minor modiﬁca-
tionstothetrainingrecipeweremadetoreducethenumberoftrainingtargetsin
acousticmodeltrainingfrom732to260.ThemodelwastrainedusingtheKaldi
toolkit [21], using speaker adaptive training on top of LDA and MLLT features
[22]. Prior to computing the forced alignments, the lexicon transducer of the
ASR system was modiﬁed to be able to align incomplete words. The transcripts
created for the ﬁles were checked against the lexicon and pronunciations for
missing and incomplete words were generated by using a grapheme-to-phoneme
(g2p) model trained on the original lexicon3.
4 Experiments
SCI and SES were computed for each of the 214 ﬁles in the dataset. Pearson’s
correlationbetweentheSCIandSESoverall214ﬁlesisat0.142andonlyshows
averyweaklinearrelationshipbetweenthetwoindices.Thisisconﬁrmedbythe
distributionplotsinFig.2,andtheirregularplotforSESvaluesoverhigherSCI
values in Fig.1. Comparing the SCI and SES directly, the absolute diﬀerence
is less than 0.1% points for 114 recordings, which is indicated by the plot in
Fig.1. This is exactly the part of the data that has no labeled modiﬁcations in
it,whichissupportedbyacorrelationof1betweenSCIandSESforthispartof
thedata.ThisshowsthatSCIandSESareidenticalforsampleswithoutspeech
modiﬁcations.
2 Kaldi recipe available at https://github.com/uhh-lt/kaldi-tuda-de.
3 G2P tool available online at https://www-i6.informatik.rwth-aachen.de/web/
Software/g2p.html.392 S. P. Bayerl et al.
Fig.1.PlotofSESoverSCIvaluescomputedfromlabelsforeveryﬁleinthedataset.
Crosses representing samples that contain modiﬁed speech, dots representing samples
without.
Fig.2. Value distribution of SCI and SES scores in the dataset (N =214).Towards Automated Assessment of Stuttering and Stuttering Therapy 393
Table 1. Phone duration (in seconds) distributions descriptive statistics.
Dataset N Mean phone Phone dur. Phone dur. Percent
dur. at 90th P at 95th P outlier
Inside disﬂuency 9818 0.230 0.570 0.850 3.14
Before disﬂuency 7898 0.199 0.460 0.670 2.12
Before to after disﬂuency 23227 0.192 0.480 0.730 2.58
All phones 73410 0.150 0.330 0.520 1.99
Fluent 41195 0.109 0.200 0.310 0.84
Fig.3.Fluent,All,andinsidedisﬂuencyphonelengthdistributionsplottedasrelative
portions of phone durations. Area under step function represents percentage of values
inside a 0.1s wide phone length interval.
Table 1 shows the descriptive statistics about the created phone subsets. In
this context, outliers were deﬁned as phones of which the duration is at least
three times the standard deviation σ greater than the mean phone duration in
the overall set. The set containing all phones has about 2% outliers, which is
higherthantheexpectedvalueforthisdeﬁnitionofoutliers.Thediﬀerencetothe
set containing only ﬂuent speech as well as the set inside labeled disﬂuencies is
moststriking.Fluentspeechonlycontains0.84%outliersandtheaveragephone
durationrelativetothesetcontainingallphonesis27%shorter.Phonesinsidea
disﬂuency compared to the set containing all phones are on average 53% longer.
Relative diﬀerence between average duration of phones inside ﬂuently labeled
speech compared to speech inside disﬂuencies is 111%. These numbers show a
clear relationship between phone duration and stuttering related disﬂuencies.394 S. P. Bayerl et al.
It can be concluded that especially phone durations starting from the 90th
percentile within a sample can be very useful in diﬀerentiating stuttered speech
from normal speech. The plot in Fig. 3 supports this observation. It contains
histogramplotsoftherelativeportionofphonedurationsin0.1swideintervals.
This shows that apart from the phones with a duration below 0.2s, the relative
number of phones inside these 0.1 second wide intervals is greatest for phones
inside disﬂuencies. Looking at the relative fraction of phones above or below a
duration of 0.2s, might be enough to diﬀerentiate between ﬂuent and disﬂuent
speech.
5 Conclusion
The SCI provides an accurate measure with similar properties as the SES for
speakers who speak mostly ﬂuent or do not use a special speech technique.
The advantage of SCI is its ability to account for modiﬁed speech of PWS who
underwent therapy and regained a level of ﬂuency and control that is more
eﬀective than stuttering, even though speech may not be classiﬁed as natural or
normal. An extensive comparison between the objective measures SSI-4 (%SS
and mean duration of the three longest symptoms), SES and SCI, as well as a
comparison of these procedures with subjective stuttering severity rating scales
will be a part of future work.
Thedatashowedthatthereisaclearrelationbetweenthedurationofphones
and their relative position to stuttering events. As indicated here, a normal
speech recognition system can be easily modiﬁed to distinguish ﬂuent and dis-
ﬂuentspeechinutterancesbasedonheuristicmeasuresaslongasitcanproduce
alignments.Forthistherecognitionsystemneedstobeabletorecognizeincom-
pletewordsandsyllablerepetitions.Thisinsightwillbeusedtobuildautomatic
stuttering recognition systems that can diﬀerentiate diﬀerent levels of ﬂuency.
Thecomprehensivelylabeleddatasetenablesfutureexplorationofdiﬀerentkinds
ofdisﬂuenciesandtheuseofstatisticallearningmethodssuchassupportvector
machines or neural networks. By classifying the amount of ﬂuent, disﬂuent and
modiﬁed speech in a speech sample, the automated and continuous calculation
of the SCI can provide a reliable measure for stuttering severity and therapy
success.Thiswillprovidevaluablefeedbacktotheclientaswellasthetherapist.
Acknowledgements. TheauthorsthanktheInstitutderKasselerStottertherapiefor
their support and excellent collaboration. This work is supported by a research grant
oftheBayerischesStaatsministeriumfu¨rBildungundKultus,WissenschaftundKunst
as well as the BAYWiss (Bayerisches Wissenschaftsforum).
References
1. Alharbi,S.,Hasan,M.,Simons,A.J.,Brumﬁtt,S.,Green,P.:Alightlysupervised
approach to detect stuttering in children’s speech. In: Proceedings of Interspeech
2018, pp. 3433–3437. ISCA (2018)Towards Automated Assessment of Stuttering and Stuttering Therapy 395
2. Amir, O., Shapira, Y., Mick, L., Yaruss, J.S.: The speech eﬃciency score (SES):
a time-domain measure of speech ﬂuency. J. Fluency Disord. (2018). https://doi.
org/10.1016/j.jﬂudis.2018.08.001
3. Anders, K., Rudorf, E.: Kompendium der Akademischen Sprachtherapie und
Logop¨adie: Bd.3, chap. In: Kohlhammer, W. (ed.) Stottern bei Jugendlichen und
Erwachsenen, pp. 225–241 (2017)
4. Carlson, N.R.: Physiology of Behavior, 11th edn. Pearson, London (2012)
5. Chee,L.S.,Ai,O.C.,Hariharan,M.,Yaacob,S.:MFCCbasedrecognitionofrepe-
titionsandprolongationsinstutteredspeechusingk-NNandLDA.In:2009IEEE
StudentConferenceonResearchandDevelopment(SCOReD),pp.146–149.IEEE
(2009)
6. Ellis, J.B., Ramig, P.R.: J. Fluency Disord. 34(4), 295–299 (2008). https://doi.
org/10.1016/j.jﬂudis.2009.10.004
7. Euler, H.A., Gudenberg, A.W.V., Jung, K., Neumann, K.: Computergestu¨tzte
Therapie bei Redeﬂussst¨orungen: Die langfristige Wirksamkeit der Kasseler Stot-
tertherapie(KST).Sprache·Stimme·Geh¨or33(04),193–202(2009).https://doi.
org/10.1055/s-0029-1242747
8. Euler, H., Gudenberg, A.W.V.: Die Kasseler Stottertherapie (KST). Ergebnisse
einercomputer-gestu¨tztenBiofeedbacktherapief¨orErwachsene1.Sprache-stimme-
gehor 24, 71–79 (2000). https://doi.org/10.1055/s-2000-11084
9. Ingham,R.J.,Kilgo,M.,Ingham,J.C.,Moglia,R.,Belknap,H.,Sanchez,T.:Eval-
uation of a stuttering treatment based on reduction of short phonation intervals.
J. Speech Lang. Hear. Res. 44(6), 1229–1244 (2001)
10. Borden, G.J., Baer, T., Kenney, M.K.: Onset of voicing in stuttered and ﬂuent
utterances. J. Speech Hear. Res. 28, 363–72 (1985). https://doi.org/10.1044/jshr.
2803.363
11. Karimi,H.,O’Brian,S.,Onslow,M.,Jones,M.:Absoluteandrelativereliabilityof
percentage of syllables stuttered and severity rating scales. J. Speech Lang. Hear.
Res. 57(4), 1284–1295 (2014)
12. Lickley,R.:Disﬂuencyintypicalandstutteredspeech.In:Fattorisocialiebiologici
nella variazione fonetica-Social and Biological Factors in Speech Variation (2017)
13. Mallard, A., Kelley, J.: The precision ﬂuency shaping program: replication and
evaluation. J. Fluency Disord. 7(2), 287–294 (1982)
14. Milde, B., K¨ohn, A.: Open source automatic speech recognition for German. In:
Proceedings of ITG 2018 (2018)
15. Mundada, M., Gawali, B., Kayte, S.: Recognition and classiﬁcation of speech and
its related ﬂuency disorders. Int. J. Comput. Sci. Inf. Technol. (IJCSIT) 5(5),
6764–6767 (2014)
16. No¨th,E.,etal.:Automaticstutteringrecognitionusinghiddenmarkovmodels.In:
Sixth International Conference on Spoken Language Processing (2000)
17. Ochi,K.,Mori,K.,Sakai,N.:Automaticevaluationofsoftarticulatorycontactfor
stuttering treatment. Proc. Interspeech 2018, 1546–1550 (2018)
18. Onslow, M., Packman, A., Harrison, E., et al.: The Lidcombe Program of Early
Stuttering Intervention: A Clinician’s Guide. Pro-ed, Austin (2003)
19. Packman, A., Attanasio, J.S.: Theoretical Issues in Stuttering (2004)
20. Packman, A., Onslow, M., Doorn, J.V.: Prolonged speech and modiﬁcation of
stuttering: perceptual, acoustic, and electroglottographic data. J. Speech Lang.
Hear. Res. 37(4), 724–737 (1994)
21. Povey, D., et al.: The Kaldi speech recognition toolkit. In: IEEE 2011 Workshop
(2011)396 S. P. Bayerl et al.
22. Povey,D.,Kuo,H.K.J.,Soltau,H.:Fastspeakeradaptivetrainingforspeechrecog-
nition. In: Ninth Annual Conference of the International Speech Communication
Association (2008)
23. Riley, G.: SSI-4 Stuttering Severity Instrument, 4th edn. (2009)
24. Starkweather, C.W.: Fluency and Stuttering. Prentice-Hall, Inc., Upper Saddle
River (1987)
25. S´wietlicka, I., Kuniszyk-J´o´zkowiak, W., Smo(cid:4)lka, E.: Hierarchical ANN system for
stuttering identiﬁcation. Comput. Speech Lang. 27(1), 228–242 (2013)
26. Webster, R.L.: An operant response shaping program for the establishment of
ﬂuency in stutterers. Final report (1972)
27. Yairi, E., Ambrose, N.G.: Early childhood stuttering I: persistency and recovery
rates. J. Speech Lang. Hear. Res. 42(5), 1097–1112 (1999)Synthesising Expressive Speech – Which
Synthesiser for VOCAs?
B
Jan-Oliver Wu¨lﬁng, Chi Tai Dang, and Elisabeth Andr´e( )
Human-Centred Multimedia, University of Augsburg, Universit¨atsstrasse 6a,
86159 Augsburg, Germany
{wuelfing,dang,andre}@hcm-lab.de
http://www.hcm-lab.de
Abstract. Inthecontextofpeoplewithcomplexcommunicationneeds
whodependonVoiceOutputCommunicationAids,theabilityofspeech
synthesisers to convey not only sentences, but also emotions would be a
greatenrichment.Thelatterisessentialandverynaturalininterpersonal
speechcommunication.Hence,weareinterestedintheexpressivenessof
speech synthesisers and their perception. We present the results of a
studyinwhich82participantslistenedtodiﬀerentsynthesisedsentences
withdiﬀerentemotionalcontoursfromthreesynthesisers.Wefoundthat
participants’ratingsonexpressivenessandnaturalnessindicatethatthe
synthesiser CereVoice performs better than the other synthesisers.
·
Keywords: Complex Communication Needs Voice Output
· ·
Communication Aid Expressive Speech Synthesis Online survey
1 Introduction
How often do we vocally speaking people use our tone of voice to communicate
our intentions, wishes, or desires to a communication partner throughout the
day? Depending on the emotions to be conveyed, the tone of voice is portrayed
by a variation of prosodic features (rhythm, speed and pitch, etc.) and voice
quality [5]. For instance, a sad person has diﬀerent tone of voice than a happy
one. The ﬁrst one typically speaks slower and lower pitched than the latter one.
AsHoﬀmannandWu¨lﬁngpointedoutinasurvey[11]with129participants,
people who cannot or almost not articulate themselves vocally would like to
do the same with the help of their VOCA (Voice Output Communication Aid).
TheseVOCAsfallintothegroupoftechnologiesinthedomainofAAC(Alterna-
tive and Augmentative Communication).VOCAstaketextinputandsynthesise
the input as auditory output. Yet, the possibilities and potentials of synthesis-
ers have not been used extensively. In the past, industry has mainly focused on
naturalness neglecting variability in expressive style. However, as text-to-speech
synthesiserscontinuetoimprove,thequestionarisesofwhethersynthesisersmay
help people use VOCAs to express their feelings, wishes, and intentions as well.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.397–408,2020.
https://doi.org/10.1007/978-3-030-58323-1_43398 J.-O. Wu¨lﬁng et al.
As a ﬁrst step to answer this question, we investigate the expressiveness
of three freely available synthesisers in terms of recognised emotions and their
naturalness in terms of perceived pronunciation quality. For this purpose, we
conducted a survey with 82 participants in order to investigate which of these
synthesisers shows the highest expressiveness and naturalness for sentences gen-
erated for the German language. We decided to evaluate MaryTTS v5.21 devel-
oped collaboratively by the German Research Center for Artiﬁcial Intelligence
and Saarland University and eSpeak2 v1.48.04 developed by Jonathan Duddig-
ton and maintained by Reece Dunn. Both are open source synthesisers. eSpeak
providesvoicescreatedbyusingformantsynthesis.MaryTTSprovidesbothunit
selection and voices based on Hidden-Markov Models (HMM) [13]. As a third
synthesiser, we chose the commercial CereVoice unit selection speech system3
v4.0.6 developed by CereProc’s Ltd. CereVoice is a commercial-grade real-time
ESS (Expressive Speech Synthesis) system [2]. For our study, we used an aca-
demic licence provided by CereProc Ltd.
All three synthesisers have capabilities to manipulate prosodic features and
make use of a markup language that more or less follows the industry standard
SSML (Speech Synthesis Markup Language) v1.14. eSpeak uses SSML, however,
with fewer options to manipulate. For MaryTTS, MaryXML5 serves as its own
datarepresentationformatwhichfacilitatesthesynthesisofprosodicutterances
- the syntax is similar to SSML. In addition to SSML support, CereVoice oﬀers
CereVoiceXMLextensions6foremotionalsynthesiscontrol.Inourpreviouswork
[16], we evaluated how a VOCA that enables the speciﬁcation of certain emo-
tional states via Emojis would be perceived by users with CCN. To this end, we
presentedthemwithaﬁrstprototypeVOCA‘EmotionTalker’(ET)intheirdaily
environment. Here, we focus on which speech synthesiser to use for enhancing a
VOCA with expressive speech. To this end, we compared three publicly avail-
able speech synthesisers (eSpeak, MaryTTS, CereVoice) in a perception study
with 82 participants. Our long-term objective is to pave the way towards a new
generation of VOCAs that convey emotions and personality.
2 Related Work
Recently, the naturalness of synthesised speech has signiﬁcantly improved. In
some cases, it has become hard to distinguish artiﬁcially created voices from
human voices. This is in particular true for commercial speech synthesisers,
such as CereVoice. In the area of speech synthesis, basically two approaches
have been used: unit selection approaches and statistical parametric synthesis
approaches (see [3] for a recent survey). Unit selection approaches make use
1 http://mary.dfki.de (accessed 02/06/20).
2 http://espeak.sourceforge.net (accessed 02/06/20).
3 https://www.cereproc.com/en/products/academic (accessed 11/06/20).
4 https://www.w3.org/TR/speech-synthesis11/ (accessed 02/06/20).
5 http://mary.dfki.de/documentation/maryxml/ (accessed 12/06/20).
6 https://www.cereproc.com/de/products/sdk (accessed 12/06/20).Synthesising Expressive Speech – Which Synthesiser for VOCAs? 399
of a large inventory of human speech units that are subsequently selected and
combinedbasedonthesentencetobesynthesised.Statisticalparametricsynthe-
sis approaches create acoustic models from recorded speech (for example, using
Hidden Markov Models or Deep Neural Networks) that are used to reconstruct
synthesised speech from the generated parameters. Usually, more natural syn-
thesis results are obtained by unit selection approaches. However, unit selection
approaches oﬀer little ﬂexibility to manipulate speech parameters in a way that
diﬀerent emotional styles are conveyed. To give users more control over the syn-
thesised speech, speciﬁc extensions for the industry standard SSML have been
developed, such as CereProc XML extensions or MaryXML, that enable users
to create diﬀerent styles of expressive speech.
To evaluate the quality of the produced speech, a variety of perceptual qual-
ity dimensions of synthetic speech, such as intelligibility and naturalness, have
been deﬁned (see [10] for an overview) that are also employed in the annual
‘Blizzard’ challenge7 on advancing speech synthesis. Also, the emotional atmo-
sphereofasceneandthemoodsofthecharactershavebeenincludedasaquality
dimension in audiobook synthesis tasks. Wagner et al. [15] point out that the
evaluation of TTS is still using criteria from the early days of speech synthesis
researchandargueforauser-centeredapproachthatconsidersalargerdiversity
of usersincluding genderand age. A user-centeredapproach is in particular rec-
ommendedforAACuserswhowouldliketocommunicatewithexpressivevoices
asshowninourpreviousresearch[16].WhendevelopingVOCAswithexpressive
speech, the question arises of how to enable AAC users to control the quality of
speech in an easy manner. Recent work on expressive VOCAs (see [16] and [8])
makes use of expressive keyboards that include Emojis to specify the emotions
to be conveyed. While such interfaces enable an easy speciﬁcation of the emo-
tionalcontent,theyprovidetheAACuseronlywithalimitedamountofcontrol
overthesynthesisedspeech.However,whenbeingengagedinaconversation,the
ﬁne-grained control of a large number of parameters that would ensure a high
quality of expressive speech is no option. For this reason, we decided to focus in
our study on a few set of parameters that can be easily mapped on emotions to
be conveyed without requiring extensive ﬁne-tuning.
3 Study
In order to evaluate the expressive capabilities of the three synthesisers to be
consideredforintegrationintoaVOCA,weperformedanonlinesurvey.Partici-
pants were acquired through a mailing list at the ﬁrst author’s home university,
the news-site of the department to which the authors are aﬃliated, and a forum
entry especially for AAC users and their personal assistants.
3.1 Online Survey
The online survey consisted of 27 WAV-ﬁles (3 sentences * 3 emotions * 3 syn-
thesisers) which were prepared in advance. For the study, we relied on Ger-
man voices. In particular, we used the following voices: eSpeak (Formant, male,
7 http://www.festvox.org/blizzard (accessed 02/06/20).400 J.-O. Wu¨lﬁng et al.
de), MaryTTS (HMM, female, bits1-hsmm), CereVoice (Unit Selection, female,
Gudrun).FollowingMurrayetal.[12],weselectedthreeemotionallyneutralsen-
tences. ‘Emotionally neutral’ means that the semantics of a sentence does not
provide any clue on the speaker ’s emotion. For example, one of the sentences
was “Ich kann da dru¨ben Leute sehen” (engl. “I can see people over there”). In
order to convey the emotions (happy, sad, angry), we used SSML-markups to
manipulate pitch, volume, rate and contour. In light of later integration into an
easy-to-useVOCAGUI,wedidnotexploitthefullpotentialofXMLextensions
toenablemoresophisticatedemotionalcontrol.Theonlinesurveyandsentences
werereviewedbyseveralresearchersintermsofwordingandconveyedemotions.
Structure. The survey had three parts. First of all, participants had to agree
to a DPA (Data Processing Agreement) in order to continue. Then, they had to
provide demographic data including age, sex, and cultural background (in order
to exclude any disposition). Next, participants had to listen to the 27 sentences.
After each audio clip, they were asked in the online survey to type the sentence
heard, to indicate the emotion perceived, and how satisﬁed they were with their
choiceoftheselectedemotion.Afterevaluatingall27sentences,theparticipants
were presented again with three sentences explicitly indicated as happy, sad, or
angry. This time, participants had to mark how satisﬁed they were with the
naturalness and expressivity of the corresponding speech synthesiser. This third
part was designed as a double check of the second part.
Questions. In the second part of the survey, we presented the participants
with a forced response choice. Following the approach of Murray et al. [12], we
included two additional emotions (fear and disgust) and a neutral state as dis-
tractors. That is, we disguised the number and the category of the emotions
actually being tested. The participants had to listen to short sentences played
back through WAV-ﬁles in a randomised order of the speech synthesisers. The
ﬁrstquestioninparttwo“Please,writetheheardsentenceintothebox”(transl.)
was asked in order to identify any acoustic issues. The second question “Which
emotion do you link to the sentence” provided us with the perceived emotion.
The last question “How satisﬁed are you with the choice of the perceived emo-
tion” served as a conﬁdence measure for the previous answer. The third part
of the online survey served to get information on the participants’ subjective
impression of the speech synthesiser. Participants were asked “Please, evaluate
the synthesiser XX in respect ...”, “... to its articulation”, and “... to its expres-
sivity”.
4 Results
The online survey was conducted between February and May 2019 with 82
German-speaking participants, who ﬁlled in the survey completely. We had 32
male/50 female participants aged between 18 and 65years (M = 28.78,SD =Synthesising Expressive Speech – Which Synthesiser for VOCAs? 401
10.61). In addition, the participants had the opportunity to state their origin.
The large majority came from Germany. In addition, Austria, Poland, Rus-
sia, Asia, Latin, and Turkey were stated. Participants needed on average 798 s
(SD =151.41) to complete the survey.
Intotal,participantshadtoevaluatethreesentencesforthreesynthesisersfor
eachofthethreeemotions,i.e.,foreachsynthesiser,theyhadtocorrectlyassign
emotions to nine sentences. The highest number of correctly assessed emotions
were: Seven correct for CereVoice by one participant, six correct for MaryTTS
by one participant, and four correct for eSpeak by nine participants. Regarding
the emotions, the highest number of correctly assessed emotions was achieved
for Sadness (eight hits by 16 participants), followed by Angry (ﬁve hits by six
participants) and Happiness (four hits by three participants).
4.1 Average Number of Recognised Emotions
Which of the synthesisers expresses which of the emotions best? In order to
answer this question, we look at the correctly assessed emotions. Table1 gives
an overview of the mean values and the corresponding standard deviations for
thenumberofcorrectlyassessedemotions fromthesynthesisedsentencesacross
all three synthesisers and separately for each of the three emotion classes. If we
consider only the correctly assessed emotions independent of the synthesisers
(aggregated over all synthesisers), then the class Sad was recognised best with
3.68 sentences (SD = 1.92), followed by the class Angry with M = 2.22,SD =
1.60. The class Happy was expressed the worst of all (M = 1.28,SD = 1.11).
When all emotion classes are considered together for each of the synthesisers,
CereVoice scores best with an average of 2.78 (SD = 1.42) correctly assessed
emotions,closelyfollowedbyMaryTTSwith2.62(SD =1.54)correctlyassessed
emotions. The synthesiser eSpeak has the worst average score of 1.78 (SD =
1.31) correctly assessed emotions.
Table1.Meansandstandarddeviationsforthenumberofcorrectlyassessedemotions.
“Assessed” is abbreviated with A.
#A. emotions per synthesiser (0..9) #A. emotions overall (0..9)
E - eSpeak M =1.78,SD=1.31 a - Angry M =2.22,SD=1.60
M - MaryTTS M =2.62,SD=1.54 h - Happy M =1.28,SD=1.11
C - CereVoice M =2.78,SD=1.42 s - Sad M =3.68,SD=1.92
#A. emotions per synthesiser and emotion class (0..3)
Emotion E - eSpeak M - MaryTTS C - CereVoice
a - Angry M =.94,SD=.85 M =.67,SD=.74 M =.61,SD=.64
h - Happy M =.29,SD=.48 M =.62,SD=.60 M =.37,SD=.53
s - Sad M =.55,SD=.63 M =1.33,SD=.99 M =1.8,SD=.99402 J.-O. Wu¨lﬁng et al.
4.2 Performance Between Synthesisers
Which of the synthesisers has the best/worst numbers of correctly assessed
instances across all emotion classes? The answer to this question is provided
by a comparison of the mean values for assessed emotion instances of all three
synthesisers (see Table1, “#Assessed emotions per synthesiser”).
Here, a repeated measurement ANOVA [9] showed highly signiﬁcant dif-
ferences between the synthesisers (F(2,162) = 18.05,p < .001). The post-
hoc pairwise comparisons (with Bonferroni corrections) for each measured syn-
thesiser revealed that there are signiﬁcant diﬀerences between the synthe-
siser eSpeak (denoted by E) and MaryTTS (denoted by M) (E-M: p < .001,
−.84,95%−CI[−1.27,−.42]) as well as eSpeak and CereVoice (denoted by C)
(E-C: p < .001, −1.00,95% − CI[−1.40,−.60]), meaning that emotions were
in general recognised signiﬁcantly better with MaryTTS and CereVoice than
with eSpeak. Figure1 shows all mean values and standard deviations of Table1,
whereby the blue bars indicate an unequal distribution of the correctly assessed
emotionclasseswithinthesynthesisers,whichwediscussfurtherinthefollowing.
Fig.1. Means and standard deviations of correctly assessed emotions from sentences.
(Color ﬁgure online)
Performance Within Individual Synthesisers. If we consider the mean
values within individual synthesisers, which of the emotion classes is expressed
better/worse than the other classes? To answer this question, we have a more
detailed look at the individual emotion classes across the three synthesisers.
Repeated measurement ANOVAs for each of the emotion classes and syn-
thesisers revealed further diﬀerences of the correctly assessed emotions for eachSynthesising Expressive Speech – Which Synthesiser for VOCAs? 403
of the synthesisers as indicated by the blue bars in Fig.1. We found highly
signiﬁcant diﬀerences in the recognition of synthesised emotions within the
synthesiser eSpeak (F(2,162) = 22.00,p < .001). Post-hoc pairwise compar-
isons showed that the recognition of the emotions Angry, Sad, Happy (in that
order) diﬀer signiﬁcantly from well to badly recognisable. With the synthesiser
CereVoice, the highly signiﬁcant diﬀerences between all three emotion classes
(F(2,162) = 97.59,p < .001), i.e., post-hoc comparisons, showed that Sad was
recognised best and Happy worst. Smaller amount of diﬀerences were found
with MaryTTS (F(2,162) = 23.63,p < .001). The signiﬁcant diﬀerences with
MaryTTS from post-hoc pairwise comparisons showed that Sad could be better
distinguished from the other emotion classes.
4.3 Performance Between Emotion Classes
Considering the recognition of emotion classes aggregated over all synthesisers,
which emotion class is recognised best? And which synthesiser performs best on
whichemotionclass? Fortheanswertothesequestions,wecombinetheassessed
ratesofthediﬀerentemotionclassesacrossallsynthesisers(c.f.,Table1).Forthe
analysis within an emotion class we take a more detailed view on the individual
synthesisers (c.f., green bars in Fig.1).
A repeated measurement ANOVA comparing the correctly assessed emo-
tions for each emotion class showed a highly signiﬁcant diﬀerence (F(2,162) =
59.80,p<.001)incorrectlyassessedemotionsbetweentheemotionclasses.Post-
hoc pairwise comparisons revealed that the class Sad was most frequently and
thus signiﬁcantly more often correctly identiﬁed by participants (a-s: p < .001,
−1.46,95% − CI[−2.04,−.88]; h-s: p < .001, −2.40,95% − CI[−2.97,−1.84])
than for the other classes. Furthermore, the class Angry was signiﬁcantly more
often identiﬁed than the class Happy (a - h: p<.001, .94,95%−CI[.47,1.41]),
meaning that the class Happy was the worst recognisable.
PerformanceWithinIndividualEmotionClasses. Ifweconsiderthemean
values within individual emotion classes, which of the synthesisers expresses the
emotions better/worse than the other synthesisers? To answer this question, we
have a more detailed look at the means across the three synthesisers for each of
the emotion classes (c.f., blue bars in Fig.1).
For the emotion class Angry, we found signiﬁcant diﬀerences (F(2,162) =
6.03, p < .005) between the synthesisers, where post-hoc analysis identiﬁed
eSpeak as signiﬁcantly better recognisable than the other synthesisers. For the
emotion class Happy, there was a signiﬁcant diﬀerence (F(2,162) = 10.52,p <
.005) between the synthesisers in favour of MaryTTS revealed by post-hoc pair-
wisecomparisons.Finally,fortheemotionclassSad,thestatisticsshowedhighly
signiﬁcant diﬀerences (F(2,162)=59.78,p<.001), where the post-hoc analysis
identiﬁed that each of the synthesisers signiﬁcantly diﬀered from each other in
the order CereVoice, MaryTTS, and eSpeak from best to worst.404 J.-O. Wu¨lﬁng et al.
4.4 Satisfaction with the Choice of Assessed Emotions
For each of the assessed emotions, participants were asked to rate on a Likert
scale(“notsatisﬁedatall-1”,“undecided-3”,“verysatisﬁed-5”),howsatisﬁed
theywerewiththechoiceoftheassessedemotionclass.Table2containsallmean
values and standard deviations. Participants seemed to have diﬀerent degrees of
satisfaction with their choice between the synthesisers. While participants rated
on average with less than “undecided-3” for eSpeak, the ratings for MaryTTS
and CereVoice tended to be higher towards “satisﬁed - 4”. However, the mean
values for satisfaction hardly diﬀeredbetween theemotion classes (a, h,s), with
mean values slightly above “undecided - 3”.
Table 2. Overview of the means and standard deviations for the satisfaction ratings
(on a scale of 1 ... 5) for a chosen emotion (and emotion class).
For chosen emotions per synthesiser For chosen emotions
eSpeak M =2.81,SD=0.94 a - Angry M =3.12,SD=0.67
MaryTTS M =3.26,SD=0.63 h - Happy M =3.12,SD=0.71
CereVoice M =3.27,SD=0.63 s - Sad M =3.11,SD=0.74
Satisfaction ratings for chosen emotions per synthesiser and emotion
Emotion E - eSpeak M - MaryTTS C - CereVoice
a - Angry M =2.91,SD=1.10 M =3.27,SD=0.70 M =3.12,SD=0.68
h - Happy M =2.76,SD=1.03 M =3.34,SD=0.75 M =3.26,SD=0.79
s - Sad M =2.76,SD=1.00 M =3.17,SD=0.64 M =3.38,SD=0.78
4.5 Satisfaction Between Synthesisers
Which synthesiser showed the highest satisfaction with the choice on average
when all emotion classes were included? To address this question, we com-
paredthegivensatisfactionratingsbetweeneachofthesynthesisers.Asalready
indicated by Table2, the repeated measurement ANOVA showed highly sig-
niﬁcant diﬀerences between the synthesisers (F(2,162) = 27.33,p < .001).
Overall, the participants were signiﬁcantly more satisﬁed with their choice of
an emotion class while listening to sentences synthesised by MaryTTS (E-
M: p < .001, −.45,95% − CI[−.65,−.26]) and CereVoice (E-C: p < .001,
−.47,95%−CI[−.67,−.26]) than by eSpeak.
Satisfaction Within Individual Synthesisers. With which of the conveyed
emotionclassesweretheparticipantsmostsatisﬁedmeasuredbythemeanvalues
within the synthesisers? For answering this question, we compared the values
within the individual emotion classes across the three synthesisers.Synthesising Expressive Speech – Which Synthesiser for VOCAs? 405
OnlyCereVoiceshowedameasurablesigniﬁcanteﬀect(F(2,162)=3.43,p<
.05), meaning that participants were more satisﬁed with the choice of the class
Sad than with the class Angry (a-s: p=.037, −.20,95%−CI[−.39,−.01]).
4.6 Satisfaction Between Emotion Classes
We also analysed the aggregated satisfaction ratings (all synthesisers together)
fortheemotionstoinvestigatewhethersatisfactionwiththechoiceforoneofthe
emotions was rated distinctly better. However, no signiﬁcant eﬀects were found.
SatisfactionWithinIndividualEmotionClasses. Whichsynthesiserelicits
the highest satisfaction ratings for individual emotion classes? To answer this
question,weconductedANOVAsforeachoftheemotionclassesandsynthesisers.
For all emotion classes, we found signiﬁcant diﬀerences (Angry: F(2,162)=
7.56,p<.01; Happy: F(2,162)=22.199,p<.001; Sad: F(2,162)=24.459,p<
.001) between the synthesisers, where post-hoc analysis identiﬁed eSpeak as sig-
niﬁcantlylesssatisfactorywhenchoosingtheemotionclassthanbothoftheother
synthesisers. In addition, for the emotion class Sad, the post-hoc pairwise com-
parisons also revealed that satisfaction with emotions generated by CereVoice
resulted in signiﬁcantly higher ratings than with MaryTTS.
4.7 Pronunciation/Emotion
In the ﬁnal part of the online survey, participants had to rate both the pro-
nunciation and the synthesised emotions on a Likert scale from very poor (1)
to very good (5). The synthesisers were presented one after the other, and for
each synthesiser sentences with all three emotion classes were generated, which
could be listened to by the participant as often as desired before both ratings
were given. Table3 contains the mean values and standard deviations for both
ratings. The mean values indicate that eSpeak was rated worst and CereVoice
was rated best for pronunciation as well as synthesised emotions.
Table 3. Means and standard deviations for the ratings of pronunciation and synthe-
sised emotions on a scale of 1 ... 5.
Rating of Pronunciation/Emotion (1 .. 5)
Emotion E - eSpeak M - MaryTTS C - CereVoice
Pronunciation M =1.74,SD=.93 M =2.77,SD=.99 M =3.27,SD=.89
Emotion M =1.60,SD=.65 M =3.88,SD=.95 M =4.06,SD=.78406 J.-O. Wu¨lﬁng et al.
Rating of Pronunciation/Synthesised Emotion. A repeated measure-
ment ANOVA on the ratings for pronunciation showed signiﬁcant diﬀerences
between the synthesisers (F(2,162) = 88.83,p < .001). Post-hoc pairwise com-
parisonsrevealedthatpronunciationofthegeneratedsentenceswereratedfrom
best to worse in the order CereVoice, MaryTTS, and eSpeak (E-M: p < .001,
−1.02,95%−CI[−1.32,−.72]; E-C: p < .001, −1.52,95%−CI[−1.81,−1.24];
M-C: p<.001, −.50,95%−CI[−.72,−.23]).
Asimilarpicturecouldbefoundfortheratingsofthesynthesisedemotions.A
repeated measurement ANOVA on the ratings for synthesised emotions showed
signiﬁcant diﬀerences between the synthesisers (F(2,162) = 345.47,p < .001).
The post-hoc pairwise comparisons identiﬁed the synthesiser eSpeak as worse
thanMaryTTSandCereVoice(E-M:p<.001,−2.28,95%−CI[−2.56,−2.0];E-
C:p<.001,−2.46,95%−CI[−2.70,−2.23])intermsofthesynthesisedemotions.
5 Discussion
AsAylettetal.[1]mentioned,thetimetoonlymimicrythenaturalnessofhuman
voiceisover.PeopleespeciallythosewithCCN(ComplexCommunicationNeeds)
have a great need for speech synthesisers that are able to convey a variety of
expressive styles in a natural manner. This aspect is also important in light of
the rapidly increasing speech interaction and its acceptance in smarthomes [6],
to respond appropriately to the emotions of residents [7].
Researchers spent decades in developing natural sounding TTS (Text-to-
Speech) incorporating prosodic elements with diﬀerent approaches. As shown
in Table1, there are diﬀerences in correctly assessing emotions per synthesiser
(indecreasingorder:CereVoice[M=2.78,SD=1.42],MaryTTS[M=2.62,SD
= 1.54], eSpeak [M = 1.78, SD = 1.31]). These results are conﬁrmed in the ﬁnal
partoftheonlinesurvey(seeTable3).Itcomesasnosurprisethatthequalityof
the single synthesisers provided diﬀerent, but consistent subjective assessments
astheyarebasedondiﬀerentunderlyingtechniques:formantsynthesis(eSpeak),
HMM-based synthesis (MaryTTS) and unit selection (CereVoice).
Our results are in line with previous studies investigating the quality of dif-
ferent types of speech synthesisers (see, for example, the chapter on Perceptual
Quality Dimension by [10]). Formant synthesis tends to sound mechanical and
artiﬁcial while the greatest amount of naturalness is typically achieved with
unit selection. Even though we did not exploit the full potential of MaryXML
and CereProc XML to control the quality of the expressive speech, MaryTTS
and CereVoice performed better in terms of expressivity than eSpeak. CereProc
showedthebestresultsbothintermsofsatisfaction withthepronunciation, i.e.
naturalness, and ability to convey emotional states as a whole, i.e. expressive-
ness.Whileitcanbearguedthatweonlyusedsimplemarkups,wehavetotake
into account that CCN users need to be able to control their voices in an easy
and quick manner. The next step would be to integrate capabilities for expres-
sive speech into EmotionTalker by enabling AAC users to specify emotions at a
higher level of abstraction, but still communicate the intended expressive style
in a believable manner.Synthesising Expressive Speech – Which Synthesiser for VOCAs? 407
Thecurrentresearchcomplementsourpreviousresearchontheevaluationof
EmotionTalker, a ﬁrst prototype of a VOCA interface that included Emojis to
enable people specify the intended emotion. For this experiment, we relied on a
small number CCN users who tested EmotionTalker in their daily environment.
Even though we aimed to include AAC users in our current evaluation by con-
tacting an AAC forum, the current evaluation was not speciﬁcally addressed to
AACusers.Thiswasduetoourfocusonaperceptivestudywithalargenumber
of users. For the online survey, it could be objected that we could not control
theparticipants’surroundingsandtheirequipmentforlisteningtothesentences.
However,tocompletethesurvey,participants hadtolistentoallsentenceswith
all synthesisers. So, they had a direct comparison.
6 Conclusion
Our objective was to identify a natural speech synthesiser with variability in
expressive style for integration into a VOCA. To this end, we evaluated the
ability of three synthesisers (eSpeak/MaryTTS/CereVoice) to convey emotion-
ally neutral utterances in a happy, sad, or angry manner. Our assumption that
CereVoice has the best capabilities was conﬁrmed. In our online survey most of
the 82 participants rated CereVoice better than MaryTTS - eSpeak was rated
worst. As outlined by [4], people with CCN may have deﬁcits in building emo-
tional competencies during childhood. In order to improve their capabilities,
it would have potential to equip VOCAs with ESS and usable input methods.
CereVoice seems to be an adequate candidate, as our ﬁndings show.
The next step will be to extend our tests with EmotionTalker. We plan to
havepeoplewithCCNtestEmotionTalkerintheirownenvironmentinspeciﬁed
situations over one week. It has to be shown if they can socialise more easily
withaVOCAcapableofESS.Furthermore,novelsynthesisparadigmsshouldbe
takenintoaccount,seetherecentdevelopmentsontheMaryTTSarchitectureto
enable synthesis based on Deep Neural Networks [14] or the recently announced
neural speech synthesis system CereWave AI by CereProc Ltd.8
Acknowledgements. The work presented here is partially supported by ‘PROMI
- Promotion inklusive’ and the employment centre. We thank the students, Lena
Tikovsky and Ewald Heinz, for their contribution to this work.
References
1. Aylett, M.P., Cowan, B.R., Clark, L.: Siri, echo and performance: you have to
suﬀerdarling.In:ConferenceonHumanFactorsinComputingSystems,Extended
Abstracts, Glasgow, Scotland, UK. ACM, New York (2019). https://doi.org/10.
1145/3290607.3310422
2. Aylett,M.P.,Pidcock,C.J.:Addingandcontrollingemotioninsynthesisedspeech.
Tech. Rep. UK patent GB2447263A (2008)
8 https://www.cereproc.com/en/v6 (accessed 11/06/2020).408 J.-O. Wu¨lﬁng et al.
3. Aylett, M.P., Vinciarelli, A., Wester, M.: Speech synthesis for the generation of
artiﬁcialpersonality.IEEETrans.Aﬀect.Comput.11(2),361–372(2020).https://
doi.org/10.1109/TAFFC.2017.2763134
4. Blackstone, S.W., Wilkins, D.P.: Exploring the importance of emotional compe-
tence in children with complex communication needs. Perspect. Augmentative
Altern. Commun. 18(3), 78–87 (2009). https://doi.org/10.1044/aac18.3.78
5. Chafe, W.: Prosody: the music of language. In: Genetti, C., Adelman, A. (eds.)
How Languages Work - An Introduction to Language and Linguistics, 2nd edn,
pp. 236–256. Cambridge University Press, Cambridge (2019)
6. Dang,C.T.,Andre,E.:Acceptanceofautonomyandcloudinthesmarthomeand
concerns. In: Dachselt, R., Weber, G. (eds.) Mensch und Computer 2018 (MuC
2018) - Tagungsband (2018)
7. Dang,C.T.,Aslan,I.,Lingenfelser,F.,Baur,T.,Andr´e,E.:Towardssomaesthetic
smarthomedesigns:exploringpotentialsandlimitationsofanaﬀectivemirror.In:
Proceedings of the 9th International Conference on the Internet of Things. IoT
2019. Association for Computing Machinery, New York (2019). https://doi.org/
10.1145/3365871.3365893
8. Fiannaca, A.J., Paradiso, A., Campbell, J., Morris, M.R.: Voicesetting: voice
authoring UIs for improved expressivity in augmentative communication. In:
Mandryk, R.L., Hancock, M., Perry, M., Cox, A.L. (eds.) Proceedings of the 2018
CHI Conference on Human Factors in Computing Systems, CHI 2018, Montreal,
QC, Canada, 21–26 April 2018, p. 283. ACM (2018). https://doi.org/10.1145/
3173574.3173857
9. Girden, E.R.: ANOVA: Repeated Measures. Sage, Newbury Park (1992)
10. Hinterleitner, F.: Quality of Synthetic Speech. TSTS. Springer, Singapore (2017).
https://doi.org/10.1007/978-981-10-3734-4
11. Hoﬀmann, L., Wu¨lﬁng, J.O.: Usability of electronic communication aids in the
light of daily use. In: Proceedings 14th Biennial Conference of the International
Society for Augmentative and Alternative Communication, p. 259 (2010)
12. Murray,I.R.,Arnott,J.L.:Applyingananalysisofactedvocalemotionstoimprove
the simulation of synthetic speech. Comput. Speech Lang. 22(2), 107–129 (2008).
https://doi.org/10.1016/j.csl.2007.06.001
13. Schr¨oder, M., Charfuelan, M., Pammi, S., Steiner, I.: Open source voice creation
toolkitfortheMARYTTSplatform.In:INTERSPEECH2011,12thAnnualCon-
ference of the International Speech Communication Association, Florence, Italy,
27–31 August 2011, pp. 3253–3256. ISCA (2011)
14. Steiner, I., Maguer, S.L.: Creating new language and voice components for the
updated marytts text-to-speech synthesis platform. In: Calzolari, N., et al. (eds.)
ProceedingsoftheEleventhInternationalConferenceonLanguageResourcesand
Evaluation, LREC 2018, Miyazaki, Japan, 7–12 May 2018. European Language
Resources Association (ELRA) (2018)
15. Wagner, P., et al.: Speech synthesis evaluation - state-of-the-art assessment and
suggestionforanovelresearchprogram.In:Proceedingsofthe10thISCASpeech
Synthesis Workshop, pp. 105–110 (2019). https://doi.org/10.21437/SSW.2019-19
16. Wu¨lﬁng, J.-O., Andr´e, E.: Progress to a VOCA with prosodic synthesised speech.
In:Miesenberger,K.,Kouroupetroglou,G.(eds.)ICCHP2018.LNCS,vol.10896,
pp. 539–546. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94277-
3 84Perceived Length of Czech High Vowels
in Relation to Formant Frequencies
Evaluated by Automatic Speech
Recognition
B
Tom´aˇs Boˇril( ) and Jitka Veronˇkova´
Faculty of Arts, Institute of Phonetics, Charles University, N´am. Jana Palacha 2,
Praha 1, Czech Republic
{tomas.boril,jitka.veronkova}@ff.cuni.cz
Abstract. Recent studies measured signiﬁcant diﬀerences in formant
valuesintheproductionofshortandlonghighvowelpairsintheCzech
language. Perceptional impacts of such ﬁndings were conﬁrmed employ-
inglisteningtestsprovingthataperceivedvowellengthisinﬂuencedby
formantvaluesrelatedtoatongueposition.Non-nativespeakersofCzech
mayexperiencediﬃcultiesincommunicationwhentheyinterchangethe
vowel length in words, which may lead to a completely diﬀerent mean-
ingofthemessage.Thispaperanalysesperceptionoftwo-syllablewords
with manipulated duration and formant frequencies of high vowels i/i:
or u/u: in the ﬁrst syllable using automatic speech recognition (ASR)
system.Suchaproceduremakesitpossibletosetaﬁneresolutioninthe
range of examined factors. Our study conﬁrms the formant values have
a substantial impact on the perception of high vowels’ length by ASR,
comparabletomeanvaluesobtainedfromlisteningtestsperformedona
group of human participants.
· · ·
Keywords: High Czech vowels Vowel length Vowel quality
·
Automatic speech recognition Perception
1 Introduction
The acquisition of a vowel system is one of the key aspects of learning a sec-
ond language (L2). Czech vowel system consists of ﬁve pairs of short and long
monophthongs and three diphthongs [8,12,13]. Since the vowel length is phono-
logically distinctive, its improper interchange in L2 speakers’ production may
lead to a misunderstanding (e.g.., /kru:ci: farma:ri/ vs /kruci: farma:ri/ (mean-
ﬁ ﬁ
ing turkey farmers vs cruel farmers).
This research was supported by the Czech Science Foundation project No. 18-18300S
“Phonetic properties of Czech in non-native and native speakers’ communication”.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.409–417,2020.
https://doi.org/10.1007/978-3-030-58323-1_44410 T. Boˇril and J. Veronˇkov´a
Diﬀerences between formant values (i.e., vowel quality correlating with a
tongue setting in a vocal tract) of a short and a long vowel in a pair are tradi-
tionally described as insigniﬁcant (both in production and perception perspec-
tive) except for /i/ and /i:/ [5]. Later, a diﬀerentiation of short [I] and long [i:]
symbols in the international phonetic alphabet (IPA) was proposed [4].
Inadditiontostatisticalevaluationofproductiondataof[I]and[i:],[10]per-
formedaperceptionanalysisofmanipulateditemswithastimulusarraycovering
the spectral and the durational span between both vowels in one syllable where
both lengths create meaningful words with a comparable probability frequency.
Thestudyalsofoundasigniﬁcant diﬀerenceintheperceptionofBohemian(the
western part of the Czech Republic) and Moravian (the eastern region of the
Czech Republic) where Bohemians relied more on the spectrum, whereas Mora-
vians relied more on the duration. Later, [14] found diﬀerences in pronunciation
of [I] and [i:] in the speech of Czech Radio newsreaders.
[12] focuses on the production of Czech in Bohemian and Moravian regions,
and there is a clear trend of the [u] vs [u:] formant shift in the Bohemian sub-
group in addition to the previously observed [I] and [i:] relation. Spontaneous
Czech speech was analysed in [7], the formant shifts between [I] – [i:] and [u]
– [u:] were measured, and also a promising diﬀerence between the [o] and [o:]
formant positions appeared. [9] conducted a listening test with artiﬁcial one-
syllable pseudowords containing manipulations of [I] – [i:] and [u] – [u:] vowels
analogous to [10] experiment. In both Czech high vowels, the quality (formant
values) played a crucial role in a vowel length discrimination in the subgroup of
listeners from the Bohemian region.
The main purpose of this paper is to compare the automatic speech recogni-
tion(ASR)ofCzechhighvowels’lengthwithhumanperceptionin3experiments.
Toemphasizethediﬀerenceofqualities,wedecidedtousethe[U]IPAsymbol
for the short vowel and [u:] for the long vowel in the following text.
Experiment 1 examines perception of quantity (phonological length) of vow-
els [I] and [i:] based on their quality (formant values in the spectrum). ASR
evaluates items manipulated with a ﬁne resolution in both duration and for-
mant dimensions. A subset of this data set with a less detailed formant scale is
also evaluated perceptually by human participants (Bohemian region) in a lis-
teningtest.ThequestioniswhetherASRperceivestheboundarybetweenshort
and long vowels in a comparable manner and whether these results correspond
to [10].
Experiments 2 and 3 analyse ASR behaviour on vowels [U] and [u:] manip-
ulated similarly. Experiment 3 focuses on the ﬁne detail of the transition part
foundinexperiment2. Thequestionis,whethertheeﬀectofformant values has
an impact on the perceived length in compliance with the novel ﬁndings in [9],
where artiﬁcial one-syllable pseudowords were tested by human participants in
a listening test.
The ASR approach applied in this study may bring several advantages. The
number of items in a listening test is naturally limited due to the requirement
of keeping human participants entirely focused. For this reason, the number ofPerceived Length of Czech High Vowels in Relation to Formant Frequencies 411
tested factors and the resolution of coverage of their span have to be notably
decreased in many experiments. The purpose of such experiments is to map a
subjectiveperceptionofrandomindividualsandthentoestimatethemeanvalue
ofthepopulation.ASRsystemsaretrainedonalargesampleofthepopulation,
andhencetheyalsomayprovideevaluation similar toanaveragerepresentative
of the population. Such a procedure can be repeated many times with diﬀerent
settings and a large number of items, which would be impossible with human
participants of listening tests.
2 Method
2.1 Experiment 1
For the ﬁrst experiment, we created 147 manipulated items (21 formant steps
and 7 duration steps) using Praat [2] and rPraat [3] as follows. A minimal pair
consisting of two words [vIrI] (meaning viruses) and [vi:rI] (meaning vortices)
was chosen to serve as boundaries lying on a diagonal of a two-dimensional
duration–formant space to be explored. The advantage of the analysis of vowel
in the ﬁrst syllable is that it is not prone to phrase-ﬁnal lengthening [15].
We recorded both words in a slow speech rate by an adult female speaker in
a quiet low-reverb room (PCM uncompressed, the sample rate of 32kHz, 16-bit
depth).EstimatedmedianvaluesofformantfrequenciesF1–F4ofthetarget(ﬁrst
syllable)short[I]were405,2295,2866,and4099Hzandofthelong[i:]were305,
2700,3000,and4099Hz(weroundedthefourthformantvaluesinbothvowelsto
thesamenumberbecauseinstantaneousvaluesreachedalargevariabilityaround
roughly the same values in both short and long vowels). For the manipulation
purposes, we chose the record of [vi:rI] as a basis because stimuli with shorter
durations of the target vowel can be easily created by truncating the original
long vowel.
Theupper-partspectrumofthebasisstimulusobtainedbyahigh-passHann
ﬁlter with a cut-oﬀ frequency of 4500Hz was stored as a separate signal to be
returned to manipulated signals at the ﬁnal step of stimuli creation to obtain a
more natural sound with a full range of the spectrum.
To obtain the source (excitation) signal and formant object, the basis stim-
ulus was resampled to 16000Hz and processed using the Burg method of linear
predictive coding (LPC) with a prediction order of 15 (leading to max. 7 for-
mant frequencies detected), 25 ms segmentation window length with 5 ms time
step and pre-emphasis frequency of 50Hz. Note: preliminary, prediction orders
of 16 and 15 were examined in all experiments, the order of 16 in experiment
1 lead to an unnatural, artiﬁcial distortion at high frequencies; the order of 16
was necessary for experiments 2 and 3. To avoid random jumps in the formant
object, formant trajectories were subsequently smoothed by a formant-tracking
algorithm with 4 formant tracks.
In the next step, formant frequencies in the time interval of the ﬁrst vowel
durationweremanipulatedbetweenthevaluesof[I]and[i:]in21linearsteps.For412 T. Boˇril and J. Veronˇkov´a
eachstep,themanipulatedsoundwascreatedbyﬁlteringthesource(excitation)
signal with the formant ﬁlter.
Finally,tocreatethewholesetoftargetstimuli,eachsoundﬁlewasobtained
by a concatenation of the ﬁrst part of the original basis stimulus (until the ﬁrst
vowel), the shortened vowel from formant-manipulated signals with the upper-
part spectra signal superposed, and the remaining part of the basis stimulus.
The target vowel was shortened to durations in the range from 90ms to 300ms
in 7 linear steps.
Automatic Speech Recognition. To evaluate manipulated stimuli by an
automatic speech recognition system (ASR), we concatenated all stimuli in a
random order into one long sound ﬁle. Each item was separated by a short
pause and a neutral nonmanipulated word [vlakI] (meaning trains) by the same
speaker to reduce possible interferences of two successive manipulated stimuli
and also to clearly distinguish the boundaries of tested items in case the item
was not recognized properly, e.g.., as two separate one-syllable words.
In total, we prepared ﬁve replications of the experiment, i.e., ﬁve diﬀerent
permutations of all manipulated items with diﬀerent random order to avoid a
possible eﬀect of the order of stimuli.
To evaluate the concatenated sound ﬁle, we employed a commercial state-
of-the-art ASR system Beey by NEWTON Technologies [6] set to the Czech
language recognition and with additional text postprocessing switched oﬀ.
Although all nonmanipulated ﬁller-words [vlakI] were recognized correctly,
the ASR occasionally had problems with the recognition of manipulated items
(probably due to their overall lower quality) and recognized them as a diﬀerent
wordoracoupleoftwoone-syllablewords,e.g.,[vi:lI](fairies),[bIlI](theywere),
[I vI] (also you) or [bI jI] (would her). Not surprisingly, the consonants were
aﬀected,andthevowelsremainedeither[I]or[i:].Forthisreason,wefocusedon
the length of the ﬁrst-syllable vowel [I] or [i:] in such cases, ignoring mismatches
in consonants.
For each item, the resulting score was calculated as a mean value of all ﬁve
replications of the experiment.
Listening Test. To compare the results of ASR with human perception, we
performed a listening test with 20 participants (native speakers of Czech, both
maleandfemalestudents,medianage=23years)usingcomfortableheadphones
in a quiet room. To keep them focused throughout the test, we decided to select
a subset of items only. The resolution of the vowel duration scale was kept the
same, i.e., 7 linear steps between 90ms and 300ms. The resolution of formant
transition was reduced to 5 discrete steps, resulting to 35 items. In addition to
these “items-of-interest”, other 15 two-syllable words with diﬀerent vowels were
included as distractors. Each of the total of 50 items in the test was initiated
with a short desensitization beep sound.
The listening test was administrated using Praat multiple forced-choice
(ExperimentMFC)environment[2].Afterashorttrainingset(6items)toresolvePerceived Length of Czech High Vowels in Relation to Formant Frequencies 413
possible problems and questions, the main test with 50 items in a random order
for each listener was performed. Each item could be played three times at the
most.Thetaskwastoclickonabuttonwiththewordclosesttothesound(both
words with a short and a long vowel in the ﬁrst syllable were oﬀered). After the
ﬁrst 25 items, the participants were instructed to take a short break and listen
to a song included in the test folder.
2.2 Experiment 2
In the second experiment, we created a set of stimuli focused on short [U] and
long [u:]. We recorded an adult male voice saying [krUci:] (meaning cruel in
plural) and [kru:ci:] (meaning turkey adjective).
Estimated median values of formant frequencies F1 – F4 of the target (ﬁrst
syllable) short [U] were 360, 906, 2774, and 3994Hz, and of the long [u:] were
288,567,2774,and3994Hz(weroundedthethirdandthefourthformantvalues
inbothvowelstothesamenumberbecauseinstantaneousvaluesreachedalarge
variability around roughly same values in both short and long vowels).
The process of manipulation was conducted in the same manner as in the
experiment 1; the LPC prediction order was set to 16. The transition between
two formant boundaries was divided into 19 linear steps. The duration of the
vowel in the ﬁrst syllable ranged from 90ms to 300ms in 7 linear steps.
This time, only the ASR task was performed with ﬁve random permutations
of stimuli. Each item was concatenated with a nonmanipulated word [farma:rI]
ﬁ
(meaning farmers), both variants creating a meaningful phrase with a similar
probabilityfrequency,i.e.,theASRshouldnotpreferonevariantattheexpense
of the other.
2.3 Experiment 3
The third experiment continued with the same original records of experiment
2, but we aimed at the middle transient area. The formant axes were focused
on the lower two thirds (as compared to experiment 2) with detailed 21 steps,
and the duration focused on the middle part ranging from 125ms to 230ms in
9 detailed steps.
3 Results
Weareawareofthefactourﬁndingsdependonaspeechrate,aprosody,andan
individual speaker’s vocal space area; therefore we do not want to interpret our
resultsasabsolutevaluesofboundariesbetweenshortandlongvowelperception.
Sincethisdependencecanbearesultofacomplexcombinationofmanyfactors,
we do not even normalise duration and formant values because it could imply
a universal rule. Rather than that, we focus on the shape of boundaries in the
duration – formants relation which reﬂects the fact the vowel length perception
is inﬂuenced by formant values, i.e., vowel quality.414 T. Boˇril and J. Veronˇkov´a
3.1 Experiment 1
The results of ASR are depicted in Fig. 1a; a grey value of each rectangle rep-
resents a mean value of ﬁve replications of the experiment. Due to the statistic
approach of ASR, some items were classiﬁed diﬀerently in some of the replica-
tions, which is mostly the case of items near the visible edge between short and
long area.
Fig.1.Evaluationof[I]and[i:]vowelsinexperiment1.Thevowelismanipulatedboth
in duration and formant values, Fratio stands for ratio on the range between formant
values of natural [I] and [i:]. Shades of grey represent mean values of evaluated vowel
lengths from (a) 5 realisations of ASR, (b) 20 participants of the listening test (white
= long, black = short).
The shortest items (duration of 90 ms) were identically identiﬁed as short
vowel [I]. All other items above the 90 ms duration were split into short [I] and
long[i:]withanalmosthorizontalboundaryimplyingASRusedthevowelquality
(i.e., spectrum) as the main cue to diﬀerentiate these two variants. This result
complies with Bohemian Czech listeners in [9] (analysing artiﬁcial one-syllable
words), although the ASR boundary seems slightly more horizontal.
We tested a statistic signiﬁcance of duration and Fratio (a ratio on the span
between typical formant values of the short and the long vowel) eﬀects using
mixed-eﬀects models with logistic regression (binomial family for binary out-
come)[1]in[11].Bothﬁxedeﬀects(durationandFratio)werecentredandstan-
dardised,replicationwasarandomeﬀect.Themodelformula(includingrandom
slopes) is length∼duration+Fratio+(1+duration+Fratio|replication), p-
values were obtained by likelihood ratio tests of the full model with the eﬀect
against the model without the eﬀect.
For both Fratio and duration eﬀects, p < 0.001. We also passed a subset
of data with a duration equal or larger than 160ms, and for the Fratio, p-value
remained<0.001;however,fortheduration,p=0.2554.ThisﬁndingcorrespondsPerceived Length of Czech High Vowels in Relation to Formant Frequencies 415
with 1a very well because vowel quality seems to be the main cue for longer
durations.
Figure 1b represents the mean values of 20 participants of our listening test.
These results are similar to ASR decision in Fig. 1a, although the Fratio scale
is sampled in much fewer steps. However, for durations equal to or larger than
230ms, some listeners evaluated items with Fratio = 0 (i.e., [I]) as long. Statis-
tical evaluation of both ﬁxed eﬀects was conducted analogously to the one with
the ASR, subject (human participant) being a random eﬀect. For both Fratio
and duration, p < 0.001.
3.2 Experiment 2
Figure2arepresentsresultsofASRevaluatingrecordswithmanipulated[U]/[u:]
vowels.Fordurationslowerthanorequalto125ms,allvowelswererecognizedas
short despite the Fratio. For longer durations, the eﬀect of the Fratio is visible.
For both Fratio and duration, p < 0.001.
These ASR results are closely comparable to the relations observed in the
listening tests of one-syllable pseudo-words in [9].
3.3 Experiment 3
Theresultsofexperiment3(i.e.,detailzoomofthetransitionareaofexperiment
2) are depicted in Fig. 2b. The impact of vowel quality on recognized length is
apparent and compatible with observations in [9]. For both Fratio and duration
factors, p < 0.001.
Fig.2. Evaluation of [U] and [u:] vowels. Fratio stands for ratio of the range between
formantvaluesofnatural[U]and[u:].Shadesofgreyrepresentmeanvaluesof5evalu-
atedvowellengthsbyASRin(a)experiment2,(b)experiment3(white=long,black
= short).416 T. Boˇril and J. Veronˇkov´a
4 Conclusions
Inthetaskoftheevaluationofperceivedphonologicalvowellength,ASRtrained
on an extensive sample of population reached results comparable with listening
tests conducted on human subjects. The recent ﬁndings of the impact of vowel
quality on perceived length of Czech high vowels in one-syllable pseudo-words
[9] were conﬁrmed on real two-syllable words in this paper.
Due to its phonological status in Czech, a mismatch in the vowel length
could lead to misunderstandings and generally diﬃcult communication, which
is typical of foreign learners of the Czech language. Interestingly, based on our
informal observation, we can say the vast majority of na¨ıve L1 users of Czech
language is not aware of these diﬀerences in quality of short and long pairs of
high vowels. Teachers of L2 Czech learners, especially during the pronunciation
training, should be aware of the fact that their perception of the phonological
vowel length could be inﬂuenced not only by the vowel duration but also by the
quality of short and long high vowel pairs.
The ASR technique may bring advantages in the process of evaluation in
such a way that the count of items is not limited and the range of examined
parameters can be covered in much more detail than in listening tests. On the
other hand, a combination with these perception experiments is recommended
as they may uncover additional eﬀects such as region, sex or age of the listener.
References
1. Bates,D.,M¨achler,M.,Bolker,B.,Walker,S.:Fittinglinearmixed-eﬀectsmodels
using lme4. J. Stat. Softw. 67(1), 1–48 (2015)
2. Boersma, P., Weenink, D.: Praat: doing phonetics by computer [Computer pro-
gram]. Version 6.1.10 (2020). http://www.praat.org/
3. Boˇril,T.,Skarnitzl,R.:ToolsrPraatandmPraat.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,
I.,Pala,K.(eds.)TSD2016.LNCS(LNAI),vol.9924,pp.367–374.Springer,Cham
(2016). https://doi.org/10.1007/978-3-319-45510-5 42
4. Dankoviˇcov´a, J.: Czech. J. Int. Phonetic Assoc. 27(1–2), 77–80 (1997)
5. H´ala, B.: Akustick´a podstata samohl´asek. Cˇesk´a akademie vˇed a umˇen´ı (1941)
6. NEWTON Technologies: Beey [web-based platform]. Version 0.7.16.5 (2020).
https://editor.beey.io
7. Paillereau,N.,Chl´adkov´a,K.:SpectralandtemporalcharacteristicsofCzechvow-
els in spontaneous speech. AUC PHILOLOGICA 2019(2), 77–95 (2019)
8. Palkov´a, Z.: Fonetika a fonologie ˇceˇstiny: s obecny´m u´vodem do problematiky
oboru. Univerzita Karlova, vydavatelstv´ı Karolinum (1994)
9. Podlipsky´, V.J., Chl´adkov´a, K., Sˇim´aˇckov´a, Sˇ.: Spectrum as a perceptual cue to
vowel length in Czech, a quantity language. J. Acoust. Soc. Am. 146(4), EL352–
EL357 (2019). Acoustical Society of America
10. Podlipsky´, V.J., Skarnitzl, R., Vol´ın, J.: High front vowels in Czech: a contrast in
quantity or quality? In: Proceedings of Interspeech, vol. 2009, pp. 132–135 (2009)
11. RCoreTeam:R:ALanguageandEnvironmentforStatisticalComputing.RFoun-
dationforStatisticalComputing,Vienna,Austria(2020).https://www.R-project.
org/Perceived Length of Czech High Vowels in Relation to Formant Frequencies 417
12. Sˇim´aˇckov´a, Sˇ., Podlipsky´, V.J., Chl´adkov´a, K.: Czech spoken in Bohemia and
Moravia. J. Int. Phonetic Assoc. 42(2), 225–232 (2012)
13. Skarnitzl, R., Sˇturm, P., Vol´ın, J.: Zvukov´a ba´ze ˇreˇcov´e komunikace: Foneticky´
a fonologicky´ popis ˇreˇci. Univerzita Karlova v Praze, Nakladatelstv´ı Karolinum
(2016)
14. Skarnitzl, R.: Dvoj´ı i vˇcesk´e vy´slovnosti. Naˇseˇreˇc 95(3), 141–153 (2012)
15. Vol´ın,J.,Skarnitzl,R.:TemporaldowntrendsinCzechreadspeech.In:Proceedings
of Interspeech, vol. 2007, pp. 442–445 (2007)Inserting Punctuation to ASR Output
in a Real-Time Production Environment
Pavel Hlub´ık1,2, Martin Sˇpanˇel1, Marek Boha´ˇc1,
B
and Lenka Weingartova´1( )
1 NEWTON Technologies, Na Pankr´aci 1683/127, 140 00 Prague, Czech Republic
{pavel.hlubik,martin.spanel,marek.bohac,lenka.weingartova}@newtontech.cz
2 Faculty of Information Technology, Czech Technical University in Prague,
Prague, Czech Republic
Abstract. The output of a speech recognition system is a continuous
stream of words that has to be post-processed in various ways, out of
which punctuation insertion is an essential step. Punctuated text is far
morecomprehensibletothereader,canbeusedforsubtitling,andisnec-
essary for further NLP processing, such as machine translation. In this
article,wedescribehowstate-of-the-artresultsintheﬁeldofpunctuation
restorationcanbeutilizedinaproduction-readybusinessenvironmentin
theCzechlanguage.Arecurrentneuralnetworkbasedonlongshort-term
memory is employed, making use of various features: textual based on
pre-trained word embeddings, prosodic (mainly temporal), morphologi-
cal, noise information, and speaker diarization. All the features except
morphological tags were found to improve our baseline system. As we
work in a real-time setup, it is not possible to employ information from
the future of the word stream, yet we achieve signiﬁcant improvements
using LSTM. The usage of RNN also allows the model to learn longer
dependenciesthananyn-gram-basedlanguagemodelcan,whichweﬁnd
essentialfortheinsertionofquestionmarks.ThedeploymentofanRNN-
basedmodelthusleadstoarelative22.6%decreaseinpunctuationerrors
and improvement in all metrics but one.
· ·
Keywords: Automatic speech recognition Czech language
Punctuation insertion
1 Introduction
Inoureverydaybusinesspracticeweemployautomaticspeechrecognition(ASR)
for diﬀerent purposes, from meeting transcriptions to subtitling or media mon-
itoring. We have long keenly felt the absence of a reliable punctuation adding
system in our target languages, in this case Czech. The usefulness of punctua-
tion marks (full stops, commas, question marks, etc.) is twofold - it signiﬁcantly
improvesthecomprehensibilityofatextforahumanreader,butalsoitisneces-
saryforfurthernaturallanguageprocessing(NLP)oftherecognizedtext,which
Supported by the Technology Agency of the Czech Republic (No. FW01010468).
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.418–425,2020.
https://doi.org/10.1007/978-3-030-58323-1_45Inserting Punctuation in a Real-Time Production Environment 419
requires reliable sentence boundaries. Morphosyntactic tagging, machine trans-
lation, dialogue analysis and other advanced algorithms all display signiﬁcantly
worsened performance without - or with incorrect - sentence boundaries. There
isampleanecdotalevidenceforambiguitycausedbyincorrectlyplacedcommas,
such as “stop clubbing, baby seals” or “let’s eat kids”, but the truth is that in
real-worldspeechtechnologyapplications,theseerrorsmaywellrendertheASR
output unusable without manual corrections by human editors.
While our company does employ human editors to create 100% transcripts
for speciﬁc purposes, adding punctuation to recognized text was by our internal
review evaluated as one of the priorities to shorten editing time and speed up
the process (the other being emergent content out-of-vocabulary tokens).
Our previous approach to punctuation adding was rule-based. Regarding
commas, contexts with high-probability of a comma were mined from text cor-
pora and reﬁned by linguists. The Czech language has quite a rigid system of
comma placement based on syntax, therefore these rules were able to add com-
mas with high precision around subordinate clauses. Other contexts, such as
parentheses, vocatives, or enumerating items in lists, could not be captured.
Full stops were added based on length of non-speech events (mainly pauses),
which also is not without speciﬁc disadvantages. Both systems were comple-
mentedwithasetofblacklists,statingcontextswherepunctuationmarksshould
not occur. No attempt was made to create rules for question mark placement.
Due to the aforementioned limitations of our baseline system, a neural net-
work-based approach was developed, the results of which are presented here.
Working in a production environment comes with its own set of advantages
and limitations. One of the main advantages is access to data - our company
transcribes approximately 920 TV, radio and internet broadcasts a week, which
arethencorrectedbyhumanannotators.Ourchieflimitationliesinthefactthat
punctuation needs to be inserted into streamed data in real-time. The nature
of our workﬂow does not allow for processing ﬁnished ASR outputs, but needs
to run in parallel. This brings up the challenge of inserting punctuation into
unﬁnished sentences or phrases.
Within this experiment, we considered adding full stops, commas and ques-
tion marks only, since other punctuation symbols were too rare in our datasets.
1.1 Related Works
Many previous works about punctuation restoration have been published. Usu-
ally n-grams are used in multiple ways: they can be directly used to train a
language model [15], or they can serve as a basis for a set of rules (as in our
case) [1,2]. N-gram models unfortunately suﬀer from sparsity issue [18], which
hurts their ability of generalization. In recent years, recurrent neural networks
(RNN),namelytheirtype-longshort-termmemory(LSTM),[5]-provedtobe
abletogeneralizemuchbetterwithregardtounseensequences,partially dueto
their ability to work with larger contexts than any n-gram. This also leads us to
believe that a model based on LSTM could be suitable for predicting question
marks, as these usually depend on the beginning of a sentence.420 P. Hlub´ık et al.
We base our approach on the work of Tilk and Aluma¨e [18], where a two-
stagemodelisproposed.First,apurelytextualmodelistrainedontheWikipedia
dataset, subsequently the output of its last hidden layer is utilized as high-level
input features for a second stage, which also takes pause duration into account.
This is followed by a bidirectional RNN with attention mechanism [19], which
unfortunately does not ﬁt the constraints of our setup.
Worth mentioning is also the use of convolutional neural networks (CNN).
As CNNs prove to be eﬀective in various tasks regarding sequences – sentence
classiﬁcation or sentiment analysis [6,7], it might be useful to employ them for
punctuation insertion as well [3]. A proposed approach is to consider a sliding
window of m words, a sample matrix formed with embedding of these words
by convolutional ﬁlters of various sizes and then predict punctuation after word
wm/2.
2 Method
As mentioned above, our approach is constrained by our production workﬂow.
Our system incorporates many technological submodules and its back-end is
complex. In this paper we focus our attention solely on the submodule adding
punctuation to recognized words. For a diagram of the whole system see Fig.1.
Fig.1. Diagram of our ASR workﬂow. Voice-to-text (V2T) and diarization (DIA)
modules take audio input and produce slightly delayed real-time streams of timed
events. Post-processing module (PP) provides formatting of numbers, abbreviations,
titles, etc. The delay between V2T and DIA event streams is variable, the stream
merger module compensates for this variability between event streams.
The stream merger is the connection point between the whole processing
chain and the punctuation submodule. The voice-to-text (V2T) module [11–13]
produces two types of events: words and non-speech events. At the end time of
everyV2Teventthereisaslotthatcancontaindocumentevents(e.g.speakeror
language change). Slots after word events may also contain punctuation marks.
Some slots are disabled by the post-processing (PP) module that binds speciﬁc
words together. The stream merger also implements several heuristic overrides
(e.g. changing comma after the last word of the utterance into a full stop).Inserting Punctuation in a Real-Time Production Environment 421
2.1 Baseline Punctuation Module
Themainmotivationforourworkistoimproveourcurrentpunctuationmodule
by incorporating machine learning algorithms. The baseline module is a hierar-
chicchainofpartialtasksstartingwiththosewithhighestprecision(inorderto
limitexcesspunctuation):i)rule-basedaddingofcommas,ii)black-listblocking
of slots, iii) utilization of diarization and iv) tempo-based addition of full stops.
Commaadditionisbasedonlargetextcorporaanalysisemployedtoextracta
setofn-gramrules(upto2wordsbeforeandafterpunctuationposition).These
rules are implemented via weighted ﬁnite-state transducers as shown in [1,2].
These rules were optimized towards high precision and cannot process language
phenomenasuchasenumerationsorparentheses.Black-listslotblockingisaset
of rules that prevent the punctuation module from placing full stops in certain
slots, such as after prepositions or inside frequent collocations.
Full stop placing is triggered in free slots followed by longer non-speech
events.Thelengththresholdofnon-speechisadaptedviaaﬂowingwindowthat
logs the duration of last 20 observed non-speech events (hesitations, breaths,
silences, etc.). Moreover, full stops are automatically placed into slots where
speaker change occurs. This rule can also override an already placed comma.
2.2 Dataset
The dataset used for this task consists of machine transcriptions of various
broadcasts from the Czech TV and radio collected over several months. These
machine transcriptions do not contain any punctuation. We also possess man-
ual corrections of these transcriptions, in which mistakes of the ASR system
are corrected by human editors and punctuation is added. To mimic the con-
ditions of the model’s future deployment, we did not wish to train it on these
corrected transcripts, therefore a sequence alignment of machine and manually
corrected transcripts is performed, and punctuation is projected into original
machine transcriptions, which can be then used as labelled data.
As is the usual practice, we split the dataset into training, validation and
test parts.1 For number of ﬁles and tokens in each part, see Table1.
Table 1. Dataset size. Tokens are speech events provided by the ASR system.
Split Files Tokens
Train 11204 12.5M
Validation 467 480k
Test 448 421k
1 The test set is publicly available on: http://newtontech.net/punctuator/tsd2020
testdata.zip.422 P. Hlub´ık et al.
2.3 Features
Inanattempttoutilizealltheinformationwepossesaboutthetext,wetriedto
employ various feature types from previous works. With regard to information
related to prosody of speech, we decided to incorporate temporal features only.
Thereasonsweretwofold:otherprosodicfeatures,suchasF0contoursorenergy
features, were not shown to add much to the accuracy of punctuation insertion
[4,14],theirextractionfromthespeechsignalismorediﬃcultandrequireslonger
computing time, thereby forbidding their application in our real-time use case.
Consequently, we opted to include 4 types of features that could be reliably
acquired directly from the raw ASR output:
Textual features consisted of 300-dimensional word embeddings, obtained
by a pre-trained fasttext embedder [10]. They were based on n-grams of char-
acters rather than single words to more eﬀectively deal with unknown words.
Information about morphology is also to some extent contained in the embed-
dings and it saves us the need to lemmatize words.
Prosodic features (primarily temporal) were related to both words and
non-speech segments. We extracted word duration, word tempo, type of non-
speechsegment(pause,breath,hesitationorothernoise)anditsduration.Word
tempoindicatesrelativeincreaseordecreaseofwordduration,therebycapturing
phrase-ﬁnal lengthening and other changes. It was computed as the ratio of real
word duration to its predicted duration based on phonemes contained in the
word and word length. Reference phoneme durations were taken from [20].
Morphological featureswereextractedusingtheMorphoDiTatagger[16].
Diarization features were employed in a 3-dimensional vector. The ﬁrst
position (0 or 1) indicates whether a speaker change point occurred during the
word or preceding noise events. The second position represents the time oﬀset
ofthechangepoint fromthebeginningoftheword,thethirdposition beingthe
oﬀset value if the change point occurred within a preceding noise.
2.4 Model
ThemodelweuseisanLSTM-basedRNNwith2hiddenlayers.Theﬁrstoneis
a dense layer of size 100 with softmax activations acting as an input ﬁlter, the
second one an LSTM layer with 200 units. The output layer is a softmax layer
of size 4, which corresponds to the number of classes we use.
In training, the ﬁrst three predicted labels are omitted and the others are
shifted to the left, so that the network predicts punctuation with delay of three
words, utilizing the features of three words after predicted punctuation.
The input vectors for the model have the dimension of 396 and consist of
concatenated feature vectors of all types described above.
We employed Tensorﬂow 2.0 to create and train our models. In all exper-
iments, we trained the model with the Adam optimizer [8] and learning rate
α=10−4.OneGPUwasutilizedfortraining,whichwehavefoundtobringsuf-
ﬁcient speedup. Dropout rate of 0.2 is applied to all hidden layers, which seems
to be enough to prevent overﬁtting. Convergence of the model usually occurs
after 300 epochs, which corresponds to several hours in our training setup.Inserting Punctuation in a Real-Time Production Environment 423
3 Experiments and Results
One of our goals during the experiments was to evaluate the contribution of dif-
ferenttypesoffeatures.AsexpectedwithregardtoﬁndingsofTilkandAluma¨e
[18], temporal prosodic features (which include pause duration) are useful for
insertingfullstops.Amodeltrainedonprosodicfeaturesonlyoutperformedour
baseline in inserting full stops, however failed terribly when it came to commas,
withrecall<1%.Thiscorroboratestheﬁndingsof[4]and[9],andisinagreement
with our knowledge about non-ﬁnal prosodic boundaries, which can be marked
bymeansofdiﬀerentprosodicevents(seee.g.[17]),sinceprosodicfeatureswork
in synergy and can to some extent substitute one another.
Morphological features were employed in a hope they would be beneﬁcial in
certainsituations,suchasinsertingcommasintoenumerations.Thisassumption
did not hold. Excluding these features from training vectors did not worsen
performance of our model and a model trained exclusively on them performed
very poorly overall.
The most signiﬁcant ﬁnding about feature importance is the model’s sen-
sitivity to diarization features. In the training data, speaker change points co-
occurwithfullstopsandquestionmarks,asspeakersusuallyﬁnishtheirsentence
beforeanotherspeakertakestheﬂoor.Butthisaccountsdirectlyonlyforapprox-
imately a third of full stops/question marks present in the data set. We found
out that if we omit these features - replace them with zeros - to mimic a situa-
tion when the diarization system is temporarily unavailable, performance drops
signiﬁcantly. This drop cannot be explained by errors around speaker change
points only, as there are not enough of these. We hypothesize that a decision to
insert a full stop greatly changes the model’s hidden state, which in turn aﬀects
many future predictions.
3.1 Performance Comparison and Discussion
One of our main concerns regarding evaluation of the model is the comparison
withourbaselinesystem.Wemeasuretheperformanceofbothmodelsinasetup
that simulates conditions under which the model is going to be used in produc-
tion. The model is deployed on server and receives one word at a time, which
willbethecaseinproduction.Ourneuralnetwork-basedmodeloutperformsthe
baselineinallmetricsexceptone:Thebaselinesystemshowshigherprecisionin
inserting commas. Full results can be found in Table2.
From the comparison of the results it can be seen that the new model
reducesnumberoferrorsinpunctuationby22.6%,whichshouldbringavaluable
improvement in our workﬂow. On the other hand, the neural network seems to
struggle with precision when inserting commas. The baseline, rule-based model
is very precise in this regard, with precision of 80.7%. Neural network trades
some of this precision for recall, which might not necessarily be a desired result
in a production case. We have concluded from pilot testing that excess commas
are perceived by human readers as more noticeable errors than missing ones.424 P. Hlub´ık et al.
Table 2. Comparison of performance: precision and recall for full stops (.), commas
(,) and question marks (?), plus a total error rate. Precision is a rate of true positives
(TP)overthenumberofTPplusfalsepositives(FP).RecallmeansthenumberofTP
over the number of TP+FP. Total error rate is the fraction of misclassiﬁed samples.
Prec(.) Rec(.) Prec(,) Rec(,) Prec(?) Rec(?) Err.
Baseline 0.587 0.421 0.807 0.572 – – 0.0903
Neural network 0.720 0.660 0.739 0.641 0.611 0.172 0.0699
Relative improvement 22.7% 56.7% −8.4% 12.0% – – 22.6%
Furthermore, we hypothesize that the use of RNN allows the model to learn
longerdependencies,whichisessentialfortheinsertionofquestionmarks,which
are usually coded at the beginning of the sentence.
It should be noted that classes in our data set are not evenly distributed.
There is one majority class of “blank” symbol (i.e. no punctuation). Generally,
training algorithms are known to suﬀer due to nonuniform prior distribution of
classes. However, due to satisfying results and a higher desirability of precision
at the expense of recall, we were not forced to tackle the issue.
4 Conclusion and Future Work
We presented a RNN-LSTM-based approach to inserting punctuation into ASR
output utilizing pre-trained word embeddings, prosodic features consisting of
temporal information about words and non-speech events (e.g. pause duration,
word tempo, noise type), morphological and diarization features. This approach
was tested in a production-ready environment, where the data stream is pro-
cessed word by word, without the possibility to look into the future.
This new model achieved a total error rate of 6.99% and outperformed our
baseline (a rule-based model) in all metrics but one. The relative decrease of
punctuation errors is 22.6%. Our results imply that the least useful feature in
our experiment were morphological tags, which did not add much to overall
performance. On the other hand, word embeddings, prosodic and diarization
features all contributed to performance improvement. The diarization features
especially seem to add a signiﬁcant value.
Inthefuturewewouldliketoexploresomeotherpossibilities.Asthebaseline
model performs well on commas, we would like to utilize its potential. One
way could be including its predictions into feature vectors used by the network.
Another way might be training an ensemble model per se.
Inputtextpreprocessingisalsoanareawheremanymoreexperimentscanbe
done. When manually evaluating punctuated text, we noticed a lot of mistakes
aroundnumbers.Whileourevidenceforthisclaimremainsspeculative,wewould
liketoexplorethecontributionofsubstitutingallnumberswithasinglenumber
token. We also decided not to lemmatize words in a hope that more meaning
would be preserved in word sequences, yet lemmatization remains a standard
step in an NLP pipeline and should be at least evaluated in our case.Inserting Punctuation in a Real-Time Production Environment 425
References
1. Boh´aˇc, M., Blavka, K.: Using suprasegmental information in recognized speech
punctuation completion. In: TSD (2014)
2. Boh´aˇc, M., Rott, M., Kov´aˇr, V.: Text punctuation: an inter-annotator agreement
study. In: TSD (2017)
3. Che,X.,Wang,C.,Yang,H.,Meinel,C.:Punctuationpredictionforunsegmented
transcript based on word vector. In: LREC (2016)
4. Christensen, H., Gotoh, Y., Renals, S.: Punctuation annotation using statistical
prosodymodels.In:ITRWonProsodyinSpeechRecognitionandUnderstanding,
pp. 35–40 (2001)
5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–80 (1997)
6. Kalchbrenner, N., Grefenstette, E., Blunsom, P.: A convolutional neural network
for modelling sentences. In: Proceedings of the 52nd Annual Meeting of the ACL
(Volume 1: Long Papers), pp. 655–665 (2014)
7. Kim, Y.: Convolutional neural networks for sentence classiﬁcation. In: EMNLP,
pp. 1746–1751 (2014)
8. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: Interna-
tional Conference on Learning Representations (2014)
9. Levy, T., Silber-Varod, V., Moyal, A.: The eﬀect of pitch, intensity and pause
duration in punctuation detection. In: 2012 IEEE 27th Convention of Electrical
and Electronics Engineers in Israel, pp. 1–4 (2012)
10. Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., Joulin, A.: Advances in pre-
training distributed word representations. In: LREC (2018)
11. Nouza, J., et al.: Making Czech historical radio archive accessible and searchable
for wide public. J. Multimedia 7, 159–169 (2012)
12. Nouza, J., et al.: Speech-to-text technology to transcribe and disclose 100,000+
hoursofbilingualdocumentsfromhistoricalCzechandCzechoslovakradioarchive.
In: Interspeech (2014)
13. Nouza, J., Sˇafaˇr´ık, R., Cˇerva, P.: ASR for South Slavic languages developed in
almost automated way. In: Interspeech, pp. 3868–3872 (2016)
14. O¨ktem,A.,Farru´s,M.,Wanner,L.:AttentionalparallelRNNsforgeneratingpunc-
tuation in transcribed speech. In: SLSP, pp. 131–142 (2017)
15. Stolcke, A., et al.: Automatic detection of sentence boundaries and disﬂuencies
based on recognized words. In: ICSLP (1998)
16. Strakov´a, J., Straka, M., Hajiˇc, J.: Open-source tools for morphology, lemmatiza-
tion, POS tagging and named entity recognition. In: Proceedings of 52nd Annual
Meeting of the ACL: System Demonstrations, pp. 13–18 (2014)
17. Swerts, M.: Prosodic features at discourse boundaries of diﬀerent strength. J.
Acoust. Soc. Am. 101, 514–21 (1997)
18. Tilk,O.,Alum¨ae,T.:LSTMforpunctuationrestorationinspeechtranscripts.In:
Interspeech (2015)
19. Tilk,O.,Alum¨ae,T.:Bidirectionalrecurrentneuralnetworkwithattentionmech-
anism for punctuation restoration. In: Interspeech (2016)
20. Weingartov`a, L.: Identiﬁkace mluvˇc´ıho v tempor´aln´ı dom´enˇe ˇreˇci [Speaker iden-
tiﬁcation in the temporal domain of speech]. Ph.D. thesis, Charles University in
Prague (2015)Very Fast Keyword Spotting System
with Real Time Factor Below 0.01
Jan Nouza(B) , Petr Cˇerva , and Jindˇrich Zˇdˇa´nsky´
Institute of Information Technologies and Electronics,
Technical University of Liberec, Studentska 2, 46117 Liberec, Czech Republic
{jan.nouza,petr.cerva,jindrich.zdansky}@tul.cz
Abstract. In the paper we present an architecture of a keyword spot-
ting (KWS) system that is based on modern neural networks, yields
goodperformanceonvarioustypesofspeechdataandcanrunveryfast.
We focus mainly on the last aspect and propose optimizations for all
the steps required in a KWS design: signal processing and likelihood
computation,Viterbidecoding,spotcandidatedetectionandconﬁdence
calculation.Wepresenttimeandmemoryeﬃcientmodellingbybidirec-
tionalfeedforwardsequentialmemorynetworks(analternativetorecur-
rent nets) either by standard triphones or so called quasi-monophones,
and an entirely forward decoding of speech frames (with minimal need
forlookback).Severalvariantsoftheproposedschemeareevaluatedon
3largeCzechdatasets(broadcast,internetandtelephone, 17hintotal)
and their performance is compared by Detection Error Tradeoﬀ (DET)
diagramsandreal-time(RT)factors.Wedemonstratethatthecomplete
system can run in a single pass with a RT factor close to 0.001 if all
optimizations(includingaGPUforlikelihoodcomputation)areapplied.
· ·
Keywords: Spoken term detection Keyword spotting Deep neural
· ·
network Feedforward sequential memory network Real-time factor
1 Introduction
Keywordspotting(KWS)isafrequentlyusedtechniqueinspokendataprocess-
ingwhosegoalistodetectselectedwordsorphrasesinspeech.Itcanbeapplied
oﬀ-line for fast search in recorded utterances (e.g. telephone calls analysed by
police[1]),largespokencorpora(likebroadcastarchives[2]),ordatacollectedby
call-centres [3]. There are also on-line applications, namely for instant alerting,
used in media monitoring [4] or in keyword activated mobile services [5].
The performance of a KWS system is evaluated from two viewpoints. The
primary one is a detection reliability, which aims at missing as few as possible
keywords occurring in the audio signal, i.e. to achieve a low miss detection rate
(MD), while keeping the number of false alarms (FA) as low as possible. The
second criterion is a speed as most applications require either instant reactions,
or they are aimed at huge data (thousands of hours), where it is appreciated
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.426–436,2020.
https://doi.org/10.1007/978-3-030-58323-1_46Very Fast Keyword Spotting System with Real Time Factor Below 0.01 427
if the search takes only a small fraction of their duration. The latter aspect is
often referred to as a real-time (RT) factor and should be signiﬁcantly smaller
than 1.
There are several approaches to solve the KWS task [6]. The simplest and
often the fastest one, usually denoted as an acoustic approach, utilizes a strat-
egysimilartocontinuousspeechrecognitionbutwithalimitedvocabularymade
of the keywords only. The sounds corresponding to other speech and noise are
modelled and captured by ﬁller units [7]. An LVCSR approach requires a very
large continuous speech recognition (LVCSR) system that transcribes the audio
ﬁrst and after that searches for the keywords in its text output or in its inter-
nal decoder hypotheses arranged in word lattices [8]. This strategy takes into
account both words from a large representative lexicon as well as inter-word
context captured by a language model (LM). However, it is always slower and
fails if the keywords are not in the lexicon and/or in the LM. A phoneme lattice
approach operatesonasimilarprinciplebutwithphonemes(usuallyrepresented
bytriphones)asthebasicunits.Thekeywordsaresearchedwithinthephoneme
lattices [9]. The crucial part of all the 3 major approaches consist in assign-
ing a conﬁdence score to keyword candidates and setting thresholds for their
acceptance or rejection. The basic strategies can be combined to get the best
propertiesofeach,asshowne.g.in[10,11],andingeneral,theyadoptatwo-pass
scheme.
The introduction of deep neural networks (DNN) into the speech processing
domain has resulted in a signiﬁcant improvement of acoustic models and there-
forealsointheaccuracyoftheLVCSRandphonemebasedKWSsystems.Vari-
ousarchitectureshavebeenproposedandtested,suchasfeedforwardDNNs[12],
convolutional (CNN) [13] and recurrent ones (RNN) [14]. A combination of the
Long Short-Term Memory (LSTM) version of the latter together with the Con-
nectionistTemporalClassiﬁcation(CTC)method,whichisanalternativetothe
classic hidden Markov model (HMM) approach, have become popular, too. The
CTCprovidesthelocationandscoringmeasureforanyarbitraryphonesequence
as presented e.g. in [15]. Moreover, modern machine learning strategies, such as
trainingdataaugmentationortransferlearninghaveenabledtotrainKWSalso
for various signal conditions [16] and languages with low data resources [17].
The KWS system presented here is a combination of several aforementioned
approachesandtechniques.Itallowsforsearchinganyarbitrarykeyword(s)using
an HMM word-and-ﬁller decoder that accepts acoustic models based on various
types of DNNs, including feedforward sequential memory networks that are an
eﬃcient alternative to RNNs [19]. An audio signal is processed and searched
within a single pass in a frame synchronous manner, which means that no inter-
mediate data (such as lattices) need to be precomputed and stored. This allows
forveryshortprocessingtime(under0.01RT)inanoﬀ-linemode.Moreover,the
execution time can be further reduced if the same signal is searched repeatedly
with a diﬀerent keyword list. The system can operate also in an on-line mode,
where keyword alerts are produced with a small latency. In the following text,428 J. Nouza et al.
we will focus mainly on the speed optimization of the algorithms, which is the
main and original contribution of this paper.
2 Brief Description of Presented Keyword Spotting
System
The system models acoustic events in an audio signal by HMMs. Their smallest
unitsarestates.Phonemesandnoisesaremodelledas3-statesequencesandthe
keywords as concatenations of the corresponding phoneme models. All diﬀerent
3-state models (i.e. physical triphones in a tied-state triphone model) also serve
astheﬁllers.Henceanyaudiosignalcanbemodelledeitherasasequenceofthe
ﬁllers, or - in presence of any of the keywords – as a sequence of the ﬁllers and
the keyword models. During data processing, the most probable sequences are
continuouslybuiltbytheViterbidecoderandiftheycontainkeywords,theseare
located and further managed. The complete KWS system is composed of three
basic modules. All run in a frame synchronous manner. The ﬁrst one – a signal
processing module - takes a frame of the signal and computes log-likelihoods
for all the HMM states. The second one – a state processing module – controls
Viterbirecombinationsforallactivekeywordsandﬁllerstates.Thethirdone–a
spot managing module – focuses on the last states of the keyword/ﬁller models,
computes diﬀerences in accumulated scores of the keywords and the best ﬁller
sequences,evaluatestheirconﬁdencescoresandthosewiththescoreshigherthan
athresholdarefurtherprocessed.Thisschemeassuresthatthedataisprocessed
almost entirely in the forward direction with minimum need for look-back and
storage of already processed data.
3 KWS Speed and Memory Optimizations
Thepresentedworkextends–inasigniﬁcantway–theschemeproposedin[18].
Therefore, we will use a similar notation here when explaining optimizations in
the three modules. The core of the system is a Viterbi decoder that handles
keywords w and ﬁllers v in the same way, i.e. as generalized units u.
3.1 Signal Processing Module
It computes likelihoods for each state (senone) using a trained neural network.
This is a standard operation which can be implemented either on a CPU, or
on a GPU. In the latter case, the computation may be more than 1000 times
faster. Yet, we come with another option for a signiﬁcant reduction in the KWS
execution.
The speed of the decoder depends on the number of units that must be pro-
cessedineachframe.Wecannotchangethekeywordnumberbutletusseewhat
can be done with the ﬁllers. Usually, their list is made of all diﬀerent physical
triphones, which means a size of several thousands of items. If monophones areVery Fast Keyword Spotting System with Real Time Factor Below 0.01 429
used instead, the number of ﬁllers would be equal to their number, i.e. it would
be smaller by 2 orders and the decoder would run much faster, but obviously
with a worse performance.
We propose an optional alternative solution that takes advantages from
both approaches. We model the words and ﬁllers by something we call quasi-
monophones, which can be thought as triphone states mapped to a monophone
structure. In each frame, every quasi-monophone state gets the highest likeli-
hood of the mapped states. This simple triphone-to-monophone conversion can
beeasilyimplementedasanadditionallayeroftheneuralnetworkthatjusttakes
maxvaluesfromthemappednodesinthepreviouslayer.Thebeneﬁtisthatthe
decoder handles a much smaller number of diﬀerent states and namely ﬁllers.
In the experimental section, we demonstrate the impact of this arrangement on
KWS system’s speed and performance.
3.2 State Processing Module
The decoder controls a propagation of accumulated scores between adjacent
states. At each frame t, new score d is computed for each state s of unit u by
adding log likelihood L (provided by the previous module) to the higher of the
scores in the predecessor states:
d(u,s,t)=L(s,t)+max[d(u,s−i,t−1)] (1)
i=0,1
Let us denote the score in the unit’s end state sE as
D(u,t)=d(u,sE,t) (2)
and T(u,t) be the frame where this unit’s instance started. Further, we denote
two values dbest and Dbest:
dbest(t)=max[d(u,s,t)] (3)
u,s
Dbest(t)=max[D(u,t)] (4)
u
The former value serves primarily for pruning, the latter is propagated to
initial states s of all units in the next frame:
1
d(u,s1,t+1)=L(s1,t+1)+max[Dbest(t),d(u,s1,t)] (5)
3.3 Spot Managing Module
This module computes acoustic scores S for all words w that reached their last
states. This is done by subtracting these two accumulated scores:
S(w,t)=D(w,t)−Dbest(T(w,t)−1) (6)430 J. Nouza et al.
The word score S(w,t) needs to be compared with score S(vstring,t) that
would be achieved by the best ﬁller string vstring starting in frame T(w,t) and
ending in frame t.
R(w,t)=S(vstring,t)−S(w,t) (7)
In[18],theﬁrstterminEq.7iscomputedbyapplyingtheViterbialgorithm
within the given frame span to the ﬁllers only. Here, we propose to approximate
its value by this simple diﬀerence:
S(vstring,t)∼=Dbest(t)−Dbest(T(w,t)−1) (8)
The left side of Eq. 8 equals exactly the right one if the Viterbi backtrack-
ing path passes through frame T(w,t), which can be quickly checked. A large
experimental evaluation showed that this happens in more than 90% cases. In
the remaining ones, the diﬀerence is so small that it has a negligible impact on
further steps.
Hence, by substituting from Eq. 6 and Eq. 8 into Eq. 7 we get:
R(w,t)=Dbest(t)−D(w,t) (9)
The value of R(w,t) is related to the conﬁdence of word w being detected in
the given frame span. We just need to normalize it and convert it to a human-
understandable scale where number 100 means the highest possible conﬁdence.
We do it in the following way:
R(w,t)
C(w,t)=100−k (10)
(t−T(w,t))NS(w)
The R value is divided by the word duration (in frames) and its number of
HMM states Ns, which is further multiplied by constant k before subtracting
the term from 100. The constant inﬂuences the range of the conﬁdence values.
We set it so that the values are easily interpretable by KWS system users (see
Sect. 5.4).
The previous analysis shows that the spot managing module can be made
very simple and fast. In each frame, it just computes Eq. 9 and 10 and the
candidates with the conﬁdence scores higher than a set threshold are registered
in a time-sliding buﬀer (10 to 20 frames long). A simple ﬁlter running over the
buﬀer content detects the keyword instance with the highest score and sends it
to the output.
3.4 Optimized Repeated Run
Inmanypracticalapplications,thesameaudiodataissearchedrepeatedly,usu-
ally with diﬀerent keyword lists (e.g. during police investigations). In this case,
the KWS system can run signiﬁcantly faster if we store all likelihoods and two
additional values (dbest and Dbest) per frame. In the repeated run, the signal
processing part is skipped over and the decoder can process only the keywords
because all information needed for optimal pruning and conﬁdence calculation
is covered by the 2 above mentioned values.Very Fast Keyword Spotting System with Real Time Factor Below 0.01 431
4 System and Data for Evaluation
4.1 KWS System
The KWS system used in the experiments is written in C language and runs
on a PC (Intel Core i7-9700K). In some tasks we employ also a GPU (GeForce
RTX 2070 SUPER) for likelihood computation.
We tested 2 types of acoustic models (AM) based on neural networks. Both
accept16kHzaudiosignals,segmentedinto25mslongframesandpreprocessed
to 40 ﬁlter bank coeﬃcients. The ﬁrst uses a 5-layer feedforward DNN trained
onsome1000hofCzechdata(amixofreadandbroadcastspeech).Thesecond
AM utilizes a bidirectional feedforward sequential memory network (BFSMN)
similartothatdescribedin[19].Wehavebeenusingitasaneﬀectivealternative
of RNNs. In our case, it has 11 layers, each covering 4 left and 4 right temporal
contexts. This AM was trained on the same source data augmented by about
400h of (originally) clean speech that passed through diﬀerent codecs [20]. For
both types of the NNs we have trained triphone AMs, for the second also a
monophone and quasi-monophone version.
4.2 Dataset for Evaluation
Three large datasets have been prepared for the evaluation experiments, each
covering a diﬀerent type of speech (see also Table 1). The Interview dataset
contains10completeCzechTVshowswithtwo-personstalkinginastudio.The
Stream dataset is made of 30 shows from Internet TV Stream. We selected the
shows with heavy background noise, e.g. Hudebni Masakry (Music Masacres
in English). The Call dataset covers 53 telephone communications with call-
centers(inseparatedchannels)anditisamixofspontaneous(client)andmainly
controlled (operator) speech. All recordings have been carefully annotated with
time information (10 ms resolution) added to each word.
Table 1. Datasets for evaluation and their main parameters.
Dataset Speech type Signal type Total duration [min] # keywords
Interview Planned Studio 272 3524
Stream Informal Heavy noise 157 1454
Call Often spontaneous Telephone 613 2935
5 Experimental Evaluation
5.1 Keyword List
Ourgoalwastotestthesystemunderrealisticconditionsand,atthesametime,
to get statistically conclusive results. A keyword list of 156 word lemmas with432 J. Nouza et al.
555 derived forms was prepared for the experiments. For example, in case of
keyword“David”weincludeditsderivedforms“David”,“Davida”,“Davidem”,
“Davidovi”, etc. in order to avoid false alarms caused by words being substrings
ofothers.Thelistwasmadebycombining80mostfrequentwordsthatoccurred
in each of the datasets, from which some were common and some appeared
only in one set. The searched word forms had to be at least 4 phonemes long.
The mean length of the listed word forms was 6.9 phonemes. The phonetic
transcriptions were automatically extracted from a 500k-word lexicon used in
our LVCSR system.
5.2 Filler Lists
Thelistofﬁllerswascreatedautomaticallyforeachacousticmodel.Thetriphone
DNN model generated 9210 ﬁllers and the triphone BFSMN produced 10455 of
them.Incontrasttotheselargenumbers,themonophoneandquasi-monophone
BFSMN model had only 48 ﬁllers (representing 40 phonemes + 8 noises).
5.3 Evaluation Conditions and Metrics
A word was considered correctly detected if the spotted word-form belonged to
the same lemma as the word occurring in the transcription at the same instant
- with tolerance ±0.5 s. Otherwise it was counted as a false alarm. For each
experimentwecomputedMissedDetection(MD)andFalseAlarm(FA)ratesas
a function of acceptance threshold value, and drawn a Detection Error Tradeoﬀ
(DET) diagram with a marked Equal Error Rate (EER) point position.
5.4 Evaluation Results
The Interview dataset was used as a development data, on which we experi-
mented with various models, system arrangements and also user preferences. In
accord with them, the internal constant k occurring in Eq. 10 was set to locate
the conﬁdence score equal to 75 close to the EER point. The ﬁrst part of the
experiments focused on the accuracy of the created acoustic models. We tested
thetriphoneDNNand3versionsoftheBFSMNone.Theirperformanceisillus-
trated by DET curves in Fig. 1, where also the EER values are displayed. It is
evident that the BFSMN-tri model performs signiﬁcantly better than the DNN
one, which is mainly due to its wider context span. This is also a reason why
even its monophone version has performance comparable to the DNN-tri one.
The proposed quasi-monophone BFSMN model shows the second best perfor-
mance but the gap between it and the best one is not that crucial, especially if
we take into account its additional beneﬁts that will be discussed later.
Similar trends can be seen also in Fig. 2 and Fig. 3 where we compare the
same models (excl. the monophone BFSMN) on the Stream and Call datasets.
In both cases, the performance of all the models was worse (when compared to
that of the Interview set) as it can be seen from the positions of the curves andVery Fast Keyword Spotting System with Real Time Factor Below 0.01 433
KWS DET - Interview dataset
50
BFSMN-mono    EER=16%
45 BFSMN-quasi-mono    EER=11%
BFSMN-tri    EER=9%
40 DNN-tri    EER=16%
%]
S [35
N
TIO30
C
TE25
E
D
D 20
E
S
S15
MI
10
5
0
0 5 10 15 20 25 30 35 40 45 50
FALSE ALARMS [%]
Fig.1. KWS results for the Interview dataset in form of DET curves drawn for 4
investigated neural network structures.
the EER values. This is due to the character of speech and signal quality as
explained is Sect. 4.2. Yet, we can notice the positive eﬀect of the training of
the BFSMN models on the augmented data (with various codecs), especially on
the Call dataset. Again, the gap between the best triphone and the proposed
quasi-monophone version seems to be not that critical.
KWS DET - Stream dataset
50
BFSMN-quasi-mono    EER=21%
45 BFSMN-tri    EER=19%
DNN-tri    EER=29%
40
%]
S [35
N
TIO30
C
TE25
E
D
D 20
E
S
S15
MI
10
5
0
0 5 10 15 20 25 30 35 40 45 50
FALSE ALARMS [%]
Fig.2. DET curves compared for 3 models on the Stream dataset
Now, we shall focus on the execution time of the proposed scheme. As
explained in Sect. 3, the three modules of the KWS system can be split into
2parts:theﬁrstwiththesignalprocessingmodule,thesecondwiththeremain-
ing two. Both can run together on a PC (in a single thread), or if extremely434 J. Nouza et al.
KWS DET - Call dataset
50
45
40
%]
S [35 BFSMN-quasi-mono    EER=23%
N BFSMN-tri    EER=18%
TIO30 DNN-tri    EER=40%
C
TE25
E
D
D 20
E
S
S15
MI
10
5
0
0 5 10 15 20 25 30 35 40 45 50
FALSE ALARMS [%]
Fig.3. DET curves compared for 3 models on the Call dataset
fast execution is required, the former can be implemented on a GPU. We tested
both approaches and measured their RT factors. Similar measurements (across
allthetreedatasets)wereperformedalsointhesecondpartforalltheproposed
variants and operation modes (see Table 2 for results.). The total RT factor is
obtained by adding the values for selected options in each of the two parts.
Table 2. Execution times for proposed KWS variants expressed as RT factors.
System part, variant, mode Real-time factor
Part 1 (signal proc. module)
on CPU 0.12
on GPU 0.0005
Part 2 (rest of KWS system)
triphone BFSMN 0.012
quasi-mono BFSMN 0.002
triphone BFSMN, repeated 0.009
quasi-mono BFSMN, repeated 0.001
Let us remind that the proposed quasi-monophone model performs slightly
worse but it oﬀers two practical beneﬁts: a) a speed that can get close to 0.001
RT (if a GPU is used for likelihood computation) and b) a small disk memory
consumption in case of repeated runs (with diﬀerent keywords) because only
48×3 + 2 = 146 ﬂoat numbers per frame need to be stored. Moreover, the
speed of the proposed KWS system is only slightly inﬂuenced by the number of
keywords. A test made with 10.000 keywords (instead of 555 ones used in the
main experiments) showed only twice slower performance.Very Fast Keyword Spotting System with Real Time Factor Below 0.01 435
6 Conclusion
In this contribution we focus mainly on the speed aspect of a modern KWS
system, but at the same time we aim at the best performance that is available
thanks to the advances in deep neural networks. The used BFSMN architecture
has several beneﬁts for practical usage. In contrast to more popular RNNs, it
canbeeﬃcientlyandfasttrainedonalargeamount(severalthousandsofhours)
of audio and at the same time yields performance comparable to more complex
RNNs and LSTMs as shown in [19]. Its phoneme accuracy is high (due its large
internal context) so that it ﬁts both to acoustic KWS systems as well as to
standard speech-to-text LVCSR systems. The latter means that it is well suited
foratandemKWSschemewhereauserrequiresthatthesectionswithdetected
keywords are immediately transcribed by a LVCSR system. In our arrangement
this can be done very eﬀectively by reusing some of the precomputed data. (Let
us recall that if we use the quasi-monophones, their values are just max values
from the original triphone neural network and hence both acoustic models can
be implemented by the same network with an additional layer.)
The results presented in Sect. 5 allow for designing an optimal conﬁguration
that takes into account the three main factors: accuracy, speed and cost. If the
main priority is accuracy and not the speed, the KWS system can run on a
standard PC and process data with a RT factor about 0.1. When very large
amounts of records must be processed within very short time then the addition
of a GPU and the adoption of the proposed quasi-monophone approach will
allow for completing the job in time that can be up to 3 orders shorter than the
audio duration.
WeevaluatedtheperformanceonCzechdatasetsasthesewereavailablewith
precise human checked transcriptions. Obviously, the proposed architecture is
language independent and we plan to utilize it for other languages investigated
in our project.
Acknowledgments. ThisworkwassupportedbytheTechnologyAgencyoftheCzech
Republic (Project No. TH03010018).
References
1. Zheng, N., Li, X.: A robust keyword detection system for criminal scene analysis.
In5thIEEEConferenceonIndustrialElectronicsandApplications,Taichung,pp.
2127–2131 (2010)
2. Cardillo, P.S., Clements, M., Miller, M.S. Phonetic searching vs. LVCSR: how to
ﬁndwhatyoureallywantinaudioarchives.Int.J.SpeechTechnol.5,9–22(2002)
3. Zhou, X., Dai, D., Xie, B., Li, X.: Multidimensional evaluation platform for call
center speech service quality based on keyword spotting. In: Yang, Y., Ma, M.
(eds.) Proceedings 2nd International Conference on Green Communications and
Networks 2012. Lecture Notes in Electrical Engineering, vol. 225, pp. 535–544.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35470-0 66
4. Oh,Y.,Park,J.-S.,Park,K.-M.:Keywordspottinginbroadcastnews.In:Global-
Network-Oriented Information Electronics, Sendai, Japan, pp. 208–213 (2007)436 J. Nouza et al.
5. Michaely,A.H.,Zhang,X.,Simko,G.,Parada,C.Aleksic,P.:Keywordspottingfor
Googleassistantusingcontextualspeechrecognition.In:IEEEAutomaticSpeech
RecognitionandUnderstandingWorkshop(ASRU),Okinawa,pp.272–278(2017)
6. Szoke,I.,etal.:Comparisonofkeywordspottingapproachesforinformalcontinu-
ous speech. In: INTERSPEECH 2005, Lisbon, pp. 633–636 (2005)
7. Rohlicek,J.R.,Russell,W.,RoukosS.,Gish,H.:ContinuoushiddenMarkovmod-
elingforspeaker-independentwordspotting.In:ICASSP,Glasgow,UK,vol.1,pp.
627–630 (1989)
8. Weintraub, M.: LVCSR log-likelihood ratio scoring for keyword spotting. In:
ICASSP 1995, Detroit, vol. 1, pp. 297–300 (1995)
9. Foote,J.,Young,S.,Jones,G.,Jones,K.S.:Unconstrainedkeywordspottingusing
phone lattices with application to spoken document retrieval. Comput. Speech
Lang. 11, 207–224 (1997)
10. Motlicek, P., Valente, F., Szoke, I.: Improving acoustic based keyword spotting
using LVCSR lattices. In ICASSP 2012, Kyoto, pp. 4413–4416 (2012)
11. Akbacak,M.,Burget,L.,Wang,W.,vanHout,J.:Richsystemcombinationforkey-
wordspottinginnoisyandacousticallyheterogeneousaudiostreams.In:ICASSP
2013, Vancouver, BC, pp. 8267–8271 (2013)
12. Chen, N.F., Lee, C.-H.: A hybrid HMM/DNN approach to key-word spotting of
short words. In: Interspeech 2013, Lyon, pp. 1574–1557 (2013)
13. Palaz,D.,Synnaeve,G.,Collobert,R.:Jointlylearningtolocateandclassifywords
using convolutional networks. In: Interspeech 2016, San Francisco, pp. 3660–3664
(2016)
14. Lengerich, C., Hannun, A.: An end-to-end architecture for keyword spotting and
voice activity detection. In: NIPS 2016, Barcelona, Spain (2016)
15. Zhuang,Y.,Chang,X.,Qian,Y.,Yu,K.:Unrestrictedvocabularykeywordspotting
using LSTM-CTC. In: Interspeech 2016, San Francisco, pp. 938–942 (2016)
16. Ko, T., Peddinti, V., Povey, D., Khudanpur, S.: Audio augmentation for speech
recognition. In: Proceedings Interspeech 2015, Dresden, pp. 3586–3589 (2015)
17. Gales,M.J.F.,Knill,K.M.,Ragni,A.,Rath,S.P.:Speechrecognitionandkeyword
spotting for low-resource languages: babel project research at CUED. In: SLTU-
2014, pp. 16–23 (2014)
18. Nouza,J.,Silovsky,J.:Fastkeywordspottingintelephonespeech.Radioengineer-
ing 18(4), 665–670 (2009)
19. Zhang, S., Jiang, H., Xiong, S., Wei, S, Dai, L.: Compact feedforward sequential
memorynetworksforlargevocabularycontinuousspeechrecognition.In:Proceed-
ings Interspeech 2016, San Francisco, pp. 3389–3393 (2016)
20. M´alek, J., Zˇd´ansky´, J., Cˇerva, P.: Robust recognition of conversational telephone
speech via multi-condition training and data augmentation. In: Sojka, P., Hor´ak,
A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2018.LNCS(LNAI),vol.11107,pp.324–333.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-00794-2 35On the Eﬀectiveness of Neural Text
Generation Based Data Augmentation
for Recognition of Morphologically
Rich Speech
B
Bal´azs Tarj´an1,2( ) , Gyo¨rgy Szasz´ak1, Tibor Fegyo´1,2, and P´eter Mihajlik1,3
1 Department of Telecommunications and Media Informatics,
Budapest University of Technology and Economics, Budapest, Hungary
{tarjanb,szaszak,mihajlik}@tmit.bme.hu
2 SpeechTex Ltd., Budapest, Hungary
fegyo@speechtex.com
3 THINKTech Research Center, V´ac, Hungary
Abstract. Advanced neural network models have penetrated Auto-
matic Speech Recognition (ASR) in recent years, however, in language
modeling many systems still rely on traditional Back-oﬀ N-gram Lan-
guage Models (BNLM) partly or entirely. The reason for this are the
high cost and complexity of training and using neural language mod-
els, mostly possible by adding a second decoding pass (rescoring). In
our recent work we have signiﬁcantly improved the online performance
of a conversational speech transcription system by transferring knowl-
edge from a Recurrent Neural Network Language Model (RNNLM) to
the single pass BNLM with text generation based data augmentation.
In the present paper we analyze the amount of transferable knowledge
anddemonstratethattheneuralaugmentedLM(RNN-BNLM)canhelp
to capture almost 50% of the knowledge of the RNNLM yet by drop-
pingtheseconddecodingpassandmakingthesystemreal-timecapable.
We also systematically compare word and subword LMs and show that
subword-based neural text augmentation can be especially beneﬁcial in
under-resourced conditions. In addition, we show that using the RNN-
BNLM in the ﬁrst pass followed by a neural second pass, oﬄine ASR
results can be even signiﬁcantly improved.
· · ·
Keywords: Speech recognition Neural text generation RNNLM
· ·
Dataaugmentation Callcenterspeech Morphologicallyrichlanguage
1 Introduction
Deep learning has penetrated machine learning in the past years, including
speech technology and language modeling in particular [5,12]. Despite the suc-
cess of this architectural paradigm shift, application of Neural Network Lan-
guage Models (NNLM) in a single decoding pass is still challenging due to their
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.437–445,2020.
https://doi.org/10.1007/978-3-030-58323-1_47438 B. Tarj´an et al.
structureandcomputationalcomplexity.NNLMscanstillbeusedinASR,when
passingtothe2-passdecodingscheme:intheﬁrstpass,asmallfootprintgeneric
Language Model (LM) is used, and the output of this step is a simpliﬁed recog-
nition network with reduced search space. On this reduced lattice, a second
decoding pass is applied with the NNLM for rescoring the hypotheses obtained
intheﬁrstpass.Althoughbysplittingthedecodingintotwopartswecanlever-
age knowledge of the NNLMs and demonstrate signiﬁcant Word Error Rate
Reduction (WERR), it also introduces considerable processing delay [4,5,12].
Therefore, techniques exploiting the capabilities of NNLMs in a single-pass
decoding approach have received particular attention recently [9,13]. A possible
technique is to augment the in-domain training data with a large text corpus
generated by an NNLM [1,3]. Of course, there is a compromise: the augmented
modelisnomoresuitableforcapturinglongcontexts,andlosecapabilitytosup-
portcontinuousspacefeatures.Sofartherehasbeennothroughoutevaluationof
what NNLM capabilities can be transferred by neural text based data augmen-
tation and how these compare to traditional Back-oﬀ N-gram Language Models
(BNLM),especiallyforthemorphologicallyrichlanguages.Theonlyexceptionis
our earlier study for Hungarian [14] showing that by combining subword lexical
modelingwithtextbasedapproximationofNNLM(referredtoasRNN-BNLM)
we can greatly improve the performance of an online ASR system.
In this paper we signiﬁcantly extend our previous work: (1) we quantify
the amount of knowledge that can be transferred from the NNLM to single
pass decoding with a BNLM augmented with data generated by the NNLM;
(2) we show that the performance of oﬄine decoding can also be signiﬁcantly
improved if we apply the augmented model in the ﬁrst-pass for generating the
lattice; (3) we evaluate the impact of training corpus size on the eﬀectiveness of
the data augmentation method. Rich morphology, per se, results in extremely
large vocabularies, which constitutes a challenge for language modeling. Since
data sparsity problems can be often handled by estimating language models on
statically derived subword units (such as morphs) [2,6], we will also evaluate
morph-based models in our experiments.
In a related work, Suzuki et al. [13] use a domain balanced mixture of the
training corpora to train a shallow RNNLM for text generation and improve
speech recognition results for Japanese, Korean and English tasks. For Korean
subword-based language models are also utilized, but only for text generation,
sinceinthelanguagemodeloftheASRsystemsubwordsaremerged.Usingsub-
wordunitsforlanguagemodelsandASRhasbeenmostlyconsideredforFinnish
and Estonian, which are morphologically very rich languages [2,6]. In [4], the
authorsmanagedtooutperformword-basedbaselinemodelonFinnishandEsto-
nian conversations by training subword RNNLMs and utilizing them in the sec-
ond pass to rescore ASR lattices. N-gram based approximation of RNNLM was
alsoinvestigatedinarecentpaper[9],wheresubwordandcharacter-basedmod-
els were trained for Finnish and Arabic OOV keyword search tasks. Although
the interpolation of approximated RNNLM and BNLM models improved OOVEﬀectiveness of Neural Text Based Data Augmentation for ASR 439
retrievaltheproposedsystemwasnotevaluatedonin-vocabularytokensandno
Word Error Rate (WER) was presented either.
2 Data and Methods
2.1 Database
Conventional Training Data. Data for modeling word units are taken from
the Hungarian Call Center Speech Database (HCCSD). The HCCSD corpus
contains real conversations recorded in customer service centers. The conver-
sations are transcribed and validated by human proofreaders. A total of 3.4M
word tokens could be used allowing for a dictionary 100K distinct word forms.
In order to speed up training, the ﬁnal vocabulary was limited to the most 50K
wordforms.TheremainingOut-Of-Vocabulary(OOV)wordswerereplacedwith
(cid:2)unk(cid:3) and the sentence endings were mapped to the (cid:2)eos(cid:3) symbol. Training cor-
pus statistics are summarized in Table 1.
Table 1. Training and test database statistics
Training Validation Evaluation
Duration [h:m] 290:07 7:31 12:12
# of word tokens 3,401,775 45,773 66,312
# of morph tokens 3,822,335 57,849 84,385
MorphSegmentedTrainingData. MorphologicallyrichlanguageslikeHun-
garian show heavy agglutination and hence vocabulary gets much larger. This
also results in higher variability regarding word sequences, and estimation of
model parameters becomes less accurate. Segmenting words into smaller units
is driven by the idea to both decrease vocabulary size and increase sequential
consistency in morph sequences [6]. Morfessor [2] is a popular algorithm for
segmenting words into subword units as it iteratively ﬁnds the optimal decom-
position of vocabulary words into subword units, called morphs. In [10] it was
shown that Morfessor can outperform the nowadays so popular character-level
Byte Pair Encoding (BPE) algorithm.
The training corpus contained 3.8M units after applying Morfessor and
decomposingwordsintomorphs(seeTable1).Thenumberofvocabularyentries
decreased to around 1/3 of the word vocabulary, that is 32K entries cover-
ing the same text corpora as the word based model. The morph vocabulary
was ﬁnally limited 30K morphs based on frequency, in order to provide enough
trainingsamplesto(cid:2)unk(cid:3).Morphsinnon-word-initialpositionwereadditionally
tagged by the ‘+’ sign to preserve this syntactic information relative to original
word boundaries. The following example illustrates a morph-based tokenization440 B. Tarj´an et al.
(decomposition) of the sentence ‘well I will discuss this with my wife’:
Conventional tokenization: h´at megbesz´elem a nejemmel
Morph-based tokenization: h´at meg +besz´el +em a nejem +mel
Development and Test Data. Forvalidationandtesting,twofurtherdisjoint
datasetswerecreatedusing20hofconversations,reservedfromtheHCCSDcor-
pus(seeTable1).Thevalidationsetisrequiredfortheoptimizationofthehyper-
parameters (e.g. Morfessor segmentation, control training of language models),
whereas the evaluation set is used for performance evaluation and comparison
of the models.
2.2 Language Modeling Methods
Back-Oﬀ N-Gram Models. N-grammodelsarestatistical,count-basedmod-
elsestimatedonlargetextcorpora.Back-oﬀN-gramLanguageModels(BNLM)
formed the state-of-the-art in language modeling for ASR over several decades,
and still today, for a number of tasks they are the primary choice, especially in
systems requiring real-time or smaller footprint setups. All BNLMs in this work
are estimated with the SRI language modeling toolkit [11] and smoothed with
Chen and Goodman’s modiﬁed Kneser-Ney discounting.
Recurrent Neural Language Model. We implemented1 a 2-layered LSTM
structureaccordingtotheschemepresentedin[14].Afterﬁne-tuningthehyper-
parameters on the validation set, we use a batch size of 32 sequences, composed
of 35 tokens each (tokens can be either words or morphs). LSTM states are pre-
served between the batches (stateful LSTM). The 650 dimensional embedding
vectors were trained from scratch, as transfer learning from existing Hungarian
pretrained embeddings proved to be suboptimal. After trying several optimiz-
ers, we decided on the traditional, momentum accelerated Stochastic Gradient
Descent(SGD)algorithm. Theinitial learningratewassetto1,whichishalved
after every epoch where the cross entropy loss increases. To prevent overﬁtting
dropout layers are used with keep probabilities of 0.5. Early stopping with a
patience of 3 epochs is also applied.
Text Generation Based Data Augmentation. Approximation of a NNLM
with a back-oﬀ ngram language model can be achieved in several diﬀerent
ways [1,3]. In [1] three such methods are described and evaluated, coming to
a conclusion that the so called text generation based data augmentation yields
thebestresults.ThemainideaofthisapproachistoestimatetheBNLMparam-
etersfromalargetextcorpusgeneratedbyaNNLM.Inourwork,wegenerated
100millionwords/morphswiththecorrespondingRNNLM(RNN-BNLM100M)
thatwasformerlytrainedonthein-domaintrainingset.Inordertogetaninsight
1 https://github.com/btarjan/stateful-LSTM-LM.Eﬀectiveness of Neural Text Based Data Augmentation for ASR 441
howthecorpussizeinﬂuencesthelanguagemodelcapabilities,wealsogenerated
alargertextcorpuswith1billionmorphs(RNN-BNLM1B).Toachievethebest
results the models trained on augmented text (RNN-BNLMs) are interpolated
with the baseline models (BNLM + RNN-BNLM). Interpolation weights are
optimized on the development set.
3 Results and Discussion
3.1 Experimental Setup
40 dimensional MFCC vectors were used as input features for a Factored Time
DelayNeuralNetwork(TDNN-F)acousticmodeltrainedapplyingLF-MMIcri-
terion in a similar manner as in [7] using the Kaldi Toolkit [8]. The matrix
size (hidden-layer dimension) was 768 and the linear bottleneck dimension was
80 resulting in a total number of 6M parameters in the twelve hidden layers.
Acoustic and language model resources were compiled into weighted ﬁnite-state
transducers and decoded with our in-house ASR decoder, called VoXerver.
Table 2. WER of the online ASR system using the proposed language models
# of Memory WERR over
Token WER
Model n-grams usage Word/Morph
type [%]
[million] [GB] BNLM [%]
BNLM 5.0 1.3 21.9
d
or RNN-BNLM100M 4.8 0.9 22.5 -2.6*
W
BNLM+RNN-BNLM100M 7.0 1.5 21.3 2.7*
BNLM 5.1 1.0 21.1 3.4*
RNN-BNLM100M 8.5 1.1 21.1 3.7* 0.3
ph RNN-BNLM1B 7.2 0.9 20.5 6.4* 3.2*
r
o BNLM+RNN-BNLM100M 7.9 1.1 20.4 6.8* 3.5*
M
7.2 1.1 20.2 7.7* 4.5*
BNLM+RNN-BNLM1B
46.6 5.9 19.9 8.8* 5.6*
* Sign indicates signiﬁcant diﬀerence compared to Word or Morph-based BNLM
modelsandwastestedwithWilcoxonsigned-ranktest(p<0.05).
3.2 Online ASR Results with Data Augmentation
We perform single-pass decoding with 4-gram BNLM and RNN-BNLM models
andcalculateWERontheevaluationset.Inordertoensurethefaircomparison
among the modeling approaches, we pruned each RNN-BNLM so that they had
similar runtime memory footprint as the baseline BNLM models (≈1 GB). The
most promising model, where the baseline is augmented with 1 billion token
corpus(BNLM+RNN-BNLM1B)however,isalsoevaluatedinasetupallowing
for larger memory consumption to determine the full capability of the model.442 B. Tarj´an et al.
Fig.1.ImpactoftrainingdatalimitationontheWERofbaselineandaugmentedLMs
and the corresponding relative WERRs
Results with Original Training Corpus. First we discuss the online ASR
results(seeTable2)obtainedwithmodelstrainedontheoriginaltrainingcorpus
(3.4M word/3.8M morph tokens). The n-gram model estimated on the corpus
that was generated with the word-based RNNLM (RNN-BNLM 100M) has a
slightly higher WER than the baseline word-based BNLM (2.6% relative WER
increase), but with the interpolated model (BNLM + RNN-BNLM 100M) we
are able to signiﬁcantly outperform both of them (2.7% rel. WERR).
Switching to subword setups we observed the following results: the simple
actofreplacingwordswithsubwordsinthebaselineBNLMalreadyyieldsasig-
niﬁcant WER reduction (3.4% rel.). The LM trained on the 100-million-morph
generated corpus (RNN-BNLM 100M) has the same WER as the morph-based
BNLM (21.1% WER). However using a ten times larger corpus to train the
approximativemodelreversesthistrend:morph-basedRNN-BNLM1Bmodelis
theﬁrstaugmentationmodelthatoutperformsabaselineBNLMbyitself,with-
out taking any beneﬁt from interpolation (20.5% WER). When adding interpo-
lation,wecanleverageafurtherincreaseinperformance.BNLM+RNN-BNLM
1BmodelcanreduceWERofmorph-basedBNLMby5%oreven6%ifruntime
memory consumption is not a restricting factor. All in all, with morph-based
neuraltextgenerationwemanagedtoreducetheWERofourcallcenterspeech
transcription system by 9% relative while preserving real-time operation.
Impact of Training Corpus Size. The RNNLM used to generate augmen-
tation data is trained on in-domain training corpus, hence we assume that the
amount of available training data is closely related to the eﬀectiveness of this
modelingapproach.Inordertoconﬁrmthishypotheses,werepeatedtheexperi-
mentsfromtheprevioussection,butwelimitedthesizeoftrainingdatabase(see
Fig. 1). The original corpus containing 3.4M tokens was reduced to two smaller
corporafollowing alog-uniformly spacedscale(600K and100K). Wefoundthat
in case of word-based modeling the less training data is used the smaller is the
beneﬁtofdataaugmentation.Incontrast,morph-basedaugmentedmodelseven
increase their advantage over the baseline for smaller training sets. Based onEﬀectiveness of Neural Text Based Data Augmentation for ASR 443
Fig.2.Acomparisonofonline(baselineLM:BNLM,augmentedmodel:BNLM+RNN-
BNLM) and oﬄine (rescoring BNLM with RNNLM: BNLM+RNNLM, rescoring
BNLM+RNN-BNLM with RNNLM: BNLM+RNN-BNLM+RNNLM) ASR results
with word and morph based lexical modeling. * indicates oﬄine, 2-pass decoding
the above, we conclude that text based augmentation can be indeed eﬀective in
under-resourced conditions, if it is paired with subword lexical modeling app-
roach so that the RNNLM has enough samples for learning.
3.3 Comparing Online and Oﬄine ASR Results
In this section we compare the performance of the original RNNLM applied for
2-pass, oﬄine decoding and the RNN-BNLMs in order to assess the amount of
knowledge that can be transferred to the online ASR system (see Fig. 2). With
oﬄine,2-passdecoding,thebaselineWERcanbereducedby≈12–13%(BNLM
+ RNNLM). Word-based augmentation can capture 22% of this WERR as it
reduces the WER by 2.7% compared to the 12.2% of 2-pass decoding. Using
morph-based lexical modeling and a 10 times larger augmentation corpus the
relative WERR can be increased to 5.6% (Morph BNLM + RNN-BNLM). On
this basis we can conclude that up to 45% of the WERR (5.6% from 12.9%)
potentialholdbytheRNNLMcanbetransferredtotheﬁrstpassofthedecoding.
TextbaseddataaugmentationwasintroducedtoimproveonlineASRresults
by transferring knowledge from the neural model to the BNLM. However, we
found that even oﬄine speech recognition can beneﬁt from this approach. The
lastcolumninFig.2showsthatsigniﬁcant(p=0.01)WERRcanbeachievedif
the lattice used for rescoring is generated with the augmented model (BNLM +
RNN-BNLM + RNNLM) instead of the original BNLM (BNLM + RNNLM).
4 Conclusions
In this paper neural LMs were used to transfer their knowledge to traditional
back-oﬀLMsbygeneratingsamplesforprobabilityestimation.Themorphologi-
calcomplexityofHungarianwastreatedbyusingmorph-basedmodelsevaluated
on a call center ASR task. We found that by generating a text with 1 billion444 B. Tarj´an et al.
morphs,theWERcanbereducedby9%relativewhilepreservingreal-timeoper-
ation. The investigated neural text based data augmentation technique proved
to be especially eﬀective in under-resourced conditions provided that subword-
based modeling is applied.
With the augmented LMs we managed to transfer ≈45% of WERR of the
oﬄine, 2-pass conﬁguration to our online system. Finally, we also showed that
augmented LMs can improve not only online but oﬄine ASR results if they are
used for generating the lattice for the 2nd decoding pass.
Acknowledgements. The research was supported by the CAMEP (2018-2.1.3-
EUREKA-2018-00014) and NKFIH FK-124413 projects.
References
1. Adel, H., Kirchhoﬀ, K., Vu, N.T., Telaar, D., Schultz, T.: Comparing approaches
to convert recurrent neural networks into backoﬀ language models for eﬃcient
decoding. Interspeech 2014, 651–655 (2014)
2. Creutz, M., Lagus, K.: Unsupervised discovery of morphemes. In: Proceedings
ACL-02 Workshop on Morphological and Phonological Learning, vol. 6, Morris-
town, NJ, USA, pp. 21–30 (2002)
3. Deoras, A., Mikolov, T., Kombrink, S., Karaﬁat, M., Khudanpur, S.: Variational
approximation of long-span language models for LVCSR. In: 2011 IEEE Inter-
national Conference on Acoustics, Speech, and Signal Processin, pp. 5532–5535.
IEEE, May 2011
4. Enarvi,S.,Smit,P.,Virpioja,S.,Kurimo,M.:Automaticspeechrecognitionwith
very large conversational ﬁnnish and estonian vocabularies. IEEE/ACM Trans.
Audio Speech Lang. Process. 25(11), 2085–2097 (2017)
5. Irie, K., Zeyer, A., Schl, R., Ney, H., Gmbh, A.: Language modeling with deep
transformers. Interspeech 2019, 3905–3909 (2019)
6. Kurimo,M.,etal.:Unlimitedvocabularyspeechrecognitionforagglutinativelan-
guages. In: HLT-NAACL 2006, Morristown, NJ, USA, pp. 487–494 (2007)
7. Povey, D., et al.: Semi-orthogonal low-rank matrix factorization for deep neural
networks. In: Interspeech 2018, ISCA, ISCA, pp. 3743–3747, September 2018
8. Povey,D.,etal.:Thekaldispeechrecognitiontoolkit.In:IEEE2011Workshopon
AutomaticSpeechRecognition&Understanding.IEEESignalProcessingSociety
(2011)
9. Singh, M., Virpioja, S., Smit, P., Kurimo, M.: Subword RNNLM approximations
for out-of-vocabulary keyword search. Interspeech 2019, 4235–4239 (2019)
10. Smit, P., Virpioja, S., Kurimo, M.: Improved subword modeling for WFST-based
speechrecognition.In:Interspeech2017.ISCA,ISCA,pp.2551–2555,August2017
11. Stolcke, A.: SRILM - an extensible language modeling toolkit. In: Proceedings
International Conference on Spoken Language Processing, Denver, US, pp. 901–
904 (2002)
12. Sundermeyer,M.,Schlueter,R.,Ney,H.:LSTMneuralnetworksforlanguagemod-
eling. Interspeech 2012, 194–197 (2012)Eﬀectiveness of Neural Text Based Data Augmentation for ASR 445
13. Suzuki,M.,Itoh,N.,Nagano,T.,Kurata,G.,Thomas,S.:ImprovementstoN-gram
language model using text generated from neural language model. In: ICASSP
2019–2019 IEEE International Conference on Acoustics, Speech, and Signal Pro-
cessing, pp. 7245–7249 (2019)
14. Tarj´an, B., Szasz´ak, G., Fegy´o, T., Mihajlik, P.: Investigation on N-gram approx-
imated RNNLMs for recognition of morphologically rich speech. In: Mart´ın-Vide,
C.,Purver,M.,Pollak,S.(eds.)SLSP2019.LNCS(LNAI),vol.11816,pp.223–234.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-31372-2 19Context-Aware XGBoost for Glottal
Closure Instant Detection in Speech
Signal
B
Jindˇrich Matouˇsek( ) and Michal Vraˇstil
Department of Cybernetics, New Technology for the Information Society (NTIS),
Faculty of Applied Sciences, University of West Bohemia, Plzenˇ, Czech Republic
jmatouse@kky.zcu.cz, vrastilm@students.zcu.cz
Abstract. In this paper, we continue to investigate the use of classi-
ﬁers for the automatic detection of glottal closure instants (GCIs) in
the speech signal. We introduce context to extreme gradient boosting
(XGBoost) and show that the context-aware XGBoost outperforms its
context-freeversion.Theproposedcontext-awareXGBoostisalsoshown
to outperform traditionally used GCI detection algorithms on publicly
available databases.
· · ·
Keywords: Glottal closure instant (GCI) Pitch mark Detection
· ·
Classiﬁcation Extreme gradient boosting Context-awareness
1 Introduction
Detectionofglottal closure instants (GCIs),alsocalledpitch marks orepochs,in
speechsignalswasshowntobeusefulinmanypracticalapplications,especiallyin
thosewherepitch-synchronous speechprocessingwasrequired(seee.g.[7,9,23]).
GCI detection could be viewed as a task of determining peaks in voiced parts of
the speech signal that correspond to the moment of glottal closure, a signiﬁcant
excitation of a vocal tract during the speaking.
Many algorithms were proposed to detect GCIs in the speech signal. Tradi-
tionally, theyexploitsomeexpertknowledgeandhand-craftedrulesandthresh-
olds to identify GCI candidates from local maxima of various speech represen-
tations (e.g. linear predictive coding like in DYPSA [20], YAGA, wavelet com-
ponents, multi-scale formalism (MMF) [11]), glottal ﬂow (GEFBA) [12], proba-
bilistic source-ﬁlter model (PSFM) [19], and/or from discontinuities or changes
insignalenergy(Hilbertenvelope,Frobeniusnorm,zero-frequencyresonator,or
This research was supported by the Czech Science Foundation (GA CR), project No.
GA19-19324S, and by the grant of the University of West Bohemia, project No. SGS-
2019-027.Computationalresourcesweresuppliedbytheproject“e-InfrastrukturaCZ”
(e-INFRALM2018140)providedwithintheprogramProjectsofLargeResearch,Devel-
opment and Innovations Infrastructures.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.446–455,2020.
https://doi.org/10.1007/978-3-030-58323-1_48Context-Aware XGBoost for Glottal Closure Instant Detection 447
SEDREAMS [8]). Dynamic programming is then often used to reﬁne the GCI
candidates [19,20,22].
Recently, classiﬁcation-based approaches were re-introduced by Matouˇsek et
al. [16–18]. In this approach, GCI detection could be viewed as a two-class clas-
siﬁcation problem: whether or not a peak in a speech waveform represents a
GCI [3]. The advantage is that once a training dataset is available, classiﬁer
parameters are set up automatically without manual tuning. It was shown that
classiﬁcation-based GCIdetection, andespecially theonebasedonextreme gra-
dient boosting (XGBoost), was able to perform very well and consistently out-
performed traditionally used algorithms on several test datasets [18].
In this paper, we continue to investigate the use of XGBoost for GCI detec-
tion.Morespeciﬁcally,weintroducecontexttoXGBoostdetectionandexamine
whetherthepresenceofneighboringGCIcandidatescanimproveGCIdetection.
2 Experimental Data
Experiments were performed on clean 16kHz-sampled speech recordings (here-
after referred to as UWB). The recordings were primarily intended for speech
synthesis. We used 88 utterances (≈11.5min of speech) for the development of
the proposed classiﬁers, and 20 test utterances (≈2.5min of speech) were held
out for an unbiased comparison with other methods. The set of utterances was
the same as in [14] – it comprised various Czech (male and female), Slovak
(female), German (male), US English (male), and French (female) voices. Most
voices were part of both the development and test datasets. Reference GCIs
produced by a human expert using both speech and electroglottograph (EGG)
signals were available for each utterance and were synchronized with the corre-
sponding minimum negative sample in the speech signal.
Speech waveforms were processed in the same way as in [18]. They were
mastered to have equal loudness, low-pass ﬁltered by a zero-phase Equiripple-
designed ﬁlter with 0.5 dB ripple in the pass band, 60 dB attenuation in the
stop band, and with the cutoﬀ frequency 800Hz to reduce the high-frequency
structure in the speech signal. The signals were then zero-crossed to identify
peaks (both of the negative and positive polarity) that are used for feature
extractioninfurtherprocessing.Sincethepolarityofspeechsignalswasshownto
have an important impact on the performance of a GCI detector [15], all speech
signals were switched to have the negative polarity, and only the negative peaks
weretakenasthecandidatesfortheGCIplacement.Forthepurposeoftraining
and testing, the location of each reference GCI was mapped to a corresponding
negativepeakintheﬁlteredsignal.Therewere98227and20338candidatepeaks
in the development and test datasets respectively (marked by both ◦ and • in
Fig.1),56025and10807ofthemcorrespondedtotrueGCIs(markedby•only).
3 Experiments and Results
Inallfollowingexperiments,weusedextreme gradient boosting (XGBoost)[5]as
the GCI detection model. It is an eﬃcient implementation of gradient boosted448 J. Matouˇsek and M. Vraˇstil
Fig.1.Illustrationoffeatureextraction:amplitudeofanegativepeak(A),amplitude
ofapositive peak (B), diﬀerence betweentwonegative peaks (C),widthofanegative
peak (D), correlation between waveforms of two negative peaks (E). GCI candidates
are marked by ◦, true GCIs by •.
decision trees designedfor speedand performance that dominated many Kaggle
competitions. XGBoost was also shown to outperform other classiﬁers in the
GCI detection problem [18].
To evaluate the performance of the proposed models, standard classiﬁcation
measures like recall (R), precision (P), F1, and area under the receiver operat-
ing characteristic curve (AUC) were utilized. Scikit-learn [21] and XGBoost [5]
toolkits were employed to train and evaluate the proposed models.
3.1 Baseline Model
The baseline XGBoost system did not use any information about the detec-
tion/prediction of neighboring peaks; it trained and predicted each peak inde-
pendently on the neighboring peaks [18]. Inspired by [3], the input features for
classiﬁcation were associated with negative peaks in the low-pass ﬁltered speech
waveforms.Eachpeakisdescribedbyasetoflocaldescriptorsreﬂectingtheposi-
tion and shape of other 3 neighboring peaks [16]: the amplitudes of the given
negativepeakand6neighboring(3priorand3subsequent)negativepeaks(7fea-
tures, denoted as A in Fig. 1), amplitudes of 6 neighboring positive peaks (6,
B), the time diﬀerence between the given negative peak and each of the neigh-
boring negative peaks (6, C), the width of the given negative peak (a distance
between two zero-crossings) and each of the neighboring negative peaks (7, D),
the correlation of the waveform around the given negative peak and the wave-
formsaroundeachoftheneighboringnegativepeaks(6,E).Thebaseline32fea-
tureswerethenextendedwithotheracousticandspectralfeatures:zero-crossingContext-Aware XGBoost for Glottal Closure Instant Detection 449
rate(ZCR),logenergy,harmonic-to-noiseratio(HNR),voiced/unvoiced,the1st
(F1) and 2nd (F2) formant frequencies, F1/F2 distance and ratio, peak ratio
to 6 neighboring peaks, spectral centroid/bandwidth/roll-oﬀ, peak slope, max-
imum of power spectral density function estimate (PSD) and its corresponding
frequency, and mel-frequency cepstral coeﬃcients (MFCCs). All these features
were calculated from 10ms-long speech segments extracted around every peak
candidate, resulting in a total number of 66 features. Recursive feature elim-
ination (RFE) with cross-validation was applied to select important features
automatically [17]. Finally, the most correlated features were removed, so that
the resulting feature set consisted of 50 features. Extensive XGBoost model
hyper-parameter tuning using grid search with repeated 10-fold cross-validation
(with 10 repetitions) was conducted on the development dataset [18].
Note that although no knowledge of the prediction of neighboring peaks is
available(i.e.,0neighboringpeaksaretakenintoaccountatthepredictionlevel),
the feature set comprises the context of 3 neighboring peaks1 (GCI candidates)
implicitly on the feature level (denoted as F3P0 hereafter). To switch oﬀ the
impact of the context, features related to the neighboring peaks were discarded.
The model with no contextual features was denoted as F0P0 and consisted of
20 features.
3.2 Context Modeling
The idea behind context modeling is that a classiﬁer can use information about
the prediction of n neighboring peaks. We believe that octal halving/doubling
errorscanbeeliminatedinthisway.Therefore,contextmodelingcanbeseenas
an alternative to dynamic programming-based GCI post-processing.
To avoid a bias towards the training data and to ensure a generalization
performance on unseen data, out-of-fold (OOF) predictions were utilized:
1. The baseline context-free classiﬁers at the prediction level F3P0 and F0P0
were10-foldcross-validatedandout-of-foldprobabilistic predictions werecol-
lected.
2. Contextual information at the prediction level (the out-of-fold predictions of
n preceding and n succeeding peaks denoting the probability of the previ-
ous/current/next peak to be a GCI) together with the original features of a
current peak were then used to train a set of contextual classiﬁers F3Pn and
F0Pn(wheren=1–10standsforthecontextlength)ontheout-of-folddata.
ToevaluatetheinﬂuenceofthecontextmodelingonGCIdetectionaccuracy,
the context-free models F3P0 and F0P0 were used to make predictions on the
test dataset. The predictions of n neighboring peaks were then combined with
theoriginalfeaturesofeachcurrentpeakandinputtedtothecontextualmodels
F3Pn and F0Pn which in turn yielded unbiased predictions on the test dataset.
It can be clearly seen in Fig. 2 that information about the prediction of
neighboring peaks does help, especially in the case of F0Pn which does not
1 3 preceding and 3 succeeding peaks were found optimal [16].450 J. Matouˇsek and M. Vraˇstil
Fig.2.TheimpactofneighboringpeaksonthepredictionoftheactualpeakforF0Pn
and F3Pn systems with respect to AUC score on the test dataset.
contain any other contextual information at the feature level. It is also evident
thatusingcontextonbothfeatureandpredictionlevels(thecaseofF3Pn)yields
the best results. The comparison of the best contextual systems F0P5 (for no
neighboring peak at the feature level and 5 neighboring peaks at the prediction
level)andF3P7(3neighboringpeaksatthefeatureleveland7neighboringpeaks
at the prediction level), and their baselines F0P0, F3P0 is given in Table 1.
Table 1. Comparison of classiﬁers’ GCI detection performance on the test dataset
(left) and the corresponding statistical signiﬁcance according to McNemar’s test [6]
(right). The symbols (cid:3) and > mean that the row classiﬁer is signiﬁcantly better at
the signiﬁcance level α = 0.01 and α = 0.05 respectively than the column classiﬁer.
The symbol = means that the respective classiﬁers perform the same.
Model R (%) P (%) F1 (%) AUC (%) Model F3P7 F3P0 F0P5 F0P0
F3P7 97.79 99.05 98.42 98.36 F3P7 = > (cid:3) (cid:3)
F3P0 97.50 99.03 98.26 98.21 F3P0 < = (cid:3) (cid:3)
F0P5 97.34 98.67 98.00 97.92 F0P5 (cid:4) (cid:4) = (cid:3)
F0P0 96.37 98.38 97.36 97.28 F0P0 (cid:4) (cid:4) (cid:4) =
4 Comparison with Other Methods
In order to compare the proposed classiﬁers with diﬀerent GCI detection algo-
rithms, standard GCI detection measures that concern the reliability and accu-
racy of the GCI detection algorithms were used [20]. The former includes theContext-Aware XGBoost for Glottal Closure Instant Detection 451
Table 2. Summary of the performance of the GCI detection algorithms for the four
datasets.
DatasetMethod IDR(%)MR(%)FAR(%) IDA(ms)A25(%) E10(%)
F3P7 96.78 2.34 0.88 0.24 98.73 95.66
F3P0 96.60 2.56 0.85 0.23 98.79 95.54
SEDREAMS 93.12 4.00 2.88 0.28 98.10 91.69
UWB MMF 85.08 11.43 3.48 0.47 97.85 83.55
DYPSA 89.64 6.25 4.11 0.37 98.04 88.22
REAPER 92.81 5.51 1.69 0.27 98.00 91.45
GEFBA 91.24 7.68 1.08 0.22 98.89 90.34
PSFM 88.17 9.71 2.12 0.39 98.27 86.88
F3P7 94.19 2.80 3.01 0.37 98.59 92.90
F3P0 94.04 2.93 3.03 0.36 98.58 92.74
SEDREAMS 91.80 3.03 5.16 0.45 97.37 90.02
BDL MMF 90.42 4.63 4.95 0.56 97.15 87.87
DYPSA 89.43 4.38 6.19 0.54 97.13 86.89
REAPER 93.24 4.39 2.37 0.56 98.01 91.47
GEFBA 87.93 10.05 2.02 1.02 99.11 87.18
PSFM 87.05 9.65 3.30 0.71 96.95 84.50
F3P7 96.64 1.34 2.01 0.17 99.73 96.39
F3P0 96.49 1.57 1.95 0.19 99.71 96.22
SEDREAMS 94.66 1.13 4.21 0.17 99.67 94.36
SLT MMF 92.44 5.29 2.26 0.40 99.17 91.78
DYPSA 93.25 2.91 3.84 0.32 99.39 92.75
REAPER 95.57 1.66 2.77 0.19 99.67 95.27
GEFBA 94.85 2.62 2.53 0.17 99.76 94.63
PSFM 86.95 10.46 2.60 0.45 99.26 86.42
F3P7 96.82 2.31 0.87 0.24 98.63 95.83
F3P0 96.60 2.56 0.85 0.22 98.76 95.68
SEDREAMS 92.30 6.03 1.66 0.29 99.12 91.76
KED MMF 90.16 7.16 2.68 0.35 98.99 89.52
DYPSA 90.27 7.07 2.65 0.30 99.25 89.72
REAPER 91.05 8.18 0.78 0.28 99.47 90.67
GEFBA 88.51 10.36 1.13 0.21 99.74 88.30
PSFM 89.47 9.59 0.94 0.39 99.22 88.85452 J. Matouˇsek and M. Vraˇstil
percentage of glottal closures for which exactly one GCI is detected (identiﬁca-
tion rate, IDR), the percentage of glottal closures for which no GCI is detected
(miss rate, MR), and the percentage of glottal closures for which more than
one GCI is detected (false alarm rate, FAR). The latter includes the percentage
of detections with the identiﬁcation error ζ ≤ 0.25 ms (accuracy to ±0.25ms,
A25)andstandarddeviationoftheidentiﬁcationerrorζ (identiﬁcationaccuracy,
IDA). In addition, we use a more dynamic evaluation measure [13]
N −N −N −N
E10= R ζ>0.1T0 M FA (1)
N
R
thatcombinesthereliabilityandaccuracyinasinglescoreandreﬂectsthelocal
pitchperiod T0 pattern(determinedfromthereferenceGCIs).NR standsforthe
numberofreferenceGCIs,NM isthenumberofmissingGCIs(correspondingto
MR),NFA isthenumberoffalseGCIs(correspondingtoFAR),andNζ>0.1T0 is
thenumberofGCIswiththeidentiﬁcationerrorζ greaterthan10%ofthelocal
pitch period T . For the alignment between the detected and reference GCIs,
0
dynamic programming was employed [13].
We compared the proposed classiﬁers with six existing state-of-the-art GCI
detection methods:
– Speech Event Detection using the Residual Excitation And a Mean-based Sig-
nal (SEDREAMS) [8], shown in [9] to provide the best of performances com-
pared to other methods;
– fast GCI detection based on Microcanonical Multiscale Formalism (MMF)
[11];
– DynamicProgrammingProjectedPhase-SlopeAlgorithm (DYPSA)[20]avail-
able in the VOICEBOX toolbox;
– Google’s Robust Epoch And Pitch EstimatoR (REAPER) [2];
– Glottal closure/opening instant Estimation Forward-Backward Algorithm
(GEFBA) [12];
– Probabilistic source-ﬁlter model (PSFM) [19].
Weusedtheimplementationsavailableonline;nomodiﬁcationsofthealgorithms
were made. Since all algorithms (except REAPER) estimate GCIs also during
unvoiced segments, their authors recommend ﬁltering the detected GCIs by the
output of a separate voiced/unvoiced detector. We applied an F contour esti-
0
mated by the REAPER algorithm for this purpose. There is no need to apply
suchpost-processingonGCIsdetectedbytheproposedclassiﬁcation-basedapp-
roachsincethevoiced/unvoicedpatternwasincludeddirectlyinthefeatureset.
To obtain consistent results for all methods, the detected GCIs were shifted
towards the neighboring minimum negative sample in the speech signal.
4.1 Test Datasets
Firstly, the evaluation was carried out on the UWB test dataset (≈2.5min of
speech) described in Sect. 2. GCIs produced by a human expert were used as
reference GCIs.Context-Aware XGBoost for Glottal Closure Instant Detection 453
Secondly,twovoices,aUSmale(BDL)andaUSfemale(SLT)fromtheCMU
ARCTIC databases intended for unit selection speech synthesis [1] were used as
a test material. Each voice consists of 1132 phonetically balanced utterances of
a total duration of ≈54min per voice. Additionally, KED TIMIT database [1]
comprising453phoneticallybalancedutterances(≈20min)ofaUSmalespeaker
wasalso usedforthe evaluation. All thesedatasets compriseclean speech.Since
therearenohand-craftedGCIsavailable forthesedatasets, GCIsdetectedfrom
contemporaneous EGG recordings by the Multi-Phase Algorithm (MPA) [13]
(again shifted towards the neighboring minimum negative sample in the speech
signal) were used as the reference GCIs. Original speech signals were downsam-
pledto16kHz.Itisimportanttomentionthatnovoicefromthesedatasetswas
part of the training dataset used to train the proposed classiﬁers.
4.2 Results
TheresultsinTable2conﬁrmthattheproposedcontext-awareXGBoostmodel
(F3P7) consistently outperforms the standard context-free XGBoost (F3P0).
It is also evident that both XGBoost models (and especially the contextual
one) generally perform very well for all tested datasets. They excel in terms
of reliability, especially with respect to the identiﬁcation (IDR) and miss (MR)
rates, and also in terms of the dynamic detection accuracy (E10). As for the
accuracy,theyalsoperformedreasonablywellastheyoftenachievedthesecond-
best results (behind the GEFBA algorithm which, however, tends to miss GCIs
quiteoften)intermsofidentiﬁcationaccuracy(IDA)andofthesmallestnumber
of timing errors higher than 0.25 ms (A25).
5 Conclusions
In this paper, we showed that the introduction of context to XGBoost classiﬁer
improvedGCIdetection.Theproposedcontext-awareXGBoostoutperformedits
context-free version, improving GCI detection accuracy F1 by 0.16% to 98.42%
(statisticallysigniﬁcantatthesigniﬁcancelevelα=0.05).Fromamorepractical
point of view, the improvement means that, on average, 2.28 peaks would be
classiﬁedbetterand1.45GCIswouldbeidentiﬁedbetterina10s-longutterance.
The context-aware XGBoost also yielded very good results when compared to
other existing state-of-the-art methods on several test datasets.
Inourfuturework,weplantoinvestigatewhetheradeeplearningalgorithm
could further increase the performance of the proposed classiﬁcation-based GCI
detectionmethod[4,10,24].Robustnessoftheproposedmethodtonoisysignals
and/or to emotional or expressive speech will also be researched.
References
1. FestVox Speech Synthesis Databases. http://festvox.org/dbs/index.html454 J. Matouˇsek and M. Vraˇstil
2. REAPER: Robust Epoch And Pitch EstimatoR. https://github.com/google/
REAPER
3. Barnard,E.,Cole,R.A.,Vea,M.P.,Alleva,F.A.:Pitchdetectionwithaneural-net
classiﬁer. IEEE Trans. Signal Process. 39(2), 298–307 (1991). https://doi.org/10.
1109/78.80812
4. Bul´ın, M., Sˇm´ıdl, L., Sˇvec, J.: On using stateful LSTM networks for key-phrase
detection.In:Ekˇstein,K.(ed.)TSD2019.LNCS(LNAI),vol.11697,pp.287–298.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-27947-9 24
5. Chen, T., Guestrin, C.: XGBoost: reliable large-scale tree boosting system. In:
Conference on Knowledge Discovery and Data Mining (2016). https://doi.org/10.
1145/2939672.2939785
6. Dietterich,T.:Approximatestatisticaltestsforcomparingsupervisedclassiﬁcation
learning algorithms. Neural Comput. 10, 1895–1923 (1998)
7. Drugman, T., Alku, P., Alwan, A., Yegnanarayana, B.: Glottal source processing:
from analysis to applications. Comput. Speech Lang. 28(5), 1117–1138 (2014).
https://doi.org/10.1016/j.csl.2014.03.003
8. Drugman,T.,Dutoit,T.:Glottalclosureandopeninginstantdetectionfromspeech
signals. In: INTERSPEECH, Brighton, Great Britain, pp. 2891–2894 (2009)
9. Drugman,T.,Thomas,M.,Gudnason,J.,Naylor,P.,Dutoit,T.:Detectionofglot-
talclosureinstantsfromspeechsignals:aquantitativereview.IEEETrans.Audio
SpeechLang.Proces.20(3),994–1006(2012).https://doi.org/10.1109/TASL.2011.
2170835
10. Goyal, M., Srivastava, V., Ap, P.: Detection of glottal closure instants from raw
speech using convolutional neural networks. In: INTERSPEECH, Graz, Austria,
pp. 1591–1595 (2019). https://doi.org/10.21437/Interspeech.2019-2587
11. Khanagha,V.,Daoudi,K.,Yahia,H.M.:Detectionofglottalclosureinstantsbased
on the microcanonical multiscale formalism. IEEE/ACM Trans. Audio Speech
Lang. Proces. 22(12), 1941–1950 (2014). https://doi.org/10.1109/TASLP.2014.
2352451
12. Koutrouvelis, A.I., Kafentzis, G.P., Gaubitch, N.D., Heusdens, R.: A fast method
for high-resolution voiced/unvoiced detection and glottal closure/opening instant
estimationofspeech.IEEE/ACMTrans.AudioSpeechLang.Proces. 24(2),316–
328 (2016). https://doi.org/10.1109/TASLP.2015.2506263
13. Leg´at, M., Matouˇsek, J., Tihelka, D.: A robust multi-phase pitch-mark detection
algorithm. In: INTERSPEECH, Antwerp, Belgium, vol. 1, pp. 1641–1644 (2007)
14. Leg´at, M., Matouˇsek, J., Tihelka, D.: On the detection of pitch marks using a
robust multi-phase algorithm. Speech Commun. 53(4), 552–566 (2011). https://
doi.org/10.1016/j.specom.2011.01.008
15. Leg´at, M., Tihelka, D., Matouˇsek, J.: Pitch marks at peaks or valleys? In:
Matouˇsek,V.,Mautner,P.(eds.)TSD2007.LNCS(LNAI),vol.4629,pp.502–507.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74628-7 65
16. Matouˇsek,J.,Tihelka,D.:Classiﬁcation-baseddetectionofglottalclosureinstants
from speech signals. In: INTERSPEECH, Stockholm, Sweden, pp. 3053–3057
(2017).https://doi.org/10.21437/Interspeech.2017-213
17. Matouˇsek,J.,Tihelka,D.:Glottalclosureinstantdetectionfromspeechsignalusing
votingclassiﬁerandrecursivefeatureelimination.In:INTERSPEECH,Hyderabad,
India, pp. 2112–2116 (2018). https://doi.org/10.21437/Interspeech.2018-1147
18. Matouˇsek,J.,Tihelka,D.:Usingextremegradientboostingtodetectglottalclosure
instants in speech signal. In: IEEE International Conference on Acoustics Speech
andSignalProcessing,Brighton,UnitedKingdom,pp.6515–6519(2019).https://
doi.org/10.1109/ICASSP.2019.8683889Context-Aware XGBoost for Glottal Closure Instant Detection 455
19. Mv,A.R.,Ghosh,P.K.:PSFM-aprobabilisticsourceﬁltermodelfornoiserobust
glottal closure instant detection. IEEE/ACM Trans. Audio Speech Lang. Proces.
26(9), 1645–1657 (2018). https://doi.org/10.1109/TASLP.2018.2834733
20. Naylor, P.A., Kounoudes, A., Gudnason, J., Brookes, M.: Estimation of glottal
closureinstantsinvoicedspeechusingtheDYPSAalgorithm.IEEETrans.Audio
Speech Lang. Proces. 15(1), 34–43 (2007). https://doi.org/10.1109/TASL.2006.
876878
21. Pedregosa, F., et al.: Scikit-learn: machine learning in Python. J. Mach. Learn.
Res. 12, 2825–2830 (2011)
22. Sujith,P.,Prathosh,A.P.,Ramakrishnan,A.G.,Ghosh,P.K.:Anerrorcorrection
schemeforGCIdetectionalgorithmsusingpitchsmoothnesscriterion.In:INTER-
SPEECH, Dresden, Germany, pp. 3284–3288 (2015)
23. Tihelka,D.,Hanzl´ıˇcek,Z.,J˚uzov´a,M.,V´ıt,J.,Matouˇsek,J.,Gr˚uber,M.:Current
state of text-to-speech system ARTIC: a decade of research on the ﬁeld of speech
technologies.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2018.LNCS
(LNAI),vol.11107,pp.369–378.Springer,Cham(2018).https://doi.org/10.1007/
978-3-030-00794-2 40
24. Yang, S., Wu, Z., Shen, B., Meng, H.: Detection of glottal closure instants from
speechsignals:aconvolutionalneuralnetworkbasedmethod.In:INTERSPEECH,
Hyderabad,India,pp.317–321(2018).https://doi.org/10.21437/Interspeech.2018-
1281LSTM-Based Speech Segmentation
Trained on Diﬀerent Foreign Languages
B
Zdenˇek Hanzl´ıˇcek( ) and Jakub V´ıt
NTIS - New Technology for the Information Society, Faculty of Applied Sciences,
University of West Bohemia, Univerzitn´ı 22, 306 14, Plzenˇ, Czech Republic
{zhanzlic,jvit}@ntis.zcu.cz
http://www.ntis.zcu.cz/en
Abstract. Thispaperdescribesexperimentsonspeechsegmentationby
using bidirectional LSTM neural networks. The networks were trained
on various languages (English, German, Russian and Czech), segmen-
tation experiments were performed on 4 Czech professional voices. To
be able to use various combinations of foreign languages, we deﬁned
a reduced phonetic alphabet based on IPA notation. It consists of 26
phones,allincludedinalllanguages.Toincreasethesegmentationaccu-
racy,weappliedaniterativeprocedurebasedondetectionofimproperly
segmented data and retraining of the network. Experiments conﬁrmed
theconvergenceoftheprocedure.AcomparisonwithareferenceHMM-
based segmentation with additional manual corrections was performed.
· ·
Keywords: Speech segmentation Neural networks LSTM
1 Introduction
The aim of speech segmentation is to determine phone boundaries in a speech
recording with a given orthographic transcription. For words with several pro-
nunciationforms,theproperphonetictranscriptionisselected.Thesegmentation
process could also involve insertion of pauses. An accurate speech segmentation
is important for many application in the ﬁeld of speech processing.
For a long time, HMM-based speech segmentation [1] was a predominant
method. Recently, neural networks play an important role in almost all speech
processingapplications.Oneofthemostemployedtypeislongshort-termmem-
ory (LSTM) recurrent neural network [2,5].
A common system for speech segmentation is language dependent, i.e. it is
trained and run on the same language. The aim of this research is to analyze
This research was supported by the Czech Science Foundation (GA CR), project
No. GA19-19324S, and by the grant of the University of West Bohemia, project
No. SGS-2019-027. Computational resources were supplied by the project “e-
Infrastruktura CZ” (e-INFRA LM2018140) provided within the program Projects of
Large Research, Development and Innovations Infrastructures.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.456–464,2020.
https://doi.org/10.1007/978-3-030-58323-1_49LSTM-Based Speech Segmentation Trained on Diﬀerent Foreign Languages 457
whether it is feasible to use a LSTM network to segment speech data of another
languagethanithasbeentrainedon.Thiswouldmakepossibletosegmentdata
of a new language, for which no reference training data are available.
Forourexperiments,weutilizedCzech,English,GermanandRussianspeech
data sets recorded for a unit selection TTS system [10]. Our ﬁrst segmentation
experiments were performed on Czech data because the most accurate refer-
ence segmentation was available for these voices; the other languages were used
to train segmentation models. We assume that the proposed approach can be
applied to other language combinations of training and segmentation data.
This paper is organized as follows: Sect.2 describes the phonetic alphabet
used for our multi-language experiments. Section 3 gives an overview of the
neuralnetworkarchitectureandthesegmentationprocess.Experimentsandthe
evaluation are presented in Sect.4. Conclusions and plans for the future work
are given in Sect.5.
2 Phonetic Alphabet
A common phonetic alphabet used for computer applications is SAMPA [11]
which uses a limited set of 7-bit ASCII symbols. However, it is deﬁned for indi-
vidual languages and diﬀerent language-speciﬁc phones can be assigned to the
same symbol. Therefore, SAMPA is not directly applicable for multi-language
tasks. Recently, various modiﬁcations, extensions or redeﬁnitions of the original
SAMPAwereintroduced,e.g.X-SAMPA,thatallowtoovercomethislimitation.
We decided to use directly the International Phonetic Alphabet (IPA) [7]
whichgenerallydescribesalldistinctivespeechsoundsandallowsastraightand
naturallyconsistentcombinationofdiﬀerentlanguages.Nophonetictransforma-
tionbetweendiﬀerentlanguagesisneeded.Thecompletelistofphonesincluded
in our data, including diacritic combinations, is presented in Table 1.
The complete set contains 127 phones; some of them are included only in
one language, e.g. phones r,˚r, EU, dz, c, é, ñ, r, and G are speciﬁc for the Czech
ﬁ ﬁ
"
language. This could be a signiﬁcant complication of the segmentation process
since those phones would not be included in the training data and the network
wouldnotrecognizethem.Tocopewiththisproblem,wehavedeﬁnedareduced
phonetic alphabet based on IPA notation, where all similar phones are joint
together and represented by one symbol. As an extra eﬀect, compacting the
phonetic alphabet increases the robustness of the segmentation process [3,4,6].
The reduction process consists of the following steps
1. removing all punctuation marks, e.g. for vowel duration (a:, O:, ˘e, etc.), con-
sonant syllabicity (r, l, m, etc.), palatalization (pj, bj, dj, etc.), nasalization
" " "
(˜a, ˜E, ˜o:, etc.),
2. splitting all composed (strongly coarticulated) phones to single phones, e.g.
diphthongs (EI, E@, aU, etc.), aﬀricates (ts, tS, dz, dZ, etc.),
3. unifying acoustically similar phones – see Table 2.458 Z. Hanzl´ıˇcek and J. V´ıt
Table 1. The complete list of phones in particular languages.
IPA Languages IPA Languages IPA Languages IPA Languages
p cs,de,en,ru,sk pf de pj ru b cs,de,en,ru,sk
bj ru t cs,de,en,ru,sk ts cs,de,ru,sk tS cs,de,en,sk
tj ru d cs,de,en,ru,sk dz cs,sk dZ cs,de,en,sk
dj ru c cs,sk é cs é sk
ﬀ
k cs,de,en,ru,sk kj ru g cs,de,en,ru,sk gj ru
P cs,de,en,sk m cs,de,en,ru,sk mj ru m cs,en
"
M cs,de,en,sk n cs,de,en,ru,sk n sk nj ru
¯
n en ñ cs,sk ñ sk N cs,de,en,sk
" ﬀ
N: de r cs,de,ru,sk ˚ﬁr cs rj ru
ﬁr cs r cs,sk r: sk f cs,de,en,ru,sk
" "
fj ru v cs,de,en,ru,sk vj ru T en
D en s cs,de,en,ru,sk sj ru z cs,de,en,ru,sk
zj ru S cs,de,en,sk Z cs,de,en,sk ù ru
ü ru ç de x cs,de,ru,sk xj ru
G cs,sk K de h de,en H cs,en,sk
V sk ô en j cs,de,en,ru,sk ô cs,de,en,sk
lj ru l cs,en,sk l: sk L sk
" "
w en C: ru ý: ru ë ru
tC ru i de,ru i: cs,de,en,sk ˘ı ru
e de,ru eI en e: de ˘e ru
E cs,de,en,ru,sk EU cs E@ en ˜E de
˜E: de E: cs,de,sk a cs,de,ru,sk aI de,en
aU cs,de,en ˜a de ˜a: de a: cs,de,sk
A en A: en O de,sk OI en
OY de O: en o cs,de,ru oU cs,en
˜o: de o: cs,de,sk u de,ru,sk u: cs,de,en,sk
y de y: de ø de ø: de
œ de œ: de 6 en 2 en
1 en,ru 0 en,ru I cs,de,en,ru,sk IE sk
Ia sk Iu sk I@ en I sk
“
Y de U cs,de,en,ru UO sk U@ en
U sk @ cs,de,en,ru @U en 8 ru
“
5 de,ru æ de,en,ru,sk 3 en 3: enLSTM-Based Speech Segmentation Trained on Diﬀerent Foreign Languages 459
Table 2. Reduced alphabet – deﬁnition of similar phones.
Reduced Assigned Reduced Assigned Reduced Assigned Reduced Assigned
phone phones phone phones phone phones phone phones
t t c d d é m m M n n N ñ
r r R K ô s s T z z D S S ù C
Z Z ü ý x ç x h H H G l l L ë
v v w i i 1 I y Y e e E ø œ a a A 2 5 æ
o O o 6 8 u u 0 U @ @ 3
Remaining (unassigned) phones p b k g P f j
Theresultingreducedalphabetcontains26phones.Theselectionofassigned
phoneswasperformedmanuallywithastrictrequirementtoshareallphonesby
all languages.
3 System Architecture
The network for acoustic modeling consists of 2 bidirectional LSTM layers fol-
lowedbylinearprojectionandsoftmaxactivation.Thenetworkinputsvectorsof
13 normalized MFCCs and outputs classiﬁcation scores (that can be considered
asposteriorprobabilities)ofparticularphones–seeFig.1.Thespeechwaveform
isdownsampledto16kHz,pre-processedbyapreemphasisﬁlterandparameter-
ized with 5 ms frame shift. During training, cross entropy loss is minimized. We
did not use any network for duration modeling within this research.
3.1 Segmentation Procedure
Foragivenutterancethenetworkoutputsaclassiﬁcationscorematrix,inwhich
rows correspond to phones and columns to speech frames. According to given
phonetic transcription, lines of score matrix corresponding to particular phones
are selected and composed to a new matrix in which the optimal alignment
between frames and phones is found by a simple application of dynamic pro-
gramming – see Fig. 1. Since each word can have more phonetic transcriptions,
the alignment procedure is not performed on a single matrix but on a structure
composed of matrices for particular words [3].
3.2 Detection of Incorrectly Segmented Phones
During the segmentation process, the most suitable phonetic transcription of
each word is selected and aligned with the speech data. However, the result is
not always fully correct. The selected transcription may not exactly correspond
tothespokenwordduetoamispronunciation,text-speechmismatchorduetoan
improper phonetic transcription. Problem could be also caused by the acoustic
model that can be insuﬃciently robust to work properly on a given voice.460 Z. Hanzl´ıˇcek and J. V´ıt
Fig.1. Neural network for acoustic modeling.
We proposed a simple procedure for detection of such improperly segmented
phones, referred to as invalid phones. First, all frames aligned with particular
phones are checked whether the highest classiﬁcation score corresponds to the
given phone. When it is not fulﬁlled for more than 1/3 of frames, the phone is
supposed to be invalid.
Whentheacousticmodelfails,thesegmentationcancontainmarkedlyshort
or long segments assigned to particular phones, especially when no duration
model is used to reduce/mask that problem. Therefore, the mean duration is
calculatedforparticularphonesandphoneinstanceswithdurationoutofinterval
50–200% of the mean value are also taken as invalid.
3.3 Iterative Segmentation Process
The segmentation performed by models trained on diﬀerent languages can be
inaccurate due to diﬀerent phone sets in the training and segmentation data. It
cancontainmanyinvalidphonesasdeﬁnedinthepreviousparagraph.Retraining
ofthenetworkbyusingthisinitialsegmentation(excludinginvalidphones)could
improvetheresult.Theretraining-segmentationprocedurecanberepeateduntil
the resulting segmentation is not signiﬁcantly changing.LSTM-Based Speech Segmentation Trained on Diﬀerent Foreign Languages 461
Table 3. Training and segmentation data.
Language Speaker #sentences #words #phones Duration
Trainingdata Czech(CZ) Male 12,487 107,102 561,878 13:25:27
Female 12,136 118,875 627,466 16:11:25
German(DE) Male 20,096 132,529 625,515 17:30:51
Female 13,001 95,136 468,585 12:30:35
English(EN) Male 19,909 132,872 448,860 10:59:11
Female 11,482 115,684 442,606 12:27:51
Russian(RU) Male 20,829 108,395 539,609 10:55:17
Female 20,829 108,402 539,509 13:22:08
Segmentationdata Czech(CZ) Male1 12,240 119,922 631,698 13:44:02
Male2 12,150 119,166 627,525 15:00:54
Female1 12,151 119,113 628,081 14:58:42
Female2 12,708 108,909 570,100 12:40:33
4 Experiments and Results
4.1 Experimental Data
For our experiments, we used speech data recorded by professional voice tal-
ents for the purposes of unit selection speech synthesis [9]; particular voices are
describedinTable3.Speechdatawassupplementedbyaphoneticsegmentation
created by a HMM-based segmentation procedure with various additional cor-
rection procedures [8]. Czech voices selected for the segmentation experiments
have been utilized for many years in a TTS system [10] and lots of additional
manualcorrectionshavebeenmadeovertime,thereforetheirphonetictranscrip-
tion and segmentation can be considered very accurate and applicable for our
segmentation experiments.
4.2 Initial Segmentation
We trained individual LSTM networks for all languages by using training data
described in Table 3. Besides, we trained one network for all foreign languages
(without Czech) together. All networks were applied on the segmentation data
and the segmentation accuracy was evaluated by comparison with the reference
segmentationusingthemeansegmentationerror.ResultsarepresentedinFig.2;
results for the multi-language network are labeled with XX.462 Z. Hanzl´ıˇcek and J. V´ıt
Fig.2. Mean segmentation error for various training data and the number of units in
bothLSTMlayers.Thelowerpartofeachbarcorrespondstotheerrordeterminedfor
valid phones only. The complete bar corresponds to all phones.
Notsurprisingly,thebestresultswereobtainedfortheCzechnetwork,where
the training and segmentation data are phonetically consistent, and also for the
multi-language network, which is supposed to be the most robust one. Figure 2
also proves the relevance of invalid phones detection: The mean segmentation
error without invalid phones is signiﬁcantly lower, i.e. the detection procedure
successfully revealed the badly segmented data.
4.3 Iterative Segmentation
Weperformedseveralstepsoftheiterativesegmentationprocedure;thestarting
points were segmentation results from the previous paragraph (0-th iteration).
Theevaluationbythemeansegmentationerrorandthenumberofinvalidphones
ispresentedinFig.3.Thelabelsinallgraphsrefertolanguagesusedinthe0-th
iteration,althoughthetraining/segmentationprocedureranwithspeechdataof
particular speakers. In all cases, segmentation error and the number of invalid
phones were decreasing during 2–3 iterations. The ﬁnal values are diﬀerent for
particularlanguagesofinitialnetworks.Theresultingaccuracyisconsistentwith
results for the initial segmentation, i.e. the best accuracy was obtained with the
Czech and multi-language initial networks.
Aninterestingissueisalsotheabilityofanetworkwiththeselectedtopology
to learn a given segmentation. We performed a simple experiment: the network
wastraineddirectlyusingthereferencesegmentationandusedtore-segmentthe
training data. In this manner the mean segmentation error and the number of
invalid phones were determined for particular speakers; the values are included
as reference lines in Fig.3.LSTM-Based Speech Segmentation Trained on Diﬀerent Foreign Languages 463
Fig.3. Mean segmentation error and the number of invalid phones.
5 Conclusion
This paper presented an initial research on LSTM-based speech segmentation
used with diﬀerent language of training and segmentation data. We performed
experiments with models trained on Czech, German, English and Russian data
andalsomodeltrainedonallforeignlanguagestogether.Thesegmentationpro-
cedurewastestedonCzechspeechdata,however,weassumethatthisapproach
isapplicabletootherlanguagecombinationsoftraining/segmentationdata.Pri-
mary results demonstrate that native (Czech in this study) and multi-language
networks can accomplish comparable results.
In our future work, we intend to include other languages, e.g. French, Slovak
and Polish. The segmentation accuracy should be analyzed in more detail. The
deﬁnition of reduced alphabet should be also based on a proper data analysis.
Since the speech data were originally recorded for speech synthesis, the segmen-
tationshouldbeevaluatedbyitsapplicationinaTTS-system.Wewillalsofocus
on the network robustness to be able to segment speech data of worse quality,
that was not originally recorded for the purposes of speech synthesis, e.g. audio
books, which are more diﬃcult to work with.464 Z. Hanzl´ıˇcek and J. V´ıt
References
1. Brugnara, F., Falavigna, D., Omologo, M.: Automatic segmentation and labeling
of speech based on hidden Markov models. Speech Commun. 12, 357–370 (1993)
2. Graves, A.: Supervised Sequence Labelling with Recurrent Neural Networks. SCI,
vol. 385. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-24797-2
3. Hanzl´ıˇcek,Z.,V´ıt,J.,Tihelka,D.:LSTM-basedspeechsegmentationforTTSsyn-
thesis. In: Ekˇstein, K. (ed.) TSD 2019. LNCS (LNAI), vol. 11697, pp. 361–372.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-27947-9 31
4. Haubold,A.,Kender,J.R.:Alignmentofspeechtohighlyimperfecttexttranscrip-
tions. In: Proceeding of ICME, pp. 224–227 (2007)
5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9,
1735–1780 (1997)
6. Hoﬀmann, S., Pﬁster, B.: Text-to-speech alignment of long recordings using uni-
versal phone models. In: Proceedings of Interspeech, pp. 1520–1524 (2013)
7. InternationalPhoneticAssociation:HandbookoftheInternationalPhoneticAsso-
ciation: A Guide to the Use of the IPA. Cambridge University Press, Cambridge
(1999)
8. Matouˇsek, J., Tihelka, D., Psutka, J.: Experiments with automatic segmentation
forCzechspeechsynthesis.In:Matouˇsek,V.,Mautner,P.(eds.)TSD2003.LNCS
(LNAI), vol. 2807, pp. 287–294. Springer, Heidelberg (2003). https://doi.org/10.
1007/978-3-540-39398-6 41
9. Matouˇsek,J.,Tihelka,D.,Romportl,J.:Buildingofaspeechcorpusoptimisedfor
unit selection TTS synthesis. In: Proceedings of LREC (2008)
10. Tihelka,D.,Hanzl´ıˇcek,Z.,J˚uzov´a,M.,V´ıt,J.,Matouˇsek,J.,Gr˚uber,M.:Current
state of text-to-speech system ARTIC: a decade of research on the ﬁeld of speech
technologies.In:Sojka,P.,Hor´ak,A.,Kopeˇcek,I.,Pala,K.(eds.)TSD2018.LNCS
(LNAI),vol.11107,pp.369–378.Springer,Cham(2018).https://doi.org/10.1007/
978-3-030-00794-2 40
11. Wells, J.: SAMPA computer readable phonetic alphabet. In: Gibbon, D., Moore,
R.,Winski,R.(eds.)HandbookofStandardsandResourcesforSpokenLanguage
Systems, pp. 684–732. Mouton de Gruyter, Berlin and New York (1997)Complexity of the TDNN Acoustic Model
with Respect to the HMM Topology
B
Josef V. Psutka1,2( ) , Jan Vanˇek2 , and Aleˇs Praˇz´ak2
1 Department of Cybernetics, University of West Bohemia, Pilsen, Czech Republic
2 NTIS-NewTechnologiesfortheInformationSociety,UWB,Pilsen,CzechRepublic
{psutka j,vanekyj,aprazak}@kky.zcu.cz
Abstract. In this paper, we discuss some of the properties of training
acousticmodelsusingalattice-freeversionofthemaximummutualinfor-
mation criterion (LF-MMI). Currently, the LF-MMI method achieves
state-of-the-art results on many speech recognition tasks. Some of the
keyfeaturesoftheLF-MMIapproachare:trainingDNNwithoutinitial-
ization from a cross-entropy system, the use of a 3-fold reduced frame
rate and the use of a simpler HMM topology. The conventional 3-state
HMM topology was replaced in a typical LF-MMI training procedure
with a special 1-stage HMM topology, that has diﬀerent pdfs on the
self-loop and forward transitions. In this paper, we would like to dis-
cussboththediﬀerenttypesofHMMtopologies(conventional1-,2-and
3-state HMM topology) and the advantages of using biphone context
modeling over using the original triphone or a simpler monophone con-
text.Wewouldalsoliketomentiontheimpactofthesubsamplingfactor
to WER.
· · ·
Keywords: Speechrecognition Acousticmodeling HMMtopology
Lattice-free MMI
1 Introduction
Lattice-free maximum mutual information (LF-MMI) HMM-DNN models [9]
achievedstate-of-the-artworderrorrates(WER)ondiﬀerentwell-knownspeech
databases such as Switchboard and Wall Street Journal (WSJ) in recent years
[5,9]. The conventional HMM topology in ASR is a 3-state left-to-right HMM
that can be traversed in a minimum of 3 frames. This topology was replaced in
the typical LF-MMI training procedure with a topology that can be traversed
in one frame. This one-state HMM topology has diﬀerent pdfs on the self-loop
and forward transitions. The observations are associated with the arcs (rather
thanthestates).ThistopologyisbasedonsimilaritytoConnectionistTemporal
Classiﬁcation(CTC)[3].Inthisarticle,wewouldliketomentiondiﬀerenttypes
of HMM topologies.
In[9],itwasalsoproposedaneﬀectiveapproachusingleft-biphonestoallow
context-dependent modeling. We would like to discuss the advantages of using
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.465–473,2020.
https://doi.org/10.1007/978-3-030-58323-1_50466 J. V. Psutka et al.
biphone context modeling over the use of the original triphone or a simpler
monophonecontext.Wewouldalsoliketomentiontheimpactofthesubsampling
factor to WER.
Table 1. Statistics of train and test data-sets.
Train Dev Test
# of speakers 2670 10 20
# of sentences 581k – –
# of tokens 3.1M 26k 53k
Dataset length [hours] 738 3.3 6.7
Last but not least, we also describe the computational and memory require-
ments of the TDNN LF-MMI acoustic model (AM). Even in the case of trans-
ferring the calculation of AM to the GPU, we may be interested in memory
demands, especially for low-performance graphics cards, or to answer the ques-
tion of how many parallel tasks can we run on a single GPU at a time.
The following sections brieﬂy describe the training and testing datasets. In
Sect.3,wedescribeatypicalTDNNLF-MMIsetup.Theexperimentsandresults
are described in Sect.4. The conclusions are presented in Sect.5.
2 Training and Testing Data
All experiments were performed using a high-quality Czech speech corpus. This
corpus consists of multiple read-speech databases and contains a total of 2670
diﬀerentspeakers.Eachspeakerreadatleast150sentences.Atotalof738738h
of speech data were available. No speech augmentation was performed. Data
were sampled at 16kHz with a resolution of 16 bits.
The test part contains recordings from 30 people (15 men and 15 women),
where each speaker read randomly selected newspaper articles for 20min. Our
test, therefore, includes a total of 10 10h of speech. We also excluded a devel-
opment subset from this test set. Details of the training and testing ﬁles are
summarized in Table 1.
3 Experimental Setup
3.1 Acoustic Feature Extraction
Thefront-endisbasedontheMel-frequencycepstralcoeﬃcients(MFCC).These
40-dimensional features were used not only as an input to the DNN but also
to the GMM. Only in the case of training basic HMM-GMM model, delta and
delta-deltafeatureswereaddedtotheoriginalcoeﬃcients.Nomeanandvariance
normalizations were used. No i-vectors or other speaker adaptation techniques
wereusedinthefeatureextractionprocesseither.Featurevectorswerecalculated
every 10ms (100 frames per second) from the 32ms frames.Complexity of AM - HMM Topology 467
3.2 Acoustic Modeling
Structure and parameters of the acoustic models in LVCSR system were tuned
using KALDI toolkit [8].
GMM: The ﬁrst step is building a monophone acoustic model. A monophone
AM is trained from the ﬂat start using the MFCCs features (static + delta +
delta delta). Secondly, we trained the triphone AM. As the number of triphones
is typically too large, decision trees are used to tie their states. We also applied
linear discriminant analysis (LDA) and Maximum Likelihood Linear Transform
(MLLT) over a central frame spliced across ±3 frames. LDA+MLLT project
the concatenated frames into 40 dimensions space. We used the feature-space
MaximumLikelihoodLinearRegression(fMLLR)andSpeaker-adaptivetraining
procedure (SAT) to adapt GMM models. The whole training data were forced
aligned using the resulting HMM-GMM model. This alignment is necessary as
an input for DNN training [7–9] as opposed to the end-to-end approach (such
as [6] and [4]), where this information is not needed.
TDNN CE: Time Delay Neural Networks (TDNN) have shown to be eﬀective
in modeling long-range temporal dependencies [14]. The TDNNs used for cross-
entropy(CE)trainingwereslightlymodiﬁedcomparedtothosepresentedin[7].
The ﬁrst splicing was the Linear Discriminant Analysis (LDA) transforms layer
(−2,−1,0,1,2).Subsequentlayersthenhadcontexts(−1,0,1),(−1,0,1),(−3,
0,3)and(−6,−3,0).The(−1,0,1)meansthattheﬁrstlayersees3consecutive
frames of input thus the (−3, 0, 3) means that the hidden layer sees 3 frames of
the previous layer, separated by 3 frames. In total, we have ﬁve hidden layers of
ReLu activation function with 650 nodes. The softmax output layer computes
posteriors for clustered GMM based triphone states (4408 states). The overall
context is therefore 13 frames to the left and the 7 to the right. State-level
Minimum Bayes Risk (sMBR) [13] has been used to improve previously trained
models to achieve state-of-the-art results.
TDNN LF-MMI: Maximum mutual information (MMI) [1] is a discrimina-
tive objective function that aims to maximize the probability of the reference
transcription while minimizing the probability of all other transcriptions. The
denominator graph has traditionally been estimated using n-best lists or later
using lattices.
Povey et al. [9] applied MMI training with HMM-DNN models using a full
denominator graph (hence the name lattice-free) by using a phone language
model(insteadofawordlanguagemodel).Insteadofaframe-levelobjective,the
log-probability of the correct phone sequence as the objective function is used.
The LF-MMI (Lattice-Free Maximum Mutual Information) training procedure
hasasequencediscriminative trainingcriterionwithouttheneedforframe-level
cross-entropypre-training.InregularLF-MMI,allutterancesaresplitintoﬁxed-
size chunks (usually 150 frames) to make GPU computations eﬃcient [9]. This468 J. V. Psutka et al.
is done using the alignments from the HMM-GMM system. Standard setup:
12 TDNN layers; dimension in the hidden layers is 1024, bottleneck dimension
is 128; context is ±28 i.e. context per layer 1 1 1 0 3 3 3 3 3 3 3 3.
3.3 Language Modeling
Our ASR system is using the universal trigram back-oﬀ Language Model (LM)
with the mixed-case vocabularies with more than 1.2M words. Our training
text corpus contains thedata fromnewspapers (520M tokens), webnews (350M
tokens),subtitles(200Mtokens),andtranscriptionsofsomeTVprograms(175M
tokens, details can be found in [11]). The resulting LM has 35M bigrams and
almost 30M trigrams. Although we have a comprehensive vocabulary, the per-
centage of OOV words on the test set is 1.3%.
3.4 Decoding
All recognition experiments were performed using our in-house real-time ASR
system. This LVCSR system is optimized for low latency in the real-time (RT)
operation with very large vocabularies. To enable recognition with a vocabulary
containing more than one million words in RT, we speeded up decoding using
a parallel approach (Viterbi search on CPU and DNN segments scores on GPU
[12]).Theoptimalweight/tradeoﬀbetweentheacousticmodelandthelanguage
model was set on the development data for each recognition experiment.
4 Experiments
4.1 HMM-topology
Figure1 shows a simple left-right HMMs with 1-, 2- and 3- emitting states. The
conventionalHMMtopologyinASRisa3-stateleft-to-rightHMM[15](3state)
(shown in Fig.1(f)). This HMM can be traversed in a minimum of 3 frames.
The 3state skip (shown in Fig.1(e)) has additional skip transitions. The skip
transitions are used because of frame subsampling (i.e., because the output rate
at the end of the network is 33.33Hz compared to typical setups where it is
100Hz[9,10]).Asimilarextension(additionofaskiptransition)wasperformed
for the classical 2-state HMM (shown in Fig.1(c) and (d)).
The conventional HMM 3-state topology was replaced in a typical LF-MMI
training procedure with a special 1-stage topology (see Fig.1(b), that has dif-
ferent pdfs on the self-loop and forward transitions (highlighted in bold in the
image). The observations are associated with the arcs (rather than the states).
AllHMMtransitionsprobabilitiesareﬁxedanduniformbecauseadjustingthem
does not improve the recognition results [9]. In Table2 we can see the impact of
a diﬀerent HMM topologies on the ﬁnal WER. In all experiments, a frame sub-
sampling factor of 3 was used. It can be seen that the 1-state models (arc-based
or state-based HMMs) perform slightly better than a 2 or 3-state model.Complexity of AM - HMM Topology 469
4.2 Context-Dependent Modeling
As the past studies have shown [2] context-dependent(CD) modeling has been a
fundamentalpartofHMM-basedmodels,notonlyforHMM-GMM,butalsofor
HMM-DNN.CommonlyusedCDphonesarebiphonesandtriphones.Thehigher
ordercontextisrarelyusedduetohighcomputationalburdens.Analternativeto
CDmodeling,iscontext-independent(CI)modeling,i.e.monophones.CImodels
have a signiﬁcantly smaller number of states and can be simply used without
state tying.
Fig.1. Diﬀerent HMM topologies. The circles represents the states and arrows the
transitions.Inputandoutputstatesarenon-emittingandareshowninblack.a)1state,
b) 1statearc, c) 2state skip, d) 2state, e) 3state skip and f) 3state
Table 2. Impact of the HMM-topology on recognition results
WER %
Monophone Biphone Triphone
1state 10.58 9.95 10.09
1statearc 10.29 9.86 10.02
2state skip 10.57 10.07 10.17
3state skip 10.62 10.09 10.15
A pruned context-dependency left biphone tree is used for regular LF-MMI
training. In Table2, it can be seen the advantages of using biphone context
modeling over the use of triphone or a simpler monophone context.470 J. V. Psutka et al.
4.3 Subsampling
As in [9], we investigated an impact of frame subsampling factor. Subsampling
factor 3 speeds up training by a factor of 2. Subsampling can save a lot of
calculations if the implementation supports this. An example of the impact of
the subsampling factor on recognition results is depicted on the Table3.
Table 3. Impact of subsampling factor on recognition results (triphones CD tree was
used)
HMM topology Subsampling factor # of clustered phonetic states WER %
1state 1 3320 11.42
1state 3 3040 10.09
1statearc 1 2992 10.86
1statearc 3 2888 10.02
2state 1 2936 10.80
2state skip 3 2968 10.17
3state 1 4232 11.30
3state skip 3 4120 10.15
4.4 Implementation Issues
Unfortunately, ourLVCSR systemisbasedonthe classicHMMimplementation
[15]. HMMs observations in “HTK-style” are associated with states rather than
arcs. This brings the need to transform arc-based to state-based HMM (espe-
cially for the 1statearc HMM). The Fig.2 shows an example of such a transfor-
mation. However, it can be seen from the ﬁgure that after the conversion, the
originalsingle-statearc-basedmodel(withthediﬀerentpdfsassociatedwiththe
self-loopandforwardtransitions)changedtotwostatesstate-basedmodel.Thus
both HMMs may emit the same outputs: either a, or ab, or abb, etc.
Fig.2. An example of arc-based transformation to state-based HMM.
4.5 Computational Aspect of DNN Acoustic Models
Inthispaper,wehaveevaluatedthreetypesofcontext-dependentacousticmod-
els: triphone, left biphone and monophone. They shared the same structure andComplexity of AM - HMM Topology 471
diﬀer only in the last layer and number of outputs. In some experiments, a
subsampling factor of 3 was used. Subsampling can save a lot of calculations if
the implementation supports this and the TDNN context is equal to the sub-
sampling factor and/or its multiples. However, the speed-up is not equal to the
subsampling factor because only a part of NN is subsampled. The total number
of parameters for the 12 TDNN layers is approximately 6.4M, and the esti-
mated number of MAD operations (multiply-and-add operations for which the
GPU is particularly suitable) required to evaluate the input signal (without
downsampling) is 317Mper second.Dueto thesubsampling and thebottleneck
NN structure, the NN inference may be run on a low-end GPU even in multi-
ple parallel copies. Note that this estimate assumes absolute reusing of already
computedintermediateresultsintheTDNNlayers.Moreover,wehaveexcluded
the last layer with a diﬀerent number of outputs for a clear comparison.
5 Conclusion
Thispaperdiscussedandcomparedsomeinterestingparametersofacousticmod-
elingusingTDNNLF-MMIintermsofcomputationalcomplexity.Theinﬂuence
ofHMMtopologywasinvestigated,1-2-andclassical3-statesHMMweretested
and compared. Various context dependencies (monophone, left biphone and tri-
phone)werealsoinvestigated. Theeﬀectofsubsamplingwasalsoanalyzed.The
most interesting results are summarized in Table4.
Table 4. Summary of the recognition results
WER [%]
GMM 19.95
12.05
TDNN CE
sMBR 11.51
TDNN LF-MMI 3state triph. 11.30
2state triph. 10.80
subsmpl. = 3 1state biph. 9.95
1statearc biph. 9.86
1state monoph. 10.58
New methods based on the use of TDNN have yielded more than 10% abso-
luteimprovementcomparedtothesolutionbasedonthestandardHMM-GMM.
The best recognition result (9.86% WER) was achieved using 1statearc HMM
topology together with left-biphones. TDNN CE with sMBR also worked very
similarlytoTDNN LF-MMIfora3statetriphonemodel(withoutsubsampling).
A very interesting result was also achieved using a monophone model with
1state HMM topology. In this case, there were as many outputs of the DNN as472 J. V. Psutka et al.
there were diﬀerent phonemes in the task. With an appropriate network struc-
ture, we can even evaluate every third frame (using subsampling). Although
there was a deterioration of 0.72 % (from the best result), this result was very
promising,becauseusingmonophonesinsteadofbiphonessigniﬁcantlysimpliﬁed
the whole decoding process.
As future work, we would like to analyze the inﬂuence of the DNN con-
text. We are mainly interested in its possible minimization with regard to the
recognition results and use in real-time. We would also like to perform similar
experiments on well-known speech databases and verify the obtained results by
a test of statistical signiﬁcance.
Acknowledgments. This paper was supported by the project LO1506 of the Czech
Ministry of Education, Youth and Sports under the program NPU I.
References
1. Bahl, L., Brown,P., de Souza, P., Mercer, R.: Maximum mutual information esti-
mation of hidden Markov model parameters for speech recognition. In: ICASSP
1986, vol. 11, pp. 49–52 (1986)
2. Deng,L.,Acero,A.,Dahl,G.,Yu,D.:Context-dependentpre-traineddeepneural
networksforlargevocabularyspeechrecognition.IEEETrans.AudioSpeechLang.
Process. 20, 30–42 (2012)
3. Graves, A., Fern´andez, S., Gomez, F.: Connectionist temporal classiﬁcation:
labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In:Proceed-
ingsoftheInternationalConferenceonMachineLearning,ICML2006,pp.369–376
(2006)
4. Hadian, H., Sameti, H., Povey, D., Khudanpur, S.: End-to-end speech recognition
using lattice-free MMI. In: Interspeech 2018, pp. 12–16 (2018)
5. Han,K.J.,Hahm,S.,Kim,B.,Kim,J.,Lane,I.R.:Deeplearning-basedtelephony
speech recognition in the wild. In: Interspeech 2017, pp. 1323–1327 (2017)
6. Hannun, A., et al.: Deep speech: scaling up end-to-end speech recognition.
abs/1412.5567 (2014). http://arxiv.org/abs/1412.5567
7. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. In: Interspeech 2015, pp. 3214–
3218 (2015)
8. Povey, D., et al.: The Kaldi speech recognition toolkit. In: IEEE 2011 Workshop
on Automatic Speech Recognition and Understanding, January 2011
9. Povey,D.,etal.:Purelysequence-trainedneuralnetworksforASRbasedonlattice-
free MMI. In: Interspeech 2016, pp. 2751–2755 (2016)
10. Sak, H., Senior, A.W., Rao, K., Beaufays, F.: Fast and accurate recurrent neural
network acoustic models for speech recognition. In: Interspeech 2015, pp. 1468–
1472 (2015)
11. Sˇvec,J.,Hoidekr,J.,Soutner,D.,Vavruˇska,J.:Webtextdataminingforbuilding
large scale language modelling corpus. In: Habernal, I., Matouˇsek, V. (eds.) TSD
2011.LNCS(LNAI),vol.6836,pp.356–363.Springer,Heidelberg(2011).https://
doi.org/10.1007/978-3-642-23538-2 45
12. Vanek,J.,Trmal,J.,Psutka,J.V.,Psutka,J.:Optimizedacousticlikelihoodscom-
putation for NVIDIA and ATI/AMD graphics processors. IEEE Trans. Audio
Speech Lang. Process. 20(6), 1818–1828 (2012)Complexity of AM - HMM Topology 473
13. Vesely´,K.,Ghoshal,A.,Burget,L.,Povey,D.:Sequence-discriminativetrainingof
deep neural networks. In: Interspeech 2013, pp. 2345–2349 (2013)
14. Waibel,A.,Hanazawa,T.,Hinton,G.,Shikano,K.,Lang,K.J.:Phonemerecogni-
tionusingtime-delayneuralnetworks.IEEETrans.Acoust.SpeechSignalProcess.
37(3), 328–339 (1989)
15. Young, S.: The HTK hidden Markov model toolkit: Design and philosophy.
Entropic Cambridge Research Laboratory, Ltd, vol. 2, pp. 2–44 (1994)DialogueLeyzer: A Dataset for Multilingual
Virtual Assistants
B B
Marcin Sowan´ski1,2( ) and Artur Janicki2( )
1 Samsung R&D Institute Poland, Warsaw, Poland
m.sowanski@samsung.com
2 Warsaw University of Technology, Warsaw, Poland
a.janicki@tele.pw.edu.pl
Abstract. In this article we present the Leyzer dataset, a multilingual
textcorpusdesignedtostudymultilingualandcross-lingualnaturallan-
guage understanding (NLU) models and the strategies of localization of
virtual assistants. The proposed corpus consists of 20 domains across
three languages: English, Spanish and Polish, with 186 intents and a
wide range of samples, ranging from 1 to 672 sentences per intent. We
describe the data generation process, including creation of grammars
and forced parallelization. We present a detailed analysis of the cre-
atedcorpus.Finally,wereporttheresultsfortwolocalizationstrategies:
train-on-targetandzero-shotlearningusingmultilingualBERTmodels.
·
Keywords: Virtual assistant Multilingual natural language
· ·
understanding Text corpus Machine translation
1 Introduction
Virtual assistants (VAs) have been available since 1960s, but the release of their
recent generation on smartphones and embedded devices has opened them to a
broaderaudience.Themostpopulardevelopmentapproachforsuchsystemsisto
releaseinitialsetoflanguages,usuallyEnglishastheﬁrst,andthenthefollowing
languages.Althoughtheremightbevariousreasonsforchoosingsuchapproach,
it is clear that adding support for new languages is a time- and cost-consuming
process.
There are over 6900 living languages in the world, from which more than 91
have over 10 million users. If we want to build an unfragmented e-society, we
have to develop methods that will allow us to create multilingual VAs also for,
so called, low-resource languages.
InthisworkwepresentLeyzer1,adatasetcontainingalargenumberofutter-
ancescreatedforthepurposeofinvestigationofcross-lingualtransferlearningin
1 Named after Ludwik Lejzer Zamenhof, a Polish linguist and the inventor of the
international language Esperanto, the most widely used constructed international
auxiliary language in the world. https://en.wikipedia.org/wiki/L. L. Zamenhof.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.477–486,2020.
https://doi.org/10.1007/978-3-030-58323-1_51478 M. Sowan´ski and A. Janicki
naturallanguageunderstanding(NLU)systems.WebelievethatLeyzerpresents
greatopportunitiestoinvestigatemultilingualandcross-lingualNLUmodelsand
localization strategies, which allow translating and adapting an NLU system to
a speciﬁc country or region. While creating our dataset, we focused particularly
on testing localization strategies that use machine translation (MT) and multi-
lingual word embeddings. First localization strategy that we tested was the so
called train-on-target, where the training corpus of a system is translated from
one language to another and the model trained from this corpus is tested on
a parallel testset that was created manually by language experts (LEs). Sec-
ond localization strategy tested was zero-shot learning, where the system that
usedmultilingualembeddingsissupposedtogeneralizefromthelanguageitwas
trained on to new languages that it will be later tested on. Finally, we report
results for two types of baseline models that were trained either on single lan-
guage data only or on all data available in three languages at once.
To the best of our knowledge, Leyzer is the largest dataset in terms of the
number of domains, intents (where intent is understood as an utterance-level
conceptrepresentingsystemfunctionalityavailablefortheuser)andslots(where
slot is deﬁned as a word-level concept representing the parameters of a given
intent)intheareaofmultilingualdatasetsfocusedonproblemsofthelocalization
ofVAdatasets.Ithasbeenpubliclyreleased,withthecodetoallowreproduction
of the experiments and is available at https://github.com/cartesinus/leyzer.
2 Related Datasets
There exist a couple of text corpora which are often used in the context of VAs,
which can be divided into two groups.
Table 1. Statistics of existing corpora compared to Leyzer, proposed in this work.
First group consists of resources designed to train and test VAs without focusing on
multilingual setup. Second group concerns multilingual VAs.
Dataset Languages #Utterances #Domains #Intents #Slots
ATIS[11] en 5871 1 26 83
Larsonetal.[8] en 23,700 10 150 0
Liuetal.[9] en 25,716 19 64 54
Snips[5] en,fr 2,943/1,136 – 7 72
Schusteretal.[12] en,es,th 43,323/8,643/5,083 3 12 11
Leyzer(thiswork) en,es,pl 3779/5425/7053 20 186 86
The ﬁrst group, represented by The Air Travel Information System (ATIS)
[11] dataset consists of spoken queries from ﬂight domain in the English lan-
guage. ATIS has a small number of intents and is heavily unbalanced with most
utterancesbelongingtothreeintents.Larsonetal.[8]createdadatasettostudyLeyzer Corpus 479
out-of-scope queries that do not fall into any of the system’s supported intents.
Presentedcorpusconsistsof23,700queriesequallydistributedamong150intents
which can be grouped into 10 general domains. Yet another corpus for English
is the one created by Liu et al. [9]. Their dataset, created as a use case of a
home robot, can be used to train and compare multiple NLU platforms (Rasa,
Dialogﬂow,LUISandWatson).Thedatasetconsistsof25,716Englishsentences
from 21 domains that can be divided into 64 intents and 54 slot types.
The Snips [5] dataset represents the second category of VAs datasets that
were designed to train and evaluate multilingual VAs. The dataset has a small
numberofintents;eachintent,however,hasalargenumberofsentences.Schuster
et al. [12] proposed a multilingual dataset for English, Spanish and Thai to
studyvariouscross-lingualtransferscenarios.Thedatasetconsistsof3domains:
Alarm,ReminderandWeatherwithsmallnumberofintentsandslots(11intents
and 12 slots in total). Diﬀerent languages have diﬀerent number of sentences,
withEnglishhaving43,323,Spanish8,643andThaihaving5,083ones.Itfollows
that there is a large number of sentences per intent and per slot type.
Table1 summarizes the existing corpora used to test VAs, and compares
them with our dataset, proposed in this article. There are many multi-domain
and multi-intent resources for English from which to choose. However, to the
bestofourknowledge,thereexistnomultilingualresourceswithmanydomains,
intents and slot types.
3 Our Dataset
We designed our dataset to be useful mostly in the following two areas related
to VAs:
– development and evaluation of VAs, and
– creation and localization of the dataset into other languages in order to have
a parallel multilingual dataset.
Commercial VA systems often face multiple challenges:
1. Numberoflanguagesandtheirlinguisticphenomena,whichrepresentsachal-
lengeofbuildingamultilingualsystemandhandlingphenomenasuchasﬂex-
ion, which has impact on slot recognition,
2. Number of domains and their distribution, that introduce two major chal-
lenges:
(a) howtotrainamodeltoequallyrepresenteachdomain,evenifourtrainset
is not balanced in terms of number of sentences per domain,
(b) how to treat sentences that are similar or identical in more than one
domain,
3. Number of intents and how they diﬀer. This introduces a problem of having
multiple intents that diﬀer only by one parameter or word,
4. Numberofslotsandtheirvalues,thatintroducesachallengeofhowtotraina
modelthatwillrecognizeslotsnotbytheirvaluesbutratherbytheirsyntactic
function in the sentence.480 M. Sowan´ski and A. Janicki
We approached these typical problems by creating a dataset (Table 2) that
consists of a large number of intent classes (186), yet also contains a wide range
of samples per intent class, ranging from 1 to 672 sentences per intent. We
selected three languages that represent separate language families (Germanic,
Romance, Slavic) to address problems typical for multilingual systems.
There is no easy mapping between the intents in Leyzer and these of Larson
et. al. [8], some intents however, overlap. When comparing the intents of Leyzer
and the intents in the corpus created by Liu et al. [9] we found out that out
of their 18 domains (called scenarios in [9]) we could match seven domains in
Leyzer. Similarly to Schuster et al. [12], in our paper we tested train-on-target
and one zero-shot scenarios. When compared to Schuster et al., our dataset
consistsofmoreintentsandslots,which,webelieve,mayhavesigniﬁcantimpact
on the results, especially for the train-on-target scenario. If an NLU system
has hundreds of closely-related intents, MT systems may easily fail to properly
distinguish them, which, as a consequence, may lead to a lot of mismatches.
Leyzer diﬀers from corpora such as MultiWoz [1], because our dataset con-
tains isolated utterances instead of dialogues. We wanted to create a resource
that is controllable and cheap in terms of the time needed to create or modify
it. We also wanted to demonstrate that VAs able to handle hundreds of intents
and slots are still a challenging task.
3.1 Creation of Corpus
Generation of Leyzer consisted of four steps: creating base grammars, creating
target grammars, applying forced parallelization, slot expansion and splitting
data into train-, dev- and testsets. They are brieﬂy characterized below.
In contrast to approaches such as MultiWoz, where utterances are usually
gathered with the use of crowdsourcing, we decided to use grammars that are
written by qualiﬁed LEs. We believe that all concerns on grammar-based gener-
atedtext,namelyontheirlackofnaturalness,canbeeliminatediftheprocedure
of quality control is implemented. We think that grammar-based corpora have
two noteworthy advantages: they are cheap in generation and remodeling, and
they can cover all possible ways to express a given intent, which crowdsourced
approaches can easily miss.
Base Grammars Creation. Starting with English, we created 20 grammars
with sentence patterns in the JSpeech Grammar Format (JSGF). Initial set of
intents in each domain was inspired by example commands available in Almond
VirtualAssistant[2].SlotvalueswerecrawledfromtheInternetorcreatedman-
ually. Depending on slot type, we gathered from a few to a few hundreds values
for each slot.
All sentence patterns in the corpus were generated from grammars. Each
of such patterns represents possible way to utter a sentence without explicitly
giving the content of the slots. Later on, grammars were ﬁlled with the slot
values. Since sentences generated in such fashion might contain some unnaturalLeyzer Corpus 481
expressions or grammatical errors, we requested veriﬁcation by LEs. Wherever
it was possible, incorrect sentences were ﬁxed, and if that was not possible,
sentences were removed.
TargetGrammarsCreation. Thesameprocedureswereusedtocreatetarget
grammars. To have intents and slots with same meaning in all languages, LEs
were asked to create grammars with intents which represent the same meaning
asinEnglish,butatthesametime,representthemostnaturalwayofexpressing
such an intent in the target language. Slot values were either crawled or created
manually.
Forced Parallelization. Although,asdiscussedinthepreviousstep,thesame
intents will have the same meaning in all languages, there is no sentence-to-
sentence mapping between diﬀerent languages. It is so because intents can be
expressed diﬀerently across languages and our creation procedure did not imply
parallel translations. To mitigate this problem, we decided to create a paral-
lel subset of our corpus that can be used as a testset for cross-lingual exper-
iments. All English patterns were machine translated into Polish and Spanish
with Google Translate and then veriﬁed and ﬁxed by the LEs in the OmegaT2
tool.
Slot Expansion. Patterns for all languages, as presented in Table3, were
expanded with slot values that were previously crawled or manually created.
Wepaidalotofattention togatheringenoughslotvaluessothatduringexpan-
sion each pattern, if possible, has a diﬀerent slot value. This way, we were able
toavoidthesystematicerrorofthesystemthatmemorizestheslotsonthebasis
of their values. Once the patterns were expanded, the LEs veriﬁed them and
changed them, if needed.
Data Split. The last step of corpus creation was splitting it into three parts:
trainset, testset and development set. To create the testset, we ﬁrst created
parallelsentences,asdescribedinForcedParallelizationstep,andlaterexpanded
the slots. Then, we selected at least one sentence from each intent which at the
same time was available in all three languages. This way it will be possible to
test cross-lingual scenarios. The training and development parts of the corpus
weretakenfromthetargetgrammarpatternsthatwereexpandedwiththeslots.
Up to 10% of such expansion formed the development set, while the remaining
part formed the trainset.
3.2 Domain Selection
Following[2]weused20domains,whichrepresentpopularapplicationsthatcan
be used on mobile devices, computers or embedded devices. We can categorize
them into groups with similar functions:
2 A computer-assisted translation tool: https://omegat.org/.482 M. Sowan´ski and A. Janicki
– Communication with Email, Facebook, Phone, Slack and Twitter domains
in that group. All these domains contain a kind of command to send a mes-
sage.
– Internet with Web Search and Wikipedia. The aim of these domains is to
search for information on the web and, therefore, these domains will have a
lot of open-title queries.
– Media and Entertainment with Spotify and YouTube domains in that
group. The root function of these applications is to ﬁnd content with name
entities connected with artists or titles.
– Devices with Air Conditioner and Speaker domains. These domains repre-
sent simple physical devices that can be controlled by voice.
– Self-management with Calendar and Contacts. These domains consist of
actions that involve time planning and people.
– Other non categorized domains represent functions and language not com-
mon to the other categories. In that sense, remaining domains can be repre-
sented as intentionally not matching other domains.
Table 2. Statistics of sentences, intents and slots across domains and languages in
Leyzer dataset.
Domain #Intents #Slots #EnglishUtt. #SpanishUtt. #PolishUtt.
Airconditioner 13 3 48 61 52
Calendar 8 5 69 120 190
Contacts 12 4 306 481 615
Email 11 7 294 315 301
Facebook 7 4 48 581 193
Fitbit 5 3 89 116 263
GoogleDrive 11 5 55 241 305
Instagram 10 6 144 471 579
News 4 3 31 30 42
Phone 5 4 192 283 130
Slack 13 8 268 268 295
Speaker 7 2 73 72 43
Spotify 18 7 633 827 823
Translate 9 6 462 109 452
Twitter 6 3 147 270 122
Weather 10 5 154 159 123
Websearch 7 2 167 291 1498
Wikipedia 8 1 200 234 162
Yelp 12 5 222 142 326
Youtube 10 3 177 354 539
Total 186 86 3779 5425 7053Leyzer Corpus 483
As mentioned above, several domains diﬀer in size to better reﬂect propor-
tions from the real world problems where some applications will only have a
few possible ways to express commands, while the other ones will have almost
inﬁnite number of valid expressions.
3.3 Intent and Slot Selection
There is a close relationship between intents and slots in our corpus, as the
intents represent functions or actions that users want to perform, while the
slots are the parameters of these intents. In many cases intents represent the
same action, but they have been distinguished on the basis of the number of
parameters. During the creation of intents our principle was that intents must
diﬀerfromeachothereitherbythelanguage(diﬀerentimportantkeywords)orby
the number of slots they have. The reason for that is purely pragmatic because
in order to avoid system’s unstability we cannot have two identical sentences
withdiﬀerentintents.Themodelinputisasentenceanditsoutputistheintent,
so if in the training corpus we had two identical sentences pointing to diﬀerent
intents, then the model would not able to learn to which intent this sentence
should be assigned.
Table 3. Representative patterns from selected domains of the corpus.
Domain Intent SentencePattern
Calendar AddEventWithName Addaneventcalled$EVENT NAME
Email ShowEmailWithLabel Showmemyemailswithlabel$LABEL
Facebook ShowAlbumWithName Showphotosinmyalbum$ALBUM
Slack SendMessageToChannel Send$MESSAGEto$CHANNELonslack
Spotify PlaySongByArtist Play$SONGby$ARTIST
Translate TranslateTextToLanguage Translate$TEXTto$TRG LANG
Weather OpenWeather What’stheweatherlike
Websearch SearchTextOnEngine Google$TXT QUERY
The slots in our corpus can be categorized into two groups:
– Open-titled – where the value of the slot can be treated as inﬁnite and
therefore cannot be listed. Open-title slots are challenging for NLU systems
because they force them to generalize the unseen data.
– Close-titled – where the values of the slots can be listed.
4 Experiments
4.1 Experimental Setup
As an architecture for all of our experiments we used the Joint BERT architec-
ture [4] implemented in the NeMo toolkit [7]. We used the pre-trained multilin-
gual cased BERT model [13] consisting of 12-layers and 110 M parameters. If484 M. Sowan´ski and A. Janicki
not stated otherwise, we trained models for 100 epochs and saved checkpoints
for each one. All checkpoint were evaluated on test part of corpora. Reported
results come from the checkpoint which achieved the highest score in the tests.
Thebatchsizewas128.Adam[6]wasusedforoptimizationwithaninitiallearn-
ingrateof2e−5.Thedropoutprobabilitywassetto0.1.Wetrainedeachmodel
independentlywithall dataavailable inthetrainingpartofcorpus.Inall ofour
experiment we used the ﬁrst version of our corpus (0.1.0).
4.2 Testing Scenarios
We evaluated the proposed corpus using the following four scenarios:
– Single-language Models – here we trained each language independently
on all sentences available in the trainset and we evaluated the model on a
testset.
– Multi-language Model–inthisexperimentwetrainedonemodelusingall
trainingdataavailableforallthreelanguagesandindependentlyevaluatedit
for each language.
– Train-on-target–similartostrategyproposedbyCettoloetal.[3],weused
Google Translate to translate English patterns into Polish and Spanish, and
expanded them with target slot values.
– Zero-shot Learning – to test this scenario we trained English model with
multilingual casedBERTfrom the English part ofLeyzer trainset and tested
it on Polish and Spanish testsets.
We used the accuracy to evaluate the performance of intent prediction and the
standard BIO structure to calculate macro F1-score that does not take label
imbalance into account. We used the evaluation metric implemented in scikit-
learn [10] and provided in the NeMo evaluation script. Using this script, we
tested each model epoch, and the results for the ones that scored best on both
the intent and the slot level are presented in Table4.
4.3 Results and Discussion
The Single-BERT models scored relatively low on both the intent and the slot
level, yielding 47%, 52% and 69% intent accuracy for English, Polish and Span-
ish, respectively. We believe that reason for that is a large number of intent
classesinourcorpus,which,bytheway,wasamotivationtocreatesuchcorpus.
In order to give some perspective to our experiments, we trained the model
on the training part of the ATIS dataset with the same parameters as in the
Single-BERT scenario. When evaluated on the test part of ATIS, we received
97.31%ontheintentand55.23%ontheF1-macroslotlevel(and97.11%forF1-
micro). Those results suggest that easier problems, such as ATIS, can be easily
learned by Single-BERT model.
TheMulti-BERTexperimentscoredbetterthantheSingle-BERTmodelson
bothintentandslotlevel.WebelievethatthereasonforthisisthatmultilingualLeyzer Corpus 485
Table 4.ResultsforNeMomodelstrainedonvariousconﬁgurationsofLeyzercorpus
Model type Language Intent acc. Slot F1 macro
Single-BERT English 46.58 45.07
Polish 51.66 54.56
Spanish 68.88 67.79
Multi-BERT English 62.80 76.48
Polish 64.17 74.83
Spanish 72.26 84.66
Train-on-target Polish 41.67 40.70
Spanish 46.42 52.38
Zero-shot Polish 13.82 15.39
Spanish 30.21 24.13
modelhadmoredatatolearnhowtoseparateintentclassesandeliminateincon-
sistencies.Presentedresultssuggestthatmultilingualmodelsmightbeneﬁtfrom
joint learning on multiple languages, at least for problems that are formulated
as in this paper.
The train-on-target models scored low when compared to the Single-BERT
models. We think that the MT errors, especially in the most important compo-
nents of the sentence (usually verbs) led to a drastic performance drop. On the
intent classiﬁcation level the accuracy for Polish and Spanish were respectively
9.9% and 22.5% relative lower than the baseline.
Thezero-shotscenarioscoredverylowwhencomparedtotheSingle-BERTor
the train-on-target experiments. Large number of intent classes, combined with
diﬀerent slot values in each language is a non-trivial problem, and, apparently,
more sophisticated methods are needed.
The results presented in this article may seem unsatisfactory, especially if
we compare them to other VA publications. However, it is noteworthy that a
search for the best architecture and parameters was not an intent of this work
– we rather wanted to set the baselines and to show complexity of the MT
problem for the proposed data. We aimed to create a challenging corpus which
can be a subject of future works, such as the localization of VAs with the use of
train-on-target and zero-shot learning scenarios.
5 Conclusions and Future Work
In our work we introduced a new dataset, named Leyzer, designed to study
multilingual and cross-lingual NLU models and localization strategies in VAs.
We also demonstrated the results for the models trained on our corpus that can
set the baseline for further work.
In the future we plan to extend our dataset to new languages and increase
the number of sentences per intent. Another line of work that we consider is to486 M. Sowan´ski and A. Janicki
add follow-up intents, as this would allow to build a fully autonomous VA from
our dataset.
The Leyzer dataset, the translation memories and the detailed experiment
results presented in this paper are available at https://github.com/cartesinus/
leyzer. We hope that this way we will foster further research in machine trans-
lation for the virtual assistants.
Acknowledgements. We thank Ma(cid:2)lgorzata Misiaszek for her help in verifying the
quality of our corpus and improving its consistency.
References
1. Budzianowski, P., et al.: MultiWOZ - a large-scale multi-domain wizard-of-Oz
dataset for task-oriented dialogue modelling. In: Proceedings of the 2018 Con-
ferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,
pp. 5016–5026. Association for Computational Linguistics (2018). https://www.
aclweb.org/anthology/D18-1547
2. Campagna,G.,Ramesh,R.,Xu,S.,Fischer,M.,Lam,M.S.:Almond:thearchitec-
tureofanopen,crowdsourced,privacy-preserving,programmablevirtualassistant.
In:Proceedingsofthe26thInternationalConferenceonWorldWideWeb,pp.341–
350 (2017)
3. Cettolo, M., Corazza, A., De Mori, R.: Language portability of a speech under-
standing system. Comput. Speech Lang. 12(1), 1–21 (1998)
4. Chen,Q.,Zhuo,Z.,Wang,W.:BERTforjointintentclassiﬁcationandslotﬁlling
(2019)
5. Coucke,A.,etal.:Snipsvoiceplatform:anembeddedspokenlanguageunderstand-
ing system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190
(2018)
6. Kingma,D.P.,Ba,J.:Adam:amethodforstochasticoptimization.In:Proceedings
ofthe6thInternationalConferenceonLearningRepresentations(ICRL2015),San
Diego, CA (2015)
7. Kuchaiev, O., et al.: NeMo: a toolkit for building AI applications using neural
modules (2019)
8. Larson, S., et al.: An evaluation dataset for intent classiﬁcation and out-of-scope
prediction. In: Proceedings of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP 2019), Hong Kong, China (2019)
9. Liu, X., Eshghi, A., Swietojanski, P., Rieser, V.: Benchmarking natural lan-
guage understanding services for building conversational agents. arXiv preprint
arXiv:1903.05566 (2019)
10. Pedregosa, F., et al.: Scikit-learn: machine learning in Python. J. Mach. Learn.
Res. 12, 2825–2830 (2011)
11. Price,P.:Evaluationofspokenlanguagesystems:theATISdomain.In:Proceedings
of the Speech and Natural Language Workshop, Hidden Valley, PA (1990)
12. Schuster, S., Gupta, S., Shah, R., Lewis, M.: Cross-lingual transfer learning for
multilingual task oriented dialog. In: Proceedings of the 2019 Annual Conference
of the North American Chapter of the Association for Computational Linguistics
(NAACL-HLT 2019), Minneapolis, MN (2019)
13. Wolf,T.,etal.:Huggingface’stransformers:State-of-the-artnaturallanguagepro-
cessing. ArXiv abs/1910.03771 (2019)Registering Historical Context
for Question Answering in a Blocks
World Dialogue System
B
Benjamin Kane( ) , Georgiy Platonov, and Lenhart Schubert
University of Rochester, Rochester, NY 14627, USA
{bkane2,gplatono,schubert}@cs.rochester.edu
Abstract. Task-orienteddialogue-basedspatialreasoningsystemsneed
tomaintainhistoryoftheworld/discoursestatesinordertoconveythat
the dialogue agent is mentally present and engaged with the task, as
well as to be able to refer to earlier states, which may be crucial in col-
laborative planning (e.g., for diagnosing a past misstep). We approach
theproblemofspatialmemoryinamulti-modalspokendialoguesystem
capable of answering questions about interaction history in a physical
blocksworldsetting.Weemployapipelineconsistingofavisionsystem,
speech I/O mediated by an animated avatar, a dialogue system that
robustly interprets queries, and a constraint solver that derives answers
basedon3Dspatialmodelling.Thecontributionsofthisworkincludea
semantic parser competent in this domain and a symbolic dialogue con-
textallowingforinterpretingandansweringfree-formhistoricalquestions
using world and discourse history.
· · ·
Keywords: Question answering Blocks world Semantic parsing
Discourse context
1 Introduction
Intelligent,task-orienteddialogueagentsthatinteractwithhumansinaphysical
setting are a long-standing AI goal and have received renewed attention in the
last10or20years.However,theyhavegenerallylackedthesortofrecallofearlier
discourse and perceived “world” situations and events—an episodic memory—
neededtoprovideasenseofsharedcontextualawarenessand,ultimately,abasis
for diagnosing past errors, planning to re-achieve an earlier situation, repeating
a past action sequence, etc.
The blocks world domain is an ideal setting for developing prototypes with
such capabilities.
Inthiswork,wepresentaspeech-basedquestion-answeringsystemforaphys-
icalblocksworldfeaturingavirtualagent,thatnotonlymodelsandunderstands
This work was supported by DARPA grant W911NF-15-1-0542, NSF NRT Graduate
Training Grant 2019–2020, and NSF EAGER Award IIS-1940981.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.487–494,2020.
https://doi.org/10.1007/978-3-030-58323-1_52488 B. Kane et al.
spatial relations but is able to register historical context and answer questions
about the session history, such as “Which block did I just move?”, “Where was
the Toyota block before I moved it?”, “Did the Target block ever touch the Tex-
aco block?”, “Was the Twitter block always between two red blocks?”, etc. Since
explicitstorageofdetailedsuccessivescenemodelswouldbediﬃculttoextendto
generalcomplexsettingsaswellasbeingcognitivelyimplausible(peopleseemto
reconstruct past situations from high-level properties [15]), we maintain a com-
pact symbolic record of changes to the world, allowing reconstruction of past
states when combined with current spatial observations.
2 Related Work
Early studies featuring the blocks world include [18] and [3], both of which
maintainedsymbolicmemoryofblocks-worldstates.Theydemonstratedimpres-
siveplanningcapabilities,buttheirworldsweresimulated,interactionwastext-
based, and they lacked realistic understanding of spatial relations.
Modern eﬀorts in blocks worlds include work by Perera et al. [13], which is
focused on learning spatial concepts (staircases, towers, etc.) based on verbally-
conveyedstructuralconstraints,e.g.,“Theheightisatmost3”,aswellasexplicit
user-given examples and counterexamples.
Bisk et al. [2] use deep learning to transduce verbal instructions into block
displacements in a simulated environment.
Some deep learning based studies achieve near-perfect scores on the CLEVR
question answering dataset [10,12]. A common limitation of these approaches is
reliance on unrealistically simple spatial models and domain-speciﬁc language
formalisms,andinrelationtoourwork,thereisnoquestionansweringfunction-
ality or episodic memory.
We are not aware of any recent study in a physical blocks world domain
that makes use of spatial memory in answering questions about past states and
events.
Outside of the blocks world domain, the TRAINS and TRIPS systems [4,5]
werenoteworthyfortheirdialogue-basedproblemsolvingabilityinavirtualmap
environment and their support of planning through temporal reasoning based
on Allen Interval Logic [1]. A system aimed at human-like performance on a
virtual reality map recall task [11] was based on the LIDA symbolic cognitive
architecture and represented spatial context using a grid representation of the
world and hierarchical “place nodes” with progressively updated activations.
Recent deep-learning based approaches to modelling spatial episodic mem-
oryinclude[16]and[6].Theformerusesanunsupervisedencoder-decodermodel
to represent episodic memory as latent embeddings, and show that this model
can allow a robot to recall previous visual episodes in a physical scene. The lat-
ter introduces a neuro-symbolic Structured Event Memory (SEM) model which
is capable of segmenting events in video data and reconstructing past memory
items.Thesemethods,however,donotreadilylendthemselvestouseforreason-
ingabouthistoricalrelationsorinteractionsinablocksworldquestionanswering
system.Session Context-Based QA in the Block World 489
3 Blocks World System and Eta Dialogue Manager
Figure1a, b depict our physical blocks world (consisting of a square table with
several cubical blocks, two Kinect sensors and a display) and the system’s soft-
warearchitecture.Theblocksarecolor-codedasgreen,red,orblue,andmarked
with corporate logos which serve as unique identiﬁers. The system uses audio-
visual I/O: the block tracking module periodically updates the block position-
ing information by reading from the Kinect cameras and an interactive avatar,
David, is used for communication. The block arrangement is modeled as a 3D
sceneinBlender,whichactsassystem’s“mentalimage”ofthestateoftheworld.
Blocks world setup Dialogue pipeline
Fig.1. System overview. (Color ﬁgure online)
The Eta dialogue manager (DM) is responsible for semantic parsing and
dialogue control. Eta is designed to follow a modiﬁable dialogue schema, the
contentsofwhichareformulas inepisodiclogic[17]withopenvariablesdescrib-
ingsuccessivesteps(events)expectedinthecourseoftheinteraction.Theseare
either realized directly as instantiated actions, or expanded into sub-schemas1.
In order to instantiate schema steps and interpret user inputs, the DM uses
hierarchical pattern transduction,similarlytothemechanismusedbytheLISSA
system [14] to extract context-independent gist clauses given the prior utter-
ance. Transduction hierarchies specify patterns at their nodes to be matched to
input,withterminalnodesprovidingresulttemplates,orspecifyingasubschema.
The pattern templates look for particular words or word features (including
“wildcards” matching any word sequence of some length). Eta uses gist clause
extraction for tidying-up the user’s utterance, and then derives an unscoped
logical form (ULF) [9] (a preliminary form of the episodic logic syntax of the
dialogue schema) from the tidied-up input. ULF diﬀers from similar semantic
representations,e.g.,AMR,inthatitisclosetothesurfaceformofEnglish,type-
consistent, and covers a rich set of phenomena. To derive ULFs, we introduced
semantic composition into the transduction trees. The resulting parser is quite
1 Intended actions obviated by earlier events may be deleted.490 B. Kane et al.
eﬃcient and accurate for the domain at hand. The input is recursively broken
intoconstituents,suchasaVPsegment,untilalexicalsubroutinesuppliesULFs
for individual words, which are propagated back up and composed into larger
expressions by the “calling” node. The eﬃciency and accuracy of the approach
lies in the fact that hierarchical pattern matching can segment utterances into
meaningful parts, so that backtracking is rarely necessary.
Anexampleofatransductiontreebeingusedforparsingahistoricalquestion
into ULF is shown and described in Fig.2. As can be seen from this example,
the resulting ULF retains much of the surface structure, but uses semantic typ-
ing and adds operators to indicate plurality, tense, aspect, and other linguistic
phenomena. Eta also has a limited coreference module utilizing syntactic con-
straints, recency, and other heuristics.
Fig.2. An example ULF parse, with the input shown in red, and the resulting ULF
(at each composition step) shown in green. The nodes with rectangles represent ULF
composition nodes, where the numbers in the upper box correspond to the indices of
the lower boxes (if no upper boxes, simple concatenation). All unframed nodes are
patternstobematchedtothecorrespondingspanofinputtext.(Colorﬁgureonline)
4 Historical Question-Answering
To answer historical questions, the DM needs to maintain a dialogue context
including some sort of spatial episodic memory, so that the ULF obtained from
parsing can be resolved into operations over this episodic memory. Based on
the cognitive considerations discussed in [15], we maintain a high-level symbolic
memorywithwhichtheagentcanreconstructpastscenes,ratherthanadetailed
visual or vector-based memory.Session Context-Based QA in the Block World 491
The vision system records the centroid coordinates and moves of blocks in
real time. On the DM side, a “perceive-world” action in the schema causes the
DMtorequestULFperceptionsfromthevisionsystem.Werelyonasimplelin-
ear,discretetimerepresentation.Thetemporalentities(|Now0|etc.)arerelated
toeachotherandtoperceivedactionspropositionally,makinguseoftheepisodic
operators described in [17]. Based on this context, the DM can eﬃciently recon-
struct a scene at any past time by backtracking from perceived block locations,
andevaluateapproximatespatialrelationshipsbasedoncentroidcoordinates.A
simpliﬁed example is shown in the top half of Fig.3.
AnexampleofansweringahistoricalquestiongivenaULFparseisshownin
the bottom half of Fig.3. Phrases with head types “adv-e”, “adv-f”, and “ps”,
indicate temporal constraints that are applied during the scene reconstruction
algorithm,andtheirsemantictypesallowthemtobeliftedtothesentencelevel.
frequency modiﬁers) map a set of times to a subset of those times, whereas
binary modiﬁers take two times and map to a truth value. Any constraint may
be further modiﬁed by a “mod-a” term (e.g. “just.mod-a”), which modiﬁes how
that mapping is applied.
NotethattheexampleinFig.3isambiguous;theanswercouldbe“A,D,C”
or“A,C”.Infact,wefoundthatmanynaturalhistoricalquestionsaresimilarly
underspeciﬁed, presenting a major source of diﬃculty. To deal with this issue,
the DM’s pragmatic module attempts to infer temporal constraints in these
ambiguous cases – in this particular example, Eta would infer the constraint
“most recently”.
ThealgorithmshownextractstheuninvertedbaseULFrelation,whereargu-
mentsarerepresentedasentitiesorvariables(possiblywithrestrictionsfornoun
modiﬁers). This base ULF, and any temporal constraints, are used to compute
a list of times with attached facts through backtracking over past times. In the
case of a binary constraint with a complex noun phrase or relative clause, this
algorithm is applied recursively (as shown by the red constituents in Fig.3).
Thealgorithmwouldlikewisebeappliedrecursivelyinthecaseofaquerywhere
the historical content is embedded in a noun phrase, e.g. “the ﬁrst block that I
moved”.
Once a list of ﬁnal times is obtained, an answer is generated by making the
appropriatesubstitutionsinthequeryULF,applyingsyntactictransformations,
andconvertingtosurfaceform.Ifnorelationsareobtained,theDM’spragmatics
module will attempt to respond to any presuppositions of the query, based on
the work in [8]. For instance, if the query is “What block was the Twitter block
on?”, Eta will respond “The Twitter block wasn’t on any block.”
5 Evaluation and Discussion
Since the COVID-19 pandemic made testing the physical blocks world system
on-site impossible, the authors had to resort to evaluating using a virtual envi-
ronment that mirrors our setup, sans the physical block tracker and the audio
I/O.However,asthecrucialcomponentsbeingevaluated(parser,DM,andspa-
tial context) were unchanged, the results should not be aﬀected.492 B. Kane et al.
Fig.3. A simpliﬁed example of how the context is represented and how the DM uses
thecontexttocomputerelationsgiventemporalconstraints(tophalf),andanexample
oftheDMdeterminingananswerfromaspeciﬁchistoricalquery(bottomhalf).(Color
ﬁgure online)
We enlisted 4 student volunteers for the user study, both native and non-
native English speakers. The participants were instructed to move the blocks
aroundandaskgeneralquestionsaboutchangesintheworld,withnorestrictions
on wording. After the system displayed its answer, the participants were asked
to provide feedback on its quality by marking it as correct, partially correct or
incorrect. Each participant contributed about 100 questions or above (primarily
historical questions, but also including some non-historical spatial questions).
Each session started with the blocks positioned in a row at the front of the
table. The participants were instructed to move the blocks arbitrarily to test
the robustness and consistency of the spatial models. The data is presented in
Table1. Non-historical questions, as well as a few malformed questions, were
excluded when computing accuracy.
We ﬁnd these results encouraging, given the pragmatic richness of the task
and the unrestricted form of the questions. About 77% of Eta’s answers were
judged to be fully correct, with accuracy rising to 80% when including partially
correctanswers.Weﬁndthatthesemanticparseritselfisveryreliable,with94%Session Context-Based QA in the Block World 493
Table 1. Evaluation data.
Total number of questions asked 496
Well-formed historical questions 387
Correct answers 297 (77% of 387)
Partially correct answers 13 (3% of 387)
Incorrect answers 77 (20% of 387)
Number of correctly parsed questions 363 (94% of 387)
Accuracy (correct + partially correct) 80%
of grammatical sentences being parsed correctly. Parsing failures accounted for
a third of the incorrect answers.
Analyzing the remaining incorrect answers, we ﬁnd that a major source of
error is in the handling of under-speciﬁed historical questions, as described in
Sect.4. There are many nuances to how humans naturally interpret these, that
are diﬃcult to capture with simple set of pragmatic rules.
Forexample,Etawillplausiblyinterpret“WhatblocksdidImovebeforethe
Twitterblock?”asmeaning“WhatblocksdidIrecently movebeforeImovedthe
Twitterblock?”;howeveriftheuserinsteadasked“HowmanyblocksdidImove
beforetheTwitterblock?”,itseemsthatthequestionerreallymeans“Howmany
blocksdidIever movebeforeImovedtheTwitterblock?”.Currently,Etawould
add “recently” for the latter case, which would be incorrect. In future work, we
aimtoinvestigatethisphenomenonfurtherandimprovethepragmaticinference
module to handle these cases correctly. In addition, the tense structure in some
more complex questions violated our simplifying assumption of discrete linear
time. In future work, we plan to look into the use of more general temporal
reasoning systems such as the tense trees described in [7] to enable more robust
handling of diﬀerent aspectual forms and more complex embedded clauses.
6 Conclusion
Wehaveextendedaspatialquestionansweringsysteminaphysicalblocksworld
systemwiththeabilitytoanswerfree-formhistoricalquestionsusingasymbolic
dialogue context, keeping track of a record of block moves and other actions.
A custom semantic parser allows historical questions to be parsed into a logical
form,whichisinterpretedinconjunctionwiththecontexttogenerateananswer.
Weobtainedanaccuracyof80%,whichwebelieveisastrongresultinviewofthe
free-form and often underspeciﬁed nature of the historical questions that users
asked, though it also leaves much room for improvement. Overall, we believe
that the pragmatic richness and complexity that we’ve observed in historical
question-answering indicates that further work towards representing episodic
memory and enabling dialogue systems to reason about historical context will
be fruitful in this sparsely researched area.494 B. Kane et al.
References
1. Allen, J.F., Ferguson, G.: Actions and events in interval temporal logic. J. Logic
Comput. 4(5), 531–579 (1994)
2. Bisk,Y.,Shih,K.J.,Choi,Y.,Marcu,D.:Learninginterpretablespatialoperations
in a rich 3D blocks world. In: 32nd AAAI Conference on Artiﬁcial Intelligence
(2018)
3. Fahlman, S.E.: A planning system for robot construction tasks. Arti. Intell. 5(1),
1–49 (1974)
4. Ferguson, G., Allen, J.: Trips: an integrated intelligent problem-solving assistant.
In: AAAI (1998)
5. Ferguson, G., Allen, J., Miller, B.: Trains-95: towards a mixed-initiative planning
assistant.In:Proceedingsof3rdInternationalConferenceonArtiﬁcialIntelligence
Planning Systems, pp. 70–77 (1996)
6. Franklin,N.T.,Norman,K.A.,Ranganath,C.,Zacks,J.M.,Gershman,S.J.:Struc-
tured event memory: a neuro-symbolic model of event cognition. bioRxiv (2019)
7. Hwang, C.H., Schubert, L.K.: Interpreting tense, aspect and time adverbials: a
compositional, uniﬁed approach. In: Gabbay, D.M., Ohlbach, H.J. (eds.) ICTL
1994. LNCS, vol. 827, pp. 238–264. Springer, Heidelberg (1994). https://doi.org/
10.1007/BFb0013992
8. Kim, G., et al.: Generating discourse inferences from unscoped episodic logical
formulas. In: Proceedings of 1st International Workshop on Designing Meaning
Representations, pp. 56–65. ACL (2019)
9. Kim, G.L., Schubert, L.: A type-coherent, expressive representation as an initial
step to language understanding. In: Proceedings of 13th International Conference
on Computational Semantics-Long Papers, pp. 13–30 (2019)
10. Kottur,S.,Moura,J.M.,Parikh,D.,Batra,D.,Rohrbach,M.:Clevr-dialog:adiag-
nosticdataset formulti-roundreasoninginvisual dialog (2019). arXiv:1903.03166
11. Madl,T.,Franklin,S.,Chen,K.,Trappl,R.:SpatialworkingmemoryintheLIDA
cognitive architecture. In: Proceedings of International Conference on Cognitive
Modelling, pp. 384–390 (2013)
12. Mao,J.,Gan,C.,Kohli,P.,Tenenbaum,J.B.,Wu,J.:Theneuro-symbolicconcept
learner:Interpretingscenes,words,andsentencesfromnaturalsupervision(2019).
arXiv:1904.12584
13. Perera, I., Allen, J., Teng, C.M., Galescu, L.: Building and learning structures in
a situated blocks world through deep language understanding. In: Proceedings of
1stInternationalWorkshoponSpatialLanguageUnderstanding.pp.12–20(2018)
14. Razavi, S., Schubert, L., Ali, M., Hoque, H.: Managing casual spoken dialogue
usingﬂexibleschemas,patterntransductiontrees,andgistclauses.In:5thAnnual
Conference on Advances in Cognitive Systems (2017)
15. Rensink, R.: Scene Perception, pp. 151–155. Oxford University Press, Oxford
(2001)
16. Rothfuss, J., Ferreira, F., Aksoy, E., You, Z., Asfour, T.: Deep episodic memory:
encoding,recalling,andpredictingepisodicexperiencesforrobotactionexecution.
IEEE Rob. Autom. Lett. 3, 4007–4014 (2018)
17. Schubert, L., Hwang, C.: Episodic logic meets little red riding hood: a compre-
hensive, naturalrepresentation for language understanding.In: NaturalLanguage
Processing and Knowledge Representation: Language for Knowledge and Knowl-
edge for Language (2000)
18. Winograd,T.:Understandingnaturallanguage.Cogn.Psychol.3(1),1–191(1972)At Home with Alexa: A Tale of Two
Conversational Agents
Jennifer Ureta , Celina Iris Brito, Jilyan Bianca Dy, Kyle-Althea Santos,
B
Winfred Villaluna, and Ethel Ong( )
De La Salle University, Manila, Philippines
{jennifer.ureta,ethel.ong}@dlsu.edu.ph
Abstract. Voice assistants in mobile devices and smart speakers oﬀer
the potential of conversational agents as storytelling peers of children,
especially those who may have limited proﬁciency in spelling and gram-
mar. Despite their prevalence, however, the built-in automatic speech
recognition features of voice interfaces have been shown to perform
poorly on children’s speech, which may aﬀect child-agent interaction.
In this paper, we describe our experiments in deploying a conversa-
tional storytelling agent on two popular commercial voice interfaces -
Google Assistant and Amazon Alexa. Through post-validation feedback
fromchildrenandanalysisofthecapturedconversationlogs,wecompare
the challenges encountered by children when sharing their stories with
thesevoiceassistants.WealsousedtheBilingualEvaluationUnderstudy
to provide a quantitative assessment of the text-to-speech transcription
quality. We found that voice assistants’ short waiting time and the fre-
quentyetmisplacedinterruptionsduringpausesdisruptthethinkingpro-
cess of children. Furthermore, disﬂuencies and grammatical errors that
naturally occur in children’s speech aﬀected the transcription quality.
· ·
Keywords: Conversational agents Voice interfaces Storytelling
1 Introduction
Stories abound in the everyday conversations of children. Through storytelling,
children can gain an understanding of their world and express their experiences
through recollection and sharing with others. This collaborative storytelling
is not limited to human-to-human interaction, but may also be manifested in
human-to-robot interaction [20].
Commercialvoiceassistants,suchasApple’sSiri,GoogleAssistantandAma-
zon’sAlexa,thatareubiquitouslyembeddedinmobilephones,tabletsandsmart
speakers [11], such as the Google Home and Amazon Echo, enable users to give
voice commands and queries in natural language. Children who have limited
proﬁciency in spelling and grammar can utilize these technologies to express
their narrative by talking to the voice assistant. Not only is voice more natural
Supported by DOST-PCIEERD.
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.495–503,2020.
https://doi.org/10.1007/978-3-030-58323-1_53496 J. Ureta et al.
for children to carry a conversation; they can also do away with less structure
typically required from written text [16].
Despite their prevalence, however, the built-in automatic speech recognition
features of voice interfaces have been shown to perform poorly on children’s
speech. Furthermore, while studies on child-agent interaction through conversa-
tionalinterfacesaresurfacing[5,8,10],therearethosethatreportedproblemsof
usingvoiceinterfacesbynon-nativeEnglishspeakers[6,19].Thesetwochallenges
may aﬀect the child-agent interaction.
Ourstudyseekstoinvestigatethechallengesthatchildrenencounterduringa
collaborativestorytellingsessionwithvoice-basedconversationalagents.Wepay
particularfocusontwopopularvoiceinterfaces,AmazonAlexarunninginEcho
andGoogleAssistantinHome,wherewedeployedourcollaborativestorytelling
agent.InSect.2,wegiveashortreviewofrelatedworkonstorytellingandvoice
interfaces,followedbyadescriptionofthedesignofourcollaborativestorytelling
agentinSect.3.InSect.4,wepresentourresultsfromconductingvalidationwith
children and analyzing the transcription quality.
2 Related Work
Voice interfaces can be used to engage children in learning activities. However,
previousstudieshavereportedthechallengesofvoiceinterfacesinunderstanding
thespeechofyoungchildren,asthebuilt-inautomaticspeechrecognitionfacility
performed poorly on children’s speech [5,8]. The problem is mainly attributed
to the diﬀerences in children’s speech from adults’ speech not only in content
but also in patterns of stress, intonation and prosody [10,13]. This can lead to
incorrect transcription of input speech, which causes misunderstanding and the
generation of inappropriate responses [16].
Despite the challenges, voice interfaces in robots and digital assistants have
foundapplicationsashomeworktutors[22],diagnostictoolsforreadingdisorders
[12]andsocialassistance[9].Astorytellingsystemcomposedofbothreaderand
listener robots was developed by [21] to investigate the eﬀect of a listener robot
acting as a side-participant in a storytelling situation with children. They found
that children preferred storytelling with the listener robot than without it.
Children employ diﬀerent repair strategies when they encounter problems
with voice-driven interfaces [2]. Instead of giving up, they often showed persis-
tence by repeating themselves, augmenting their speech, adjusting the tone and
pronunciation of their words and substituting with a simpler word [2,14].
Unlike task-oriented voice assistants typically found in retail applications,
storytelling agents have to engage children in a conversation that is ﬂuent, user-
directedandconsistentwiththecontextofthesharedstory[15].Thismimicsthe
spontaneous turn-taking that typically occurs in human-to-human storytelling.
Thesharingthatoccursbetweenstorytellersisbeneﬁcialinimprovingchildren’s
listeningandcomprehensionskills,enhancingtheirabilitytointeractwithothers
[18], and using language to express their thoughts and sense of the world [3,4].At Home with Alexa: A Tale of Two Conversational Agents 497
3 Method
Theconversationalstorytellingagent(CSA)wasdeployedontwovoiceassistants
(VA), namely Google Assistant and Alexa, using the smart speakers Google
Home and Amazon Echo, respectively, as the physical interface. An issue we
ran into while porting the agent was that Alexa does not allow getting the
whole user input due to security issues. For Alexa to process the user’s input,
speciﬁc keywords need to be detected for mapping to the appropriate intent, so
we required participants to add the phrase “my turn” before their actual input.
10 children between the age of 7 to 11years old were invited to participate
in the study. They were briefed regarding the keywords they could use to start
andendthesessions,toseekforhelpandtotakethedialogueturninthecaseof
Alexa.EachparticipantwasaskedtosharetheirownstorywitheachoftheVA.
While the original intent was to have the child share two diﬀerent stories, in the
courseoftestingwithAlexa,itwasobservedthattheagentexitstheapplication
when the participant takes time to craft his/her story, thus exceeding the built-
in 7-second wait time of the device. Because of this, 7 out of 10 children used
the same story for both devices.
Anobservationchecklist wasusedtomonitorthechild-agentinteraction,with
a focus on instances where children enjoy or get frustrated with storytelling. A
post-interview and survey form asked participants to rate the agent’s collabo-
rative features using a 5-point Likert scale and to give qualitative feedback on
the interaction. The verbal conversation and the transcription of speech to text
were also recorded.
ResultswereanalyzedusingthreecriteriageneralUX,frustrationandenjoy-
ment.ThegeneralUX criterionassessestheCSAandtheparticipant’scapability
tounderstandandfolloweachother’scommandsandresponses.Thefrustration
criterion [1,7] refers to aspects of the interaction with the CSA that caused the
participants to feel bored, irritated or distracted. The enjoyment criterion is
used to assess the quality of the conversation based on the satisfaction of the
participants and includes acknowledgement of the CSA as an actual being that
childrenwould talkto again. WealsousedtheBilingual Evaluation Understudy
(BLEU) [17] to analyze and determine the quality of the text-to-speech transla-
tion performed by each of the voice assistants.
4 Results
4.1 User Feedback
Table1 presents the results of user feedback from the survey forms. Requiring
thephrase“myturn” forAlexamadeitdiﬃcultandunnaturalforthechildrento
carryonasmoothconversationﬂowwhichledtoalowergeneralUXscorecom-
pared to Google Assistant. There were also instances wherein they forget to say
the keywords before starting their story. Occurrences of misunderstanding also
aﬀected the score, especially when Alexa and Google Assistant misinterpreted
the words in the child’s input. Examples of these are shown in Table2. When498 J. Ureta et al.
Table 1. Average rating from participants’ survey forms.
Criteria Google Alexa
General UX 3.67 3.61
Frustration 3.93 3.63
Enjoyment 3.73 3.47
misunderstandings occur, participants tend to get thrown oﬀ and confused at
ﬁrst.Theywillthentrytomakesenseofwhattheagentsaid.Iftheyareunable
to do so, they tend to give up and continue on with their story.
In the conversation with P2, Alexa misheard “insects” as “sex” (lines 2 and
4),and“he then” as“heathen” inline6.Atﬁrst,P2triedtocorrectthemistake
byrepeatingwhathesaidinline3.ButwhenAlexarepeatedthesamemistake,
P2 just ignored the misunderstanding and continued with his/her story. For P7
and P10, there were clear indications of their confusion, as seen in lines 3 of
their respective log. For P7, he/she was initially thrown oﬀ by the mistake that
the interface made but did not try to correct it at all and just continued with
the story. For P10, he/she got really confused with the misunderstanding and
was only able to move on with the story when the storytelling agent issued the
prompt “So what happens next?” in line 6.
Children’s frustration and enjoyment in sharing stories with the conversa-
tional agent were aﬀected by the latter’s ability to understand the utterance
in order to generate appropriate responses and the frequent need for children
to have to repeat themselves as a repair strategy. These instances of misunder-
standing are detailed in the next section.
4.2 Transcription Quality
BLEU is a metric which is primarily used to evaluate the quality of machine-
translated text by comparing a candidate translation to one or more reference
translations.BLEUscoresrangefrom0to1,withvaluescloserto1representing
a higher similarity with the reference text.
BLEU was used to evaluate how eﬀective the voice interfaces were in tran-
scribing the speech of the children. We used the manually transcribed utterance
of the participants from the recorded vouce conversations as the reference and
compared the transcriptions of the voice assistants against these. As shown in
Table3,thenumberofdialogueturns(input)thechildtookaswellastheBLEU
scores for both interfaces were calculated. Based from the results, Google Assis-
tant has a higher BLEU score than Alexa for all transcribed conversations.
Table3 also indicates the number of instances when Alexa asked each par-
ticipant to repeat their utterances because the agent failed to “hear” the input.
Everytimethishappens,aBLEUscoreof0isassignedtotheutterancebecause
an empty string is compared with the actual utterance of the participant found
in the reference transcription. The higher the number of repetitions, the lowerAt Home with Alexa: A Tale of Two Conversational Agents 499
Table 2. Examples of misunderstanding between the agent and the children.
Excerptofconversationof P2withAlexa(A)
1 P2: theﬁshwouldgouptotherivereverydaytoseeiftherewereinsects
2 A: Ineedhelp,pleaseusesex inasentence
3 P2: theﬁshwouldgouptotherivereverydaytoseeiftherewereinsects
4 A: Ineedhelp,pleaseusesex inasentence
5 P2: he then wouldgobackiftherewerenone
6 A: Tellsomethingaboutheathen
7 P2: itwouldthengotothereefandrestandwaitforthenextday
Excerptofconversationof P7withGoogleAssistant(GA)
1 P7: afterweplaysoccer meandmyfriendgottiredandwentbackatmyhouse
2 GA: Ineedhelp,pleaseusesoftware inasentence
3 P7: Idon’tknowwhatthatmean(misunderstanding)
4 GA: Themooniscold
5 P7: mymomgiveusdrinks...mymomgiveusdrinksandtowel
Excerptofconversationof P10withGoogleAssistant(GA)
1 P10: whenhegottherehegotthebookswereinteresting
2 GA: Whatiftheerica gotget?
3 P10: Yes?What(P10 got confused)
4 GA: Tellsomethingabouterica
5 P10: What?(P10 got confused)
6 GA: Sowhathappensnext?
7 P10: whenheopenedthebookitwasntreallythatbig
Table 3. BLEU scores for Alexa and Google assistant.
Alexa Google assistant
P Inputs Repeats BLEU Inputs Repeats BLEU
P1 7 4 0.20 12 0.61
P2 15 2 0.56 23 0.90
P3 34 19 0.35 18 0.91
P4 23 9 0.51 19 0.93
P5 23 10 0.27 8 0.56
P6 13 4 0.42 10 0.76
P7 18 7 0.46 13 0.64
P8 11 4 0.28 5 0.80
P9 13 3 0.56 16 0.94
P10 25 5 0.51 17 0.77500 J. Ureta et al.
the BLEU score. Those participants who made at least 30% repetitions have
BLEU scores that fall below 0.50.
Another factor that aﬀected the BLEU score is the total number of words
uttered by a participant. This is illustrated in the plot in Fig.1. From the plot,
we get a correlation coeﬃcient of 0.1334; the more words that participants say
and were heard by Alexa, the better the score becomes. The 2 outlying cases in
the given plot are from P3 and P5. P3 spoke in a soft voice and would at times
mumble. For P5, despite his/her loud and clear voice, Alexa still misinterpreted
the words he/she said, as seen in lines 11–13 of the transcribed log in Table4.
This may be due to P5 speaking fast in some parts which resulted in Alexa
mishearing the words.
Table 4. Examples of Alexa misinterpreting the participants’ utterances.
Line Transcribed Alexa
11 He gone to the mall to buy a new rigan to the multiply on you
game! game
12 he go and buy the game he wanted higoandbuythegive you 1 that
13 when he played the game he loved when he played the game he loved
it so he bought the game from the it so he the game from the store
store and gone back to his home and gun back to his home
TheplotforGoogleAssistantisshowninFig.2.Ithasacorrelationcoeﬃcient
of-0.0359,whichmeansthereisnocorrelationbetweenthenumberofwordsand
the BLEU score that generally ﬂoats above 0.5.
Multiple disﬂuencies and grammatical errors can be found in natural child
speech [8]. Disﬂuencies like the ones shown in Table5 for P5 and P9 often led
Alexa to mistake them as the end of an utterance. These problems in the par-
ticipants’ speech also aﬀect the BLEU scores negatively.
Fig.1. Correlation between the BLEU Fig.2. Correlation between the BLEU
score and the number of words per par- score and the number of words per par-
ticipant with Alexa. ticipant with Google Assistant.At Home with Alexa: A Tale of Two Conversational Agents 501
Table 5.Examplesofdisﬂuenciesandgrammaticalerrorsinparticipants’utterances.
Alexa
P3 then in math class I didn’t understand
P3 If I had 5 mistakes, then I will fail
P4 and then I ate recess
P5 He - he go .. gone to the mall to buy a new game
P5 so-so Jacob and his little sister play the played till 12 pm O clock
P7 after we play soccer me and my friend got tired and went back
P9 he was disappoint - ed that but his friend comforted him
Google assistant
P1 but thay already made an idea they sell drinks
P4 until in december 4 it should be returned
P5 and when he go camping
5 Conclusion
Conversationalagentscanengagechildrenincollaborativestorytellingtodevelop
their language and literacy skills. In this paper, we deployed our conversational
storytelling agent in voice interfaces, particularly Amazon Alexa and Google
Assistant, to provide access to children with limited proﬁciency in spelling and
grammar. Our results are in line with those reported in previous studies where
voice interfaces perform poorly on children’s speech. Our ﬁndings showed that
these problems, combined with the short waiting time, frequent interruptions
duringpausesandmishearingwordsaﬀectthechild-agentinteractionandledto
diﬃculties in carrying out a smooth turn-based storytelling session.
WealsousedBLEUscorestocomparethetranscriptionqualityofAlexaand
Google Assistant, with an average of 0.34 and 0.65 respectively. Analysis of the
conversationlogsshowedahighincidenceofrepetitionsamongparticipantswhen
conversingwithAlexa,leadingtoBLEUscoresthatfallbelow0.50.Disﬂuencies
that are descriptive in children’s speech also aﬀect the BLEU scores negatively.
Forfuturework,worderrorrate(WER)shouldbeconsideredandacomparison
between the two metrics in the area of storytelling can also be done. While
children employ repair strategies and do their best to ﬁnish their stories, their
inabilitytocarryonasmoothconversationﬂowwiththestorytellingagentledto
frustrationandnon-enjoymentoftheinteraction.Futurevoiceinterfacedesigners
should take into account the spontaneous nature of children’s storytelling, their
limitedvocabularyandpronunciationabilities,andthelengthoftimetheyneed
to formulate their stories.502 J. Ureta et al.
References
1. Blythe,M.,Reid,J.,Wright,P.,Geelhoed,E.:Interdisciplinarycriticism:analysing
theexperienceofriot!alocation-sensitivedigitalnarrative.J.Behav.Inf.Technol.
25(2), 127–139 (2006)
2. Cheng, Y., Yen, K., Chen, Y., Chen, S., Hiniker, A.: Why doesn’t it work? voice-
driven interfaces and young children’s communication repair strategies. In: Pro-
ceedingsof17thACMConferenceonInteractionDesignandChildren,pp.337–348.
ACM (2018)
3. Duranti, A., Goodwin, C.: Rethinking Context: Language as an Interactive Phe-
nomenon. Cambridge University Press, Cambridge (1992)
4. Engel,S.:TheStoriesChildrenTell:MakingSenseoftheNarrativesofChildhood.
W H Freeman & Co. Ltd., New York (1995)
5. Gerosa, M., Giuliani, D., Narayanan, S., Potamianos, A.: A review of ASR tech-
nologies for children’s speech. In: Proceedings of the 2nd Workshop on Child,
Computer and Interaction, WOCCI 2009, pp. 1–8, November 2009
6. Harwell, D.: The accent gap: how Amazon’s and Google’s smart speakers leave
certain voices behind, July 2018
7. Hone, K.S., Graham, R.: Towards a tool for the subjective assessment of speech
system interfaces (SASSI). Natural Lang. Eng. 6(3–4), 287–303 (2000)
8. Kennedy, J., et al.: Child speech recognition in human-robot interaction: evalua-
tionsandrecommendations.In:Proceedingsofthe2017ACM/IEEEInternational
Conference on Human-Robot Interaction, pp. 82–90 (2017)
9. Keren, G., Fridin, M.: Kindergarten social assistive robot (kindsar) for children’s
geometricthinkingandmetacognitivedevelopmentinpreschooleducation:apilot
study. Comput. Hum. Behav. 35, 400–412 (2014)
10. Lovato,S.,Piper,A.M.:“Siri,isthisyou?”:understandingyoungchildren’sinter-
actions with voice input systems. In: Proceedings of the 14th International Con-
ference on Interaction Design and Children, pp. 335–338, June 2015
11. Lovato, S.B., Piper, A.M., Wartela, E.A.: ’hey google, do unicorns exist?’: con-
versational agents as a path to answers to children’s questions. In: Proceedings of
the 18th ACM International Conference on Interaction Design and Children, pp.
301–313 (2019)
12. Maier, A., et al.: An automatic version of a reading disorder test. ACM Trans.
Speech Lang. Process. 7, 15 (2011)
13. Meinedo,H.,Trancoso,I.:AgeandgenderdetectionintheI-DASHproject.ACM
Trans. Speech Lang. Process. 7, 16 (2011)
14. Most,T.:Theuseofrepairstrategiesbychildrenwithandwithouthearingimpair-
ment. Lang. Speech Hearing Serv. Schools 33(2), 112–123 (2002)
15. Ong,D.T.,DeJesus,C.R.,Gilig,L.K.,Alburo,J.B.,Ong,E.:Adialoguemodelfor
collaborative storytelling with children. In: Proceedings of the 26th International
Conference on Computers in Education, pp. 205–210. APSCE (2018)
16. Ong, E., Alburo, J.B., De Jesus, C.R., Gilig, L.K., Ong, D.T.: Challenges posed
by voice interface to child-agent collaborative storytelling. In: Proceedings of the
22nd Conference of the Oriental COCOSDA, pp. 1–6, October 2019
17. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: A method for automatic
evaluationofmachinetranslation.In:Proceedingsofthe40thAnnualMeetingon
Association for Computational Linguistics, pp. 311–318, July 2002
18. Peck,J.:Usingstorytellingtopromotelanguageandliteracydevelopment.Reading
Teach. 43(2), 138–141 (1989)At Home with Alexa: A Tale of Two Conversational Agents 503
19. Pyae, A., Sciﬂeet, P.: Investigating diﬀerences between native English and non-
nativeEnglishspeakersininteractingwithavoiceuserinterface:Acaseofgoogle
home. In: Proceedings of the 30th Australian Conference on CHI, pp. 548–553,
December 2018
20. Sun,M.,Leite,I.,Lehman,J.,Li,B.:Collaborativestorytellingbetweenrobotand
child: a feasibility study. In: Proceedings 2017 Conference on Interaction Design
and Children, pp. 205–214, June 2017
21. Tamura,Y.,Kimoto,M.,Shiomi,M.,Iio,T.,Shimohara,K.,Hagita,N.:Eﬀectsofa
listenerrobotwithchildreninstorytelling.In:Proceedingsofthe5thInternational.
Conference on Human Agent Interaction, pp. 35–43. ACM, NY (2017)
22. Ward,W.,Cole,R.,Bolan˜os,D.,Buchenroth-Martin,C.,Svirsky,E.,Weston,T.:
Mysciencetutor:aconversationalmultimediavirtualtutor.J.Educ.Psychol.105,
1115 (2013)ConversIAmo: Improving Italian Question
Answering Exploiting IBM Watson
Services
B
Chiara Leoni( ), Ilaria Torre , and Gianni Vercelli
DIBRIS, University of Genoa, Viale F. Causa 13, 16145 Genoa, Italy
chiara.leoni@outlook.com, {ilaria.torre,gianni.vercelli}@unige.it
Abstract. Chatbots, conversational interfaces and NLP have achieved
considerable improvements and are spreading more and more in every-
day applications. Solutions on the market allow their implementation
easily in diﬀerent languages, but the proposals for the Italian language
are not so eﬀective as the English ones. This paper introduces Con-
versIAmo, the prototype of a conversational agent which implements
a question answering system in Italian on a closed domain concerning
artiﬁcial intelligence, taking the answers from online articles. This sys-
tem integrates IBM services (Watson Assistant, Discovery and Natural
LanguageUnderstanding)withfunctionsdevelopedwithinConversIAmo
and Tint, an open-source tool for the analysis of the Italian language.
Our QA pipeline turned out to give better results than those obtained
from using Watson Discovery service on its own, as for precision, F1-
score and correct answer ranking (on average +12%, +21% and +20%
respectively). Our main contribution is to address the need for an eﬀec-
tive but easy-to-apply method aimed to improve performances of IBM
Watson services for the Italian language. In addition, the AI domain is
a new one for an Italian conversational agent.
· ·
Keywords: Conversational agents Question answering IBM
· ·
Watson Artiﬁcial intelligence Italian language
1 Introduction
Artiﬁcialintelligencehasbecomeatrendingtopicinthelastfewyearsandboth
researchers and companies are striving to improve results in many areas, from
medical diagnosis to any kind of business process and products. Meanwhile,
Software-as-a-Service (SaaS) solutions oﬀered by the IT corporations on the
market are now integrating powerful AI-as-a-Service (AIaaS) products, includ-
ing conversational and QA (Query Answering) modules. Task-oriented dialog
agents, designed for speciﬁc goals and set up to have short conversations to
reach their purpose, include digital assistants on smartphones or home con-
trollers (Siri, Cortana, Alexa, Google Now/Home, etc.), which can give travel
directions, control home appliances, ﬁnd restaurants, and so forth, and many
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.504–512,2020.
https://doi.org/10.1007/978-3-030-58323-1_54ConversIAmo: Italian Question Answering Exploiting IBM Watson Services 505
companies deploy their own task-based conversational agents to support cus-
tomers. They are usually based on a knowledge structure representing the most
common intents expressed by the user and related to the task. This implies
a ﬁnite number of cases, designed by a dialog designer, each one with many
input examples to train the classiﬁcation, sets of entities for the given domain
andpresetanswers(e.g.,GoogleDialogﬂow,AmazonLex,MicrosoftBotFrame-
work, Oracle Digital Assistant and IBM Watson Assistant as the most famous
ones).
However, while several conversational and QA systems exist in English lan-
guage, few examples are available in Italian (e.g., [4,6,12]), and performances
using commercial tools are on average lower.
In this paper we present a QA prototype system, ConversIAmo, which uses
unstructured text documents in Italian language in the Artiﬁcial Intelligence
(AI) domain. Based on Watson services, ConversIAmo integrates new modules
that we developed and Tint, an Italian NLP tool, with the goal of improving
performances compared to Watson Discovery on Italian questions.
Watson Discovery Service (WDS from now on) showed experimentally good
recall but low precision and weak ranking of the correct answers (around 51%
of correct answers among the ﬁrst three), unlike when the English version is
used for English texts. Since the relevance of ranking is a well-known principle
in interactive IR and QA systems, the main goal, that guided our approach,
was to improve this measure, keeping the recall high. Results seem encouraging
since we obtained that by questioning our ConversIAmo prototype on a dataset
on AI in Italian language, precision, F1-score and accuracy outperform WDS
(respectively +12%, +21% and +20%).
Our contribution consists not only in the creation of a question-answer
dataset in Italian language regarding the topic of AI, which can be used for
further research, but mainly in the proposal of an easy-to apply method to inte-
grate IBM NL-related services with new modules to create an Italian speaking
agent about AI. This is the ﬁrst chatbot in Italian on AI domain and, to the
best of our knowledge, also in other languages.
The paper is structured as follows: Sect.2 presents related works, Sect.3
describes ConversIAmo, Sect.4 its experimental evaluation, results and
discussion.
2 Background and Related Works
Withthesurgeoftechnologiesavailable,conversationalagentsarebeingusedin
many diﬀerent ﬁelds. Beyond oﬀ-the-shelf solutions, diﬀerent technologies allow
to create custom dialog systems that can be trained and specialized on spe-
ciﬁc application scenarios. IBM Watson, is one of the most widely-adopted QA
frameworks [6]. It combines several diﬀerent techniques for NLP, IR and ML
and uses IBM’s DeepQA software and the Apache UIMA (Unstructured Infor-
mation Management Architecture) framework implementation. Its eﬀectiveness506 C. Leoni et al.
increases when it is extensively trained [1,11], but this task is very time con-
suming, thus other approaches have been proposed, such as using automati-
cally generated question-answer pairs [8]. Several conversational agents have
been developed using Watson services. Recently it has been exploited in [13]
to build a multilingual student support system regarding exam stress, in [10] to
build a parallel programming assistant (PAPA) to support programmers, in [1]
to provide answers about programs and other issues in a university.
However,whileperformancesareusuallygoodwithdialogagentsinEnglish,
the eﬀectiveness with other languages is not the same. Italian in particular,
is not currently fully supported by Watson services, as reported in [2,6]. Our
experience conﬁrms this gap. We found that Watson Discovery did not get the
same precision and accuracy that we got with English texts. We had to develop
new modules that were used in the answer selection and ranking process to
improvethesemeasuresonItaliantexts.Conversely,recallwasveryhigh(around
95%) and not further improvable with ConversIAmo. Recent systems that use
Watson services with Italian text are described in [6] and [4]. The former is
a virtual assistant that supports students and staﬀ of a smart campus. The
authors compare the trained and untrained versions ﬁnding improvements after
training, even though they notice that there were no improvements with out-of-
scope questions, maybe due to the language, which is not fully supported [6].
In [4], the authors describe a pipeline for Italian that uses IBM services and
tools for Italian language processing. The same group also developed a query
expansion module to improve the retrieval phase [5]. Like to us, they exploit
Tint for linguistic analysis.
InadditiontotheQAsystemsabove,fewexamplescanbefurthermentioned
among Italian-based QA systems [12,14]. Some of the reasons might be diﬃcul-
ties related to the highly variable structures of queries, that may impact the
question analysis and interpretation, and shortage of multilingual datasets [14].
3 ConversIAmo Prototype
ConversIAmo is a QA prototype that exploits Watson Assistant (WA), Watson
DiscoveryService(WDS),andWatsonNaturalLanguageUnderstanding(NLU).
Moreover, it includes Tint and ad-hoc modules to increase performances for
Italian language. WA is the IBM platform to build and manage chatbots, WDS
istheservicetoretrieveinformationfromunstructureddataandNLUtoextract
metadata from text such as entities, keywords, categories, relations, etc. Tint is
anopen-sourcetoolforNLPinItalian,basedonStanfordCoreNLP(tint.fbk.eu).
ConversIAmo Question Answering Pipeline is shown in Fig.1.
Setup and Training
ConversIAmo dialog ﬂow has been created using the WA graphical interface on
IBM Cloud which allows to insert nodes that correspond to diﬀerent intents of
the user’s question. As typical in conversational agents, intents are associated
to question classes, which are mapped to corresponding Lexical Answer Types
(LAT). As in many other QA systems [4,6,9], question classes are organized inConversIAmo: Italian Question Answering Exploiting IBM Watson Services 507
a taxonomy. The main classes are factoid questions and description questions,
each organized into sub-classes. Factoid class refers to questions that expect
speciﬁc entities as answer types, while Description class refers to questions that
expectasanswerdescriptivetextthatconcernsthetopic(s)ofthequestion,e.g.,
“what’s the diﬀerence between supervised learning and unsupervised learning?”
The sub-classes we considered for factoid questions are: HUMAN (split in: indi-
vidual and group), NUMERIC (split in: count, date, money, percent, period),
LOCATION and ENTITY. Each sub-class is associated to a set of suitable
entity types. E.g., HUMAN group sub-class expects Organization or Company
as entity types. Description questions are split in GENERIC DESCRIPTION
(DESCRIPTION) and DEFINITION. A further class is ABBREVIATION. For
each class, examples are to be provided in order to train WA to classify them.
We provided 10 examples for each sub-class on our AI domain in Italian.
Finally, the setup includes document ingestion and information extraction.
Once prepared and formatted as required, the documents on the AI subject
domain are uploaded on WDS to let it extract concepts and named entities,
which are then imported into WA to be used in the QA ﬂow.
QA Flow
TheQAﬂowistheprocessperformedeachtimetheusersubmitsaquestionand
the system has to ﬁnd some answers to output (see Fig.1).
Fig.1. Question answering scheme.
Question Analysis. This is the ﬁrst phase of all QA systems and the funda-
mentalonetodeterminethesuccessoftheprocesssinceitsoutputwillbeusedin
the next steps. We deploy diﬀerent services (WA, NLU and Tint) to accomplish
this task, which includes three processes, other then the morpho-syntactic and
syntacticanalysiscarriedoutusingTint.(i)Questionclassiﬁcation ishandledby
WA, which is in charge of handling directly the user’s question. The intent rec-
ognizedbyWAintheuserquestionisoneofthequestionclassesspeciﬁedabove,508 C. Leoni et al.
whichcorrespondstotheexpectedLAT.(ii)Namedentitiesextraction isaccom-
plished by combining named entities recognized by WDS, and imported in WA,
withthoseextractedbyNLU.Theirintersectionresultedtobethemosteﬀective
combination. (iii) As for keywords extraction we have developed and tested two
diﬀerent approaches: the simplest, exploiting the keywords extracted by NLU,
and a more complex one, developed with the tool Tint, speciﬁcally tailored for
Italian. Thanks to its linguistic analysis, it allows to identify the linguistic fea-
tures and select those candidates to become keywords: nouns, adjectives and
verbs. Then, looking for direct dependencies between these words with any of
theothersselected,inparticularbetweennounsandtheiradjectives,wemerged
related keywords. Overall, this method for keyword identiﬁcation includes more
keywords than those extracted by Watson NLU.
Information Retrieval. As ﬁrst, we execute stop words removal through a
functionthatenrichesthesetofItalianquestionwords(e.g.,“quali,qual,quale”,
are three forms of “which”) taking into account the context of the word.
Then the ﬂow continues with this reworked query stop words passed to Dis-
covery,whichreturnsthetextpassagesthatitretrieveswithanassociatedscore.
Answers Selection and Dialog Flow Expansion. Filtering and sorting of
text passages is the step where Watson services for Italian mainly failed. Thus,
wedonotexploitanypre-existingserviceforthisstage,butweimplementedour
own “ConversIAmo answers manager” which exploits the result of the question
analysis. We followed two diﬀerent ﬁltering strategies according to the classiﬁ-
cation of the user input, depending on whether the question belongs to: (I) the
factoid group (HUMAN, LOCATION and NUMERIC), that requires a speciﬁc
type of entity in the answer or (II) questions without any speciﬁc type of entity
tosearchfor(i.e.,descriptionquestionsandABBREVIATION).Forquestionsof
theﬁrstgroup,theﬁlteringtechniquerequiresthattextpassagescontainatleast
one entity of the type required by the question, while the others are removed.
Questions that WA is not able to classify (unclassiﬁed questions) are taken into
accountasthesecondgroupinordertotreatthemwiththelessrestrictiveapp-
roach and not to lose any answer. Then, for both groups, a scoring technique
is applied that takes each text passage with its Discovery score and increases it
depending on the percentage match between (i) the keywords and (ii) the enti-
ties in the original question and in the text passage, limited to those occurring
in the same role (as from the morpho-syntactic analysis). Once the scores have
been updated, the passages are rearranged in descending order, then the scores
are normalized and results below a certain threshold are deleted, to limit them
to the most inherent to the question.
At the end, the results are displayed to the user and s(he) is asked to check
the correct one. If an answer is selected, it is inserted in a node of WA dialogue
ﬂow within its question class branch and with the named entities identiﬁed
in the question as entry condition for the node. This is aimed to extend the
conversational agent knowledge base and improve results over time.ConversIAmo: Italian Question Answering Exploiting IBM Watson Services 509
4 Experimental Results
Question Classiﬁcation. In order to test the approach, we used a question-
answer dataset that we built since none was available in Italian nor, as far as
we know, in other languages on the subject of AI. Questions respect the prin-
ciples in [7] about the way people usually interact with machines, and follow
the example of popular question-answer datasets such as WikiQA dataset, MS
MARCO, the Stanford Question Answering Dataset, and the Italian version of
SQuAD. The dataset is composed of 110 questions on basic topics in AI, anno-
tated with their correct question type extracted manually from the corpus of
130 articles each with its correct passages of text as answers. Questions are
distributed among classes as follows: factoid questions 37.3% (NUMERIC 20%,
HUMAN 10.9% and LOCATION 6.4%, further split in sub-classes); description
questions 60% (DESCRIPTION 48.2%, DEFINITION 11.8%) and ABBREVI-
ATION 2,7%.
To test WA’s ability to classify the questions, we passed as input all the 110
questionsinourdatasetandweobtainedthefollowingresults:9wrongclassiﬁca-
tions on 110 (8%), of which 8 DESCRIPTION labeled as DEFINITION, and 13
unclassiﬁed questions (11%) that Assistant was not able to assign to any intent,
allbelongingtodescription questions.Sinceweapplythesamemethodforboth
DESCRIPTION and DEFINITION, such misclassiﬁcation has no consequences
on the answer selection, but it can impact the ﬁnal dialog node expansion.
From these data, 10 examples have proved to be enough for factoid ques-
tions training, because they usually have in common some question words (e.g.,
“Dove”, “Where” or “In quale citta`/stato/paese”, “In what/which city/state/
country” for LOCATION), while they are not enough to cover the variety of
question forms that ask for a description, resulting in misclassiﬁed or unclassi-
ﬁed questions. As explained in Sect.3, we face unclassiﬁed questions by treating
them as description questions and not inserting them in new dialog nodes.
QA Results.ToevaluatetheperformanceofConversIAmocomparedtoWDS,
we uploaded the test corpus of 130 articles, and use the widely adopted metrics
inQAandIRcommunitiesprecision,recall andF1score.Moreover,weuseaccu-
racy computedastheratioofquestionsthatreturnedatleastonecorrectanswer
within the ﬁrst 3 answers of the response [3]. By running WDS, we found that
whileperformanceisgoodasforrecall,itisnotfortheothermeasures.Accuracy
in particular is below expectations for a QA system, where the correctness of
theﬁrstanswersisofforemostimportancefortheuserexperience.Thedesignof
ConversIAmowasmostlyaimedtokeeptherecallatthathighlevel(weobtained
also a slight improvement of this parameter, though) while improving ranking,
and thus accuracy, that is the placement of the correct text passages provided
as answers. By improving ranking, we were also able to reduce the number of
answers to be returned to the user and this led to an increase of precision and
F1 score.
Figure2showstheresultsforfactoid(leftside)anddescriptiontypequestions
(right side). Each side reports the results for WDS compared to ConversIAmo.510 C. Leoni et al.
Fig.2. Results on factoid and description type questions.
Resultsforthelatterareprovidedwithtwoconﬁgurations:ConversIAmo(1)uses
Tint for keyword extraction, while ConversIAmo(2) uses NLU. Other than that
therearenodiﬀerencesbetweenthetwoversionsasforNLPprocessing,question
analysis, stop word removal and results ﬁltering.
As showed in Fig.2, for factoid questions we achieved a consistent +30% in
accuracy (from WDS 49% to our 80%), keeping therecall at its max, improving
precision andF1score byrespectivelyabout20%and35%,withthetwodiﬀerent
approaches on keyword extraction that perform almost similarly.
While we achieved very good improvements for factoid questions, only a
slight improvement has been accomplished with description questions. In these
latter, recall is kept at its max, precision and F1 score have a slight increment
(about 7% and 12% respectively), but accuracy is not signiﬁcantly improved
(only +2/3%). Again, the two versions of ConversIAmo perform similarly.
The diﬀerence in results is mostly related to the fact that factoid questions
involvedirectlyanentitytypeandtheﬁlteringtechniquewedesigned,combined
with the scoring technique, works well, while this is not the case for the broad
type of description questions. In the dataset used for the test, the questions
that fall in this class are very diﬀerent from each other and more complex (e.g.,
“what is the diﬀerence between ...”), such that improvements can diﬃcultly be
gained using rule-based approaches and without extensive training. However,
improvements could come from reﬁning the keyword extraction method used in
ConversIAmo(1),whichperformedworsethanexpected(wediscussitinSect.5).
As a general result of our approach, we got that, despite the inverse rela-
tionship between precision and recall that is typical in IR systems, and that
we also found with WDS results, our approach was able to keep recall at WDS
max level and we achieved a signiﬁcant improvement in accuracy. In detail, the
combinedresultsoffactoidanddescriptionquestions,includingtheabbreviation
class,whichperformedsimilarlytothelatter,are:+20%accuracy (from51%to
71%) and improvements also on precision (+12%) and F1 score (+21%).
5 Conclusions and Discussion
In this paper we presented ConversIAmo, the prototype of an Italian speak-
ing agent on the AI domain, the ﬁrst as far as we know. Based upon IBMConversIAmo: Italian Question Answering Exploiting IBM Watson Services 511
Watson framework (Watson Assistant, NLU, Discovery), it exploits custom-
made Java functions and Tint, a NLP tool for Italian language, in order to
improve performances, compared to IBM Watson service used on its own. The
resultsareencouragingsincewegainedimprovement inaccuracyandslightlyin
precision and F1 score, while recall was already high. In addition, ConversIAmo
isdesignedtodynamicallyfeedtheagentwithanswers,dialognodesandentities,
to improve results over time. About limits, we acknowledge that the approach
is eﬀective with factoid questions, while less with description questions. This is
true for both question classiﬁcation and QA results. The method for keyword
extraction that we proposed as an alternative to NLU keyword extraction was
intended to improve such results, but the two perform similarly. Thus, as future
workweplantoimprovethismethod,andalsotryotherapproachesforkeyword
expansion (e.g., as in [5]). Moreover we plan further tests to balance the need of
an extensive training with the deployment of an eﬀective easy-to-apply method.
References
1. Asakiewicz, C., Stohr, E.A., Mahajan, S., Pandey, L.: Building a cognitive appli-
cation using Watson DeepQA. IT Prof. 19(4), 36–44 (2017)
2. Bellomaria,V.,Castellucci,G.,Favalli,A.,Romagnoli,R.:Almawave-SLU:anew
dataset for SLU in Italian. In: Proceedings of the Sixth Italian Conference on CL
(2019)
3. Boyer, J.M.: Natural language question answering in the ﬁnancial domain. In:
Proceedings of the 28th Annual International Conference on Computer Science
and Software Engineering, pp. 189–200. IBM Corporation (2018)
4. Damiano, E., Spinelli, R., Esposito, M., De Pietro, G.: An eﬀective corpus-based
questionansweringpipelineforItalian.In:DePietro,G.,Gallo,L.,Howlett,R.J.,
Jain,L.C.(eds.)KES-IIMSS2017.SIST,vol.76,pp.80–90.Springer,Cham(2018).
https://doi.org/10.1007/978-3-319-59480-4 9
5. Esposito, M., Damiano, E., Minutolo, A., De Pietro, G., Fujita, H.: Hybrid query
expansion using lexical resources and word embeddings for sentence retrieval in
question answering. Inf. Sci. 514, 88–105 (2020)
6. Gaglio, S., Re, G.L., Morana, M., Ruocco, C.: Smart assistance for students and
peoplelivinginacampus.In:2019IEEEInternationalConferenceonSmartCom-
puting (SMARTCOMP), pp. 132–137. IEEE (2019)
7. Hill,J.,Ford,W.R.,Farreras,I.G.:Realconversationswithartiﬁcialintelligence:a
comparison between human-human online conversations and human-chatbot con-
versations. Comput. Hum. Behav. 49, 245–250 (2015)
8. Lee, J., Kim, G., Yoo, J., Jung, C., Kim, M., Yoon, S.: Training IBM Watson
usingautomaticallygeneratedquestion-answerpairs.In:50thHawaiiInternational
ConferenceonSystemSciences,HICSS2017,Hawaii,USA,4–7January2017,pp.
1–9 (2017)
9. Li,X.,Roth,D.:Learningquestionclassiﬁers.In:ProceedingsoftheInternational
Conference on Computational Linguistics, vol. 1, pp. 1–7 (2002)
10. Memeti, S., Pllana, S.: PAPA: a parallel programming assistant powered by IBM
Watson cognitive computing. J. Comput. Sci. 26, 275–284 (2018)
11. Murtaza,S.S.,Lak,P.,Bener,A.,Pischdotchian,A.:HowtoeﬀectivelytrainIBM
Watson: classroom experience. In: 2016 49th Hawaii International Conference on
System Sciences (HICSS), pp. 1663–1670. IEEE (2016)512 C. Leoni et al.
12. Pipitone, A., Tirone, G., Pirrone, R.: QuASIt: a cognitive inspired approach to
questionansweringfortheItalianlanguage.In:Adorni,G.,Cagnoni,S.,Gori,M.,
Maratea,M.(eds.)AI*IA2016.LNCS(LNAI),vol.10037,pp.464–476.Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-49130-1 34
13. Ralston, K., Chen, Y., Isah, H., Zulkernine, F.: A voice interactive multilingual
studentsupportsystemusingIBMWatson.In:201918thIEEEInternationalCon-
ference on Machine Learning and Applications (ICMLA), pp. 1924–1929 (2019)
14. Siciliani,L.,Basile,P.,Semeraro,G.,Mennitti,M.:AnItalianquestionanswering
systemforstructureddatabasedoncontrollednaturallanguages.In:Proceedings
of the Sixth Italian Conference on Computational Linguistics (2019)Modiﬁcation of Pitch Parameters in
Speech Coding for Information Hiding
B
Adrian Radej and Artur Janicki( )
Institute of Telecommunications, Warsaw University of Technology,
ul. Nowowiejska 15/19, 00-665 Warsaw, Poland
261187@pw.edu.pl, ajanicki@tele.pw.edu.pl
Abstract. The article presents a method of using F0 parameter in
speech coding to transmit hidden information. It is an improved app-
roach, which uses interpolation of pitch parameters instead of transmit-
ting exact original values. Using an example of the Speex codec, we
describesixvariantsofthismethod,namedoriginallyasHideF0,andwe
compare them by analyzing the capacity of the hidden channels, their
detectability and the decrease in quality introduced by pitch manipula-
tion.Inparticular,weperformlisteningtestsusing20participantstover-
ifyhowperceptiblethepitchmanipulationsare.Theresultsarepresented
and discussed. We prove that minor modiﬁcations of pitch parameters
are hardly perceptible, what can be used to create hidden transmission
channels.Oneofthebestproposedvariants,calledHideF0-FM,isshown
toenablehiddentransmissionatthebitrateofover120bpsatnospeech
quality degradation at all. Higher bitrates are also possible, only with
minor quality degradation and limited detectability.
· · ·
Keywords: Speech coding Information hiding Pitch Listening
·
tests Speex
1 Introduction
Large volume of encoded voice streams transmitted over the Internet has
attractedthosetryingtousethemasacarrierofhiddeninformation.Numerous
researchers have proposed various steganographic methods to be used with IP
telephony voice streams [12]. Some of these methods are based on exploiting
unused ﬁelds in protocol headers, other manipulate the encoded speech data,
yet another group modiﬁes time relationship between packets.
Existence of hidden channel can result in a major security breach. They can
be used to allow leakage of sensitive data out of, for example, a governmental
institution, or they can be used to control behavior of malicious software, which
can be used to attack a host or a network. Therefore it is very important to be
aware of various information hiding techniques and research on their counter-
measures.
A relatively small group of algorithms use pitch parameters to hide infor-
mation. One of such methods, called HideF0, was proposed in [7]. In this paper
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.513–523,2020.
https://doi.org/10.1007/978-3-030-58323-1_55514 A. Radej and A. Janicki
an improved version of this method is presented. In particular, we will try to
ﬁnd out if changes in pitch parameters are noticeable by listeners, therefore an
auditory assessment of perceptibility of pitch manipulation will be shown.
The article is organized as follows: we will start with a brief overview of
main information hiding techniques used in speech coding. Next, in Sect.3, we
will present our algorithm with particular focus on the new variants. In Sect.4
we will describe the experiments and their results. This section will be followed
by discussion in Sect.5 and conclusions in Sect.6.
2 Hiding Information Using Speech Coding
Speechcodinghasbeenusedforinformationhidingalreadyforseveralyears[5].
A large group of data hiding methods used with encoded speech is based on
a very popular steganographic method: it uses least signiﬁcant bits (LSB) to
transmit hidden data. For example, in [2] the author proposed applying the
LSB method directly to G.711 speech samples, in order to transmit the side
information to extend the conveyed acoustic bandwidth.
The LSB method can be also used for encoded speech parameters. In [15]
the authors used it with the G.723.1 codec by embedding a secret message into
various bitstream parameters (VQ index, codebook lags, pulse positions etc.),
achieving bitrates up to 133.33bps. It is noteworthy that the LSB methods are
easily prone to removing the hidden content by applying the so called content
threat removal (CTR).
Another group of algorithms replaces completely the voice payload encoded
by one codec, using another, more eﬃcient speech codec. Such a method was
called transcoding steganography [11]. The payload type in the RTP header
remainsunchanged,i.e.,itindicatesthattheoriginal,lesseﬃcientcodecisused.
The saved bits are used to hide own data.
Sofar,onlyafewsteganographicmethodsusedpitchorpitch-relatedparam-
eters to hide information. In [6] the LSB technique was used for F0 parameter
in G.729 codec. The authors achieved the steganographic bandwidth of 200bps.
In [14] a similar technique was proposed for the AMR codec.
The authors in [10] described a method dedicated to the ACELP codec,
whichwasbasedonembeddingsecretinformationintothefractionalpitchdelay
parameters, while at the same time keeping the integer pitch delay parameters
unchanged. A variant of LSB was used for information hiding in the so called
random LSB of pitch and Fourier magnitude steganography (RLPFS) [9]. The
authors claimed that they were able to create a covert channel with maximum
capacity of 266.64bps at the cost of a steganographic noise between 0.031 and
0.62 MOS, but the method was detection-resistant only when the bitrate was
ca. 44bps.
The method discussed in this article, originally proposed in [7], was based
on interpolation of the pitch contour. In its ﬁrst version it oﬀered the hidden
channel of ca. 200bps of capacity, at the steganographic cost of 0.5–0.7 MOS.
In this paper several new variants of this algorithm are proposed and evaluated.Modiﬁcation of Pitch Parameters in Speech Coding for Information Hiding 515
3 Proposed Algorithm
The proposed method takes advantage of the fact that parameters describing
thepitchvalue,usedinspeechcoding,forvoicedspeechchangerelativelyslowly
intime. This is why onecan tryto useapproximated pitch values instead ofthe
original ones and use the save bits to hide information.
Fig.1. Proposed variants of the HideF0 algorithm.
In this paper we will analyze three main variants of this method (see Fig.1):
– HideF0-FF (First-First);
– HideF0-FL (First-Last);
– HideF0-FM (First-Middle).516 A. Radej and A. Janicki
We will explain them using an example of the Speex codec, where the pitch
values are represented using four Fine Pitch parameters (FP1–FP4) per 20ms
frame, each occupying seven bits.
The ﬁrst variant (HideF0-FF) is used as a reference method [8]. It uses the
ﬁrst FP value from the current frame (FP1(n)) and the ﬁrst FP value from the
next frame (FP1(n+1)) and uses these values to approximate the remaining
ones. Next, the algorithm calculates the distance between the actual FPs and
the approximated ones. If the approximation error is below certain threshold θ
intheMSEsense,thevaluesFP2(n)..FP4(n)areusedforhiddentransmission.
On the receiver side, the hidden data are extracted and the missing FP values
are reconstructed using a simple linear approximation formula:
FP1(n+1)−FP1(n)
FPx(n)=FP1(n)+ ·x, x={2..4} (1)
4
and the resulting values are rounded to the closest integer. The frames with
manipulated pitch values are ﬂagged using any of the unused bits in IP, TCP,
UDP or RTP headers (according to [13], in the IP header alone, there are 64
bits where the data can be hidden), so that the receiver knows which frames to
extract the hidden information from.
Thesecondvariant–HideF0-FL,proposedinthispaper,usesonlyoneframe
to approximate FP values. Values FP2(n) and FP3(n) are calculated as:
FP4(n)−FP1(n)
FPx(n)=FP1(n)+ ·x, x={2,3} (2)
3
The next steps are analogous to the HideF0-FF variant.
ThethirdvariantproposedwasnamedasHideF0-FM(First-Middle).Inthis
variant the approximation is tried twice for each frame: between FP1(n) and
FP3(n), and also between FP3(n) and FP1(n+1), where the approximated
valuesaresimplyarithmeticmeansoftheneighboringones.Itisnoteworthythat
in this variant a 2-bit ﬂag is needed to inform if any approximation is required
in the ﬁrst, second or both half-frames.
4 Experiments
The three variants of the HideF0 algorithm were subjects to several experi-
ments, in order to verify their eﬀectiveness and detectability. In addition to the
base variants, we also tested their modiﬁcations, in which only a random subset
of qualiﬁed frames were used for hidden transmission – we wanted to see what
impact it would have on the detectability of hidden transmission. In this paper
we denote them as “Rnd” variants, in contrast to “All” variants, using every
frame meeting the MSE condition for data hiding. In total, for all six variants
of the HideF0 algorithm we ran the following experiments:
– Measurement of quality loss against the capacity of the hidden channel;
– Perceptibility tests for various capacities of the hidden channel;Modiﬁcation of Pitch Parameters in Speech Coding for Information Hiding 517
Fig.2. Quality decrease (steganographic cost) in the function of capacity of hidden
channel (steganographic bandwidth).
Fig.3. Results of perceptibility tests for various variants of the HideF0 algorithm.
Value “−1” denotes “speech modiﬁcations imperceptible”, value “1” denotes “speech
modiﬁcations clearly perceptible.”
– Detectability tests for various capacities of the hidden channel.
All experiments were run for the narrowband Speex codec mode 5 (quality
8), working with bitrate 15kbps, using the US English recordings taken from518 A. Radej and A. Janicki
theTIMITcorpus[3].WeemulatedhiddentransmissionwithsixvariousHideF0
variantsandvariousvaluesofthresholdθtoallowhiddentransmissionatvarious
(the so called steganographic) bitrates. The details of the experiments and their
output are presented below.
4.1 Quality Loss vs. Capacity of Hidden Channel
First we measured what quality decrease (the so called steganographic cost)
wasassociatedwithpitchmanipulationcausedbyhiddentransmission.Weana-
lyzedthemeanopinionscorelowqualityobjective(MOS-LQO)resultsreturned
by the Perceptual Evaluation of Speech Quality (PESQ) algorithm [1]. As the
audio material we used 24 male and 24 female recordings, each lasting ca. 30s,
composed out of the TIMIT audio ﬁles downsampled to 8kHz.
TheresultsareshowninFig.2.ItcanbeseenthattheHideF0methodallows
hidden transmission at no quality decrease at the ca. 50bps for the FF and FL
variants, and above 120bps for the FM variants. When moving to the bitrates
at the level of 200bps, the PESQ-estimated speech quality decrease caused by
pitchapproximationwasatthelevelof0.4MOS(forFMvariants),0.5MOS(for
the FM-All and FL-All variants) and 1.2 MOS for random variants of FM and
FL. For bitrates over 350bps the quality loss for all variants exceeded 1.4 MOS.
4.2 Perceptibility of Pitch Modiﬁcations
Inthesecondtestwewantedtoverifyhowmuchthepitchapproximationcaused
by the hidden transmission is perceptible. The testing methodology applied was
similar to the preference tests used, e.g., for synthetic speech evaluation [4].
Twenty listeners, aged 19–30 yrs, were exposed to 60 random, gender-balanced
TIMIT speech recordings, which were transcoded with various share of manip-
ulated frames. The listeners were in a quiet environment and used headphones.
Foreachaudioﬁletheparticipantswereaskedtoanswerthequestion:“Hasthis
recording been manipulated?”. The possible answers were “No” (scored as −1),
“Diﬃcult to say” (scored as 0) and “Yes” (scored as 1). The scores were then
averaged across the participants.
TheresultsaredisplayedinFig.3.Itshowsthatforthebitratesbelow200bps
the perceptibility of the FM variants was clearly lower than for the remaining
HideF0 variants and yielded values below −0.4. For bitrates 200–270bps the
perceptibility was similar to that of the FF-All variant and oscillated around 0,
which denotes “Diﬃcult to say”. For bitrates over 300bps most of the listeners
noticed that the audio was manipulated, with the score for FM-All being the
lowest (ca. 0.2). Interestingly, even if no pitch approximation was used at all
(left bottom end of each line in the plot), a few participants pointed them also
as suspicious.
4.3 Detectability of Hidden Transmission
In the third test we wanted to test if the use of HideF0 can be easily detected
by analysis of the bitstream of encoded speech. Following the state-of-the-artModiﬁcation of Pitch Parameters in Speech Coding for Information Hiding 519
Fig.4.AreaundertheROCcurve(AUC)fordetectionofhiddentransmissionassum-
ing method-aware detection system.
techniques we used the machine learning approach. We tried several algorithms,
such as support vector machines (SVM) or Na¨ıve Bayes (NB), but we achieved
thebestresultsfortheRandomForest(RF)classiﬁer,thereforethebelowresults
are presented for this algorithm only.
First we tried a na¨ıve approach, i.e., we assumed that we have no prior
knowledge of a technique used to hide data, which is usually the case. We tried
to train the RF classiﬁer based on 2/3 of the encoded speech data (with and
without pitch manipulation), using the remaining data as a testing set. As a
featurevectorweusedhistogramsofbytevaluesofthevoicepayload.Depending
on hidden channel capacity, the RF classiﬁer yielded the area under the ROC
curve (AUC) in the range of 0.50–0.52, what indicated a random classiﬁcation.
To consider an opposite case, we assumed the worst-case detection scenario:
the full knowledge of the data hiding technique, so that the detection algorithm
was aware that HideF0 was used and in which variant. In this case the feature
vector consisted of diﬀerence values (deltas) between the actual FP values and
the approximated ones, using the approximation formulae for the respective
HideF0 variant, calculated for a window spanning over 2.5s. The RF algorithm
was tested using the data, where 3/4 of encoded speech was benign and 1/4 of
encoded speech was manipulated using HideF0.
We analyzed AUC and precision of the hidden transmission detection for
various HideF0 variants. The results are depicted in Figs.4 and 5, respectively.
They show that for lower bitrates (i.e., when the steganographic cost equals 0)
thedetectionwashardlypossible,despitebeingawareofthedatahidingmethod
- the AUC was below 0.55. For bitrates over 200bps the AUC exceeded 0.9 for520 A. Radej and A. Janicki
Fig.5.Precisionofdetectionofhiddentransmissionassumingmethod-awaredetection
system.
all variants, what meant that the detection became easier. But even for bitrates
exceeding300bpstheprecisionofdetectionforsomeoftheHideF0variants(FF,
FM-Rnd)wasbelow96%,whatwouldresultinarelativelyhighfalsealarmrate.
5 Discussion
Approximationofpitch-relatedparametersismostlyeﬀectivewhentheseparam-
eterschangeslowlyandmonotonously.Thisisthecasewhenthespeechisvoiced.
When the speech is unvoiced, the F0 is undetermined and pitch-related param-
eters (such as FP in Speex) take unpredictable values, therefore their approxi-
mation results in high diﬀerences and is easily noticeable. Only the frames for
which the approximation error is below certain threshold θ take part in hiding
data. The presented capacity values of the hidden channel were averaged across
the analyzed recordings, but it must be remembered that the actual capacity
depends on the voice activity of the speaker and the amount of voiced speech
within the transmitted signal.
In several tests the variant HideF0-FM yielded the best results. This can be
easily explained by the fact that it is easier to interpolate a single pitch value
between two neighbors than two or three values in a row. Therefore such an
interpolation can happen more often, so that this variant can lead to higher
bitrates of hidden transmission at lower quality cost. It is noteworthy, however,
that since such an interpolation can happen twice per frame, it requiresanother
bitﬂaghiddenintheTCP,UDPorRTPheader,whatincreasesthesidechannel
capacity by 50bps and may also have impact on increased detectability of the
method as a whole.Modiﬁcation of Pitch Parameters in Speech Coding for Information Hiding 521
The HideF0 “Rnd” variants which involved random use of the frames for
hidden transmission, did not turn out to be very successful in terms of qual-
ity, when comparing them with “All” methods for the same size of the hidden
channel.Thiscanbeexplainedbythefactthatcreatingahiddenchannelofthe
same capacity as for the “All” variant, required random selection of frames out
of a larger set of frames, among which there might be frames with much higher
approximationerror.Thiscouldlowerthequalityandincreasetheperceptibility
of such a manipulation. However, the “Rnd” variants may turn out to be quite
diﬃcult to detect for lower steganographic bitrates using statistical detection
methods, but this would need to be veriﬁed in additional experiments.
The applied perceptibility test requires some comments. The listeners were
informed that the recordings might have been manipulated, therefore they paid
specialattentiontoanydistortionsintheperceivedspeech.Thisisprobablythe
reasonwhysomeofthelistenersfoundﬁleswithnomodiﬁcationsalsoasmanip-
ulated. We think that in real environment speech transmitted with such quality
wouldraisenosuspicionsatall.Thereforewesuspectthattheperceptibilitytest
was biased toward increased perceptibility of HideF0.
Thelistenersturnedouttobemoderatelysensitivetopitchmanipulation:for
the PESQ-estimated quality degradation of 1.0 MOS (which is relatively high)
the perceptibility oscillated around 0, what meant that on average the listeners
were not sure if any manipulation took place.
6 Conclusions
In this paper we discussed the idea of using interpolation of pitch parameters
in encoded speech bitstream for the purpose of hidden transmission. The main
objective was to verify experimentally how eﬃcient such hidden channels would
beandiftheyareeasytoperceiveanddetect.Theseelementswerenovelinthis
article:
– NewvariantsoftheHideF0method,namedFirst-Last(FL)andFirst-Middle
(FM), were proposed and tested.
– Versions“Rnd”withrandomsubsetofframesusedtohidedatawereproposed
and evaluated.
– Perceptual listening tests with 20 participants were conducted and they
results were analyzed.
– Experiments with detection of all six variants were run and their outcomes
were analyzed.
We can conclude that the discussed methods, especially the newly proposed
HideF0-FM variant, can pose a security risk, as they allow to create a relatively
highly eﬃcient hidden channel (with the steganographic bitrate over 120bps)
at no quality decrease, with lack of perceptibility and detectability, while the
originalversionofthisalgorithm(namedhereasHideF0-FF)oﬀeredthecostless
variant at the bitrate of ca. 50bps only. The HideF0-FM methods can also
provide higher steganographic bitrates (around 200bps), while still not being522 A. Radej and A. Janicki
well noticed by listeners nor eﬃciently detected by a trained classiﬁer, even if it
is method-aware.
Acknowledgements. This work has been partially funded under the SIMARGL
project,whichhasreceivedfundingfromtheEuropeanUnion’sHorizon2020research
and innovation programme under grant agreement No. 833042.
References
1. Recommendation P.862: Perceptual evaluation of speech quality (PESQ): An
objective method for end-to-end speech quality assessment of narrow-band tele-
phone networks and speech codecs (2001)
2. Aoki,N.:AbandextensiontechniqueforG.711speechusingsteganography.IEICE
Trans. Commun. 89(6), 1896–1898 (2006)
3. Garofolo, J., Lamel, L., Fisher, W., Fiscus, J., Pallett, D., Dahlgren, N.: TIMIT
acoustic-phoneticcontinuousspeechcorpus.LinguisticDataConsortium,Philadel-
phia, US (1993)
4. Gu,Y.,Kang,Y.:Multi-taskWaveNet:amulti-taskgenerativemodelforstatisti-
calparametricspeechsynthesiswithoutfundamentalfrequencyconditions.In:Pro-
ceedingsoftheInterspeech2018,Hyderabad,India,pp.2007–2011(2018).https://
doi.org/10.21437/Interspeech.2018-1506
5. Huang,Y.,Liu,C.,Tang,S.,Bai,S.:Steganographyintegrationintoalow-bitrate
speech codec. IEEE Trans. Inf. Forensics Secur. 7(6), 1865–1875 (2012)
6. Iwakiri,M.,Matsui,K.:Embeddingatextintoconjugatestructurealgebraiccode
excited linear prediction audio codecs. Inf. Process. Soc. Jpn. 39(9), 2623–2630
(1998)
7. Janicki,A.:NovelmethodofhidinginformationinIPtelephonyusingpitchapprox-
imation. In: Proceedings of the International Workshop on Cyber Crime (IWCC
2015), Toulouse, France, pp. 429–435, August 2015
8. Janicki, A.: Pitch-based steganography for Speex voice codec. Secur. Commun.
Netw. 9(15), 2923–2933 (2016). https://doi.org/10.1002/sec.1428
9. Kheddar,H.,Bouzid,M.,Megias,D.:PitchandFouriermagnitudebasedsteganog-
raphy for hiding 2.4 kbps MELP bitstream. IET Signal Proc. 13(3), 396–407
(2019). https://doi.org/10.1049/iet-spr.2018.5339
10. Liu,X.,Tian,H.,Huang,Y.,Lu,J.:Anovelsteganographicmethodforalgebraic-
code-excited-linear-prediction speech streams based on fractional pitch delay
search. Multimed. Tools Appl. 78(7), 8447–8461 (2018). https://doi.org/10.1007/
s11042-018-6867-7
11. Mazurczyk,W.,Szaga,P.,Szczypiorski,K.:Usingtranscodingforhiddencommu-
nicationinIPtelephony.Multimed.ToolsAppl.70(3),2139–2165(2012).https://
doi.org/10.1007/s11042-012-1224-8
12. Mazurczyk, W., Szczypiorski, K.: Steganography of VoIP streams. In: Meersman,
R.,Tari,Z.(eds.)OTM2008.LNCS,vol.5332,pp.1001–1018.Springer,Heidelberg
(2008). https://doi.org/10.1007/978-3-540-88873-4 6
13. Murdoch, S.J., Lewis, S.: Embedding covert channels into TCP/IP. In: Barni,
M.,Herrera-Joancomart´ı,J.,Katzenbeisser,S.,P´erez-Gonz´alez,F.(eds.)IH2005.
LNCS, vol. 3727, pp. 247–261. Springer, Heidelberg (2005). https://doi.org/10.
1007/11558859 19Modiﬁcation of Pitch Parameters in Speech Coding for Information Hiding 523
14. Nishimura,A.:Datahidinginpitchdelaydataoftheadaptivemulti-ratenarrow-
band speech codec. In: Fifth International Conference on Intelligent Information
HidingandMultimediaSignalProcessing(IIH-MSP2009),Kyoto,Japan,pp.483–
486, September 2009. https://doi.org/10.1109/IIH-MSP.2009.83
15. Xu, T., Yang, Z.: Simple and eﬀective speech steganography in G.723.1 low-rate
codes.In:InternationalConferenceonWirelessCommunicationsSignalProcessing
(WCSP2009),Nanjing,China,pp.1–4,November2009.https://doi.org/10.1109/
WCSP.2009.5371745ConfNet2Seq
Full Length Answer Generation from Spoken Questions
B
Vaishali Pal1( ) , Manish Shrivastava1 , and Laurent Besacier2
1 LTRC, International Institute of Information Technology - Hyderabad,
Hyderabad, India
vaishali.pal@research.iiit.ac.in, m.shrivastava@iiit.ac.in
2 LIG - Universit´e Grenoble Alpes, Grenoble, France
laurent.besacier@univ-grenoble-alpes.fr
Abstract. Conversational and task-oriented dialogue systems aim to
interactwiththeuserusingnaturalresponsesthroughmulti-modalinter-
faces, such as text or speech. These desired responses are in the form of
full-length natural answers generated over facts retrieved from a knowl-
edge source. While the task of generating natural answers to questions
from an answer span has been widely studied, there has been little
research on natural sentence generation over spoken content. We pro-
pose a novel system to generate full length natural language answers
fromspokenquestionsandfactoidanswers.Thespokensequenceiscom-
pactly represented as a confusion network extracted from a pre-trained
AutomaticSpeechRecognizer.Thisistheﬁrstattempttowardsgenerat-
ing full-length natural answers from a graph input (confusion network)
tothebestofourknowledge.Wereleasealarge-scaledatasetof259,788
samples of spoken questions, their factoid answers and corresponding
full-lengthtextualanswers.Followingourproposedapproach,weachieve
comparable performance with best ASR hypothesis.
· · ·
Keywords: Confusion network Pointer-generator Copy attention
·
Natural answer generation Question answering
1 Introduction
Full-length answer generation is the task of generating natural answers over
a question and an answer span, usually a fact-based phrase (factoid answer),
extracted from relevant knowledge sources such as knowledge-bases (KB) or
context passages. Such functionality is desired in conversational agents and dia-
logue systems to interact naturally with the user over multi-modal interfaces,
such as speech and text. Typical task-oriented dialogue systems and chatbots
formulatecoherentresponsesfromconversationcontextwithanaturallanguage
generation(NLG)module.Thesemodulescopyrelevantfactsfromcontextwhile
generatingnewwords,maintainingfactualaccuracyinacoherentfact-basednat-
ural response. Recent research [7,10] utilizes a pointer-network to copy words
from relevant knowledge sources. While the task of generating natural response
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.524–531,2020.
https://doi.org/10.1007/978-3-030-58323-1_56ConfNet2Seq: Full Length Answer Generation from Spoken Questions 525
to text-based questions have been extensively studied, there is little research
on natural answer generation from spoken content. Recent research on Spoken
Question Answering and listening comprehension tasks [6] extracts an answer-
span and does not generate a natural answer. This motivates us to propose the
task of generating full length answer from spoken question and textual factoid
answer. However, such a task poses signiﬁcant challenges as the performance of
the system is highly dependent on Automatic Speech Recognizer (ASR) error.
To mitigate the eﬀect of Word Error Rate (WER) on ASR predictions, a list
of top-N hypothesis, ASR lattices or confusion networks has been used in vari-
ous tasks such as Dialogue-state-tracking [2,16], Dialogue-Act detection [8] and
named-entity recognition. These tasks show that models trained using multiple
ASR hypotheses outperforms those trained top-1 ASR hypothesis. While classi-
ﬁcation and labeling tasks beneﬁt from multiple hypothesis by aggregating the
predictions over a list of ASR hypothesis, it is non-trivial to apply the same for
NLG using pointer-networks. Our proposed system aims to take advantage of
multiple time-aligned ASRhypotheses representedas a confusion network using
apointer-networktogeneratefull-lengthanswers.Tothebestofourknowledge,
there is no prior work for full length answer generation from spoken questions.
Our overall research contributions are as follows:
– We propose a novel task of full-length answer generation from spoken ques-
tion. To achieve this, we develop a ConfNet2Seq model which encodes a con-
fusion network and adapts it over a pointer-generator architecture.
– Wecomparetheeﬀectsofusingmultiplehypothesisencodedwithaconfusion
network encoder and the best hypothesis encoded with a text encoder.
– We publicly release the dataset, comprising of spoken question audio ﬁle, the
corresponding confusion network, the factoid answer and full-length answer.
2 Related Work
Spoken Language Understanding (SLU) has the additional challenge of disam-
biguation of ASR errors which drastically aﬀect performance. Several methods
have been proposed to curb the eﬀects of the WER. Word lattices from ASR
were ﬁrst used by [1] over ASR top-1 hypothesis for tasks such as named-entity
extraction and call classiﬁcation. Word confusion networks have been recently
used by [4] for intent classiﬁcation in dialogue systems and by [2,9] for dialogue
state tracking (DST). [2] show that confusion network gives comparable perfor-
mance to top-N hypotheses of ASR while [9] show that using confusion network
improves performance in both in time and accuracy. Another related task in
SLU is that of Spoken Question Answering. Recent work [6] on SQuAD dataset
introduces the task for machine listening comprehension where the context pas-
sages are in audio form. [5] released Open-Domain Spoken Question Answering
Dataset(ODSQA)withmorethanthreethousandquestionsinChineseandused
an enhanced word embedding comprising of word embedding and pingyin-token
embedding. [15] developed a QA system for spoken lectures and generates an
answer span from the video transcription.526 V. Pal et al.
3 Models
Our system generates full length answer from a textual factoid answer and spo-
ken question. We use a pointer generator architecture over two sequences, i.e.,
over the textual factoid answer sequence and the encoded question sequence
produced by the confusion network encoder. In this section, we describe the 1)
Confusion network encoder, 2) Final model over spoken question and factoid
answer. The full architecture is shown in Fig.1.
3.1 Confusion Network Encoder
A Confusion Network is a weighted directed acyclic graph with one or more
parallel arcs between consecutive nodes where each path goes through all the
nodes. Each set of parallel arcs represents time-aligned alternative words or
hypothesisoftheASRweighedbyprobability.Thetotalprobabilityofallparallel
arcs between two consecutive nodes sums up to 1. A confusion network C can
be deﬁned formally as a sequence of sets of parallel weighted arcs as:
where isthejth ASRhypothesisatpositioni,andπj itsassociatedprobabil-
i
ity. We use a confusion network encoder to transform a 2-dimensional confusion
network into an 1-dimensional sequence of embeddings as described in [8]. Each
word of the confusion network can be encoded by weighing the word embed-
ding by the ASR probability followed by a non-linear transformation as:
whereW isatrainableparameter.Eachsetofparallelarcscanbeencodedinto
1
a vector by a weighted sum over the words of the parallel arc set. The weights
measuretherelevanceofeachwordamongthealternatetime-alignedhypothesis.
The learnt weight distribution for each parallel-arc set is:
αj = (cid:2)exp(W2qij) (3)
i exp(W qj)
j 2 i
where W is a trainable parameter. The ﬁnal encoding of each set of parallel
2
arcs is: (cid:3)
βi = αijqij (4)
i
3.2 Full Length Answer Generation from Spoken Questions
We have followed a Seq2Seq with pointer generator architecture as [10] to gen-
eratefull-lengthanswersfromaquestionandfactoidanswer.However,wequery
with spoken questions instead of textual questions. The confusion network is
extractedfromspokenquestionsusingastandardASR.ThequestionisencodedConfNet2Seq: Full Length Answer Generation from Spoken Questions 527
Fig.1.Fulllengthanswergenerationfromspokenquestionandtextualfactoidanswer:
The confusion-network encoder generates a sequence of 1D-encodings of the sequence
of parallel arcs (2D graph). The ASR scores are multiplied with the global-attention
weights of the encodings to generate the copy-attention distribution of the question.
as Q = {q1,q2,...,qn} where qt is the encoding from the confusion network
encoder explained in Sect.3.1.
The factoid answer is represented as A={a1,a2,a3,...,am} where at is the
GloVe embedding [11] of a word. We encode the sequences using two 3-layered
bi-LSTMs which share weights as:
htQ =BILSTM(htQ−1,qt)
(5)
htA =BILSTM(htA−1,at)
The encoded hidden states of the 2 encoders are stacked together to produce a
single list of source hidden states, hS =[hQ;hA]. The decoder is initialized with
the combined ﬁnal states of the two encoders as h0 =hn +hm.
T Q A
The global attention weights attnt are computed on the n hidden states of
i
the question and m hidden states of the answer, stacked to produce a total of
m+n global attention weights. For each source state, hi, and decoder state, st:528 V. Pal et al.
where battn, , Wh, Ws are learnable parameters. The copy mechanism for
summarization introduced in [13] takes advantage of a word distribution over
an extended vocabulary comprising of source words and vocabulary words. The
probabilityofcopyingaword fromatextsequenceis .Tocopy
wordsfromtheconfusionnetwork,wecomputetheglobalattentionweightsover
each set of parallel-arc encodings. Here, the global attention weights denote a
probability distribution over parallel-arc sets instead of words. These attention
weights attnti are sampled to select the hidden state representation, βi, of a set
of parallel arcs. The ASR scores πi is a probability distribution over the set
of parallel words at position j in the confusion network. These are sampled to
select the most likely word from that set of parallel arcs. The ﬁnal probability
of copying a word from the confusion network is the joint-probability:
The probability of copying a word from the answer is:
The ﬁnal probability of a word output at at time t by the decoder is as
shown in
where isasoftswitchforthedecodertogeneratewordsorcopywordsfrom
thesource. istheprobabilityofgeneratingawordfromthevocabulary.
These parameters are computed as described in [13].
4 Dataset
Togeneratedataforourtask,weuse258,478samplesfromthefull-lengthanswer
generation dataset introduced in [10] where each sample consists of a question,
factoid answer and full-length answer. The samples in the dataset were chosen
from SQuAD and HarvestingQA. Each sample in our dataset is also a 3-tuple
(q,f,a)inwhichq isaspoken-formquestion,f isatext-formfactoidanswerand
a is the text-form full-length natural answer. 256,478 samples were randomly
selected as the training set, 1000 as the development set and 1000 as the test
set.Wealsoextracted470samplesfromNewsQAdatasetand840samplesfrom
Freebase to evaluate our system on cross-domain datasets.1
WeusedGoogletext-to-speechtogeneratethespokenutterancesoftheques-
tions. Google Voice en-US-Standard-B was used to generate 239,746 spoken
questions in male voice and Google Voice en-US-Wavenet-C was used to gen-
erate 16,730 spoken questions in female voice. All samples are in US accented
English. The ASR lattice was extracted using Kaldi ASR [12] and converted
to a confusion network for compact representation using SRILM [14]. We used
1 Code and dataset at: https://github.com/kolk/ConfnetPointerGenBaseline.ConfNet2Seq: Full Length Answer Generation from Spoken Questions 529
the pre-trained ASpIRE Chain Model which has been trained on Fisher English
to transcribe the spoken question and extract the ASR lattices. The training
dataset has a WER of 22.94% and test set has a WER of 37.57% on the best
hypothesisoftheASR,whilethecross-datasetevaluationtestsets-NewsQAhas
a WER of 34.60% and Freebase has a WER of 43.80%.
5 Experiments and Results
WebuiltoursystemoverOpenNMT-Py[3].Weusedabatchsizeof32,dropout
rate of 0.5, RNN size of 512 and decay steps 10000. The maximum number of
parallel arcs in the confusion network and maximum sentence length are set to
20 and 50 respectively. The confusion network contains noise and interjections
suchas*DELETE* and[noise],[laughter],uh,oh whichleadstodegradationin
system performance. To mitigate the eﬀect of such noise, we remove the whole
set of parallel arcs if all the arcs are noise and interjection words. As shown in
Table1, the pruned confusion network, named clean confnet, outperforms the
system marginally for the SQuAD/HarvestingQA dataset. We also compare the
system with a model trained on the best hypothesis of the extracted from the
ASR lattice using Kaldi. Here, the confusion network encoder is replaced with a
text encoder which shares weights with the factoid answer encoder.
As shown in Table1, we observe for SQuAD/HarvestingQA dataset that the
Best-ASR-hypothesisoutperformsthecleanconfusionnetworkmodelwitha5%
margin in BLEU score and 2% margin in ROGUE-L score. To asses the cross-
domain generalizability, we also perform cross-dataset evaluation by evaluating
our models on 840 samples of a KB based dataset (Freebase) and 470 samples
of a machine comprehension dataset (NewsQA). The clean confusion network
marginally outperforms the best-hypothesis model in ROGUE scores for cross-
dataset evaluation and gives comparable results on BLEU scores. This shows
thattheconfusionnetworksystemgeneralizesbetteroncross-domainnoisydata
and is less sensitive to noise introduced by new domains and noisy input sig-
nal, when compared with the Best-ASR-Hypothesis model. A plausible reason
to this could be that the confusion network model is itself trained on a closed
set of hypothesis, as compared to the Best-ASR-Hypothesis model which makes
simplifying assumptions about the input signal. A compelling extension to the
confusionnetworkmodelistoadaptthecopyattentionoverallthetime-aligned
hypotheses of the confusion network input. This would allow the confusion net-
work model to copy among top-N words at any given time-step of the confusion
network, instead of an erroneous word with the highest ASR score.
An example of results on a SQuAD/HarvestingQA test sample is in Table1.
– Gold Question: what was the title of the sequel to conan the barbarian?
– Top-Hypothesis: what was the title of the sequels are counting the barbar-
ian
– Factoid Answer: conan the destroyer
– Full-length Answer: the title of the sequel to conan the barbarian was
conan the destroyer530 V. Pal et al.
Table 1. Top section shows the scores on 1000 SQuAD/HarvestingQA test samples.
Bottom 2 section shows the scores for cross-dataset evaluation on a Knowledge-Base
(Freebase)datasetandamachinecomprehension(NewsQA)dataset.Foreachsection,
the top row displays the score on the best hypothesis of the confusion network, the
middlerowdisplaysthescoresontheconfusionnetwork,whilethebottomrowdisplays
the results on the pruned clean confusion network
Testdataset Input BLEU ROGUE-1 ROGUE-2 ROGUE-L
SQuAD/HarvestingQA Besthypothesis 60.26 82.43 70.61 78.21
Confnet 55.38 81.60 68.02 76.68
Cleanconfnet 55.92 81.39 67.79 76.78
Freebase Besthypothesis 43.21 71.37 51.72 64.98
Confnet 41.86 72.42 51.84 65.78
Cleanconfnet 42.89 72.54 52.77 66.39
NewsQA Besthypothesis 49.98 75.82 59.59 72.65
Confnet 53.45 76.45 60.32 72.78
Cleanconfnet 56.86 76.07 61.18 73.12
– Clean Confnet Model prediction:thetitleofthesequelstothebarbarian
was conan the destroyer
– Best-Hypothesis Model prediction: the title of the sequels are counting
the barbarian
6 Conclusion
Weproposethetaskofgeneratingfull-lengthnaturalanswersfromspokenques-
tions and factoid answer. We generated a dataset consisting of triples (spoken
question, factoid answer, full length answer) and extracted confusion network
from the questions. We have used the pointer-network over ASR graphs (confu-
sionnetwork)andshowthatitgivescomparableresultstothemodeltrainedon
thebesthypothesis.OursystemachievesaBLEUscoreof55.92%andROGUE-
L score of 76.78% on SQuAD/HarvestingQA dataset. We perform cross-dataset
evaluation to obtain a BLEU score of 42.89% and ROGUE-L score of 66.39%
on Freebase, and a BLEU score of 56.86% and ROGUE-L score of 73.12% on
NewsQA dataset.
References
1. Hakkani-Tu¨r, D., B´echet, F., Riccardi, G., Tu¨r, G.: Beyond ASR 1-best: using
wordconfusionnetworksinspokenlanguageunderstanding.Comput.SpeechLang.
20(4), 495–514 (2006). https://doi.org/10.1016/j.csl.2005.07.005
2. Jagfeld, G., Vu, N.T.: Encoding word confusion networks with recurrent neural
networks for dialog state tracking. CoRR abs/1707.05853 (2017). http://arxiv.
org/abs/1707.05853ConfNet2Seq: Full Length Answer Generation from Spoken Questions 531
3. Klein, G., Kim, Y., Deng, Y., Senellart, J., Rush, A.M.: OpenNMT: open-source
toolkitforneuralmachinetranslation.In:ProceedingsoftheACL(2017).https://
doi.org/10.18653/v1/P17-4012
4. Ladhak, F., Gandhe, A., Dreyer, M., Mathias, L., Rastrow, A., Hoﬀmeister, B.:
LatticeRnn: recurrent neural networks over lattices. In: INTERSPEECH (2016)
5. Lee, C., Wang, S., Chang, H., Lee, H.: ODSQA: open-domain spoken question
answeringdataset.CoRRabs/1808.02280(2018).http://arxiv.org/abs/1808.02280
6. Li, C., Wu, S., Liu, C., Lee, H.: Spoken squad: a study of mitigating the impact
of speech recognition errors on listening comprehension. CoRR abs/1804.00320
(2018). http://arxiv.org/abs/1804.00320
7. Liu, C., He, S., Liu, K., Zhao, J.: Curriculum learning for natural answer genera-
tion. In: IJCAI, pp. 4223–4229. ijcai.org (2018)
8. Masumura, R., Ijima, Y., Asami, T., Masataki, H., Higashinaka, R.: Neural
ConfNet classiﬁcation: fully neural network based spoken utterance classiﬁcation
usingwordconfusionnetworks.In:2018IEEEInternationalConferenceonAcous-
tics, Speech and Signal Processing, ICASSP 2018, pp. 6039–6043, April 2018.
https://doi.org/10.1109/ICASSP.2018.8462030
9. Pal, V., Guillot, F., Shrivastava, M., Renders, J.M., Besacier, L.: Modeling ASR
ambiguity for dialogue state tracking using word confusion networks. In: INTER-
SPEECH 2020, Shanghai, China (2020, in press). https://arxiv.org/abs/2002.
00768
10. Pal,V.,Shrivastava,M.,Bhat,I.:Answeringnaturally:factoidtofulllengthanswer
generation. In: Proceedings of the 2nd Workshop on New Frontiers in Summa-
rization, Hong Kong, China, pp. 1–9. Association for Computational Linguistics,
November2019.https://doi.org/10.18653/v1/D19-5401.https://www.aclweb.org/
anthology/D19-5401
11. Pennington, J., Socher, R., Manning, C.D.: Glove: global vectors for word repre-
sentation. In: EMNLP (2014)
12. Povey, D., et al.: The Kaldi speech recognition toolkit. In: IEEE 2011 Workshop
on Automatic Speech Recognition and Understanding. IEEE Signal Processing
Society, December 2011. IEEE Catalog No.: CFP11SRW-USB
13. See, A., Liu, P.J., Manning, C.D.: Get to the point: summarization with pointer-
generator networks. CoRR abs/1704.04368 (2017). http://arxiv.org/abs/1704.
04368
14. Stolcke, A.: SRILM-an extensible language modeling toolkit. In: Proceedings of
the 7th International Conference on Spoken Language Processing (ICSLP 2002),
pp. 901–904 (2002)
15. Unlu,M.,Arisoy,E.,Saraclar,M.:Questionansweringforspokenlectureprocess-
ing, pp. 7365–7369, May 2019. https://doi.org/10.1109/ICASSP.2019.8682580
16. Zhong,V.,Xiong,C.,Socher,R.:Global-locallyself-attentiveencoderfordialogue
state tracking. In: ACL (2018)Graph Convolutional Networks
for Student Answers Assessment
B
Nisrine Ait Khayi( ) and Vasile Rus
Institute for Intelligent Systems, University of Memphis, Memphis, TN, USA
{ntkhynyn,vrus}@memphis.edu
Abstract. Graph Convolutional Networks have achieved impressive
results in multiple NLP tasks such as text classiﬁcation. However, this
approach has not been explored yet for the student answer assessment
task. In this work, we propose to use Graph Convolutional Networks to
automaticallyassessfreelygeneratedstudentanswerswithinthecontext
of dialogue-based intelligent tutoring systems. We convert this task to
a node classiﬁcation task. First, we build a DTGrade graph where each
node represents the concatenation of the student answer and its corre-
sponding reference answer whereas the edges represent the relatedness
betweennodes.Second,theDTGradegraphisfedtotwolayersofGraph
ConvolutionalNetworks.Finally,theoutputofthesecondlayerisfedto
a softmax layer. The empirical results showed that our model reached
the state-of-the-art results by obtaining an accuracy of 73%.
·
Keywords: Graph Convolutional Networks Student answers
·
assessment Intelligent tutoring systems
1 Introduction
Student answers assessment or short text grading is a well-deﬁned problem in
Natural Language Processing (NLP). It is a an extremely challenging task as
students can express the same answer in multiple ways owing to diﬀerent indi-
vidual styles and varied cognitive abilities and knowledge levels. Table1 shows
fouranswers,articulatedbyfourdiﬀerentcollegestudents,toaquestionaskedby
the state-of-the-art intelligent tutoring system (ITS) DeepTutor [13]. It should
be noted that all four student answers in Table1 are correct answers to the
tutorquestion.Ascanbeseenfromthetable,somestudentswritefullsentences
(student answer A4), some others write very short answers (A3), and yet other
studentswriteelaborateanswersthatincludeadditionalconceptsrelativetothe
reference answer (A1).
Assessingthefreelygeneratedstudentanswersinconversationaltutoringcan
be achieved using various approaches. Semantic similarity is a widely adopted
and scalable approach in which the student answer is compared to a reference
answerproducedbyanexpert.Typically,anormalizedsemanticsimilarityscore,
from 0 to 1 (or from 0 to 5), between the student answer and the expert answer
(cid:2)c SpringerNatureSwitzerlandAG2020
P.Sojkaetal.(Eds.):TSD2020,LNAI12284,pp.532–540,2020.
https://doi.org/10.1007/978-3-030-58323-1_57Graph Convolutional Networks for Student Answers Assessment 533
Table 1. Examples of student answers showing the diversity of responses from Deep-
Tutor
Problem description:
Whilespeedingup,alargetruckpushesasmallcompactcar
Tutor question:
Howdothemagnitudesofforcestheyexertoneachothercompare?
Reference answer:
Theforcesfromthetruckandcarareequalandopposite
Student answers:
A1.Themagnitudesoftheforcesareequalandoppositetoeach
otherDuetoNewton’sthirdlawofmotion
A2.theyareequalandoppositeindirection
A3.equalandopposite
A4.thetruckappliesanequalandoppositeforcetothecar
is generated. A high score implies that the student answer is correct, and a low
score implies the student answer is incorrect.
More recently deep learning has shown its eﬀectiveness in solving the stu-
dents answers assessment task [1,2,9,11]. These deep learning models have the
advantage of capturing semantic and syntactic information for the text input.
Graph Convolutional Networks, in particular, have received a growing attention
recently[4,6].Graphneuralnetworkshave beeneﬀectiveat tasksthat have rich
relational structure and can preserve global structure information of a graph in
graph embeddings.
In this paper, we propose a novel approach based on Graph Convolu-
tional Networks [15], for the students answers assessment task. We construct
a DTGrade graph where each node consists of the concatenation of a student
answer and its corresponding reference answer. We model the graph with a
Graph Convolutional Network (GCN) that encodes relevant information about
its neighborhood as a real-valued feature vector. The edge between two nodes
is built using word frequency and word’s document frequency method and an
embedding based method. Then, we turn the assessment task into a node clas-
siﬁcation task.
The rest of the paper is organized as follows: Sect.2 presents a review of
several prior research works that used Graph Convolutional Networks for diﬀer-
ent NLP tasks. Section3 explains the proposed approach. Section4 summarizes
the conducted experiments to evaluate the performance of our approach and
the results obtained on the DT-Grade dataset. Finally, we discuss conclusions
and highlight future research directions to improve results and overcome the
limitations.534 N. A. Khayi and V. Rus
2 Related Work
Graph Convolutional Networks (GCN) have yielded great results in multiple
NLPtasks.Forinstance,Sahuandcolleagues[14]proposedanovelinter-sentence
relation extraction model that builds a labelled edge Graph Convolutional Net-
workonadocument-levelgraph.Theexperimentalresultsshowedthatthemodel
hasachievedacomparableperformancetostate-of-the-artneuralmodelsonthe
inter-sentence relation extraction task. Working on the same task, Zhang and
colleagues [17] proposed a novel model for the relation extraction task. Their
model consists of the following components: 1) an instance encoder based on
convolutional neural networks (CNN) to encode the instance semantics into a
vector, 2) a relational knowledge learning component that employs graph con-
volutional networks to learn explicit relational knowledge, and 3) a knowledge-
awareattentioncomponenttoselectthemostinformativeinstancethatmatches
the relevant relation. The experimental results showed that this model outper-
forms several baselines such as CNN. GCNs have been applied successfully as
well for the semantic role labeling task that can be described as the task of
discovering in texts who did what to whom. To this end, Marcheggiani and col-
leagues [12] have proposed a model that consists of the following components:
1) word embeddings, 2) a BiLSTM encoder that takes as input the embedding
representation of each word, 3) a syntax-based GCN encoder that re-encodes
the BiLSTM representation based on the predicted syntactic structure of the
sentence, and 4) a classiﬁer to predict the role associated with each word. The
empirical results showed that this based GCN model has achieved the state-of-
the-art results. GCNs have been explored successfully in text classiﬁcation. For
thispurpose,Yaoandcolleagues[16]proposedtouseGraphConvolutionalNet-
works for text classiﬁcation. They built a single text graph for the whole corpus
based on word co-occurrence and document word relations then learnt a Text
GraphConvolutionalNetworkforthecorpus.Theproposedmodelhasbeeneval-
uated using multiple benchmarks. The experimental results showed that GCN
outperforms several baselines such as Bi-Directional LSTM and LSTM. In this
work, we don’t consider a heterogenous graph where nodes present words and
documents. The nodes represent documents only as a concatenation between
student answers and reference answers. Based on these successes of Graph Con-
volutional Networks on NLP related tasks, we have explored their potential for
assessingstudentanswers.Tothebestofourknowledge,thisistheﬁrstattempt
at using GCNs for assessing student generated answers in conversational intelli-
gent tutoring systems.
3 Proposed Method
Our proposed method consists of building ﬁrst a graph from the DTGrade. The
built graph is fed into two GCN layers. Finally, we apply a classiﬁer to predict
the class of each text node (Fig. 1).Graph Convolutional Networks for Student Answers Assessment 535
Fig.1.Themodelarchitectureconsistsof:1)buildingaDTGraph,2)feedingittotwo
GCN layers, and ﬁnally 3) applying a classiﬁer
3.1 DT-Grade Graph
WebuildatextgraphfromtheDT-Gradedatasetbasedonthecitationrelation
approach [15]. We consider each document, whose content is the combination of
thestudentansweranditscorrespondingreferenceanswer,asanode.Thus,the
classiﬁcation of a pair of student answer and reference answer turns to a node
classiﬁcation task. The number of the nodes in the text graph is 900 which is
the number of instances in the DT-Grade dataset. Formally given a graph G =
(V,E)whereVandEaresetsofnodesandedges.Theweightoftheedgebetween
twonodesiscalculatedusingtwomethods:aTF-IDFmethodandanembedding
basedmethod.Intheﬁrstone,wecomputethetermfrequency-inversedocument
frequency(TF-IDF)betweentwotextnodes.Weaddanedgebetweentwonodes
iftheweightisaboveathresholdof0.9.Thesecondmethodisbasedonword2vec
embeddings. First, word2vec is used to learn a vector representation for each
word in the text representing each node. Then, we compute the Word Mover’s
Distance (WMD) to measure the similarity between two texts representing two
nodes in the graph. Texts that share many words should have smaller distances
than texts with very dissimilar words. WMD has been introduced to measure
thedistancebetweentwotextdocumentsthattakesintoaccountthealignments
betweenwords.Inthispaper,weconsiderthetextassociatedwitheachnodeasa
shortdocument.TheWMDalgorithmﬁndsthevaluesofanauxiliary‘transport’
matrix T, such that Tij describes how much da should be transported to. The
i
WMD learns T to minimize:
(cid:2)n
D(xi,xj)=Tm>i=n0 Tij||xi−xj||p2 (1)
i,j=1
(cid:3) (cid:3)
Subject to: ni,j=1 Tij =dai, ni,j=1 Tij =dbi
where: da and db are the n-dimensional normalized bag-of-vectors for the two
i i
nodes’ texts, xi ∈ Rd is the embedding vector of the ith word and p is usually
set to 1 or 2. The resulted graph is fed afterwards into a two-layers GCN, as
explained next.536 N. A. Khayi and V. Rus
3.2 Graph Convolutional Networks (GCN)
GCN is a recent class of multilayer neural networks that operate on graphs [8,
15]. For every node in the graph, GCN encodes relevant information about its
neighborhoodasareal-valuedfeaturevector.FormallygivenagraphG=(V,E)
where V and E are sets of nodes and edges. Every node is assumed to connect
withitself,i.e., (v,v)∈E foranyv.LetX ∈Rn×m beamatrixcontaining alln
nodes with their features, where m is the dimension of the feature vectors, each
rowxi ∈Risthefeaturevectorforv.Weconsid(cid:3)erAanadjacencymatrixofthe
graph G and its degree matrix D where Dii = jAij. When using GCN with
multiplelayers,theinformationaboutlargerneighborsiscaptured.Followingthe
recommendationofKipfetal.[15]thatmultiplelayersyieldbetterperformance,
weconsidermultiplelayersofGCN.Thenewk-dimensionalnodefeaturematrix
of layer L(j+1)is computed as following:
L(j+1) =p(A˜L(j)Wj) (2)
where A˜ = D−1/2AD−1/2 is the normalized symmetric adjacency matrix and
Wj is a weight matrix and p is an activation matrix and L(0) =X.
3.3 The Classiﬁer
The output of the second GCN layer is fed into a softmax layer as following:
Z =softmax(A˜ReLU(A˜XW0)Wj) (3)
where A˜ = D−1/2AD−1/2 is the normalized symmetric adj(cid:3)acency matrix, Wj,
W0 are weight parameters and softmax(xi) = exp(xi) ÷ iexp(xi). A˜XW0
containstheﬁrstlayerdocumentembeddingsand(A˜ReLU(A˜XW0)Wj)contains
the second layer document embeddings.
4 Experiments
Our experiments were conducted in the context of student generated answers in
responsetohints(intheformofquestions)inconversationalintelligenttutoring
systems. To this end, we have used a previously annotated dataset as described
next.
4.1 DT-Grade Dataset
The DT-Grade dataset [3] was created by extracting student responses from
logged tutorials interactions between 36 junior level college students and a state
oftheartITS.Duringtheinteractions,eachstudentsolved9conceptualphysics
problems – they had to provide the correct answer and a full justiﬁcation based
on Physics principles. Their answer was evaluated and if the answer was incor-
rectorincomplete,e.g.,afulljustiﬁcationwasnotprovided,adialoguefollowedGraph Convolutional Networks for Student Answers Assessment 537
in which the ITS helped the student discover the solution through personalized
scaﬀolding in the form of hints that varied in their degree of information/help
provided. Each annotation instance in the DT-Grade dataset consists of the fol-
lowing attributes: (1) problem description (describes the scenario or context),
(2) tutor question, (3) student answer (as typed by the students, i.e., without
correcting spelling and grammatical errors) and (4) reference answers. In addi-
tion,thedataincludesthecorrectnessclassofeachstudentanswer.Eachstudent
responsewascategorizedbyhumanexpertsintooneofthefollowingfourclasses:
(1) Correct: Answer is correct; (2) Correct-but- incomplete: The response pro-
vided by the student is correct, but something is missing; (3) Incorrect: Student
answer is incorrect; and (4) Contradictory: The student answer is contradicting
with the answer.
Inthiswork,weconsideronlytwoclasses:correctandincorrect.Thecorrect
answers are those labeled as “correct” in the DT-Grade dataset. All the other
instancesareconsidered“incorrect”.Asaresult,weobtainedthefollowingclass
distribution shown in Table2.
Table 2. The distribution of classes in training (800 instances) and testing data (100
instances)
Dataset Correct(%) Incorrect(%)
Training 41 59
Testing 41.59 58.41
4.2 Experimental Setting
Several experiments have been conducted with diﬀerent parameters settings to
evaluate the performance of our proposed method. To this end, we trained and
evaluated a two-layer GCN using the DTGrade dataset. In all experiments, we
trained our model for a maximum of 1000 epochs (training iterations) using
the Categorical Cross Entropy loss function and Adam optimizer [10] with a
learning rate of 0.01. We stopped the training when the validation loss does not
decrease for 100 consecutive epochs, as suggested in prior works [15]. To avoid
overﬁtting,weappliedadropoutrate=0.5.Forthegraphconvolutionlayer,we
usedahiddenlayersizeof16unitswithL2regularizationandReLUactivation.
We selected randomly 600 instances for training, 100 instances for validation,
and 200 instances as an independent test set.
Intheﬁrstsetofexperiments,wehaveusedtheTF-IDFapproachtocompute
theweightoftheDTGradegraphedges.Then,werepeatedtheexperimentwith
the following ﬁlters: 1) local pool ﬁlter [15] which is considered as a baseline
ﬁlter for Graph Convolutional Networks, 2) Chebyshev polynomial ﬁlter [7] and
3) ARMA ﬁlter [5].538 N. A. Khayi and V. Rus
In a second set of experiments, we have used the word2vec embedding with
300 dimension and WMD distance (see Sect.3.1) to compute the weight of the
edges. We report the accuracy of the model using the three ﬁlters.
4.3 Results and Analysis
Table3summarizestheresultsofusingGCNwithdiﬀerentparameterssettings.
Several observations can be made. First, the use of the TF-IDF method to com-
putetheweightsbetweentheedgesoutperformtheword2vecbasedmethodinall
experiments. The highest accuracy obtained with TF-IDF was 73% versus 70%
of the word2vec method. The performance degradation when using the embed-
ding based approach may due to adding some edges between nodes that are not
very related closes. This explains the incorrect assessment of many short stu-
dents’ responses. Added to this, the word2vec embedding based approach may
not propagate label information to the whole graph well in comparison with the
TF-IDFapproach. Second, the empirical results show that ARMA ﬁlter outper-
forms the other polynomial ﬁlters regardless the method used for weighting the
edges in the DTGraph. This is attributed to the implementation strategy of the
ARMA ﬁlter that allows better handling of the graph variations.
The results depicted in Table3 show also that Graph Convolutional Net-
works outperform the previous deep learning models: Transformer [2], Bi-GRU
Capsnet [1], LSTM and Bi-GRU by obtaining the state of the-art results on the
DTGrade dataset. Graph neural networks have been eﬀective at tasks thought
to have rich relational structure and can preserve global structure information
of a graph in graph embeddings.
Table 3. Performance of GCN using binary encoding with diﬀerent ﬁlters
Model Accuracy
GCN (TF-IDF+localpool ﬁlter) 68
GCN (TF-IDF+ Chebyshev ﬁlter) 72
GCN (TF-IDF+ARMA ﬁlter) 73(+0.5)
GCN (word2vec+WMD+ localpool ﬁlter) 62.5
GCN ((word2vec+WMD+chebyshev ﬁlter) 70
GCN ((word2vec+WMD+ARMA ﬁlter) 70
Transformer Encoder+Elmo 71
Bi-GRU+Glove 56.25
LSTM + Glove 60
Bi-GRU Capsnet+ Elmo 72.5Graph Convolutional Networks for Student Answers Assessment 539
5 Conclusion
MotivatedbygoodresultsofapplyingtheGraphConvolutionalNetworks(GCN)
in the NLP, we propose to use a GCN based model to assess the correctness
of student answers in conversational intelligent tutoring systems. This is the
ﬁrst time such model is applied for this task. The results demonstrated the
eﬀectiveness of the proposed model by yielding state of the-art results on the
DT-Gradedataset.Ahighestaccuracyof73%hasbeenachievedwhenusingthe
TF-IDF and the ARMA ﬁlter. As a future direction, we are planning to explore
more novel deep learning models that perform well on a small size of dataset
such as ours.
References
1. Ait Khayi, N., Rus, V.: Bi-GRU Capsnet for student answers assessment. In: The
2019 KDD Workshop on Deep Learning for Education (DL4Ed) in Conjunction
Withthe25thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMin-
ing (KDD 2019), Anchorage, Alaska, USA (2019)
2. Ait Khayi, N., Rus, V.: Attention based transformer for student answers assess-
ment. In: The Flairs-33rd International Conference (2020)
3. Banjade,R.,Maharjan,N.,Niraula,N.B.,Gautam,D.,Samei,B.,Rus,V.:Evalu-
ationdataset(DT-Grade)andwordweightingapproachtowardsconstructedshort
answers assessment in tutorial dialogue context. In: The 11th Workshop on Inno-
vative Use of NLP for Building Educational Applications, pp. 182–187 (2016)
4. Battaglia, P.W., et al: Relational inductive biases, deep learning, and graph net-
work. arXiv preprint arXiv:1806.01261 (2018)
5. Bianchi, F.M., Grattarola, D., Alippi, C., Livi, L.: Graph neural networks with
convolutional ARMA ﬁlters. arXiv preprint arXiv:1901.01343 (2019)
6. Cai, H., Zheng, V.W., Chang, K.: A comprehensive survey of graph embedding
problems, techniques and applications. IEEE Trans. Knowl. Data Eng. 30(9),
1616–1637 (2018)
7. Deﬀerrard,M.,Bresson,X.,Vandergheynst,P.:Convolutionalneuralnetworkson
graphs with fast localized spectral ﬁltering. In: Advances in Neural Information
Processing Systems, pp. 3844–3852 (2016)
8. Duvenaud, D., et al.: Convolutional networks on graphs for learning molecular
ﬁngerprints. In: NIPS (2015)
9. Gong, T., Yao, X.: An attention-based deep model for automatic short answer
score. Int. J. Comput. Sci. Softw. Eng. 8(6), 127–132 (2019)
10. Kingma,D.P.,Ba,J.:Adam:amethodforstochasticoptimization.arXivpreprint
arXiv:1412.6980 (2014)
11. Maharjan, N., Gautam, D., Rus, V.: Assessing free student answers in tutorial
dialogues using LSTM models. In: Penstein Ros´e, C., Mart´ınez-Maldonado, R.,
Hoppe, H.U., Luckin, R., Mavrikis, M., Porayska-Pomsta, K., McLaren, B., du
Boulay, B. (eds.) AIED 2018. LNCS (LNAI), vol. 10948, pp. 193–198. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-93846-2 35
12. Marcheggiani,D.,Titov,I.:Encodingsentenceswithgraphconvolutionalnetworks
for semantic role labeling. In: EMNLP (2017)
13. Rus, V., D’Mello, S.K., Hu, X., Graesser, A.C.: Recent advances in intelligent
tutoring systems with conversational dialogue. AI Mag. 34(3), 42–54 (2013)540 N. A. Khayi and V. Rus
14. Sahu, S.K., Christopoulou, F., Miwa, M., Ananiadou, S.: Inter-sentence rela-
tion extraction with document-level graph convolutional neural network. In: ACL
(2019)
15. Kipf,T.,Welling,M.:Semisupervisedclassiﬁcationwithgraphconvolutionalnet-
works. In: ICLR (2017)
16. Yao,L.,Mao,C.,Luo,Y.:Graphconvolutionalnetworksfortextclassiﬁcation.In:
The AAAI Conference on Artiﬁcial Intelligence, vol. 33, pp. 7370–7377 (2019)
17. Zhang, N., et al.: Long-tail relation extraction via knowledge graph embeddings
and graph convolution networks. In: NAACL-HLT (2019)Author Index
Adelani, David Ifeoluwa 273 Hévrová,Marie 348
Ait Khayi, Nisrine 532 Hlubík,Pavel 418
Alekseev,Anton 222 Hönig,Florian 386
André,Elisabeth 397 Horák, Aleš 112
Aragón,Mario Ezra 231 Hosier, Jordan 312
Argüello-Vélez,Patricia 303
Arias-Vergara, Tomas 303 Illina, Irina 377
Azarova, Irina 122 Ircing, Pavel 214,321
Barančíková, Petra 135
James, Jesin 294
Bayerl, Sebastian P. 386
Berend,Gábor 197 Janicki, Artur 477,513
Jayan, A.R. 71
Berriman, Rebekah 294
Jónsson, Haukur Páll 95
Besacier, Laurent 524
Bodnár,Jan 189 Jůzová,Markéta 340
Boháč,Marek 418
Bojar, Ondřej 135 Kabiri, Arman 153
Bořil,Tomáš 348, 409 Kalfen, Jordan 312
Brito,Celina Iris 495 Kane, Benjamin 487
Keegan, PeterJ. 294
Červa, Petr 426 King, Milton 248
Cook,Paul 153,248 Klakow, Dietrich 265,273
Kleinbauer, Thomas 265
Dahiya,Anirudh 240 Köpke, Barbara 348
Dang,ChiTai 397 Kurfalı,Murathan 79
Davody,Ali 273
Dy,Jilyan Bianca 495 Lehečka,Jan 214,321
Leoni, Chiara 504
Fegyó,Tibor 437 Level, Stephane 377
Ficsor, Tamás 197 Loftsson, Hrafn 95
Fidalgo, Robson 257 López-Monroy, A.Pastor 231
Fohr, Dominique 377
Franco,Natália 257 Macková,Kateřina 171
Funk, Adam 3 Málek,Jiří 366
Manohar, Kavya 71
George,Elizabeth Jasmi 206 Mareček, David 180
Giachanou, Anastasia 30 Marjanović,Saša 61
González, LuisC. 231 Matoušek,Jindřich 446
González-Rátiva,MaríaClaudia 303 Maynard, Diana 3
Gosztolya, Gábor 285 Medveď,Marek 112
Gurbani, VijayK. 312 Mihajlik, Péter 437
Miletic, Aleksandra 61
Hanzlíček,Zdeněk 456 Miller, Gabriel F. 356
Helali, Mossad 265 Mírovský,Jiří 50542 AuthorIndex
Mogadala, Aditya 273 Shields, Isabella 294
Montes-y-Gómez,Manuel 231 Shrivastava, Manish 240,524
Musil, Tomáš 180 Šimko,Marián 162
Símonarson,Haukur Barri 95
Nikolenko, Sergey 222 Šmídl, Luboš 214,321
Nivre, Joakim 11 Snæbjarnarson, Vésteinn 95
Nöth,Elmar 303,331,356 Sowański,Marcin 477
Nouza,Jan 426 Španěl,Martin 418
Steingrímsson,Steinþór 95
Ong,Ethel 495 Stosic, Dejan 61
Orozco-Arroyave, JuanRafael 303,331 Straka, Milan 171
Švec,Jan 214,321
Pal, Vaishali 240, 524 Szaszák, György 437
Paul, Soma 87
Pereira, Jayr 257 Tarján, Balázs 437
Pikuliak,Matúš 162 Terzić, Dušica 61
Platonov, Georgiy 487 Thomas, Aleena 273
Poláková,Lucie 50 Tihelka, Daniel 340
Ponzetto, Simone Paolo 41 Torre, Ilaria 504
Pražák,Aleš 465
Psutka, Josef V. 465 Ulčar, Matej 104
Ureta, Jennifer 495
Radej, Adrian 513
Rajan, Rajeev 71 Vaněk,Jan 465
Reister, Joëlle 386 Vásquez-Correa, JuanCamilo 331, 356
Riedhammer, Korbinian 386 Vercelli, Gianni 504
Rios-Urrego, CristianDavid 331 Veroňková,Jitka 409
Robnik-Šikonja,Marko 104 Vetráb, Mercedes 285
Rosa, Rudolf 180 Vidra, Jonáš 144
Rosso, Paolo 30, 41 Villaluna, Winfred 495
Rus, Vasile 532 Vít,Jakub 456
Vraštil, Michal 446
Sabol, Radoslav 112
Sánchez-Junquera, Javier 41 Watson, CatherineI. 294
Santos, Kyle-Althea 495 Weingartová,Lenka 418
Saxena,Prateek 87 Wülﬁng,Jan-Oliver 397
Schubert, Lenhart 487
Schuster, Maria Elke 303 Žabokrtský, Zdeněk 144,189
Ševčíková,Magda 189 Zakharov, Victor 122
Sharma, Dipti Misra 240 Žďánský, Jindřich 366,426
Sharma, Nikhita 312 Zhang, Guobiao 30