IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.14,NO.5,SEPTEMBER2006 1557
An Overview of Automatic Speaker
Diarization Systems
SueE.Tranter,Member,IEEEandDouglasA.Reynolds,SeniorMember,IEEE
Abstract—Audio diarization is the process of annotating an
input audio channel with information that attributes (possibly
overlapping) temporal regions of signal energy to their speciﬁc
sources. These sources can include particular speakers, music,
backgroundnoisesources,andothersignalsource/channelchar-
acteristics.Diarizationcanbeusedforhelpingspeechrecognition,
facilitating the searching and indexing of audio archives, and
increasingtherichnessofautomatictranscriptions,makingthem
more readable. In this paper, we provide an overview of the
approaches currently used in a key area of audio diarization,
Fig. 1. Example of audio diarization on broadcast news. Annotated
namely speaker diarization, and discuss their relative merits phenomena may include different structural regions such as commercials,
and limitations. Performances using the different techniques are differentacousticeventssuchasmusicornoise,anddifferentspeakers.(Color
comparedwithintheframeworkofthespeakerdiarizationtaskin versionavailableonlineathttp://ieeexplore.ieee.org.)
theDARPAEARSRichTranscriptionevaluations.Wealsolookat
howthetechniquesarebeingintroducedintorealbroadcastnews
systemsandtheirportabilitytootherdomainsandtaskssuchas Ingeneral,aspokendocumentisasingle-channelrecording
meetingsandspeakerveriﬁcation. thatconsists ofmultiple audio sources. Audio sources may be
Index Terms—Speaker diarization, speaker segmentation and different speakers, music segments, types of noise, etc. For
clustering. example, a broadcast news program consists of speech from
differentspeakersaswellasmusicsegments,commercials,and
soundsusedtosegueintoreports(seeFig.1).Audiodiarization
I. INTRODUCTION
is deﬁned as the task of marking and categorising the audio
THE continually decreasing cost of and increasing ac- sources within a spoken document. The types and details of
cess to processing power, storage capacity, and network the audio sources are application speciﬁc. At the simplest,
bandwidth is facilitating the amassing of large volumes of diarization is speech versus nonspeech, where nonspeech is
audio, including broadcasts, voice mails, meetings and other a general class consisting of music, silence, noise, etc., that
“spoken documents.” There is a growing need to apply au- need not be broken out by type. A more complicated diariza-
tomatic human language technologies to allow efﬁcient and tion would further mark where speaker changes occur in the
effective searching, indexing, and accessing of these informa- detectedspeechandassociatesegmentsofspeech(asegmentis
tion sources. Extracting the words being spoken in the audio a section of speech bounded by nonspeech or speaker change
using speech recognition technology provides a sound base points)comingfromthesamespeaker.Thisisusuallyreferred
for these tasks, but the transcripts are often hard to read and toasspeakerdiarization(a.k.a.“whospokewhen”)orspeaker
do not capture all the information contained within the audio. segmentation and clustering and is the focus of most current
Other technologies are needed to extract meta-data which can research efforts in audio diarization. This paper discusses the
make the transcripts more readable and provide context and techniques commonly used for speaker diarization, which
informationbeyondasimplewordsequence.Speakerturnsand allows searching audio by speaker, makes transcripts easier
sentence boundaries are examples of such meta-data, both of to read, and provides information which could be used within
whichhelpprovidearichertranscriptionoftheaudio,making speakeradaptationinspeechrecognitionsystems.Otheraudio
transcripts more readable and potentially helping with other diarization tasks, such as explicitly detecting the presence of
taskssuchassummarization,parsing,ormachinetranslation. music(e.g.,[2]),helpingﬁndthestructure ofa broadcastpro-
gram(e.g.,[3]),orlocatingcommercialstoeliminateunwanted
ManuscriptreceivedOctober11,2005;revisedApril25,2006.Thiswork audio (e.g., [4]), also have many potential beneﬁts but fall
wassupportedbytheDefenseAdvancedResearchProjectsAgencyunderGrant outsidethescopeofthispaper.
MDA972-02-1-0013 and in part by Air Force Contract FA8721-05-C-0002.
There are three primary domains which have been used for
Opinions,interpretations,conclusions,andrecommendationsarethoseofthe
authorsandarenotnecessarilyendorsedbytheU.S.Government.Thispaper speaker diarization research and development: broadcast news
isbasedontheICASSP2005HLTspecialsessionpaper(Philadelphia,PA). audio, recorded meetings, and telephone conversations. The
Theassociateeditorcoordinatingthereviewofthismanuscriptandapproving
datafromthesedomainsdiffersinthequalityoftherecordings
itforpublicationwasDr.JohnMakhoul.
S.TranteriswiththeEngineeringDepartment,CambridgeUniversity,Cam- (bandwidth, microphones, noise), the amount and types of
bridgeCB21PZ,U.K.(e-mail:sej28@eng.cam.ac.uk). nonspeech sources, the number of speakers, the durations and
D.ReynoldsiswiththeLincolnLaboratory,MassachusettsInstituteofTech-
sequencing of speaker turns, and the style/spontaneity of the
nology,Lexington,MA02420-9185USA(e-mail:dar@ll.mit.edu).
DigitalObjectIdentiﬁer10.1109/TASL.2006.878256 speech. Each domain presents unique diarization challenges,
1558-7916/$20.00©2006IEEE1558 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.14,NO.5,SEPTEMBER2006
althoughoftenhigh-levelsystemtechniquestendtogeneralize
well over several domains [5], [6]. The NIST Rich Transcrip-
tionspeakerevaluations[7]haveprimarilyusedbothbroadcast
newsandmeetingdata,whereastheNISTspeakerrecognition
evaluations [8] have primarily used conversational telephone
speechwithsummedsides(a.k.atwo-wire).
Thediarizationtaskisalsodeﬁnedbytheamountofspeciﬁc
prior knowledge allowed. There may be speciﬁc prior knowl-
edge via example speech from the speakers in the audio, such
as in a recording of a regular staff meeting. The task then be-
comes more like speaker detection or tracking tasks [9]. Spe-
ciﬁcpriorknowledgecouldalsobeexamplespeechfromjusta
fewofthespeakerssuchascommonanchorsonparticularnews
stations,orknowledgeofthenumberofspeakersintheaudio,
perhapsforateleconferenceoveraknownnumberoflines,or
maybethestructureoftheaudiorecording(e.g.,musicfollowed
Fig. 2. Prototypical diarization system. Most diarization systems have
bystory).Mostofthispriorknowledgehasbeenusedtoimprove
components to perform speech detection, gender and/or bandwidth
diarizationperformancealthoughnotallofithasprovedbeneﬁ- segmentation, speaker segmentation, speaker clustering, and ﬁnal
cialwithincurrentsystems.[10].However,foramoreportable resegmentationorboundaryreﬁnement.
speakerdiarizationsystem,itisdesiredtooperatewithoutany
speciﬁc prior knowledge of the audio. This is the general task speech/nonspeechmodelssuchasin[11],while[12]issimilar
deﬁnitionusedintheRichTranscriptiondiarizationevaluations, butfourspeechmodelsareusedforthepossiblegender/band-
whereonlythebroadcasteranddateofbroadcastareknownin widthcombinations.Noiseandmusicareexplicitlymodeledin
addition to having the audio data and we adopt this scenario [13]–[15] which haveclasses for speech, music, noise,speech
whendiscussingspeakerdiarizationsystems. music,andspeech noise,while[16]and[17]usewideband
The aim of this paper is to provide an overview of current speech, narrowband speech, music and speech music. The
speakerdiarizationapproachesandtodiscussperformanceand extra speech xx models are used to help minimize the false
potential applications. In Section II, we outline the general rejectionofspeechoccurringinthepresenceofmusicornoise,
framework of diarization systems and discuss different im- andthisdataissubsequentlyreclassiﬁedasspeech.Theclasses
plementations of the key components within current systems. can also be broken down further, as in [18], which has eight
Performance is measured in terms of the diarization error rate models in total, ﬁve for nonspeech (music, laughter, breath,
(DER) using the DARPA EARS Rich Transcription Fall 2004 lip-smack,andsilence)andthreeforspeech(vowelsandnasals,
(RT-04F)speakerdiarizationevaluationdata.SectionIVlooks fricatives,andobstruents).
at the use of these methods in real applications and the future When operating on unsegmented audio, Viterbi segmenta-
directionsfordiarizationresearch. tion,(singlepassoriterativewithoptionaladaptation)usingthe
modelsisemployedtoidentifyspeechregions.Ifaninitialseg-
II. DIARIZATIONSYSTEMFRAMEWORK mentationisalreadyavailable(forexample,theorderingofthe
keycomponentsmayallowchangepointdetectionbeforenon-
Inthissection,wereviewthekeysubtasksusedtobuildcur-
speechremoval),eachsegmentisindividuallyclassiﬁed.Min-
rentspeakerdiarizationsystems.Mostdiarizationsystemsper-
imumlengthconstraints[11],[18]andheuristicsmoothingrules
form these tasks separately, although it is possible to perform
[12],[15]mayalsobeapplied.Analternativeapproachwhich
some of the stages jointly (for example speaker segmentation
doesnotuseViterbidecoding,butinsteadabestmodelsearch
andclustering)andtheorderingofthestagesoftenvariesfrom
withmorphologicalrulesisdescribedin[19].
systemtosystem.Aprototypicalcombinationofthekeycom-
Silencecanberemovedinthisearlystage,usingaphonerec-
ponentsofadiarizationsystemisshowninFig.2.Foreachtask,
ognizer(asin[17])orenergyconstraint,orinaﬁnalstagepro-
weprovideabriefdescriptionofthecommonapproachesem-
cessingusingawordrecognizer(asin[14])orenergyconstraint
ployedandsomeoftheissuesinapplyingthem.
(asintheMITsystemforRT-03[20]).Regionswhichcontain
commercialsandthusareofnointerestfortheﬁnaloutputcan
A. SpeechDetection
also be automatically detected and removed at this early stage
The aim of this step is to ﬁnd the regions of speech in the [4],[20]
audiostream.Dependingonthedomaindatabeingused,non- For broadcast news audio, speech detection performance is
speechregionstobediscardedcanconsistofmanyacousticphe- typicallylessthan1%miss(speechinreferencebutnotinthe
nomenasuchassilence,music,roomnoise,backgroundnoise, hypothesis) and 1%–2% false alarm (speech in the hypothesis
orcross-talk. butnotinthereference),whereasformeetingaudio,theﬁgures
The general approach used is maximum-likelihood classi- aretypicallyaround1%higherforboth.Whenthespeechdetec-
ﬁcation with Gaussian mixture models (GMMs) trained on tionphaseisrunearlyinasystem,ortheoutputisrequiredfor
labeled training data, although different class models can be further processing such as for transcription, it is more impor-
used,suchasmultistateHMMs.Thesimplestsystemusesjust tant to minimize speech miss than false alarm rates, since theTRANTERANDREYNOLDS:OVERVIEWOFAUTOMATICSPEAKERDIARISATIONSYSTEMS 1559
formerareunrecoverableerrorsinmostsystems.However,the distance metric. The peaks in the distance function are then
DER, used to evaluate speaker diarization performance, treats found and deﬁne the change points if their absolute value ex-
bothformsoferrorequally. ceeds a predetermined threshold chosen on development data.
For telephone audio, typically some form of standard en- Smoothing the distance distribution or eliminating the smaller
ergy/spectrum-based speech activity detection is used since of neighboring peaks within a certain minimum duration pre-
nonspeech tends to be silence or noise sources, although the vents the system overgenerating change points at true bound-
GMM approach has also been successful in this domain with aries.SingleGaussiansaregenerallypreferredtoGMMsdueto
single-channel[21]orcross-channel[22]classes.Formeeting the simpliﬁed distance calculations. Typical window sizes are
audio, the nonspeech can be from a variety of noise sources, 1–2or2–5swhenusingadiagonalorfullcovarianceGaussian,
likepapershufﬂing,coughing,laughing,etc.andenergy-based respectively.AswithBIC,thewindowlengthconstrainsthede-
methods do not currently work well for distant microphones tectionofshortturns.
[23],[24],sousingasimplepretrainedspeech/nonspeechGMM Sincethechangepointdetectionoftenonlyprovidesaninitial
isgenerallypreferred[6],[25],[23].Aninterestingalternative basesegmentationfordiarizationsystems,whichwillbeclus-
uses a GMM, built on the normalized energy coefﬁcients of teredandoftenresegmentedlater,beingabletorunthechange
thetestdata,todeterminehowmuchnonspeechtoreject[24], pointdetectionveryfast(typicallylessthan0.01 foradi-
whilepreliminaryworkin[6]showspotentialforthefuturefor agonalcovariancesystem)isoftenmoreimportantthananyper-
anewenergy-basedmethod.Whensupported,multiplechannel formancedegradation.Infact,[11]and[19]foundnosigniﬁcant
meeting audio can be used to help speech activity detection performance degradation when using a simple initial uniform
[26]. This problem is felt to be so important in the meetings segmentationwithintheirsystems.
domainthataseparateevaluationforspeechactivitydetection Both change detection techniques require a detection
was introducedinthe spring 2005Rich Transcription meeting thresholdtobeempiricallytunedforchangesinaudiotypeand
evaluation[27]. features. Tuning the change detector is a tradeoff between the
desires to have long, pure segments to aid in initializing the
B. ChangeDetection clustering stage, and minimizing missed change points which
The aim of this step is to ﬁnd points in the audio stream producecontaminationsintheclustering.
likely to be change points between audio sources. If the input Alternatively,orinaddition,awordorphonedecodingstep
tothisstageistheunsegmentedaudiostream,thenthechange with heuristic rules may be used to help ﬁnd putative speaker
detectionlooksforbothspeakerandspeech/nonspeechchange change points such as in [18] and the Cambridge 1998–2003
points. If a speech detector or gender/bandwidth classiﬁer has systems [16], [20]. However, this approach can over-segment
beenrunﬁrst,thenthechangedetectorlooksforspeakerchange the speech data and requires some additionalmerging orclus-
pointswithineachspeechsegment. teringtoformviablespeechsegments,andcanmissboundaries
Two main approaches have been used for change detection. infastspeakerinterchangesifrelyingonthepresenceofsilence
They both involve looking at adjacent windows of data and orgenderchangesbetweenspeakers.
calculating a distance metric between the two, then deciding
C. Gender/BandwidthClassiﬁcation
whether the windows originate from the same or a different
source. The differences between them lie in the choice of dis- The aim of this stage is to partition the segments into
tancemetricandthresholdingdecisions. commongroupingsofgender(maleorfemale)andbandwidth
The ﬁrst general approach used for change detection, used (low-bandwidth: narrow-band/telephone or high-bandwidth:
in [15], is a variation on the Bayesian information criterion studio). This is done to reduce the load on subsequent clus-
(BIC)techniqueintroducedin[28].Thistechniquesearchesfor tering, provide more ﬂexibility in clustering settings (for
change points within a window using a penalized likelihood examplefemalespeakersmayhavedifferentoptimalparameter
ratio test of whether the data in the window is better modeled settings to male speakers), and supply more side information
byasingledistribution(nochangepoint)ortwodifferentdis- about the speakers in the ﬁnal output. If the partitioning can
tributions (change point). If a change is found, the window is be done very accurately and assuming no speaker appears in
resettothechangepointandthesearchrestarted.Ifnochange the same broadcast in different classes (for example both in
pointisfound,thewindowisincreasedandthesearchisredone. the studio and via a prerecorded ﬁeld report) then performing
SomeoftheissuesinapplyingtheBICchangedetectorareas this partitioning early on in the system can also help improve
follows.1)Ithashighmissratesondetectingshortturns( 2–5 performance while reducing the computational load [33]. The
s),socanbeproblematictouseonfastinterchangespeechlike potential drawback in this partitioning stage, however, is if
conversations. 2) The full search implementation is computa- a subset of a speaker’s segments is misclassiﬁed the errors
tionally expensive (order ), so most systems employ some can be unrecoverable, although it is possible to allow these
formofcomputationreductions(e.g.,[29]). classiﬁcationstochangeinasubsequentresegmentationstage,
Asecondtechniqueusedﬁrstin[30]andlaterin[13],[17], suchasin[19].
and[31]usesﬁxed-lengthwindowsandrepresentseachwindow Classiﬁcation for both gender and bandwidth is typically
byaGaussianandthedistancebetweenthembytheGaussian done using maximum-likelihood classiﬁcation with GMMs
Divergence (symmetric KL-2 distance). The step-by-step im- trained on labeled training data. Either two classiﬁers are run
plementation in [19] and system for telephone audio in [32] (one for gender and one for bandwidth) or joint models for
are similar but use the generalized log likelihood ratio as the gender and bandwidth are used. This can be done either in1560 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.14,NO.5,SEPTEMBER2006
conjunction with the speech/nonspeech detection process or parent cluster in the penalty factor represents a “local”
aftertheinitialsegmentation.Bandwidthclassiﬁcationcanalso BICdecision,i.e.,justconsideringtheclustersbeingcombined.
be done using a test on the ratio of spectral energy above and Thishasbeenshowntoperformbetterthanthecorresponding
below 4 kHz. An alternative method of gender classiﬁcation, “global”BICimplementationwhichusesthenumberofframes
usedin[17],alignsthewordrecognitionoutputofafast ASR inthewholeshow instead[20],[31],[36].
system with gender dependent models and assigns the most Slight variations of this technique have also been used. For
likelygendertoeachsegment.Thishasahighaccuracybutis example,thesystemdescribedin[18]usesessentiallythelocal
unnecessarily computationally expensive if a speech recogni- BIC score (with the number of parameters term incorporated
tionoutputisnotalreadyavailableandsegmentsideallyshould withinthepenaltyweight),butsetsdifferentthresholdsforpo-
beofareasonablesize(typicallybetween1and30s).Gender tentialboundariesoccurringduringspeechornonspeech,moti-
classiﬁcation error rates are around 1%–2% and bandwidth vated by an observation that most true speaker change points
classiﬁcationerrorratesarearound3%–5%forbroadcastnews occurred during nonspeech regions. A further example, used
audio. in the system described in [11] and [37] removes the need for
tuningthepenaltyweight ondevelopmentdata,byensuring
D. Clustering thatthenumberofparameters inthemergedandseparate
distributionsareequal,althoughthebasenumberofGaussians
Thepurposeofthisstageistoassociateorclustersegments
and,hence,numberoffreeparametersneedstobechosencare-
fromthesamespeakertogether.Theclusteringideallyproduces
fullyforoptimaleffect.Alternativestothepenaltyterm,suchas
oneclusterforeachspeakerintheaudiowithallsegmentsfrom
usingaconstant[38],theweightedsumofthenumberofclus-
a given speaker in a single cluster. The predominant approach
ters and number of segments [13], or a penalized determinant
usedindiarizationsystemsishierarchical,agglomerativeclus-
ofthewithin-clusterdispersionmatrix[34],[39]havealsohad
teringwithaBICbasedstoppingcriterion[28]consistingofthe
moderatesuccess,buttheBICmethodhasgenerallysuperseded
followingsteps:
these.AddingaViterbiresegmentationbetweenmultipleitera-
0) initializeleafclustersoftreewithspeechsegments;
tionsofclustering[31]orwithinasingleiteration[11]hasalso
1) computepair-wisedistancesbetweeneachcluster;
been used to increase performance at the penalty of increased
2) mergeclosestclusters;
computationalcost.
3) updatedistancesofremainingclusterstonewcluster;
An alternative approach described in [40] uses a Euclidean
4) iteratesteps1)–3)untilstoppingcriterionismet.
distancebetweenMAP-adaptedGMMsandnotesthisishighly
Theclustersaregenerallyrepresentedbyasinglefullcovari-
correlated with a Monte Carlo estimation of the Gaussian Di-
anceGaussian[5],[12],[15],[17],[31],[34],butGMMshave
vergence(symmetricKL-2)distancewhilealsobeinganupper
also been used [11], [19], [35], sometimes being built using
boundtoit.Thestoppingcriterionusesaﬁxedthreshold,chosen
mean-only MAP adaptation of a GMM of the entire test ﬁle
on the development data, on the distance metric. The perfor-
toeachclusterforincreasedrobustness.Thestandarddistance
manceiscomparabletothemoreconventionalBICmethod.
metric between clusters is the generalized likelihood ratio
A further method described in [15] uses “proxy” speakers.
(GLR). It is possible to use other representations or distance
Asetofproxymodelsisappliedtomapsegmentsintoavector
metrics, but these have been found the most successful within
space, then a Euclidean distance metric and an ad hoc occu-
theBICclusteringframework.Thestoppingcriterioncompares
pancy stopping criterion are used, but the overall clustering
theBICstatisticfromthetwoclustersbeingconsidered, and
framework remains the same. The proxy models can be built
,withthatoftheparentcluster, ,shouldtheybemerged,the
by adapting a universal background model (UBM) such as a
formulationbeingforthefullcovarianceGaussiancase
128 mixture GMM to the test data segments themselves, thus
making the system portable to different shows and domains
while still giving consistent performance gain over the BIC
method.
Regardless of the clustering employed, the stopping crite-
rion is critical to good performance and depends on how the
output is to be used. Under-clustering fragments speaker data
over several clusters, while over-clustering produces contam-
inated clusters containing speech from several speakers. For
where is the number of free parameters, the number of indexing information by speaker, both are suboptimal. How-
frames, the covariance matrix, and the dimension of the ever, when using cluster output to assist in speaker adaptation
featurevector.(see,e.g.,[20]foramorecompletederivation.) of speech recognition models, under-clustering may be suit-
Ifthepairofclustersarebestdescribedbyasinglefullcovari- able when a speaker occurs in multiple acoustic environments
anceGaussian,the willbelow,whereasiftherearetwo andover-clusteringmaybeadvantageousinaggregatingspeech
separatedistributions,implyingtwospeakers,the willbe fromsimilarspeakersoracousticenvironments.
high.Foreachstep,thepairofclusterswiththelowest is
E. JointSegmentationandClustering
mergedandthestatisticsarerecalculated.Theprocessisgener-
allystoppedwhenthelowest isgreaterthanaspeciﬁed An alternative approach to running segmentation and clus-
threshold, usually 0. The use of the number of frames in the teringstagesseparatelyistouseanintegratedscheme.ThiswasTRANTERANDREYNOLDS:OVERVIEWOFAUTOMATICSPEAKERDIARISATIONSYSTEMS 1561
ﬁrstdonein[13]byemployingaViterbidecodebetweeniter- meanandvariancenormalization[15]andfeaturewarping[44]
ations of agglomerative clustering, but an initial segmentation usingaslidingwindowof3s[14],[17].Thelattermethodhad
stage was still required. A more recent completely integrated previously been found by one study to be more effective than
scheme,basedonanevolutive-HMM(E-HMM)wheredetected otherstandard normalization techniqueson a speakerveriﬁca-
speakershelpinﬂuenceboththedetectionofotherspeakersand tion task on cellular data [45]. In [17], it was found the fea-
the speaker boundaries, was introduced in [41] and developed ture normalization was necessary to get signiﬁcant gain from
in [19] and [42]. The recording is represented by an ergodic theclusterrecombinationtechnique.
HMM in which each state represents a speaker and the tran- When the clusters are merged, a new speaker model can be
sitionsmodelthechangesbetweenspeakers.TheinitialHMM trainedwiththecombineddataanddistancesupdated(asin[14]
containsonlyonestateandrepresentsallofthedata.Ineachit- and[17])orstandardclusteringrulescanbeusedwithastatic
eration,ashortspeechsegmentassumedtocomefromanonde- distancematrix(asin[15]).Thisrecombinationcanbeviewed
tectedspeakerisselectedandusedtobuildanewspeakermodel asfusingintra-andinter-[43]audioﬁlespeakerclusteringtech-
byBayesianadaptationofaUBM.Astateisthenaddedtothe niques. On the RT-04F evaluation it was found that this stage
HMMtoreﬂectthisnewspeaker,andthetransitionsprobabili- signiﬁcantlyimprovesperformance,withfurtherimprovements
tiesaremodiﬁedaccordingly.Anewsegmentationisthengen- beingobtainedsubsequentlybyusingavariableprioriterative
eratedfromaViterbidecodeofthedatawiththenewHMM,and MAPapproachforadaptingtheUBMs,andbuildingnewUBMs
eachmodelisadaptedusingthenewsegmentation.Thisreseg- includingallofthetestdata[17].
mentation phase is repeated until the speaker labels no longer
change. The process of adding new speakers is repeated until G. Resegmentation
thereisnogainintermsofcomparablelikelihoodorthereisno
Thelaststagefoundinmanydiarizationsystemsisareseg-
datalefttoformanewspeaker.Themainadvantagesofthisin-
mentationoftheaudioviaViterbidecoding(withorwithoutit-
tegratedapproacharetousealltheinformationateachstepand
erations)usingtheﬁnalclustermodelsandnonspeechmodels.
to allow the use of speaker recognition-based techniques, like
Thepurposeofthisstageistoreﬁnetheoriginalsegmentbound-
BayesianadaptationofthespeakermodelsfromaUBM.
ariesand/ortoﬁllinshortsegmentsthatmayhavebeenremoved
formorerobustprocessingintheclusteringstage.Filteringthe
F. ClusterRecombination
segment boundaries using a word or phone recognizer output
In this relatively recent approach [31], state-of-the-art canalsohelpreducethefalsealarmcomponentoftheerrorrate
speaker recognition modeling and matching techniques are [31].
used as a secondary stage for combining clusters. The signal
processing and modeling used in the clustering stage of Sec-
H. FindingIdentities
tionII-Dareusuallysimple:nochannelcompensation,suchas
RASTA,since we wish to take advantageof common channel Althoughcurrentdiarizationsystemsareonlyevaluatedusing
characteristicsamongaspeaker’ssegments,andlimitedparam- “relative”speakerlabels(suchas“spkr1”),itisoftenpossibleto
eter distribution models, since the model needs to work with ﬁndthetrueidentitiesofthespeakers(suchas“TedKoppel”).
smallamountsofdataintheclustersatthestart. Thiscanbeachievedbyavarietyofmethods,suchasbuilding
Withclusterrecombination,clusteringisruntounder-cluster speaker models for people who are likely to be in the news
theaudiobutstillproduceclusterswithareasonableamountof broadcasts(suchasprominentpoliticiansormainnewsanchors
speech s . AUBMis builtontraining datatorepresent and reporters)and includingthese models inthe speakerclus-
generalspeakers.Bothstaticanddeltacoefﬁcientsareusedand teringstageorrunningspeaker-trackingsystems.
featurenormalizationisappliedtohelpreducetheeffectofthe Analternativeapproach,introducedin[46],useslinguisticin-
acousticenvironment.Maximumaposteriori(MAP)adaptation formationcontainedwithinthetranscriptionstopredictthepre-
(usually mean-only) is then applied on each cluster from the vious,current,ornextspeaker.Rulesaredeﬁnedbasedoncat-
UBM to form a single model per cluster. The cross likelihood egoryandwordN-gramschosenfromthetrainingdata,andare
ratio(CLR)betweenanytwogivenclustersisdeﬁned[31],[43] thenappliedsequentiallyonthetestdatauntilthespeakernames
havebeenfound.Blockingrulesareusedtostoprulesﬁringin
certaincontexts,forexample,thesequence“[name]reports ”
assignsthenextspeakertobe[name]unless istheword“that.”
Anextensionofthissystemdescribedin[47],learnsmanyrules
where is the average likelihood per frame of data and their associated probability of being correct automatically
giventhemodel .ThepairofclusterswiththehighestCLR fromthetrainingdataandthenappliesthesesimultaneouslyon
ismergedandanewmodeliscreated.Theprocessisrepeated the test data using probabilistic combination. Using automatic
until the highest CLR is below a predeﬁned threshold chosen transcriptions and automatically found speaker turns naturally
from development data. Because of the computational load at degrades performance but potentially 85% of the time can be
this stage, each gender/bandwidth combination is usually pro- correctlyassignedtothetruespeakeridentityusingthismethod.
cessedseparately,whichalsoallowsmoreappropriateUBMsto Although primarily used for identifying the speaker names
beusedforeachcase. givenasetofspeakerclusters,thistechniquecanassociatethe
Differenttypesoffeaturenormalizationhavebeenusedwith samenameformorethanoneinputclusterand,therefore,could
this process,namely RASTA-ﬁlteredcepstrawith10-s feature bethoughtofasahigh-levelcluster-combinationstage.1562 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.14,NO.5,SEPTEMBER2006
I. CombiningDifferentDiarizationMethods thenscoredagainstreference“ground-truth”speakersegmenta-
tionwhichisgeneratedusingtherulesgivenin[52].Sincethe
Combining methods used in different diarization systems
hypothesisspeakerlabelsarerelative,theymustbematchedap-
could potentially improve performance over the best single
propriatelytothetruespeakernamesinthereference.Toaccom-
diarization system. It has been shown that the word error rate
plishthis,aone-to-onemappingofthereferencespeakerIDsto
(WER) of an automatic speech recognizer can be consistently
thehypothesisspeakerIDsisperformedsoastomaximizethe
reducedwhencombiningmultiplesegmentationsevenifthein-
totaloverlapofthereferenceand(corresponding)mappedhy-
dividualsegmentationsthemselvesdonotofferstate-of-the-art
pothesisspeakers.Speakerdiarizationperformanceisthenex-
performance in either DER or resulting WER [48]. Indeed, it
pressed in terms of the miss (speaker in reference but not in
seems that diversitybetween the segmentation methods is just
hypothesis),falsealarm(speakerinhypothesisbutnotinrefer-
asimportantasthesegmentationqualitywhenbeingcombined.
ence), and speaker-error (mapped reference speaker is not the
ItisexpectedthatgainsinDERarealsopossiblebycombining
sameasthehypothesizedspeaker)rates.TheoverallDERisthe
differentdiarizationmodulesorsystems.
sumofthesethreecomponents.Acompletedescriptionofthe
Several methods of combining aspects of different diariza-
evaluationmeasureandscoringsoftwareimplementingitcanbe
tion systems have been tried, for example the “hybridization”
foundathttp://nist.gov/speech/tests/rt/rt2004/fall.
or“piped”CLIPS/LIAsystemsof[35]and[49]andthe“plug
Itshouldbenotedthatthismeasureistime-weighted,sothe
andplay”CUED/MIT-LLsystemof[20]whichbothcombine
DERisprimarilydrivenby(relativelyfew)loquaciousspeakers
components of different systems together. A more integrated
and it is, therefore, more important to get the main speakers
mergingmethodisdescribedin[49],while[35]describesaway
completeandcorrectthantoaccuratelyﬁndspeakerswhodonot
of using the 2002 NIST speaker segmentation error metric to
speakmuch.Thisscenariomodelssometasks,suchastracking
ﬁnd regions in two inputs which agree and then uses these to
anchorspeakersinbroadcastnewsfortextsummarization,but
trainpotentiallymoreaccuratespeakermodels.Thesesystems
theremaybeothertasks(suchasforspeakeradaptationwithin
generallyproduceperformancegains,buttendtoplacesomere-
automatictranscription,orascertainingtheopinionsofseveral
strictiononthesystemsbeingcombined,suchastherequiredar-
speakersinaquickdebate)forwhichitislessappropriate.The
chitectureorequalizingthenumberofspeakers.Analternative
same formulation can be modiﬁed to be speaker weighted in-
approachintroducedin[50]usesa“clustervoting”techniqueto
stead of time weighted if necessary, but this is not discussed
comparetheoutputofarbitrarydiarizationsystems,maintaining
here.Theutilityofeitherweightingdependsontheapplication
areasofagreementandvotingusingconﬁdencesoranexternal
ofthediarizationoutput.
judgingschemeinareasofconﬂict.
J. SequentialSpeakerClustering B. Data
Forsomeapplications,itcanbeimportanttoproducespeaker TheRT-04Fspeakerdiarizationdataconsistsofone30-min
labels immediately without collecting all of the potential data extractfrom12differentU.S.broadcastnewsshows.Thesewere
fromaparticularscenario,forexamplereal-timecaptioningof derivedfromTVshows:threefromABC,threefromCNN,two
abroadcastnewsshow.Thisconstraintpreventsthestandardhi- from CNBC, two from PBS, one from CSPAN, and one from
erarchicalclusteringtechniquesbeingused,andinsteadrequires WBN.Thestyleofshowvariedfromasetoflecturesfromafew
theclusteringtobeperformedsequentiallyoronline.Anelegant speakers(CSPAN)torapidheadlinenewsreporting(CNNHead-
solutiontothis,describedin[34],takesthesegmentsinturnand lineNews).Detailsoftheexactcompositionofthedatasetscan
decidesiftheymatchanyoftheexistingspeakerclustersusing befoundin[52].
thresholdsondistancemetricsbasedonthe generalizedlikeli-
hoodratioandapenalizedwithin-clusterdispersion.Ifamatch C. Results
isfound,thestatisticsofthematchedclusterareupdatedusing
The results from the main diarization techniques are shown
thenewsegmentinformation,whereasifnomatchisfound,the
in Fig. 3. Using a top-down clustering approach with full
segmentstartsanewspeakercluster.Thisprocessismuchfaster
covariance models, arithmetic harmonic sphericity (AHS)
thantheconventionalhierarchicalapproach,particularlywhen
distancemetricand BIC stopping criteriongavea DER ofbe-
therearealargenumberofinitialsegments,andhasbeenused
tween20.5%and22.5%[38].Thecorrespondingperformance
for both ﬁnding speaker turns [34] and for speaker adaptation
on the six-show RT diarization development data sets ranged
withinareal-timespeechrecognitionframework[51].
from 15.9% to 26.9%, showing that the top-down method
seems more unpredictable than the agglomerative method.
III. EVALUATIONOFPERFORMANCE
This is thought to be because the initial clusters contain many
InthissectionwebrieﬂydescribetheNISTRT-04Fspeaker speakers and segments may thus be assigned incorrectly early
diarizationevaluationandpresenttheresultswhenusingthekey on,leading toan unrecoverableerror.In contrast,the agglom-
techniques discussed in this paper on the RT-04F diarization erative scheme grows clusters from the original segments and
evaluationdata. should not contain impure multispeaker clusters until very
late in the clustering process. The agglomerative clustering
A. SpeakerDiarizationErrorMeasure
BIC-based scheme got around 17%–18% DER [11], [15],
A system hypothesizes a set of speaker segments each of [17], [31], with Viterbi resegmentation between each step
whichconsistsofa(relative)speaker-idlabelsuchas“Mspkr1” providingaslightbeneﬁtto16.4%[11].Furtherimprovements
or“Fspkr2”andthecorrespondingstartandendtimes.Thisis to around 13% were made using CLR cluster recombinationTRANTERANDREYNOLDS:OVERVIEWOFAUTOMATICSPEAKERDIARISATIONSYSTEMS 1563
Fig. 5. Accessing broadcast news audio from automatically derived
speaker names via a wavesurfer plug-in. (Color version available online at
Fig. 3. DERs for different methods on the RT-04F evaluation data. Each http://ieeexplore.ieee.org.)
dot represents a different version of a system built using the indicated core
technique. E-HMMnottunedontheU.S.broadcastnewsdevelopmentsets.
(Colorversionavailableonlineathttp://ieeexplore.ieee.org.) Reducing this variability is a source of ongoing work. Certain
techniquesorparametersettingscanperformbetterfordifferent
styles of show. Systems may potentially be improved by ei-
ther automatically detecting the type of show and modifying
thechoiceoftechniquesorparametersaccordingly,orbycom-
biningdifferentsystemsdirectlyasdiscussedinSectionII-A.
IV. CONCLUSIONANDFUTUREDIRECTIONS
There has been tremendous progress in task deﬁnition, data
availability, scoring measures, and technical approaches for
speaker diarization over recent years. The methods used in
broadcastnewsdiarizationarenowbeingdeployedacrossother
domains, for example, improving speaker recognition perfor-
manceusingmultispeakertrainandtestdatainconversational
telephonespeech[15]andﬁndingspeakerswithinmeetingdata
[6],[23]–[25],[53].Thelattersometimescontainsanadditional
stage when multiple microphones are present to either select
the most prominent microphone [53] or to weight the audio
signal from multiple microphones to form a single “superior”
signal before further processing [6], [24]. Different methods
Fig. 4. DER per show for the system with lowest DER. There is a large for obtaining these channel weights have been tried including
variabilitybetweenthedifferentstylesofshow.(Colorversionavailableonline equalweighting[6],[24],usingsignal-to-noiseratioestimates
athttp://ieeexplore.ieee.org.)
[6], [24], using the correlation between different channels [6],
and “delay-and-sum” beamforming [6]. However, the other
andresegmentation[15].TheCLRclusterrecombinationstage
components of the systems generally match those used in
which included feature warping produced a further reduction
broadcast news with only the pretrained models and in some
to around 8.5%–9.5% [14], [17], and using the whole of the
cases parameters being changed to reﬂect the new domain.
RT-04F evaluation data in the UBM build of the CLR cluster
Indeed,[6] is working toward the goal ofcomplete portability
recombination stage gave a ﬁnal performance of 6.9% [17].
betweendomainsbytryingtoremovedomain-speciﬁcmodels
The proxy model technique performed better than the equiva-
andparameterscompletely.
lent BIC stages, giving 14.1% initially and 11.0% after CLR
Othertasks,suchasﬁndingtruespeakeridentities(seeSec-
cluster recombination and resegmentation [15]. The E-HMM
tion II-H) or speaker tracking (e.g., in [54]) are increasingly
system, despite not being tuned on the U.S. broadcast news
using diarizationoutputasa startingpointand performanceis
developmentsets,gave16.1%DER.
approachingalevelwhereitcanbeconsidered“useful”forreal
ForthesystemwiththelowestDER,theper-showresultsare
humaninteraction.
given in Fig. 4. Typical of most systems, there is a large vari-
Additionstoapplicationswhichdisplayaudioandoptionally
abilityinperformanceovertheshows,reﬂectingthevariability
transcriptions,suchaswavesurfer1 (seeFig.5) or transcriber2
inthenumberofspeakers,thedominanceofspeakers,andthe
styleandstructureofthespeech.Mostofthevariabilityisfrom 1Wavesurferisavailablefromwww.speech.kth.se/wavesurfer
the speaker error component due to over or under clustering. 2Transcriberisavailablefromhttp://trans.sourceforge.net/1564 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.14,NO.5,SEPTEMBER2006
[6] X.Anguera,C.Wooters,B.Peskin,andM.Aguiló,“Robustspeaker
segmentation for meetings: The ICSI-SRI Spring 2005 Diarization
System,”inProc.MachineLearningforMultimodalInteractionWork-
shop(MLMI),Edinburgh,U.K.,Jul.2005,pp.402–414.
[7] BenchmarkTests:RichTranscription(RT).NIST.[Online].Available:
http://www.nist.gov/speech/tests/rt/
[8] Benchmark Tests: Speaker Recognition. NIST. [Online]. Available:
http://www.nist.gov/speech/tests/spk/
[9] A.MartinandM.Przybocki,“Speakerrecognitioninamulti-speaker
environment,”inProc.Eur.Conf.SpeechCommun.Technol.,vol.2,Aal-
borg,Denmark,Sep.2001,pp.787–790.
[10] D.Moraru,L.Besacier,andE.Castelli,“Usinga-prioriinformationfor
speakerdiarization,”inProc.OdysseySpeakerandLanguageRecogni-
tionWorkshop,Toledo,Spain,May2004,pp.355–362.
[11] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Toward Robust
speaker segmentation: The ICSI-SRI Fall 2004 Diarization System,”
in Proc. Fall 2004 Rich Transcription Workshop (RT-04), Palisades,
NY,Nov.2004,[Online].Available:http://www.icsi.berkeley.edu/cgi-
Fig. 6. Diarization information, including automatically found speaker bin/pubs/publication.pl?ID=000100.
identities, being used in the “Transcriber” tool to improve readability [12] P. Nguyen, L. Rigazio, Y.Moh,and J. C. Junqua.Rich transcription
and facilitate searching by speaker. (Color version available online at 2002sitereport.Panasonicspeechtechnologylaboratory(PSTL).pre-
http://ieeexplore.ieee.org.) sentedatProc.RichTranscriptionWorkshop(RT-02).[Online].Avail-
able:http://www.nist.gov/speech/tests/rt/rt2002/presentations/rt02.pdf
(seeFig.6),andtheinclusionincompleteretrievalsystemssuch [13] J.-L.Gauvain,L.Lamel,andG.Adda,“Partitioningandtranscriptionof
as Rough ’n’ Ready [55] and SpeechFind [56] allow users to broadcastnewsdata,”inProc.Int.Conf.SpokenLang.Process.,vol.4,
Sydney,Australia,Dec.1998,pp.1335–1338.
seethecurrentspeakerinformation,understandthegeneralﬂow
[14] X. Zhu, C. Barras, S. Meignier, and J.-L. Gauvain, “Combining
ofspeakersthroughoutthebroadcast,orsearchforaparticular speaker identiﬁcationand BICfor speakerdiarization,”in Proc.Eur.
speakerwithintheaudio.Experimentsarealsounderwaytoas- Conf. Speech Commun. Technol., Lisbon, Portugal, Sep. 2005, pp.
2441–2444.
certainifadditionaltasks,suchastheprocessofannotatingdata,
[15] D.A.ReynoldsandP.Torres-Carrasquillo,“TheMITLincolnLabora-
canbefacilitatedusingdiarizationoutput. toryRT-04Fdiarizationsystems:Applicationstobroadcastaudioand
The diarization tasks of the future will cover a wider scope telephoneconversations,”inProc.Fall2004RichTranscriptionWork-
shop(RT-04),Palisades,NY,Nov.2004.
thancurrently,bothintermsoftheamountofdata(hundredsof
[16] T. Hain, S. E. Johnson, A. Tuerk, P. C. Woodland, and S. J. Young.
hours)andinformationrequired(speakeridentity,speakerchar- Segment generation and clustering in the HTK broadcast news tran-
acteristics,orpotentiallyevenemotion).Currenttechniquesand scription system. presented at Proc. 1998 DARPA Broadcast News
Transcription and Understanding Workshop. [Online]. Available:
toolkits(forexample,ALIZE[57])willprovideaﬁrmbaseto
http://mi.eng.cam.ac.uk/reports/abstracts/hain_darpa98.html
start from, but new methods, particularly combining informa- [17] R. Sinha, S. E. Tranter, M. J. F. Gales, and P. C. Woodland, “The
tion from many different approaches (as is currently done in Cambridge University March 2005 speaker diarization system,” in
Proc. Eur. Conf. Speech Commun. Technol., Lisbon, Portugal, Sep.
thespeakerrecognitionﬁeld[58])willneedtobedevelopedto
2005,pp.2437–2440.
allowdiarizationtobemaximallybeneﬁcialtorealusersandpo- [18] D. Liu and F. Kubala, “Fast speaker change detection for broadcast
tentialdownstreamprocessingsuchasmachinetranslationand news transcription and indexing,” in Proc. Eur. Conf. Speech
Commun. Technol., vol. III, Budapest, Hungary, Sep. 1999, pp.
parsing.Additionally,furtherdevelopmentoftoolstoallowuser
1031–1034.
interactions with diarization output for speciﬁc jobs will help [19] S.Meignier,D.Moraru,C.Fredouille,J.-F.Bonastre,andL.Besacier,
focustheresearchtocontributetohigh-utilityhumanlanguage “Step-by-Stepandintegratedapproachesinbroadcastnewsspeakerdi-
arization,”Comput.SpeechLang.,no.20,pp.303–330,Sep.2005,to
technology.
bepublished.
[20] S.E.TranterandD.A.Reynolds,“Speakerdiarizationforbroadcast
ACKNOWLEDGMENT news,”inProc.OdysseySpeakerandLanguageRecognitionWorkshop,
Toledo,Spain,Jun.2004,pp.337–344.
TheauthorswouldliketothankC.Barras,J.-F.Bonastre,C. [21] S.E.Tranter,K.Yu,G.Evermann,andP.C.Woodland,“Generating
Fredouille, P. Nguyen, P. Torres-Carrasquillo, and C. Wooters andevaluatingsegmentationsforautomaticspeechrecognitionofcon-
versationaltelephonespeech,”inProc.ICASSP,vol.I,Montreal,QC,
fortheirhelpintheconstructionofthispaper.
Canada,May2004,pp.753–756.
[22] D. Liu and F. Kubala, “A cross-channel modeling approach for au-
REFERENCES tomatic segmentation of conversational telephone speech,” in Proc.
IEEEASRUWorkshop,St.Thomas,U.S.VirginIslands,Dec.2003,pp.
[1] D.A.ReynoldsandP.Torres-Carrasquillo,“Approachesandapplica- 333–338.
tionsofaudiodiarization,”inProc.IEEEInt.Conf.Acoust.,Speech, [23] D.A.vanLeeuwan,“TheTNOspeakerdiarizationsystemforNIST
SignalProcess.,vol.V,Philadelphia,PA,Mar.2005,pp.953–956. RT05s meeting data,” in Proc. Machine Learning for Multimodal
[2] J.Saunders,“Real-timediscriminationofbroadcastspeech/music,”in Interaction Workshop (MLMI), Edinburgh, UK, Jul. 2005, pp.
Proc.IEEEInt.Conf.Acoust.,Speech,SignalProcess.,vol.II,Atlanta, 440–449.
GA,May1996,pp.993–996. [24] D.Istrate,C.Fredouille,S.Meignier,L.Besacier,andJ.-F.Bonastre,
[3] Z.Liu,Y.Wang,andT.Chen,“Audiofeatureextractionandanalysis “NISTRT’05evaluation:Preprocessingtechniquesandspeakerdiariza-
forscenesegmentationandclassiﬁcation,”J.VLSISignalProcess.Syst., tiononmultiplemicrophonemeetings,”inProc.MachineLearningfor
vol.20,no.1–2,pp.61–79,Oct.1998. MultimodalInteractionWorkshop(MLMI),Edinburgh,U.K.,Jul.2005,
[4] S.E.JohnsonandP.C.Woodland,“Amethodfordirectaudiosearch pp.428–439.
withapplicationstoindexingandretrieval,”inProc.IEEEInt.Conf. [25] S.Cassidy,“ThemacquariespeakerdiarizationsystemforRT05s,”in
Acoust.,Speech,SignalProcess.,vol.3,Istanbul,Turkey,Jun.2000,pp. Proc.NISTSpringRichTranscriptionEvaluationWorkshop(RT-05s),
1427–1430. Edinburgh,UK,Jul.2005.
[5] Y.Moh,P.Nguyen,andJ.-C.Junqua,“Towarddomainindependentclus- [26] T.Pfau,D.Ellis,andA.Stolcke,“Multispeakerspeechactivitydetection
tering,”inProc.IEEEInt.Conf.Acoust.,Speech,SignalProcess.,vol. fortheICSImeetingrecorder,”inProc.IEEEASRUWorkshop,Trento,
II,China,Apr.2003,pp.85–88. Italy,Dec.2001,pp.107–110.TRANTERANDREYNOLDS:OVERVIEWOFAUTOMATICSPEAKERDIARISATIONSYSTEMS 1565
[27] J.G.Fiscus,N.Radde,J.S.Garofolo,A.Le,J.Ajot,andC.Laprun, [47] S. E. Tranter, “Who really spoke when?—Finding speaker turns and
“Therichtranscription2005springmeetingrecogntionevaluation,”in identitiesinaudio,”inProc.IEEE Int.Conf.Acoust.,Speech,Signal
Proc.MachineLearningforMultimodalInteractionWorkshop(MLMI), Process.,vol.I,Toulouse,France,May2006,pp.1013–1016.
Edinburgh,UK,Jul.2005,pp.369–389. [48] M.J.F.Gales,D.Y.Kim,P.C.Woodland,H.Y.Chan,D.Mrva,R.Sinha,
[28] S. S. Chen and P. S. Gopalakrishnam, “Speaker, environment andS.E.Tranter,“ProgressintheCU-HTKtranscriptionsystem,”IEEE
and channel change detection and clustering via the bayesian Trans.Audio,Speech,Lang,Process.,vol.14,no.5,pp.1511–1523,Sep.
information criterion,” in Proc. 1998 DARPA Broadcast News 2006.
Transcription and Understanding Workshop, Lansdowne, VA, 1998, [49] D.Moraru,S.Meignier,C.Fredouille,L.Besacier,andJ.-F.Bonastre,
pp. 127–132. “TheELISAconsortiumapproachesinspeakersegmentationduringthe
[29] B. Zhou and J. Hansen, “Unsupervised audio stream segmentation NIST 2003 Rich Transcription evaluation,” in Proc. IEEE Int. Conf.
and clustering via the Bayesian information criterion,” in Proc. Int. Acoust.,Speech,SignalProcess.,vol.1,Montreal,QC,Canada,May
Conf.SpokenLanguageProcess.,vol.3,Beijing,China,Oct.2000,pp. 2004,pp.373–376.
714–717. [50] S.E.Tranter,“Two-wayclustervotingtoimprovespeakerdiarization
[30] M.A.Siegler,U.Jain,B.Raj,andR.M.Stern,“Automaticsegmenta- performance,”inProc.IEEEInt.Conf.Acoust.,Speech,SignalProcess.,
tion,classiﬁcationandclusteringofbroadcastnews,”inProc.DARPA vol.I,Philadelphia,PA,Mar.2005,pp.753–756.
SpeechRecognitionWorkshop,Chantilly,VA,Feb.1997,pp.97–99. [51] D.Liu,D.Kiecza,A.Srivastava,andF.Kubala,“Onlinespeakeradap-
[31] C.Barras,X.Zhu,S.Meignier,andJ.-L.Gauvain,“Improvingspeaker tationandtrackingforreal-timespeechrecognition,”inProc.Eur.Conf.
diarization,” in Proc. Fall Rich Transcription Workshop (RT-04), SpeechCommun.Technol.,Lisbon,Portugal,Sep.2005,pp.281–284.
Palisades,NY,Nov.2004,[Online].Available:http://www.limsi.fr/In- [52] J.G.Fiscus,J.S.Garofolo,A.Le,A.F.Martin,D.S.Pallett,M.A.
dividu/barras/publis/rt04f_diarization.pdf. Przybocki, and G. Sanders,“Results of thefall2004 STTand MDE
[32] A.E.Rosenberg,A.Gorin,Z.Liu,andS.Parthasarathy,“Unsupervised evaluation,”inProc.Fall2004RichTranscriptionWorkshop(RT-04),
speakersegmentationoftelephoneconversations,”inProc.Int.Conf. Palisades,NY,Nov.2004.
SpokenLanguageProcess.,Denver,CO,Sep.2002,pp.565–568. [53] Q.Jin,K.Laskowski,T.Schultz,andA.Waibel,“Speakersegmenta-
[33] S.Meignier,D.Moraru,C.Fredouille,L.Besacier,andJ.-F.Bonastre, tion and clustering in meetings,” in Proc. ICASSP Meeting Recogni-
“Beneﬁtsofprioracousticsegmentationforautomaticspeakersegmen- tionWorkshop,Montreal,QC,Canada,May2004,[Online].Available:
tation,”inProc.IEEEInt.Conf.Acoust.,Speech,SignalProcess.,vol.I, http://isl.ira.uka.de/publications/SchultzJin_NIST04.pdf.
Montreal,QC,Canada,May2004,pp.397–400. [54] D.Istrate,N.Schefﬂer,C.Fredouille,andJ.-F.Bonastre,“Broadcast
[34] D.LiuandF.Kubala,“Onlinespeakerclustering,”inProc.IEEEInt. newsspeakertrackingforESTER2005campaign,”inProc.Eur.Conf.
Conf.Acoust.,Speech,SignalProcess.,vol.I,HongKong,China,Apr. SpeechCommun.Technol.,Lisbon,Portugal,Sep.2005,pp.2445–2448.
2003,pp.572–575. [55] F.Kubala,S.Colbath,D.Liu,A.Srivastava,andJ.Makhoul,“Integrated
[35] D. Moraru, S. Meignier, L. Besacier, J.-F. Bonastre, and I. technologiesforindexingspokenlanguage,”Commun.ACM,vol.43,no.
Magrin-Chagnolleau. The ELISA consortium approaches in speaker 2,pp.48–56,Feb.2000.
segmentation during the NIST 2002 speaker recognition evaluation. [56] J.H.L.Hansen,R.Huang,B.Z.M.Seadle,J.J.R.Deller,A.R.Gurijala,
presented at Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.. M.Kurimo,andP.Angkititrakul,“Speechﬁnd:Advancesinspokendoc-
[Online]. Available: http://www.lia.univ-avignon.fr/ﬁch_art/339-mor- umentretrievalforanationalgalleryofthespokenword,”IEEETrans.
icassp2003.pdf SpeechAudioProcess.,vol.13,no.5,pp.712–730,Sep.2005.
[36] M. Cettolo, “Segmentation, classiﬁcation and clustering of an [57] J.F.Bonastre,F.Wils,andS.Meignier,“Alize:Afreetoolkitforspeaker
Italian corpus,” in Proc. Recherche d’Information Assisté par Or- recogntion,”inProc.IEEEInt.Conf.Acoust.,Speech,SignalProcess.,
dinateur (RIAO), Paris, France, Apr. 2000, [Online]. Available: vol.I,Philadelphia,PA,Mar.2005,pp.737–740.
http://munst.itc.it/people/cettolo/papers/riao00a.ps.gz. [58] D.Reynolds,W.Andrews,J.Campbell,J.Navratil,B.Peskin,A.Adami,
[37] J.AjmeraandC.Wooters,“ARobustSpeakerClusteringAlgorithm,” Q.Jin,D.Klusacek,J.Abramson,R.Mihaescu,J.Godfrey,D.Jones,
inProc.IEEEASRUWorkshop,StThomas,U.S.VirginIslands,Nov. andB.Xiang,“ThesuperSIDproject:Exploitinghigh-levelinforma-
2003,pp.411–416. tionforhigh-accuracyspeakerrecognition,”inProc.IEEEInt.Conf.
[38] S. E. Tranter, M. J. F. Gales, R. Sinha, S. Umesh, and P. C. Wood- Acoust.,Speech,SignalProcess.,vol.IV,HongKong,China,Apr.2003,
land,“ThedevelopmentoftheCambridgeUniversityRT-04diarization pp.784–787.
system,”inProc.Fall2004RichTranscriptionWorkshop(RT-04),Pal-
isades,NY,Nov.2004,[Online].Available:http://mi.eng.cam.ac.uk/re-
ports/abstracts/tranter_rt04.html. SueE.Tranter(M’04)receivedtheM.Eng.degree
[39] H.Jin,F.Kubala,andR.Schwartz,“Automaticspeakerclustering,”in in engineering science, specializing in information
Proc.DARPASpeechRecognitionWorkshop,Chantilly,VA,Feb.1997, engineering,fromtheUniversityofOxford,Oxford,
pp.108–111. U.K., in 1996 and the M.Phil. degree in computer
[40] M.Ben,M.Betser,F.Bimbot,andG.Gravier,“Speakerdiarizationusing speechandlanguageprocessingfromtheUniversity
bottom-up clustering based on a parameter-derived distance between ofCambridge,Cambridge,U.K.,in1997.
adaptedGMMs,”inProc.Int.Conf.SpokenLanguageProcessing,Jeju Followingthis,sheworkedasaResearchAssistant
Island,Korea,Oct.2004,pp.2329–2332. onMultiMediaDocumentRetrievalattheUniversity
[41] S. Meignier, J.-F. Bonastre, C. Fredouille, and T. Merlin, “Evolutive ofCambridgeuntil2000,andthenonnonlinearcon-
HMM for multispeaker tracking system,” in Proc. IEEE Int. Conf. troltheoryattheUniversityofOxford.Since2002
Acoust.,Speech,SignalProcess.,vol.II,Istanbul,Turkey,Jun.2000, shehasbeenaResearchAssociateontheEffective
pp.1201–1204. AffordableReusableSpeech-To-Text(EARS)projectattheUniversityofCam-
[42] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for bridge,specializinginspeakersegmentationandclustering.
learning and adapting sound models for speaker indexing,” in Proc.
OdysseySpeakerandLanguageRecognitionWorkshop,Crete,Greece,
Jun.2001,pp.175–180. DouglasReynolds(SM’98)receivedtheB.E.E.de-
[43] D.Reynolds,E.Singer,B.Carlson,J.O’Leary,J.McLaughlin,andM. gree(withhighesthonors)andthePh.D.degreein
Zissman,“Blindclusteringofspeechutterancesbasedonspeakerand electricalengineering,bothfromtheGeorgiaInsti-
languagecharacteristics,”inProc.Int.Conf.SpokenLanguageProcess., tuteofTechnology,Atlanta.
vol.7,Sydney,Australia,Dec.1998,pp.3193–3196. HejoinedtheSpeechSystemsTechnologyGroup
[44] J.PelecanosandS.Sridharan,“FeaturewarpingforRobustspeakerver- (nowtheInformationSystemsTechnologyGroup),
iﬁcation,”inProc.OdysseySpeakerandLanguageRecognitionWork- LincolnLaboratory,MassachusettsInstituteofTech-
shop,Crete,Greece,Jun.2001,pp.213–218. nology,Cambridge,in1992.Currently,heisaSe-
[45] C. Barras and J.-L. Gauvain, “Feature and score normalization for niorMemberofTechnicalStaffandhisresearchin-
speakerveriﬁcationofcellulardata,”inProc.IEEEInt.Conf.Acoust., terestsincluderobustspeakerandlanguageidentiﬁ-
Speech, Signal Process., vol. II, Hong Kong, China, Apr. 2003, pp. cationandveriﬁcation,speechrecognition,andgen-
49–52. eralproblemsinsignalclassiﬁcationandclustering.
[46] L.Canseco-Rodriguez,L.Lamel,andJ.-L.Gauvain,“SpeakerDiariza- DouglasisaSeniorMemberofIEEESignalProcessingSocietyandaco-
tion from Speech Transcripts,” in Proc. Int. Conf. Spoken Language founderandmemberofthesteeringcommitteeoftheOdysseySpeakerRecog-
Process.,JejuIsland,Korea,Oct.2004,pp.1272–1275. nitionworkshop.