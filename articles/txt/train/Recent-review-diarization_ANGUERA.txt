356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
are made available as outputs and have been used successfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamforming is the most reliable one when no information on ture. The main drawback of model-based approaches is their re-
the location or nature of each microphone is known a priori. liance on external data for the training of speech and nonspeech
Among alternative beamforming algorithms we ﬁnd maximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions. Hybrid approaches have been proposed as a poten-
[47] which adaptively ﬁnd the optimum parameters, and min- tial solution. In most cases, an energy-based detection is ﬁrst ap-
imum variance distortionless response (MVDR) [48] when plied in order to label a limited amount of speech and nonspeech
prior information on ambient noise is available. All of these data for which there is high conﬁdence in the classiﬁcation. In a
have higher computational requirements and, in the case of the second step, the labeled data are used to train meeting-speciﬁc
adaptive algorithms, there is the danger of converging to inac- speech and nonspeech models, which are subsequently used in a
curate parameters, especially when processing microphones of model-based detector to obtain the ﬁnal speech/nonspeech seg-
different types. mentation [9], [55]–[57]. Finally, [58] combines a model-based
with a 4-Hz modulation energy-based detector. Interestingly, in-
B. Speech Activity Detection stead of being applied as a preprocessing stage, in this system
Speech activity detection (SAD) involves the labeling of SAD is incorporated into the speaker diarization process.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), times used to refer to both segmentation and clustering. While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process, and more speciﬁcally the acoustic models involved in speaker diarization and some systems, such as that reported in
the process [49]. Indeed, the inclusion of non-speech segments [6], apply distinctly independent segmentation and clustering
in speaker modelling leads to less discriminant models and thus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good described separately here.
compromise between missed and false alarm speech error rates Speaker segmentation is core to the diarization process and
has to be found to enhance the quality of the following speaker aims at splitting the audio stream into speaker homogeneous
diarization process. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech known as speaker turns. The classical approach to segmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the two sliding and possibly overlapping, consecutive windows. For
literature [50]. Initial approaches for diarization tried to solve each considered change point there are two possible hypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrst that both segments come from the same speaker ( ), and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a second that there are two different speakers ( ), and thus that
dedicated speech/nonspeech detector as preprocessing step. two different models are more appropriate. In practice, models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teria are used to determine whether they are best accounted for
knocks or non-lexical noise such as breathing, coughing, and by two separate models (and hence two separate speakers), or by
laughing, among other background noises. Therefore, highly a single model (and hence the same speaker) by using an empir-
variable energy levels can be observed in the nonspeech parts ically determined or dynamically adapted threshold [10], [59].
of the signal. Moreover, differences in microphones or room This is performed across the whole audio stream and a sequence
conﬁgurations may result in variable SNRs from one meeting of speaker turns is extracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
and pitch estimation) combined with a threshold-based decision during the last four years. The most common approach is that
have proven to be relatively ineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely on a two-class detector, with models pre-trained with e.g.,[60]–[62]. The approach requires the setting of an explicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penalty term which controls the tradeoff between missed turns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
different meetings and thus new, more robust approaches have the introduction of missing speaker turns. Most of present
been devised. They either adapt the penalty term automatically, diarization systems thus perform segmentation and clustering
i.e., the modiﬁed BIC criterion [33], [63], [64], or avoid the use simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
and some systems have been developed in order to use the BIC based on the current clustering hypothesis before the models
only in a second pass, while a statistical-based distance is used are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usually performed. In order to make the Viterbi decoding more
cross-BIC and introduced in [67] and [68], involves the com- stable, it is common to use a Viterbi buffer to smooth the state,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
according to a model tuned from the second segment and vice brief speaker turns, as in [16]. Most state-of-the-art systems
versa. In [69], different techniques for likelihood normalization employ some variations on this particular issue.
are presented and are referred to as bilateral scoring. An alternative approach to clustering involves majority voting
A popular and alternative approach to BIC-based measures is [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trast to the BIC, the GLR is a likelihood-based metric and corre- frames during decoding. This technique leads to savings in com-
sponds to the ratio between the two aforementioned hypotheses, putation but is more suited to online or live speaker diarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
order to take into account the amount of training data available
E. One-Step Segmentation and Clustering
in the two segments, a penalized GLR was proposed in [74].
The last of the dominant approaches is the Kullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems, segmentation and clustering are performed hand-in-hand
asymmetric, and thus the KL2 metric, a symmetric alternative, in one loop. Such a method was initially proposed by ICSI for
has proved to be more popular in speaker diarization when used a bottom-up system [31] and has subsequently been adopted
to characterize the similarity of two audio segments [75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally, in this section we include a newly introduced distance rithms it was initially proposed by LIA [14] as used in their latest
metric that has shown promise in a speaker diarization task. The system [16].
information change rate (ICR), or entropy can be used to char- In all cases the different acoustic classes are represented using
acterize the similarity of two neighboring speech segments. The HMM/GMM models. EM training or MAP adaptation is used to
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- model assignments, and a Viterbi algorithm is used to reassign
tion and can thus be used for speaker segmentation. Unlike the all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessing is sometimes performed several times for the frame as-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segments in a space of relevance variables, with maximum mu- ated/eliminated so that the resulting class distribution is allowed
tual information or minimum entropy. One suitable space comes to adapt to the data.
from GMM component parameters [18]. The ICR approach is The one-step segmentation and clustering approach, although
computationally efﬁcient and, in [78], ICR is shown to be more much slower, constitutes a clear advantage versus sequential
robust to data source variation than a BIC-based distance. single-pass segmentation and clustering approaches [5]–[7]. On
the one hand, early errors (mostly missed speaker turns from the
D. Clustering segmentation step) can be later corrected by the re-segmentation
Whereas the segmentation step operates on adjacent windows steps. On the other hand, most speaker segmentation algorithms
in order to determine whether or not they correspond to the same use only local information to decide on a speaker change while
speaker, clustering aims at identifying and grouping together when using speaker models and Viterbi realignment all data is
same-speaker segments which can be localized anywhere in the taken into consideration.
audio stream. Ideally, there will be one cluster for each speaker. When performing frame assignment using Viterbi algorithm
The problem of measuring segment similarity remains the same a minimum assignment duration is usually enforced to avoid
and all the distance metrics described in Section III-C may also an unrealistic assignment of very small consecutive segments
be used for clustering, i.e., the KL distance as in [10], a modiﬁed to different speaker models. Such minimum duration is usually
KL2 metric as in [61], a BIC measure as in [79] or the cross made according to the estimated minimum length of any given
likelihood ratio (CLR) as in [80] and [81]. speaker turn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENT RESEARCH DIRECTIONS
single speaker, and thus diarization algorithms can only work In this section, we review those areas of work which are still
well if the initial segmentation is of sufﬁciently high quality. not mature but which have the potential to improve diarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
performance. We ﬁrst discuss the trend in recent NIST RT eval- diarization performance and the success of these systems in
uations to use spatial information obtained from multiple micro- NIST RT evaluations would seem to support their use.
phones, which are used by many in combination with MFCCs
B. Use of Prosodic Features in Diarization
to improve performance. Then, we discuss the use of prosodic
information which has led to promising speaker diarization re- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
of overlapping speech and its correct labeling for improved di- and speech recognition (which requires invariance against
arization outputs. We then consider a recent trend towards mul- speakers) [93]. In [84], the authors present a systematic investi-
timodal speaker diarization including studies of multimodal, gation of the speaker discriminability of 70 long-term features,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despite the dominance of short-term cepstral features in speaker
we consider general combination strategies that can be used to recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowing summarizes recent work in all of these areas. suggested in [94], the consideration of patterns derived from
larger segments of speech can reveal individual characteristics
A. Time-Delay Features
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sum beamforming of multiple microphone channels, frame-based cepstral analysis. The authors use Fisher LDA as
as described in Section III-A, but also for speaker localization. a ranking methodology and sort the 70 prosodic and long-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
tracking algorithms are used, then estimates of speaker location top-ten ranked prosodic and long-term features combined with
may thus be used as alternative features, which have nowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formation is not always available. The ﬁrst work [88] that does scheme that can be applied to standard bottom-up diarization
not rely on microphone locations led to promising results, even algorithms. The initialization method is a combination of the
if error rates were considerably higher than that achieved with recently proposed “adaptive seconds per Gaussian” (ASPG)
acoustic features. Early efforts to combine acoustic features and method [96] and a new pre-clustering method in addition to
estimates of inter-channel delay clearly demonstrated their po- a new strategy which automatically estimates an appropriate
tential, e.g.,[89], though this work again relied upon known mi- number of initial clusters based on prosodic features. It outper-
crophone locations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. Overlap Detection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though A fundamental limitation of most current speaker diarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based The presence of overlapped speech, though, is common in mul-
on an entropy-based metric is used for cluster comparison in a tiparty meetings and, consequently, presents a signiﬁcant chal-
bottom-up speaker diarization system. A complete front-end for lenge to automatic systems. Speciﬁcally, in regions where more
speaker diarization with multiple microphones was proposed in than one speaker is active, missed speech errors will be incurred
[42]. Here a two-step TDOA Viterbi post-processing algorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lapped speech in diarization pertains to speaker clustering and
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms. More recently, an approach to the unsupervised dis- single speaker should not be assigned to any individual speaker
criminant analysis of inter-channel delay features was proposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
using delay features alone. mately reduces diarization performance. Approaches to overlap
In the most recent NIST RT evaluation, in 2009, all but one detection were thoroughly assessed in [97] and [98] and, even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are a small number of systems actually detects overlapping speech
rarely reported it is not possible to assess the contribution well enough to improve error rates [99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 363
speaker during overlap regions using a simple strategy of detection of the mouth is not always feasible. Therefore, other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boring segments, as well as by excluding overlap regions from visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study has been relatively little work on using global body movements
assumed ground-truth overlap detection. In [100], a real overlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out [82], [117]–[119] that show promising initial results.
that computed posterior probabilities from diarization to post However, until the work presented in [120], approaches have
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions. The main bottleneck of the achieved performance gain vised joint optimization problem. The work in [120], though,
is mainly due to errors in overlap detection, and more work on relies on multiple cameras. The ﬁrst paper that discusses joint
enhancing its precision and recall is reported in [99] and [101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech), and the best feature combination is MFCC and modu- [121]. The algorithm relies on very few assumptions and is able
lation spectrogram features [103], although comparable results to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Most importantly, as a result of training a combined audiovisual
energy, spectral ﬂatness, or harmonic energy ratio. The reported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and can result in speaker localization as side information. This way
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “who spoken when and from where.” This solution to the local-
DER improvement goes up to 37%. This way, this area has ization problem has properties that may not be observed either
potential for future research efforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. Audiovisual Diarization channel. In addition, in contrast to audio-only speaker diariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. System Combination
speaker localization in broadcast video. The authors of [105]
present a multi-modal speaker localization method using a spe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Though the results seem comparable to the state-of-the-art, the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarization using a dynamic Bayesian network (DBN) merging multiple output segmentations. Combination strategies
but tests were limited to discussions with two to three people have to accommodate differences in temporal synchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual of speaker labels. Moreover, systems involved in the combina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation when combined. Some of the combination strategies proposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
the authors demonstrate that the different shapes the mouth can tially, based on the segmentation outputs of the previous steps
take when speaking facilitate word recognition under tightly in order to reﬁne boundaries (referred to as “hybridization” or
constrained test conditions (e.g., frontal position of the subject “piped” systems in [122]). In [123] for instance, the authors
with respect to the camera while reading digits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented by two dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferent French laboratories (LIUM and IRIT) are merged and/or
motion from a person comes predominantly from the motion used sequentially, which leads to a performance gain compared
of the lower half of their face. In addition, gestural or other to results from individual systems. An original approach is pro-
nonverbal behaviors associated with natural body motion posed in [125], based on a “real” system combination. Here, a
during conversations are artiﬁcially suppressed, e.g., for the couple of systems uniquely differentiated by their input features
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
only where short term synchrony of lip motion and speech are phone calls conversations. The combination approach relies on
the basis for audiovisual localization. In a real scenario the both systems identifying some common clusters which are then
subject behavior is not controlled and, consequently, the correct considered as the most relevant. All the segments not belonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involved in a new re-classiﬁcation step based on a GMM mod- a priori knowledge available to the participants relates to the
eling of the common clusters and a maximum likelihood-based recording scenario/source (e.g., conference meetings, lectures,
decision. or coffee breaks for the meetings domain), the language (Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. Alternative Models participants may use external or background data for building
world models and/or for normalization purposes but no a priori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differ from the standard HMM insofar as they are fully nonpara-
The number of speakers is also not known.
metric (that is, the number of parameters of the system depends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization. Each meeting used in the evaluations was recorded
mixtures and deﬁnes the number of components by a measure
using multiple microphones (of different types and quality)
over distributions. The authors of [127] illustrate the use of the
which are positioned on the participants or in different locations
Dirichlet process mixtures, showing an improvement compared
around the meeting room. By grouping these microphones into
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
on transition matrices over countably inﬁnite state spaces, that
microphones (MDM), multiple mark III arrays (MM3A), and
is, no ﬁxed number of speakers is assumed, nor found through
all distant microphones (ADM). MM3A microphones are those
either split or merging approaches using classical model selec-
exclusively found within the arrays built and provided by NIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measure is placed over distributions (called a random measure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resulting HDP-HMM leads to a data-driven learning algorithm
consider them to be the most representative of standard meeting
which infers posterior distributions over the number of states.
room recording equipment. These conditions have also proven
This posterior uncertainty can be integrated out when making
to be the most popular among evaluation participants.
predictions effectively averaging over models of varying com-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
with speaker labels, which are used solely to identify the mul-
erative HMM with GMM emissions, while requiring very
tiple interventions of a given speaker, but do not need to reﬂect
little hyperparameter tuning and providing a statistically sound
the speaker’s real identity. These system outputs are compared
model. Globally, these non parametric Bayesian approaches did
to the ground-truth reference in order to obtain the overall DER.
not bring a major improvement compared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech (percentage of speech in the ground-truth but not in the
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesis but not in the ground-truth) and speaker error (per-
more, they provide a probabilistic interpretation on posterior
centage of speech assigned to the wrong speaker). The speaker
distributions (e.g., number of speakers).
error can be further classiﬁed into incorrectly assigned speakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speaker does not correspond to the real (ground-truth) speaker.
V. PERFORMANCE EVALUATION
Speaker overlap error refers to the case when the wrong number
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performance as reported during the four most recent NIST RT the same time. The inclusion of overlapping speech error in the
evaluations. The analysis focuses solely on conference meetings evaluation was restricted to a contrastive metric in the initial RT
which are the core evaluation condition. We also present an anal- evaluations but has been the primary metric since 2006. Overlap
ysis of the ground-truth references in order to underline the char- errors can be classiﬁed as missed overlap (when fewer speakers
acteristics of the data with respect to meeting sources and the than the real number are hypothesized) and false alarm overlap
different evaluation campaigns. Finally we show state-of-the-art (when too many speakers are hypothesized). In the NIST eval-
system results, collated from four NIST RT’07 and RT’09 eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note that as the DER is time-weighted, it ascribes little im-
portance to the diarization quality of speakers whose overall
A. Benchmarking Evaluations speaking time is small. Additionally, a nonscoring collar of 250
ms is generally applied either side of the ground-truth segment
Since 2004, NIST has organized a series of benchmark eval-
boundaries to account for inevitable inconsistencies in precise
uations within the Rich Transcription (RT) campaigns.3 One of
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
puts with the ground-truth, and given that the labels identifying
3See http://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 365
TABLE I
GROUND-TRUTH ANALYSIS FOR THE DATASETS OF THE LAST FOUR SPEAKER DIARIZATION EVALUATION CAMPAIGNS (RT’05 TO
RT’09) AND MEETING SOURCE. COMPARISONS ARE BASED ON THE AVERAGE SPEAKER AND TURN DURATIONS (LEFT-HALF SIDE)
AND THE PERCENTAGE OF SILENCE AND OVERLAPPING SPEECH (RIGHT-HALF SIDE)
For RT’05 the average speaker segment duration is 2.5 s. This
value decreases continuously for subsequent datasets (2.3 s for
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leads to increasingly more frequent speaker turns and increases
the chances of miss-classifying a speech segment. The average
turn segment duration is 2.1 s for RT’05. This value falls to 1.4 s
for RT’06 and remains stable for RT’07 and RT’09 (1.5 s and 1.4
Fig. 2. Examples of turn and speaker durations in the presence of overlapped s respectively). The consistent decrease in speaker/turn duration
speech and silences.
ratio highlights a general trend of increasing spontaneity and
helps to explain the differences in results from one dataset to an-
ﬁrst computes an optimum mapping between both sets of la-
other. There are no distinct differences across different meeting
bels in order to obtain the DER. This is normally performed ac-
sites.
cording to a standard dynamic programming algorithm deﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics. The percentage of silence is lower for the RT’05 and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-Truth Analysis
(10.3% and 17.5% cf. 31.5% and 24.9%). However, the RT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
were initially obtained via manual labeling of the acoustic data; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
be problematic. Therefore, more recently, an automatically gen- and NIST sites) which have overlap rates over 25% (note that
erated forced alignment has been used in order to extract more values in Table I are averaged across sites, and do not reﬂect
reliable speaker start and end points using an automatic speech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
the audio from individual head microphones (IHM). meeting (recorded by NIST) in which the overlap reaches 31%.
As meeting data come from a variety of sources some differ- Listening to this meeting we concluded that the reason of such
ences between them are expected. Furthermore, large changes in overlap is that it is not a professional meeting but a social
the ﬁnal DER scores from different evaluations would suggest rendezvous. Conversely, RT’05 and RT’09 have in average a
that there are differences between the sets of meetings used each lower percentage of silence (10% and 17%) compared to RT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferent parameters computed on the ground-truth data. In Table I, overlap might indicate that these meetings are more dynamic,
we report four of these parameters, which we found most inter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
In the left side of the table we report average speaker and turn dataset.
durations. As exempliﬁed in Fig. 2, the average speaker duration Overall, we see that, although all recordings belong to the
refers to the average time during which a speaker is active (i.e., a same task, there are large differences between the datasets used
single line in the RTTM reference ﬁles). Conversely, the average for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two perform well regardless of particular dataset characteristics. It is
statistics reﬂects the degree of overlap and spontaneity. Without important to note, however, that the NIST RT datasets discussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
average speaker and turn durations would be identical. Increases of them contributing to a single DER score. Random variations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impact on average results. It is then difﬁcult to reliably interpret
silence and of overlapping speech. results and hence also difﬁcult to draw meaningful conclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Fig. 3. DERs for the RT’07 and RT’09 (a) in multiple distant microphone (MDM) condition, and (b) single distant microphone (SDM) condition (note that
spkr_error in meeting NIST_20080201–1405 has been trimmed to ﬁt the screen, with a speaker error of 31.79% and a total DER of 49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus, there is a large variation in performance across different
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which high percentage of overlapping speech for this meeting as illus-
makes it difﬁcult to assess novel algorithms, is a critical barrier trated in Table I. Hence, the detection and appropriate treatment
to further research in our ﬁeld. Signiﬁcantly larger datasets are of overlapping speech remains an unsolved problem. In fact, the
needed in order to obtain more robust and meaningful perfor- overlap error shown in Fig. 3 is entirely due to missed overlap
mance estimates and comparisons. As a result of processing regions, as none of the speaker diarization systems considered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
for research in speaker diarization to be feasible with standard the general stability of speech activity detection (SAD) algo-
computing resources. rithms which achieve impressive levels of performance in both
MDM and SDM conditions (i.e., they are robust to the quality
C. Evaluation Results
of the signal). Values of around 1% to 2% missed speech error
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
for future research we present results for the RT’07 (Fig. 3 left ical. The main difference between MDM and SDM performance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly in the speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDM condition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall, the large variations in DER observed among the dif-
and UPC) and by selecting the result with lowest DER for each ferent meetings and meeting sets originate from the large vari-
meeting recording. Given the volatility of the results described ance of many important factors for speaker diarization, which
and studied in [3], by selecting the best result in each case we hy- makes the conference meeting domain not as easily tractable
pothesize that these results are a more meaningful estimation of as more formalized settings such as broadcast news, lectures,
the state-of-the-art performance in speaker diarization for con- or court house trials. Previous work has highlighted the difﬁ-
ference meeting data than selecting all results from any single culty in assessing the performance of speaker diarization algo-
system output. To illustrate the variation in performance for dif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In ported in Section III, current approaches to speaker diarization
both ﬁgures, errors are decomposed into the speaker error (Spkr involve a sequence of separate stages where each stage takes
error), overlap error (OVL error), false alarm speech error (FA its input from the preceding stage(s). When combined in such a
speech), and missed speech error (MISS speech). fashion, it is exceedingly difﬁcult to assess the performance of
For the MDM condition [Fig. 3(a)] the average DER for the each system component since every single one is affected by the
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performance of all previous processing stages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 is not guaranteed that improvements to one stage, for example
dataset whereas for the RT’09 dataset performance varies be- that of segmentation, will lead unequivocally to improvements
tween 5.3% and 22.2%. For the SDM condition the average in later stages, for example that of clustering. This makes the op-
DER is 11.6% and 17.7% for the RT’07 and RT’09 datasets, re- timization of different system components rather troublesome.
spectively. Performance is always poorer than that for the MDM Once again, by drawing comparisons to the speech and speaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches, as is already in progress with the now commonplace
4Eurecom was associated with the LIA for the RT’09 campaign only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “The NIST Rich Transcription 2009 (RT’09) Evaluation,” NIST, 2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tive algorithms that can detect and correctly assign overlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S. Tranter and D. Reynolds, “An overview of automatic speaker di-
arization systems,” IEEE Trans. Audio, Speech, Lang. Process., vol.
14, no. 5, pp. 1557–1565, Sep. 2006.
[3] N. Mirghafori and C. Wooters, “Nuts and ﬂakes: A study of data char-
VI. CONCLUSION AND DIRECTIONS FOR FUTURE RESEARCH acteristics in speaker diarization,” in Proc. ICASSP, 2006.
[4] X. Anguera, “Robust speaker diarization for meetings,” Ph.D. disser-
Research on speaker diarization has been developed in many
tation, Univ. Politecnica de Catalunya, Barcelona, Spain, 2006.
domains, from phone calls conversations within the speaker [5] M. Kotti, E. Benetos, and C. Kotropoulos, “Computationally efﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognition evaluations, to broadcast news and meeting record-
Speech, Lang. Process., vol. 16, no. 5, pp. 920–933, Jul. 2008.
ings in the NIST Rich Transcription evaluations. Furthermore, [6] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Multi-stage speaker
it has been used in many applications such as a front-end for diarization for conference and lecture meetings,” in Proc. Multimodal
Technol. Perception of Humans: Int. Eval. Workshops CLEAR 2007 and
speaker and speech recognition, as a meta-data extraction tool
RT 2007, Baltimore, MD, May 8–11, 2007, Revised Selected Papers,
to aid navigation in broadcast TV, lecture recordings, meetings, Berlin, Heidelberg: Springer-Verlag, 2008, pp. 533–542.
and video conferences and even for applications such as media [7] S. Jothilakshmi, V. Ramalingam, and S. Palanivel, “Speaker diariza-
tion using autoassociative neural networks,” Eng. Applicat. Artif. In-
similarity estimation for copyright detection. Also, speaker di- tell., vol. 22, no. 4-5, pp. 667–675, 2009.
arization research has led to various by-products. For example, [8] X. Anguera, C. Wooters, and J. Hernando, “Robust speaker diarization
for meetings: ICSI RT06s evaluation system,” in Proc. ICSLP, Pitts-
with the availability of recordings using multiple microphones,
burgh, PA, Sep. 2006.
a set of algorithms has been proposed in recent years both for [9] C. Wooters and M. Huijbregts, “The ICSI RT07s speaker diarization
signal enhancement and to take advantage of the extra infor- system,” in Multimodal Technologies for Perception of Humans: Inter-
national Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore,
mation that these offer. In addition, the availability of other MD, USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidel-
modalities, such as video, have started to inspire multimodal berg: Springer-Verlag, 2008, pp. 509–519.
[10] J. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, and J. Martinez, “Fast
diarization systems, thus merging the visual and the acoustic
incremental clustering of Gaussian mixture speaker models for scaling
domains. up retrieval in on-line broadcast,” in Proc. ICASSP, May 2006, vol. 5,
This paper provides an overview of the current state-of- pp. 521–524.
[11] W. Tsai, S. Cheng, and H. Wang, in Proc. ICSLP, 2004.
the-art in speaker diarization systems and underlines several
[12] T. H. Nguyen, E. S. Chng, and H. Li, “T-test distance and clustering
challenges that need to be addressed in future years. For ex- criterion for speaker diarization,” in Proc. Interspeech, Brisbane, Aus-
tralia, 2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T. Nguyen et al., “The IIR-NTU speaker diarization systems for RT
that methods can be easily ported across different domains, as 2009,” in Proc. RT’09, NIST Rich Transcription Workshop, Melbourne,
shown in Section V, where small differences in meeting data FL, 2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learning and adapting sound models for speaker indexing,” in Proc.
mance. In the meantime, larger datasets need to be compiled in Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
order for results to become more meaningful and for systems to Jun. 2001, pp. 175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
be more robust to unseen variations. Of course, with increasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA, May 8–11, 2007, Revised Selected Papers, Berlin, Heidelberg:
order to process such data in reasonable time. Still, the biggest
Springer-Verlag, 2008, pp. 520–532.
single challenge is probably the handling of overlapping [16] C. Fredouille, S. Bozonnet, and N. W. D. Evans, “The LIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09 speaker diarization system,” in Proc. RT’09, NIST Rich Tran-
scription Workshop, Melbourne, FL, 2009.
a relatively embryonic community, at least compared to the
[17] S. Bozonnet, N. W. D. Evans, and C. Fredouille, “The LIA-EURECOM
more established ﬁelds of speech and speaker recognition, there RT’09 speaker diarization system: Enhancements in speaker modelling
and cluster puriﬁcation,” in Proc. ICASSP, Dallas, TX, Mar. 14–19,
are thus outstanding opportunities for signiﬁcant advances
2010, pp. 4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D. Vijayasenan, F. Valente, and H. Bourlard, “Agglomerative infor-
approaches that currently dominate the ﬁeld. mation bottleneck for speaker diarization of meetings data,” in Proc.
ASRU, Dec. 2007, pp. 250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D. Vijayasenan, F. Valente, and H. Bourlard, “An information theoretic
brighter than the present, as more and more people acknowl- approach to speaker diarization of meeting data,” IEEE Trans. Audio,
Speech, Lang. Process., vol. 17, no. 7, pp. 1382–1393, Sep. 2009.
edge the usefulness of audio methods for many tasks that have
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994, vol. 23, pp. 727–741.
[21] G. E. Hinton and D. van Camp, “Keeping the neural networks simple by
problems underlying virtually any task that involves acoustics
minimizing the description length of the weights,” in Proc. 6th Annu.
and the presence of more than one person. Conf. Comput. Learn. Theory, New York, 1993, COLT ’93, pp. 5–13.
[22] M. J. Wainwright and M. I. Jordan, “Variational inference in graphical
models: The view from the marginal polytope,” in Proc. 41st Annu.
Allerton Conf. Commun., Control, Comput., Urbana-Champaign, IL,
ACKNOWLEDGMENT 2003.
[23] F. Valente, “Variational Bayesian methods for audio indexing,” Ph.D.
The authors would like to thank those RT participating sites dissertation, Eurecom Inst., Sophia-Antipolis, France, 2005.
that lent them their results for this study, in particular J. Luque [24] D. Reynolds, P. Kenny, and F. Castaldo, “A study of new approaches
to speaker diarization,” in Proc. Interspeech, 2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P. Kenny, “Bayesian Analysis of Speaker Diarization with Eigenvoice
NTU, Singapore. Priors,” Technical Report. Montreal, QC, Canada: CRIM, 2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[26] X. Anguera and J.-F. Bonastre, “A novel speaker binary key derived [50] J. Ramirez, J. M. Girriz, and J. C. Segura, M. Grimm and K. Kroschel,
from anchor models,” in Proc. Interspeech, 2010. Eds., “Voice activity detection. Fundamentals and speech recognition
[27] X. Anguera and J.-F. Bonastre, “Fast speaker diarization based on bi- system robustness,” in Proc. Robust Speech Recognit. Understand., Vi-
nary keys,” in Proc. ICASSP, 2011. enna, Austria, Jun. 2007, p. 460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C. Fredouille and G. Senay, “Technical improvements of the E-HMM
C. Wooters, “A fast-match approach for robust, faster than real-time based speaker diarization system for meeting records,” in Proc. MLMI
speaker diarization,” in Proc. IEEE Workshop Autom. Speech Recogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tion Understanding, Kyoto, Japan, Dec. 2007, pp. 693–698. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 359–370.
[29] G. Friedland, J. Ching, and A. Janin, “Parallelizing speaker-attributed [52] D. A. V. Leeuwen and M. Konecˇný, “Progress in the AMIDA speaker
speech recognition for meeting browsing,” in Proc. IEEE Int. Symp. diarization system for meeting data,” in Proc. Multimodal Technol. for
Multimedia, Taichung, Taiwan, Dec. 2010, pp. 121–128. Percept. of Humans: Int. Eval. Workshops CLEAR 2007 and RT 2007,
[30] X. Anguera, C. Wooters, and J. Hernando, “Friends and enemies: A Baltimore, MD, May 8–11, 2007, Revised Selected Papers, Berlin, Hei-
novel initialization for speaker diarization,” in Proc. ICSLP, Pittsburgh, delberg: Springer-Verlag, 2008, pp. 475–483.
PA, Sep. 2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera, “A robust speaker clustering algorithm,” in Proc. ASRU, Polymenakos, “The 2006 Athens information technology speech ac-
2003, pp. 411–416. tivity detection and speaker diarization systems,” in Proc. Mach. Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France, May 2006, pp. 1025–1028. 2006, pp. 385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A. Temko, D. Macho, and C. Nadeu, “Enhanced SVM training for ro-
channel change detection and clustering via the bayesian information bust speech activity detection,” in Proc. ICASSP, Honolulu, HI, 2007,
criterion,” in Proc. DARPA Broadcast News Transcription and Under- pp. 1025–1028.
standing Workshop, Lansdowne, VA, Feb. 1998, pp. 127–132. [55] X. Anguera, C. Wooters, M. Anguilo, and C. Nadeu, “Hybrid speech/
[34] H. Gish and M. Schmidt, “Text independent speaker identiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEE Signal Process. Mag., vol. 11, no. 4, pp. 18–32, Oct. 1994. Proc. Speaker Odyssey Workshop, Puerto Rico, Jun. 2006.
[35] A. Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards, J. Macias-Guarasa, [56] H. Sun, T. L. Nwe, B. Ma, and H. Li, “Speaker diarization for meeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. room audio,” in Proc. Interspeech’09, Sep. 2009.
Wrede, “The ICSI meeting project: Resources and research,” in Proc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSP Meeting Recognition Workshop, 2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronen- [58] E. El-Khoury, C. Senac, and J. Pinquier, “Improved speaker diariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tion system for meetings,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L. Lu, H.-J. Zhang, and H. Jiang, “Content analysis for audio classiﬁ-
[37] D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. cation and segmentation,” IEEE Trans. Speech Audio Process., vol. 10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no. 7, pp. 504–516, Oct. 2002.
matikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. [60] R. Li, Q. Jin, and T. Schultz, “Improving speaker segmentation via
Bernardin, and C. Rochet, “The CHIL audiovisual corpus for lecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
and meeting analysis inside smart rooms,” Lang. Resources Eval., vol. Sep. 2009, pp. 3073–3076.
41, Dec. 2007. [61] M. Ben, M. Betser, F. Bimbot, and G. Gravier, “Speaker diarization
[38] C. Fredouille, D. Moraru, S. Meignier, L. Besacier, and J.-F. Bonastre, using bottom-up clustering based on a parameter-derived distance be-
“The NIST 2004 spring rich transcription evaluation: Two-axis tween adapted gmms,” in Proc. ICSLP, Jeju Island, Korea, 2004.
merging strategy in the context of multiple distant microphone based [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript. Eval. Workshop, Montreal, QC, Canada, 2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q. Jin, K. Laskowski, T. Schultz, and A. Waibel, “Speaker segmen- vol. 4299, Lecture Notes in Computer Science, pp. 371–384.
tation and clustering in meetings,” in Proc. ICSLP, Jeju, Korea, Sep. [63] A. Vandecatseye, J.-P. Martens, J. Neto, H. Meinedo, C. Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, and J.-F. Bonastre, Cizmar, H. Papageorgiou, and C. Alexandris, “The cost278 pan-Eu-
“NIST RT05S evaluation: Pre-processing techniques and speaker di- ropean broadcast news database,” in Proc. LREC, Lisbon, Portugal, 5,
arization on multiple microphone meetings,” in Proc. NIST 2005 Spring 2004, vol. 4, pp. 873–876.
Rich Transcript. Eval. Workshop, Edinburgh, U.K., Jul. 2005. [64] K. Mori and S. Nakagawa, “Speaker change detection and speaker clus-
[41] X. Anguera, C. Wooters, B. Peskin, and M. Aguilo, “Robust speaker tering using VQ distortion for broadcast news speech recognition,” in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc. ICASSP, 2001, pp. 413–416.
system,” in Proc. NIST MLMI Meeting Recognition Workshop, Edin- [65] J. Ajmera and I. McCowan, “Robust speaker change detection,” IEEE
burgh, U.K., 2005. Signal Process. Lett., vol. 11, pp. 649–651, 2004.
[42] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speaker diarization of meetings,” IEEE Trans. Audio, Speech, Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process., vol. 15, no. 7, pp. 2011–2023, Sep. 2007. 358–361.
[43] X. Anguera, BeamformIt (The Fast and Robust Acoustic Beamformer) [67] X. Anguera and J. Hernando, “Evolutive speaker segmentation using a
[Online]. Available: http://www.xavieranguera.com/beamformit/ repository system,” in Proc. Interspeech, 2004.
[44] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
Time Series. New York: Wiley, 1949. multi-party meetings using acoustic fusion,” in Proc. ASRU, Nov. 2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp. 426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGI features for ASR,” in Proc. ICSLP, 2002, vol. 1, pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,” IEEE Signal Process. Lett., vol. 13, no. 8, pp. 509–512,
[46] M. L. Seltzer, B. Raj, and R. M. Stern, “Likelihood maximizing beam- Aug. 2006.
forming for robust hands-free speech recognition,” IEEE Trans. Speech [70] M.-H. Siu, G. Yu, and H. Gish, “Segregation of speakers for speech
Audio Process., vol. 12, no. 5, pp. 489–498, Sep. 2004. recognition and speaker identiﬁcation,” in Proc. ICASSP’91, 1991, pp.
[47] L. J. Grifﬁths and C. W. Jim, “An alternative approach to linearly con- 873–876.
strained adaptive beamforming,” IEEE Trans. Antennas Propagat., vol. [71] P. Delacourt and C. Wellekens, “DISTBIC : A speaker-based segmen-
AP-30, no. 1, pp. 27–34, Jan. 1982. tation for audio data indexing,” Speech Commun., pp. 111–126, 2000.
[48] M. Woelfel and J. McDonough, Distant Speech Recognition. New [72] S. S. Han and K. J. Narayanan, “Agglomerative hierarchical speaker
York: Wiley, 2009. clustering using incremental Gaussian mixture cluster modeling,” in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc. Interspeech’08, Brisbane, Australia, 2008, pp. 20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R. Gangadharaiah, B. Narayanaswamy, and N. Balakrishnan, “A novel
in Proc. Fall 2004 Rich Transcript. Workshop (RT04), Palisades, NY, method for two speaker segmentation,” in Proc. ICSLP, Jeju, Korea,
Nov. 2004. Sep. 2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 369
[74] D. Liu and F. Kubala, “Fast speaker change detection for broadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
news transcription and indexing,” in Proc. Eurospeech’99, Sep. 1999, Ph.D. dissertation, Univ. of California, Berkeley, 2008.
pp. 1031–1034. [102] S. Otterson and M. Ostendorf, “Efﬁcient use of overlap information in
[75] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen- speaker diarization,” in Proc. ASRU, Kyoto, Japan, 2007, pp. 686–6.
tation, classiﬁcation and clustering of broadcast news audio,” in Proc. [103] B. E. D. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech
DARPA Speech Recognit. Workshop, 1997, pp. 97–99. recognition using the modulation spectrogram,” Speech Commun., vol.
[76] P. Zochová and V. Radová, “Modiﬁed DISTBIC algorithm for speaker 25, no. 1-3, pp. 117–132, 1998.
change detection,” in Proc. 9th Eur. Conf. Speech Commun. Technol., [104] H. J. Nock, G. Iyengar, and C. Neti, “Speaker localization using audio-
Bonn, Germany, 2005, pp. 3073–3076. visual synchrony: An empirical study,” Lecture Notes in Comput. Sci.,
[77] X. Zhu, C. Barras, L. Lamel, and J.-L. Gauvain, “Speaker diarization: vol. 2728, pp. 565–570, 2003.
From broadcast news to lectures,” in Proc. MLMI, 2006, pp. 396–406. [105] C. Zhang, P. Yin, Y. Rui, R. Cutler, and P. Viola, “Boosting-based
[78] K. Han and S. Narayanan, “Novel inter-cluster distance measure com- multimodal speaker detection for distributed meetings,” in Proc. IEEE
bining GLR and ICR for improved agglomerative hierarchical speaker Int. Workshop Multimedia Signal Process. (MMSP), 2006, pp. 86–91.
clustering,” in Proc. ICASSP, Apr. 2008, pp. 4373–4376. [106] A. Noulas and B. J. A. Krose, “On-line multi-modal speaker diariza-
[79] D. Moraru, M. Ben, and G. Gravier, “Experiments on speaker tracking tion,” in Proc. 9th Int. Conf. Multimodal Interfaces ICMI ’07, New
and segmentation in radio broadcast news,” in Proc. ICSLP, 2005. York, 2007, pp. 350–357.
[80] C. Barras, X. Zhu, S. Meignier, and J.-L. Gauvain, “Improving speaker [107] Z. Ghahramani and M. I. Jordan, “Factorial hidden Markov models,”
diarization,” in Proc. DARPA RT04, 2004. Mach. Learn., vol. 29, pp. 245–273, Nov. 1997.
[81] H. Aronowitz, “Trainable speaker diarization,” in Proc. Interspeech, [108] A. K. Noulas, G. Englebienne, and B. J. A. Krose, “Mutimodal speaker
Aug. 2007, pp. 1861–1864. diarization,” IEEE Trans. Pattern Anal. Mach. Intell., 2011, preprint, to
[82] H. Hung and G. Friedland, “Towards audio-visual on-line diarization of be published.
participants in group meetings,” in Proc. Workshop Multi-Camera and [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-Modal Sensor Fusion Algorithms Applicat. –M2SFA2, Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France, 2008. Process., vol. 36, no. 2–3, pp. 117–124, 2004.
[83] G. Friedland and O. Vinyals, “Live speaker identiﬁcation in conversa- [110] T. Chen and R. Rao, “Cross-modal prediction in audio-visual commu-
tions,” in Proc. MM’08: Proc. 16th ACM Int. Conf. Multimedia, New nication,” in Proc. ICASSP, 1996, vol. 4, pp. 2056–2059.
York, 2008, pp. 1017–1018. [111] J. W. Fisher, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning joint
[84] G. Friedland, O. Vinyals, Y. Huang, and C. Muller, “Prosodic and other statistical models for audio-visual fusion and segregation,” in Proc.
long-term features for speaker diarization,” IEEE Trans. Audio, Speech, NIPS, 2000, pp. 772–778.
Lang. Process., vol. 17, no. 5, pp. 985–993, Jul. 2009. [112] J. W. Fisher and T. Darrell, “Speaker association with signal-level au-
[85] J. Luque, X. Anguera, A. Temko, and J. Hernando, “Speaker diariza- diovisual fusion,” IEEE Trans. Multimedia, vol. 6, no. 3, pp. 406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun. 2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R. Rao and T. Chen, “Exploiting audio-visual correlation in coding of
shops CLEAR 2007 and RT 2007, Baltimore, MD, May 8–11, 2007, talking head sequences,” in Proc. Int. Picture Coding Symp., Mar. 1996.
Revised Selected Papers, Berlin, Heidelberg: Springer-Verlag, 2008, [114] M. Siracusa and J. Fisher, “Dynamic dependency tests for audio-visual
pp. 543–553. speaker association,” in Proc. ICASSP, Apr. 2007, pp. 457–460.
[86] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- [115] E. K. Patterson, S. Gurbuz, Z. Tufekci, and J. N. Gowdy, “CUAVE: A
tiple distant microphone meetings: Mixing acoustic features and inter- new audio-visual database for multimodal human–computer interface
channel time differences,” in Proc. Interspeech, 2006. research,” in Proc. ICASSP, 2002, pp. 2017–2020.
[87] G. Lathoud and I. M. Cowan, “Location based speaker segmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
in Proc. ICASSP, 2003, vol. 1, pp. 176–179. Press, 2000.
[88] D. Ellis and J. C. Liu, “Speaker turn detection based on between-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nels differences,” in Proc. ICASSP, 2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J. Ajmera, G. Lathoud, and L. McCowan, “Clustering and segmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakers and their locations in meetings,” in Proc. ICASSP, 2004, vol. 1150–1153.
1, pp. 605–608. [118] H. Hung, Y. Huang, C. Yeo, and D. Gatica-Perez, “Associating audio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visual activity cues in a dominance estimation framework,” in Proc.
multiple distant microphone meetings: Mixing acoustic features and IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition (CVPR)
inter-channel time differences,” in Proc. Interspeech, 2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for mul- pp. 1–6.
tiple-distant-microphone meetings using several sources of informa- [119] N. Campbell and N. Suzuki, “Working with very sparse data to detect
tion,” IEEE Trans. Comput., vol. 56, no. 9, pp. 1212–1224, Sep. 2007. speaker and listener participation in a meetings corpus,” in Proc. Work-
[92] N. W. D. Evans, C. Fredouille, and J.-F. Bonastre, “Speaker diariza- shop Programme, May 2006, vol. 10.
tion using unsupervised discriminant analysis of inter-channel delay [120] G. Friedland, H. Hung, and C. Yeo, “Multimodal speaker diarization
features,” in Proc. ICASSP, Apr. 2009, pp. 4061–4064. of real-world meetings using compressed-domain video features,” in
[93] M. Wölfel, Q. Yang, Q. Jin, and T. Schultz, “Speaker identiﬁcation Proc. ICASSP, Apr. 2009, pp. 4069–4072.
using warped MVDR cepstral features,” in Proc. Interspeech, 2009. [121] G. Friedland, C. Yeo, and H. Hung, “Visual speaker localization aided
[94] E. Shriberg, “Higher-level features in speaker recognition,” in Speaker by acoustic models,” in Proc. 17th ACM Int. Conf. Multimedia MM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: , New York, 2009, pp. 195–202.
Springer, 2007, vol. 4343, Lecture Notes in Artiﬁcial Intelligence. [122] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, and L. Besacier,
[95] D. Imseng and G. Friedland, “Tuning-robust initialization methods for “Step-by-step and integrated approaches in broadcast news speaker di-
speaker diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18, no. 8, pp. 2028–2037, Nov. 2010. Workshop (Odyssey’04), 2006, pp. 303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speech recordings,” in Proc. IEEE Workshop Autom. Speech Recognit. glomerative and sequential clustering for speaker diarization,” in Proc.
Understand., Dec. 2009, pp. 432–437. ICASSP, Las Vegas, NV, 2008, pp. 4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E. El-Khoury, C. Senac, and S. Meignier, “Speaker diarization: Com-
Findings and implications for automatic processing of multi-party bination of the LIUM and IRIT systems,” in Internal Report, 2008.
conversations,” in Proc. Eurospeech’01, Aalborg, Denmark, 2001, pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O. Çetin and E. Shriberg, “Speaker overlaps and ASR errors in meet- speaker diarization of telephone conversations,” in IEEE Signal
ings: Effects before, during, and after the overlap,” in Proc. ICASSP, Process. Lett., Dec. 2007, vol. 14, no. 12, pp. 1040–1043.
Toulouse, France, 2006, pp. 357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K. Boakye, B. Trueba-Hornero, O. Vinyals, and G. Friedland, “Over- lems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.
lapped speech detection for improved speaker diarization in multiparty [127] F. Valente, “Inﬁnite models for speaker clustering,” in Proc. Int. Conf.
meetings,” in Proc. ICASSP, 2008, pp. 4353–4356. Spoken Lang. Process., 2006, iDIAP-RR 06–19.
[100] B. Trueba-Hornero, “Handling overlapped speech in speaker diariza- [128] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
tion,” M.S. thesis, Univ. Politecnica de Catalunya, Barcelona, Spain, Dirichlet processes,” J. Amer. Statist. Assoc., vol. 101, no. 476, pp.
2008. 1566–1581, 2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics, speech enhancement, and acoustic echo cancellation. His team led
HDP-HMM for systems with state persistence,” in Proc. ICML, Jul. LIA-EURECOM’s joint entry to the NIST Rich Transcription evaluations in
2008. 2009. He has authored or coauthored in excess of 50 peer-reviewed research
[130] M. Huijbregts and C. Wooters, “The blame game: Performance analysis articles and participates in several national and European projects, all involving
of speaker diarization system components,” in Proc. Interspeech, Aug. speech processing.
2007, pp. 1857–60. Dr. Evans is a member of the IEEE Signal Processing Society, ISCA, and
EURASIP and he serves as an Associate Editor of the EURASIP Journal on
Xavier Anguera Miro (M’06) received the Telecom- Audio, Speech, and Music Processing.
munications Engineering and European Masters in
Language and Speech (M.S.) degrees from the Uni-
versitat Politècnica de Catalunya (UPC), Barcelona,
Spain, in 2001 and the Ph.D. degree from UPC, with a Corinne Fredouille received the Ph.D. degree from
thesis on “Robust Speaker Diarization for Meetings.” the Laboratoire Informatique d’Avignon (LIA), Uni-
From 2001 to 2003, he was with Panasonic Speech versity of Avignon, Avignon, France, in 2000
Technology Lab, Santa Barbara, CA. From 2004 to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIA in 2003. Her research interests include acoustic
tional Computer Science Institute (ICSI), Berkeley, analysis, voice quality assessment, statistical
CA, where he pursued research on speaker diariza- modeling, automatic speaker recognition, speaker
tion for meetings, contributing to ICSI’s participation in the NIST RT evalua- diarization and, more recently, speech and voice
tions in 2004 (broadcast news) and 2005–2007 (meetings), obtaining state-of- disorder assessment and acoustic-based characteri-
the-art results. He brieﬂy joined LIMSI, Paris, France, in 2006. He has been zation. She has participated in several national and
with Telefonica Research, Barcelona, Spain, since 2007, pursuing research in international speaker diarization system evaluation
multimedia. His current research interests include speaker characterization (in- campaigns and has published over 15 research papers in this ﬁeld.
cluding diarization, recognition, etc.), language identiﬁcation (including a par- Prof. Fredouille is a member of the International Speech Communication As-
ticipation in NIST LRE’07 evaluation) and several topics in multimodal multi- sociation (ISCA) and secretary of the French speaking communication associ-
media analysis (e.g., video copy detection, involving the participation in NIST ation (AFCP), Special Interest Group (SIG) of ISCA.
TRECVID 2009 and 2010 evaluations). He has authored or coauthored over 50
peer-reviewed research articles. He is the main developer of the BeamformIt
toolkit, extensively used by the RT community for processing multiple micro-
phone recordings. Gerald Friedland (M’08) received the diplom and
Dr. Anguera Miro is a member of ISCA, ACM, and IEEE Signal Processing doctorate (summa cum laude) degrees in computer
Society and has been involved in the organization of several ACM and IEEE science from Freie Universität Berlin, Berlin, Ger-
conferences. He has been a reviewer for many conferences, as well as for several many, in 2002 and 2006, respectively.
journals in the multimedia domain. He is a Senior Research Scientist at the Interna-
tional Computer Science Institute (ICSI), Berkeley,
CA, an independent nonproﬁt research lab associated
with the University of California (UC) at Berkeley
Simon Bozonnet (S’08) received the diploma in where he, among other functions, is currently leading
electrical engineering from INSA de Lyon, France, the speaker diarization research. Apart from speech,
in 2008 with specialization in signal processing his interests also include image and video processing
and the Master of Research in Images and Systems and multimodal machine learning. He is a Principal Investigator on an IARPA
from INSA. He undertook his M.S. thesis at the project on video concept detection and a Co-Principal Investigator on an NGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURI grant on multimodal location estimation. Until 2009 he had been a site
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is which sponsored the research on multimodal meeting analysis algorithms.
currently pursuing the Ph.D. degree from Telecom Dr. Friedland is a member of the IEEE Computer Society and the IEEE Com-
ParisTech, Paris, France, and joined the Multimedia munication Society, and he is involved in the organization of various ACM and
Communications Department as a Ph.D. candidate IEEE conferences, including the IEEE International Conference on Semantic
with LIA-EURECOM, Sophia-Antipolis, France. Computing (ICSC), where he served as cochair and the IEEE International Sym-
As part of his studies, he spent one year at KTH (Royal Institute of posium on Multimedia (ISM2009), where he served as program cochair. He is
Technology), Stockholm, Sweden. His research interests include multimedia also cofounder and Program Director of the IEEE International Summer School
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- for Semantic Computing at UC Berkeley. He is the recipient of several research
RECOM recent submission to the NIST RT’09 evaluation and contributes his and industry recognitions, among them the Multimedia Entrepreneur Award by
expertise in speaker diarization to the national “ACAV” project which aims to the German government and the European Academic Software Award. Most re-
improve web accessibility for the visually and hearing impaired. cently, he won the ﬁrst prize in the ACM Multimedia Grand Challenge 2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea (UWS), Swansea, U.K., in 1999 and 2003, the Polytechnic University of Catalonia, Barcelona,
respectively. Spain, and the M.S. degree in computer science from
From 2002 and 2006, he was a Lecturer at UWS the University of California, San Diego, in 2009.
and was an Honorary Lecturer until 2009. He brieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon University of California, Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse (UAPV), Avignon, France, in 2006 before particular emphasis on machine learning, speech, and
moving to EURECOM, Sophia Antipolis, France, vision. He was a Visiting Scholar at the Computer
in 2007 where he is now an Assistant Professor. At Science Department, Carnegie Mellon University,
EURECOM, he heads research in speech and audio processing and is currently Pittsburgh, PA, in 2006, where he worked in computer vision and robotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr. Vinyals received a Microsoft Research Ph.D. Fellowship in 2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 