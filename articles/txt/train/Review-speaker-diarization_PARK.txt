A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27A Review of Speaker Diarization: Recent Advances with Deep Learning
Tae Jin Parka,∗, Naoyuki Kandab,∗, Dimitrios Dimitriadisb,∗, Kyu J. Hanc,∗, Shinji Watanabed,∗, Shrikanth Narayanana
aUniversity of Southern California, Los Angeles, USA
bMicrosoft, Redmond, USA
cASAPP, Mountain View, USA
dJohns Hopkins University, Baltimore, USA
Abstract
Speaker diarization is a task to label audio or video recordings with classes corresponding to speaker identity, or in short, a task
1 to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multi-
2 speaker audio recordings to enable speaker adaptive processing, but also gained its own value as a stand-alone application over
0 time to provide speaker-speciﬁc meta information for downstream tasks such as audio retrieval. More recently, with the rise of
2
deep learning technology that has been a driving force to revolutionary changes in research and practices across speech application
 
n domains in the past decade, more rapid advancements have been made for speaker diarization. In this paper, we review not
a only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization
J
  approaches. We also discuss how speaker diarization systems have been integrated with speech recognition applications and how
4
the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other.
2
By considering such exciting technical trends, we believe that it is a valuable contribution to the community to provide a survey
 
]  work by consolidating the recent developments with neural methods and thus facilitating further progress towards a more eﬃcient
S
speaker diarization.
A
Keywords: speaker diarization, automatic speech recognition, deep learning
.
s
s
e
e 1. Introduction processing techniques, for example, speech enhancement, dere-
[
  verberation, speech separation or target speaker extraction, are
 
1 “Diarize” is a word that means making a note or keeping an utilized. Voice or speech activity detection is then applied to
v event in a diary. Speaker diarization, like keeping a record separate speech from non-speech events. Raw speech signals
4
of events in such a diary, addresses the “who spoke when” in the selected speech portions are transformed to acoustic fea-
2
6 question [1, 2, 3] by logging speaker-speciﬁc salient events tures or embedding vectors. In the clustering stage, the speech
9 on multi-participant (or multi-speaker) audio data. Through- portion represented by the embedding vectors are grouped and
0 out the diarization process, the audio data would be divided labeled by speaker classes and in the post-processing stage, the
1. and clustered into groups of speech segments with the same clustering results are further reﬁned. Each of these sub-modules
0 speaker identity/label. As a result, salient events, such as non- is optimized individually in general.
1 speech/speech transition, speaker turn changes, speaker classi-
2
ﬁcation or speaker role identiﬁcation, are labeled in an auto-
: 1.1. Historical development of speaker diarization
v matic fashion. In general, this process does not require any
i prior knowledge of the speakers, such as their real identity or During the early years of diarization technology (in the
X
number of participating speakers in the audio data. Thanks to 1990s), the research focus was on unsupervised speech seg-
r
a its innate feature of separating audio streams by these speaker- mentation and clustering of acoustic events including not only
speciﬁc events, speaker diarization can be eﬀectively employed speaker-speciﬁc ones but also those related to environmental
for indexing or analyzing various types of audio data, e.g., au- or background changes [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
dio/video broadcasts from media stations, conversations in con- In this period some of the fundamental approaches to speaker
ferences, personal videos from online social media or hand-held change detection and clustering, such as leveraging Generalized
devices, court proceedings, business meetings, earnings reports Likelihood Ratio (GLR) and Bayesian Information Criterion
in a ﬁnancial sector, just to name a few. (BIC), were developed and quickly became the golden stan-
Traditionally speaker diarization systems consist of multiple, dard. Most of the works beneﬁted Automatic Speech Recogni-
independent sub-modules as shown in Fig. 1. In order to mit- tion (ASR) on broadcast news recordings, by enabling speaker
igate any artifacts in acoustic environments, various front-end adaptive training of acoustic models [10, 15, 16, 17, 18]. All
these eﬀorts collectively laid out a path to consolidate ac-
tivities across research groups around the world, leading to
∗Authors contributed equally several research consortia and challenges in the early 2000s,
Preprint submitted to Computer, Speech and Language January 26, 2021Audio  Input Diarization  Output
(RTTM)
Front-End  Speech Activity Speaker  Post
Segmentation Clustering
Processing Detection Embedding Processing
Section 2.1 Section 2.2 Section 2.3 Section 2.4 Section 2.5 Section 2.6
Fig. 1: Traditional Speaker Diarization Systems.
among which there were the Augmented Multi-party Interac- corresponding technologies to mitigate problems from the per-
tion (AMI) Consortium [19] supported by the European Com- spective of meeting environments, where there are usually more
mission and the Rich Transcription Evaluation [20] hosted by participants than broadcast news or CTS data and multi-modal
the National Institute of Standards and Technology (NIST). data is frequently available. Since these two papers, especially
These organizations, spanning over from a few years to a thanks to leap-frog advancements in deep learning approaches
decade, had fostered further advancements on speaker diariza- addressing technical challenges across multiple machine learn-
tion technologies across diﬀerent data domains from broadcast ing domains, speaker diarization systems have gone through a
news [21, 22, 23, 24, 25, 26, 27, 28, 29] and Conversational lot of notable changes. We believe that this survey work is a
Telephone Speech (CTS) [24, 30, 31, 32, 33, 34] to meeting valuable contribution to the community to consolidate the re-
conversations [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. The cent developments with neural methods and thus facilitate fur-
new approaches resulting from these advancements include, ther progress towards a more eﬃcient diarization.
but not limited to, Beamforming [42], Information Bottleneck
Clustering (IBC) [44], Variational Bayesian (VB) approaches 1.3. Overview and Taxonomy of speaker diarization
[33, 45], Joint Factor Analysis (JFA) [46, 34]. Attempting to categorize the existing, most-diverse speaker
Since the advent of deep learning in the 2010s, there has diarization technologies, both on the space of modularized
been a considerable amount of research to take the advantage speaker diarization systems before the deep learning era and
of powerful modeling capabilities of the neural networks for those based on neural networks of the recent years, a proper
speaker diarization. One representative example is extracting grouping would be helpful. The main categorization we adopt
the speaker embeddings using neural networks, such as the d- in this paper is based on two criteria, resulting in the total four
vectors [47, 48, 49] or the x-vectors [50], which most often are categories, as shown in Table 1. The ﬁrst criterion is whether
embedding vector representations based on the bottleneck layer the model is trained based on speaker diarization-oriented ob-
output of a “Deep Neural Network” (DNN) trained for speaker jective function or not. Any trainable approaches to optimize
recognition. The shift from i-vector [51, 52, 53, 54] to these models in a multi-speaker situation and learn relations between
neural embeddings contributed to enhanced performance, eas- speakers are categorized into the “Diarization Objective” class.
ier training with more data [55], and robustness against speaker The second criterion is whether multiple modules are jointly
variability and acoustic conditions. More recently, End-to-End optimized towards some objective function. If a single sub-
Neural Diarization (EEND) where individual sub-modules in module is replaced into a trainable one, such method is catego-
the traditional speaker diarization systems (c.f., Fig. 1) can rized into the “Single-module Optimization” class. On the other
be replaced by one neural network gets more attention with hand, for example, joint modeling of segmentation and cluster-
promising results [56, 57]. This research direction, although not ing [55], joint modeling of speech separation and speaker di-
fully matured yet, could open up unprecedented opportunities arization [76] or fully end-to-end neural diarization [56, 57] is
to address challenges in the ﬁeld of speaker diarization, such categorized into the “Joint Optimization” class.
as, the joint optimization with other speech applications, with Note that our intention of this categorization is to help read-
overlapping speech, if large-scale data is available for training ers to quickly overview the broad development in the ﬁeld, and
such powerful network-based models. it is not our intention to divide the categories into superior-
inferior. Also, while we are aware of many techniques that fall
1.2. Motivation into the category “Non-Diarization Objective” and “Joint Opti-
mization” (e.g., joint front-end and ASR [67, 68, 69, 70, 71, 72],
Till now, there are two well-rounded overview papers in
joint speaker identiﬁcation and speech separation [73, 74], etc.),
the area of speaker diarization surveying the development of
we exclude them in the paper to focus on the review of speaker
speaker diarization technology with diﬀerent focuses. In [2],
diarization techniques.
various speaker diarization systems and their subtasks in the
context of broadcast news and CTS data are reviewed up to
1.4. Paper Organization
the point of mid 2000s. As such, the historical progress of
The rest of the paper is organized as follows.
speaker diarization technology development in the 1990s and
early 2000s are hence covered. In contrast, the focus of [3] • In Section 2, we overview techniques belonging to the
was put more on speaker diarization for meeting speech and “Non-Diarization Objective” and “Single-module Opti-
its respective challenges. This paper thus weighs more in the mization” class in the proposed taxonomy, mostly those
2Table 1: Table of Taxonomy
Non-Diarization Diarization
Objective Objective
Section 2 Section 3.1
Single-module Front-end [58, 59, 60], speaker IDEC [64], aﬃnity matrix
Optimization embedding [61, 62, 50], speech reﬁnement [65], TS-VAD [66], etc.
activity detection [63], etc.
Out of scope Section 3.2
Joint front-end & ASR UIS-RNN [55], RPN [75], online
Joint [67, 68, 69, 70, 71, 72], joint RSAN [76], EEND [56, 57], etc.
Optimization speaker identiﬁcation & speech Section 4
separation [73, 74], etc. Joint ASR & speaker diarization.
[77, 78, 79, 80], etc.
used in the traditional, modular speaker diarization sys- each module, this section also summarizes the latest schemes
tems. While there are some overlaps with the counterpart within the module.
sections of the aforementioned two survey papers [2, 3] in
terms of reviewing notable developments in the past, this 2.1. Front-end processing
section would add more latest schemes as well in the corre-
This section describes mostly front-end techniques, used for
sponding components of the speaker diarization systems.
speech enhancement, dereverberation, speech separation, and
• In Section 3, we discuss advancements mostly leveraging speech extraction as part of the speaker diarization pipeline. Let
DNNs trained with the diarization objective where single si, f,t ∈ C be the STFT representation of source speaker i on
sub-modules are independently optimized (subsection 3.1) frequency bin f at frame t. The observed noisy signal xt, f can be
or jointly optimized (subsection 3.2) toward fully end-to- represented by a mixture of the source signals, a room impulse
end speaker diarization. response hi, f,t ∈ C, and additive noise nt, f ∈ C,
• In Section 4, we present a perspective of how speaker di- (cid:88)K (cid:88)
arization has been investigated in the context of ASR, re- xt, f = hi, f,τsi, f,t−τ + nt, f , (1)
viewing historical interactions between these two domains i=1 τ
to peek the past, present and future of speaker diarization
where K denotes the number of speakers present in the audio
applications.
signal.
• Section 5 provides information of speaker diarization chal- The front-end techniques described in this section is to esti-
mate the original source signal xˆ given the observation X =
lenges and corpora to facilitate research activities and an- i,t
({x } ) for the downstream diarization task,
chor techonology advances. We also discuss evaluation t, f f t
metrics such as Diarization Error Rate (DER), Jaccard Er-
xˆ = FrontEnd(X), i = 1, . . . , K, (2)
ror Rate (JER) and Word-level DER (WDER) in the sec- i,t
tion. where xˆ ∈ CD is the i-th speaker’s estimated STFT spectrum
i,t
• We share a few examples of how speaker diarization sys- with D frequency bins at frame t.
Although there are numerous speech enhancement, dereber-
tems are employed in both research and industry practices
beration, and separation algorithms, e.g., [81, 82, 83], herein
in Section 6 and conclude this work in Section 7 with pro-
most of the recent techniques used in the DIHARD challenge
viding summary and future challenges in speaker diariza-
series [84, 85, 86], LibriCSS meeting recognition task [87, 88],
tion.
and CHiME-6 challenge track 2 [89, 90, 91] are covered.
2.1.1. Speech enhancement (Denoising)
2. Modular Speaker Diarization Systems
Speech enhancement techniques focus mainly on suppress-
This section provides an overview of algorithms for speaker ing the noise component of the noisy speech. Single-channel
diarization belonging to the “Single-module Optimization, speech enhancement has shown a signiﬁcant improvement in
Non-Diarization Objective” class mostly modular speaker, as denoising performance [92, 93, 94] thanks to deep learning,
shown in Figure 1. Each subsection in this section corresponds when compared with classical signal processing based speech
to the explanation of each module in the traditional speaker di- enhancement [95]. For example, LSTM-based speech enhance-
arization system. In addition to the introductory explanation of ment [96, 94] is used as a front-end technique in the DIHARD
3II baseline [85], i.e., 2.1.3. Speech separation and target speaker extraction
Speech separation is a promising family of techniques when
xˆt = LSTM(X), (3) the overlapping speech regions are signiﬁcant. Similarly to
other research areas, DL-based speech separation has become
where we only consider the single source example (i.e., K = 1) popular, e.g, “Deep Clustering” [58], “Permutation Invariant
and omit the source index i. This is a regression-based approach Training” (PIT) [59], and Conv-TasNet [60]. The eﬀective-
by minimizing the objective function, ness of multi-channel speech separation based on beamforming
has been widely conﬁrmed [102, 103], as well. For example,
LMSE = ||st − xˆt||2. (4) in the CHiME-6 challenge [89], “Guided Source Separation”
(GSS) [103] based multi-channel speech extraction techniques
The log power spectrum or ideal ratio mask is often used as the have been used to achieve the top result. On the other hand,
target domain of the output st. Also, the speech enhancement single-channel speech separation techniques do not often show
used in [95] applies this objective function in each layer based any signiﬁcant eﬀectiveness in realistic multi-speaker scenar-
on a progressive manner. ios like the LibriCSS [87] or the CHiME-6 tasks [89], where
The eﬀectiveness of the speech enhancement techniques can speech signals are continuous and contain both overlapping and
be boosted multi-channel processing, including minimum vari- overlap-free speech regions. The single-channel speech sepa-
ance distortionless response (MVDR) beamforming [81]. [88] ration systems often produce a redundant non-speech or even
shows the signiﬁcant improvement of the DER from 18.3% a duplicated speech signal for the non-overlap regions, and as
to 13.9% in the LibriCSS meeting task based on mask-based such the “leakage” of audio causes many false alarms of speech
MVDR beamforming [97, 98]. activity. A leakage ﬁltering method was proposed in [104] tack-
ling the problem, where a signiﬁcant improvement of speaker
diarization performance was shown after including this pro-
2.1.2. Dereverberation
cessing step in the top-ranked system on the VoxCeleb Speaker
Compared with other front-end techniques, the major dere-
Recognition Challenge 2020 [105].
verberation techniques used in various tasks is based on statis-
tical signal processing methods. One of the most widely used
2.2. Speech activity detection (SAD)
techniques is Weighted Prediction Error (WPE) based derever-
beration [99, 100, 101].
SAD distinguishes speech segments from non-speech seg-
The basic idea of WPE, for the case of single source, i.e.
ments such as background noise. A SAD system is mostly
K = 1, without noise, is to decompose the original signal model
comprised of two parts. The ﬁrst one is a feature extraction
Eq. (1) into the early reﬂection xearly and late reverberation xlate frontend, where acoustic features such as Mel-Frequency Cep-
t, f t, f
as follows: stral Coeﬃcients (MFCCs) are extracted. The other part is a
(cid:88) classiﬁer, where a model predicts whether the input frame is
xt, f = h f,τs f,t−τ = xte,afrly + xtla, fte. (5) speech or not. These models may include Gaussian Mixture
τ Models (GMMs) [106], Hidden Markov Models (HMMs) [107]
or DNNs [63].
WPE tries to estimate ﬁlter coeﬃcients hˆwf,pt e ∈ C, which main- The performance of SAD largely aﬀects the overall perfor-
tain the early reﬂection while suppress the late reverberation
mance of the speaker diarization system because it can cre-
based on the maximum likelihood estimation.
ate a signiﬁcant amount of false positive salient events or miss
speech segments [108]. A common practice in speaker diariza-
(cid:88)L
xˆearly = x − hˆwpex , (6) tion tasks is to report DER with “oracle SAD” setup which in-
t,, f t, f f,τ f,t−τ
τ=∆ dicates that the system output is using speech activity detection
output that is identical to the ground truth. On the other hand,
where ∆ is the number of frames to split the early reﬂection and the system output with an actual speech activity detector is re-
late reverberation, and L is the ﬁlter size. ferred to as “system SAD” output.
WPE is widely used as one of the golden standard front-
end processing methods, e.g., it is part of the DIHARD and
2.3. Segmentation
CHiME both the baseline and the top-performing systems
[84, 85, 86, 89, 90]. Although the performance improvement Speech segmentation breaks the input audio stream into mul-
of WPE-based dereverberation is not signiﬁcant, it provides tiple segments so that the each segment can be assigned to a
solid performance improvement across almost all tasks. Also, speaker label. Before re-segmentation phase, the unit of the out-
WPE is based on the linear ﬁltering and since it does not intro- put of speaker diarization system is determined by segmenta-
duce signal distortions, it can be safely combined with down- tion process. There are two ways of performing speech segmen-
stream front-end and back-end processing steps. Similar to the tation for speaker diarization tasks: either with speaker change
speech enhancement techniques, WPE-based dereberberation point detection or uniform segmentation. The segmentation by
shows additional peformance improvements when applied on detecting the speaker change point was the golder standard of
multi-channel signals. the earlier speaker diarization systems, where speaker change
4points are detected by comparing two hypotheses: Hypothe- However, the process of uniformly segmenting the input sig-
sis H assumes both left and right samples are from the same nals for diarization poses some potential problems. First, uni-
0
speaker and hypothesis H assumes the two samples are from form segmentation introduces a trade-oﬀ error related to the
1
the diﬀerent speakers. Many algorithms for the hypothesis test- segment length: segments need to be suﬃciently short to safely
ing, such as Kullback Leibler 2 (KL2) [10], “Generalized Like- assume that they do not contain multiple speakers but at the
lihood Ratio” (GLR) [109] and BIC [110, 111] were proposed same time it is necessary to capture enough acoustic informa-
with the BIC method been the most widely used method. The tion to extract a meaningful speaker representation x .
j
BIC approach can be applied to segmentation process as fol-
lows: assuming that X = {x1, · · · , xN} is the sequence of speech 2.4. Speaker Representations and Speaker Embeddings
features extracted from the given audio stream and x is drawn
In this section, we explain a few popular methods for mea-
from from an independent multivariate Gaussian process:
suring the similarity of speech segments. These methods are
x ∼ N (µ , Σ ) , (7) paired with clustering algorithms, which will be explained in
i i i
the next section. We ﬁrst introduce GMM based hypothesis
where µi, Σi is mean and covariance matrix of the i-th feature testing approaches which are usually employed with segmenta-
window, two hypothesis H0 and H1 can be denoted as follows: tion approaches based on a speaker change point detection. We
then introduce well-known speaker representations for speaker
H : x · · · x ∼ N(µ, Σ) (8)
0 1 N diarization systems that are usually employed with the uniform
H1 : x1 · · · xi ∼ N (µ1, Σ1) (9) segmentation method in Section 2.4.2 and Section 2.4.3.
xi+1 · · · xN ∼ N (µ2, Σ2) (10)
2.4.1. GMM speaker model for similarity measure
Thus, hypothesis H models two sample windows with one
0 The early days of speaker diarization systems were based on
Gaussian while hypothesis H models two sample windows
1 a GMM built on acoustic features such as the MFCCs. Along
with two Gaussians. Using the Eq. (8), the maximum likeli-
with GMM based method, AHC was also employed for clus-
hood ratio statistics can be expressed as
tering, resulting in the speaker homogeneous clusters. While
R(i) = N log |Σ| − N log |Σ | − N log |Σ | , (11) there are many hypothesis testing methods for speech segment
1 1 2 2
clustering process such as greedy BIC [110], GLR [114] and
where the sample covariance Σ is from {x , · · · , x }, Σ is from KL [115] methods, greedy BIC method was the most popular
1 N 1
{x1, · · · , xi} and Σ2 is from {xi+1, · · · , xN}. Finally, a BIC value approach. While greedy BIC method also employs BIC value as
between two models is expressed: in speaker change point detection, in greedy BIC method, BIC
value is used for measuring the similarity between two nodes
BIC(i) = R(i) − λP, (12) during the AHC process. For the given nodes to be clustered,
S = {s , · · · , s }, greedy BIC method model each node s as a
where P is the penalty term [110] deﬁned as multiva1riate Gakussian distribution N (µ , Σ ) where µ and Σi are
i i i i
(cid:32) (cid:33) mean and co variance matrix of the merged samples in the node
1 1
P = d + d(d + 1) log N, (13) s . BIC value for merging the node s and s is calculated as
2 2 i 1 2
and d is dimension of the feature. The penalty weight λ is gen- BIC = n log |Σ| − n1 log |Σ1| − n2 log |Σ2| − λP, (15)
erally set to λ = 1. The change point is set when the following
equation becomes true, where λ and P value are identical to Eq. (12) and n is sample
size of the merged node (n = n + n ). During the clustering
(cid:26) (cid:27) 1 2
process, we merge the modes if Eq. (15) is negative. GMM
max BIC(i) > 0. (14)
i based hypothesis testing method with bottom-up hierarchical
clustering method was popularly used until i-vector and DNN-
As described above, the speaker change points can be detected
based speaker representations dominate the speaker diarization
by using hypothesis testing based on BIC values or other meth-
research scene.
ods such as KL2 [10], GLR [109]. However, if speech seg-
mentation is done by speaker change point detection method,
the length of each segment is not consistent. Therefore, after 2.4.2. Joint Factor Analysis and i-vector
the advent of i-vector [51] and DNN-based embeddings [61] Before the advent of speaker representations such as i-
the segmentation based on speaker change point detection was vector [51] or x-vector [50], “Universal Background Model”
mostly replaced by uniform segmentation [112, 113, 49], since (UBM) [116] framework showed success for speaker recogni-
varying length of the segment created an additional variability tion tasks by employing a large mixture of Gaussians, while
into the speaker representation and deteriorated the ﬁdelity of covering a fairly large amount of speech data. The idea of
the speaker representations. In uniform segmentation schemes, modeling and testing the similarity of voice characteristics with
the given audio stream input is segmented with a ﬁxed win- GMM-UBM [116] is largely improved by JFA [117, 118].
dow length and overlap length. Thus, the length of the unit of GMM-UBM based hypothesis testing had a problem of Max-
speaker diarization result is remains ﬁxed. imum a Posterior (MAP) adaptation that is not only aﬀected
5by speaker-speciﬁc characteristics but also other nuisance fac-
tors such as channel and background noise. Therefore, the con-
cept of supervector generated by GMM-UBM method was not
ideal. JFA tackles this problem and decompose a supervector
into speaker independent, speaker dependent, channel depen-
dent and residual components. Thus, the ideal speaker super-
vector s can be decomposed as in the Eq. (16). A term m de-
notes speaker independent component, U denotes channel de-
pendent component matrix, and D denotes speaker-dependent
residual component matrix. Along with these component ma-
trices, vector y is for the speaker factors, vector x is for the
channel factors and vector z is for the speaker-speciﬁc residual
factors. All of these vectors have a prior distribution of N(0, 1).
M(s) = m + Vy + Ux + Dz. (16)
The idea of JFA approach is further simpliﬁed by employing
Fig. 2: Diagram of d-vector model.
the so called “Total Variability” matrix T modeling both the
channel and the speaker variability, and the vector w which is
referred to as the “i-vector” [51]. The supervector M is modeled
as:
M = m + Tw, (17)
In Eq. (17), m is the session and channel-independent com-
ponent of the mean supervector. Similarly to JFA, w is as-
sumed to follow standard normal distribution and calculated by
MAP estimation, in [119]. The notion of speaker representa-
tion is popularized by i-vectors, where the speaker represen-
tation vector can contain a numerical feature that characterize
the vocal tract of each speaker. The i-vector speaker represen-
tations have employed in not only speaker recognition studies
but also in numerous speaker diarization studies [112, 120, 121]
and showed superior performance over GMM-based hypothesis
testing methods.
2.4.3. Neural Network Based Speaker Representations Fig. 3: Diagram of x-vector embedding extractor.
Speaker representations for speaker diarization has also been
heavily aﬀected by the rise of neural networks and deep learn-
ing approaches. The idea of representation learning was ﬁrst with the cross entropy loss. The d-vector embeddings are ob-
introduced for face recognition tasks [122, 123]. The fun- tained in the last fully connected layer as in Fig. 2. The d-vector
damental idea of neural network-based representations is that scheme appears in numerous speaker diarization papers, e.g., in
we can use deep neural network architecture to map the in- [49, 55].
put signal source (an image or an audio clip) to a dense vec- DNN-based speaker representations are even more improved
tor by sampling the activations of a layer in the neural network by x-vector [62, 50]. The x-vector showed a superior per-
model. The neural network based representation does not re- formance by winning the NIST speaker recognition challenge
quire eigenvalue decomposition or factor analysis model that [124] and the ﬁrst DIHARD challenge [84]. Fig. 3 shows the
involves hand-crafted design of the intrinsic factor. Also, there structure of x-vector framework. The time-delay architecture
is no assumption or requirement of Gaussianity for the input and statistical pooling layer diﬀerentiate x-vector architecture
data. Thus, the representation learning process has become from d-vector while statistical pooling layer mitigates the eﬀect
more straight-forward and the inference speed has been also im- of the input length. This is especially advantageous when it
proved compared to the traditional factor analysis based meth- comes to speaker diarization since the speaker diarization sys-
ods. tems are bound to process segments that are shorter than the
Among many of the neural network based speaker repre- regular window length.
sentations, d-vector [61] remains one of the most prominent For speaker diarization tasks, “Probabilistic Linear Distcrim-
speaker representation extraction frameworks. The d-vector inant Analysis” (PLDA) has been frequently used along with
employs stacked ﬁlterbank features that include context frames x-vector or i-vector to measure the aﬃnity between two speech
as an input feature and trains a multiple fully connected layers segments. PLDA employs the following modeling for the given
6speaker representation φ of the i-th speaker and j-th session as
i j
below:
φ = µ + Fh + Gw + (cid:15) . (18)
i j i i j i j
Here, m is mean vector, F is speaker variability matrix, G is
channel variability matrix and (cid:15) is residual component. The
terms h and w are latent variable for F and G respectively.
i i j
During the training process of PLDA, m, Σ, F and G are esti-
mated using expectation maximization (EM) algorithm where
Σ is a covariance matrix. Based on the estimated variability Fig. 4: Agglomerative Hierarchical Clustering.
matrices and the latent variables h and w , two hypotheses are
i i j
tested: hypothesis H for the case that two samples are from the
0
same speaker and hypothesis H for the case that two samples
1
are from diﬀerent speakers. The hypothesis H can be written
0
as follows:
 
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) FF G0 0G (cid:35)  hww112  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (19)
2
On the other hand, The hypothesis H can be modeled as the
1
following equation.
 
h
(cid:34) φφ12 (cid:35) = (cid:34) µµ (cid:35) + (cid:34) F0 G0 F0 G0 (cid:35)  wh121  + (cid:34) (cid:15)(cid:15)12 (cid:35) . (20) Fig. 5: General steps of spectral clustering.
w
2
3. Shift the search window to the new mean.
The PLDA model projects the given speaker representation
onto the subspace F to co-vary the most while de-emphasizing 4. Repeat the process until convergence.
the subspace G pertaining to channel variability. Using the
above hypotheses, we can calculate a log likelihood ratio. Mean-shift clustering algorithm was applied to speaker diariza-
tion task with KL distance [126], i-vector and cosine distance
s (φ1, φ2) = log p (φ1, φ2 | H0) − log p (φ1, φ2 | H1) . (21) in [112, 127] and i-vector and PLDA [128]. The advantage
of mean-shift clustering algorithm is that the clustering algo-
Ideally, stopping criterion should be 0, but in practice it varies
rithm does not require the number of clusters in advance unlike
from around zero values and the stopping criterion needs to be
k-means clustering methods. This becomes a signiﬁcant advan-
tuned on development set. The stopping criterion largely aﬀects
tage in speaker diarization tasks where the number of speakers
the estimated number of speakers because the clustering pro-
is unknown as in most of the applications.
cess stops when the distance between closest samples reaches
threshold and the number of clusters is determined by the num-
2.5.2. Agglomerative Hierarchical Clustering (AHC)
ber of remaining clusters at the step where clustering is stopped.
AHC is a clustering method that has been constantly em-
ployed in many speaker diarization systems with a number of
2.5. Clustering
diﬀerent distance metric such as BIC [110, 129], KL [115] and
After generating the speaker representations for each seg-
PLDA [84, 90, 130]. AHC is an iterative process of merging
ment, clustering algorithm is applied to make clusters of seg-
the existing clusters until the clustering process meets a crite-
ments. We introduce the most commonly used clustering meth-
rion. AHC process starts by calculating the similarity between
ods for speaker diarization task.
N singleton clusters. At each step, a pair of clusters that has the
highest similarity is merged. The iterative merging process of
2.5.1. Mean-shift
AHC produces a dendrogram which is depicted in Fig. 4.
Mean-shift [125] is a clustering algorithm that assigns the
One of the most important aspect of AHC is the stopping
given data points to the clusters iteratively by ﬁnding the modes
criterion. For speaker diarization task, AHC process can be
in a non-parametric distribution. Mean-shift algorithm follows
stopped using either a similarity threshold or a target number
the following steps:
of clusters. Ideally, if PLDA is employed as distance metric,
the AHC process should be stopped at s (φ , φ ) = 0 in Eq.
1. Start with the data points assigned to a cluster of their own. 1 2
(18). However, it is widely employed that the stopping metric
2. Compute a mean for the each group. is adjusted to get an accurate number of clusters based on a
7development set. On the other hand, if the number of speakers while choosing σ by using predeﬁned scalar value β and vari-
is known or estimated by other methods, AHC process can be ance values from the data points while the speaker diarization
stopped when the clusters created by AHC process reaches the system in [134] did not use β value for NJW algorithm. On the
pre-determined number of speaker K. other hand, in the speaker diarization system in [52], σ2 = 0.5
for NJW algorithm.
2.5.3. Spectral Clustering Aside from NJW spectral clustering algorithm, many other
Spectral Clustering is another popular clustering approach types of spectral clustering were successfully applied to speaker
for speaker diarization. While there are many variations, spec- diarization task. The speaker diarization system in [49] em-
tral clustering involves the following steps. ployed Gaussian blur for aﬃnity values, diﬀusion process Y =
XXT and row-wise max normalization (Y = X / max X ).
i j i j k ik
i. Aﬃnity Matrix Calculation: There are many ways to gen- In the spectral clustering approach appeared in [135], similar-
erate an aﬃnity matrix A depending on the way the aﬃnity ity values that are calculated from a neural network model were
value is processed. The raw aﬃnity value d is processed used without any kernel, and the unnormalized graph Laplacian
(cid:16) (cid:17)
by kernel such as exp −d2/σ2 where σ is a scaling pa- is employed to perform spectral clustering. More recently, auto-
rameter. On the other hand, the raw aﬃnity value d could tuning spectral clustering method was proposed for speaker di-
also be masked by zeroing the values below a threshold to arization task [136] where the proposed clustering method does
only keep the prominent values. not require parameter tuning on a separate development set.
The work in [136] employs binarized aﬃnity matrix with the
ii. Laplacian Matrix Calculation [131]: The graph Laplacian row-wise count p and the binarization parameter p is selected
can be calculated in the following two types; normalized by choosing the minimum value of r(p) = p/g where g rep-
p p
and unnormalized. The degree matrix D contains diagonal resents the maximum eigengap from the unnormalized graph
elements d = (cid:80)n a where a is the element of the i-th Laplacian matrix. Thus, r(p) represents how clear the clusters
i j=1 i j i j
row and j-th column in an aﬃnity matrix A. are for the given value p and p could be automatically selected
to perform spectral clustering without tuning the p-value.
(a) Normalized Graph Laplacian:
2.6. Post-processing
L = D−1/2AD−1/2. (22)
2.6.1. Resegmentation
Resegmentation is a process to reﬁne the speaker boundary
(b) Unnormalized Graph Laplacian:
that is roughly estimated by the clustering procedure. In [137],
L = D − A. (23) Viterbi resegmentation method based on the Baum-Welch al-
gorithm was introduced. In this method, estimation of Gaus-
iii. Eigen Decomposition: The graph Laplacian matrix L is sian mixture model corresponding to each speaker and Viterbi-
decomposed into the eigenvector matrix X and the diago- algorithm-based resgmentation by using the estimated speaker
nal matrix that contains eigenvalues. Thus, L = XΛX(cid:62). GMM are alternately applied.
Later, a method to represent the diarization process based on
iv. Re-normalization (optional) : the rows of X is normalized Variational Bayeian Hidden Markov Model (VB-HMM) was
so that y = x / (cid:16)(cid:80) x2 (cid:17)1/2 where x and y are the ele- proposed, and was shown to be superior as a resegmentation
i j i j j i j i j i j
ments of the i-th row and j-th column in matrix X and Y, method compared to Viterbi resegmentation [138, 139, 140].
respectively. In the VB-HMM framework, the speech feature X = (xt|t =
1, ..., T ) is assumed to be generated from HMM where each
v. Speaker Counting: Speaker number is estimated by ﬁnd- HMM state corresponds to one of K possible speakers. Given
ing the maximum eigengap. we have M HMM states, M-dimensional variable Z = (z |t =
t
1, ..., T ) is introduced where k-th element of z is 1 if k-th
t
vi. k-means Clustering: The k-smallest eigenvalues λ , λ ,...,
1 2 speaker is speaking at the time index t, and 0 otherwise. At the
λ and the corresponding eigenvectors v , v ,..., v are
n 1 2 k same time, the distribution of x is modeled based on a hidden
used to make U ∈ Rm×n where m is dimension of the row variable Y = {y |i = 1, ..., K}, twhere y is a low dimensional
k k
vectors in U. Finally, the row vectors u , u ,...,u are clus-
1 2 n vector for k-th speaker. Given these notation, the joint proba-
tered by k-means algorithm.
bility of X, Y, and Z is decomposed as
Among many variations of spectral clustering algorithm, Ng- P(X, Z, Y) = P(X|Z, Y)P(Z)P(Y), (24)
Jordan-Weiss (NJW) algorithm [132] is often employed for
speaker diarization task. NJW algorithm employs a kernel where P(X|Z, Y) is the emission probability modeled by GMM
(cid:16) (cid:17)
exp −d2/σ2 where d is a raw distance for calculating an aﬃn- whose mean vector is represented by Y, P(Z) is the transition
ity matrix. The aﬃnity matrix is used for calculating a nor- probability of the HMM, and P(Y) is the prior distribution of Y.
malized graph Laplacian. In addition, NJW algorithm involves Because Z represents the trajectory of speakers, the diarization
renormalization before the k-means clustering process. The problem can be expressed as the inference problem of Z that
(cid:82)
speaker diarization system in [133] employed NJW algorithm maximize the posterior distribution P(Z|X) = P(Z, Y|X)dY.
8Since it is intractable to directory solve this problem, Varia-
tional Bayes method is used to estimate the model parameters
that approximate P(Z, Y|X) [139, 141]. The VB-HMM frame-
work was originally designed as a standalone diarization frame-
work. However, it requires the parameter initialization to start
VB estimation, and the parameters are usually initialized based
on the result of speaker clustering. In that context, VB-HMM
can be seen as a resegmentation method, and widely used as the
ﬁnal step of speaker diarization (e.g., [142, 113]).
2.6.2. System Fusion
As another direction of post processing, there have been
a series of studies on the fusion method of multiple diariza-
tion results to improve the diarization accuracy. While it is
widely known that the system combination generally yields bet-
ter result for various systems (e.g., speech recognition [143] or
speaker recognition [144]), combining multiple diarization hy-
potheses has several unique problems. Firstly, the speaker la-
beling is not standardized among diﬀerent diarization systems.
Secondly, the estimated number of speakers may diﬀer among
diﬀerent diarization systems. Finally, the estimated time bound-
aries may be also diﬀerent among multiple diarization systems.
System combination methods for speaker diarization systems
Fig. 6: Example of DOVER system.
need to handle these problems during the fusion process of mul-
tiple hypotheses.
In [145], a method to select the best diarization result among proposed a method called DOVER-Lap, in which the speak-
multiple diarization systems were proposed. In this method, ers of multiple hypothesis are aligned by a weighted k-partite
AHC is applied on the set of diarization results where the dis- graph matching, and the number of speakers K for each small
tance of two diarization results are mesured by symmetric DER. segment is estimated based on the weighted average of multi-
AHC is executed until the number of groups becomes two, and ple systems to select the top-K voted speaker labels. Both the
the diarization result that has the smallest distance to all other modiﬁed DOVER and DOVER-Lap showed the improvement
results in the biggest group is selected as the ﬁnal diarization re- of DER for the speaker diarization result with speaker overlaps.
sult. In [146], two diarization systems are combined by ﬁnding
the matching between two speaker clusters, and then perform- 3. Recent Advances in Speaker Diarization using Deep
ing the resegementation based on the matching result. Learning
More recently, DOVER (diarization output voting error re-
duction) method [147] was proposed to combine multiple di- This section introduces various recent eﬀorts toward deep
arization results based on the voting scheme. In the DOVER learning-based speaker diarization techniques. Firstly, meth-
method, speaker labels among diﬀerent diarization systems are ods that incorporate deep learning into a single component of
aligned one by one to minimize DER between the hypotheses speaker diarization, such as clustering or post-processing, are
(the processes 2 and 3 of Fig. 6). After every hypotheses are introduced in Section 3.1, Then, methods that unify several
aligned, each system votes its speaker label to each segmented components of speaker diarization into a single neural network
region (each system may have diﬀerent weight for voting), and are introduced in Section 3.2,
the speaker label that gains the highest voting weight is selected
3.1. Single-module optimization
for each segmented region (the process 4 of Fig. 6). In case of
3.1.1. Speaker clustering enhanced by deep learning
multiple speaker labels get the same voting weight, a heuris-
Several methods that enhance the speaker clustering based
tic to break the ties (such as selecting the result from the ﬁrst
on deep learning were proposed. A deep-learning based clus-
system) is used.
tering algorithm, called Improved Deep Embedded Clustering
The DOVER method has an implicit assumption that there
(IDEC) is proposed in [149]. The goal is to transform the input
is no overlapping speech, i.e., at most only 1 speaker is as-
features, herein speaker embeddings, to become more separa-
signed for each time index. To combine the diarization hy-
ble, given the number of clusters/speakers. The key idea is that
potheses with overlapping speakers, two methods were recently
each embedding has a probability of “belonging” to each of the
proposed. In [104], the authors proposed the modiﬁed DOVER
method, where the speaker labels in diﬀerent diarization results available speaker cluster [150, 64],
are ﬁrst aligned with a root hypothesis, and the speech activ- (cid:16) (cid:17)− a+1
1 + (cid:107)z − µ (cid:107)2/a a q2 / f
istcyoroef feoarcehacsphesapkeearkiesr efsotrimeaactehdsmbaasleldseognmtehnet.wReaigj hetteadl.v[o1t4in8g] qi j = (cid:80)l (cid:0)1 + (cid:107)izi − jµl(cid:107)2/a(cid:1)− a+a1 , pi j = (cid:80)liqj 2il/ifl (25)
9and σ(·) is a nonlinear function. GNN was optimized by min-
imizing the distance between the reference aﬃnity matrix and
estimated aﬃnity matrix, where the distance was calculated by
a combination of histogram loss [151] and nuclear norm.
There are also several diﬀerent approaches to generate the
aﬃnity matrix. In [152], self-attention-based network was in-
troduced to directly generate a similarity matrix from a se-
quence of speaker embeddings. In [153], several aﬃnity ma-
trices with diﬀerent temporal resolutions were fused into single
aﬃnity matrix based on a neural network.
3.1.2. Learning the Distance Estimator
Data-driven techniques perform remarkably well on a wide
variety of tasks [154]. However, traditional DL architectures
may fail when the problem involves relational information be-
tween observations [155]. Recently, Relational Recurrent Neu-
Fig. 7: Speaker diarization with graph neural network
ral Networks (RRNN) were introduced by [155, 156, 157] to
solve this ”relational information learning” task. Speaker di-
where z are the bottleneck features, µ is the centroid of i-th arization can be seen as a member of this class of tasks, since
i i
cluster and f is the soft cluster frequency with f = (cid:80) q . the ﬁnal decision depends on the distance relations between
i i i j
The clusters are iteratively reﬁned based on a target distribu- speech segments and speaker proﬁles or centroids.
tion [150] based on bottleneck features estimated using an au- The challenges of audio segmentation are detailed in Sec-
toencoder. tion 2.3. Further, speaker embeddings are usually extracted
The initial DEC approach presented some problems. As from a network trained to distinguish speakers among thou-
such, improved versions of the algorithm have been proposed, sands of candidates [50]. However, a diﬀerent level of granular-
where the possibility of trivial (empty) clusters is addressed ity in the speaker space is required, since only a small number
(under the assumption that the distribution of speaker turns is of participants is typically involved in an interactive meeting
uniform across all speakers, i.e. all speakers contribute equally scenario. In addition to that, the distance metric used is of-
to the session). This assumption is not realistic in real meet- ten heuristic and/or dependent on certain assumptions which do
ing environments but it constrains the solution space enough to not necessarily hold, e.g., assuming Gaussianity in the case of
avoid the empty clusters without aﬀecting overall performance. PLDA [158], etc. Finally, the audio chunks are treated indepen-
An additional loss term penalizes the distance from the cen- dently and any temporal information about the past and future
troids µ , bringing the behavior of the algorithm closer to k- is simply ignored. Most of these issues can be addressed with
i
means [149]. the RRNNs in [159], where a data-driven, memory-based ap-
Based on these improvements, the loss function of the revis- proach is bridging the performance gap between the heuristic
ited DEC algorithm consists of three diﬀerent loss components, and the trainable distance estimating approaches. The RRNNs
i.e. L the clustering error, L the uniform “speaker air-time” have shown great success on several problems requiring rela-
c u
distribution constraint and L the distance of the bottleneck tional reasoning [156, 155, 159], and speciﬁcally using the Re-
MS E
features from the centroids [149], lational Memory Core (RMC) [155].
In this context, a novel approach of learning the distance
L = αLc + βLr + γLu + δLMS E (26) between such centroids (or speaker proﬁles) and the embed-
dings was proposed in [159] (Fig. 8). The diarization pro-
allowing for the diﬀerent loss functions to be weighted diﬀer-
cess can be seen as a classiﬁcation task on already segmented
ently and the weights α, β, γ and δ can be ﬁne-tuned on some
audio, Section 2.3, where the audio signal is ﬁrst segmented
held-out data.
either uniformly [160] or based on estimated speaker change
In [65], a diﬀeent approach that purify the similarity matrix
points [161]. As these segments are assumed to be speaker-
for the spectral clustering based on the graph neural network
homogeneous, speaker embeddings x for each segment are ex-
(GNN) was proposed (Fig. 7). Given a sequence of speaker j
tracted and then compared against all the available speaker pro-
embeddings {e , ...e } where N is the length of sequence. The
1 N ﬁles or speaker centroids. By minimizing a particular distance
ﬁrst layer of the GNN takes the input {x0 = e |i = 1, . . . , N}.
i i metric, the most suitable speaker label is assigned to the seg-
The GNN then computes the output of the p-th layer {x(p)|i =
i ment. The ﬁnal decision relies on a distance estimation, ei-
1, . . . , N} as followings.
ther the cosine [51] or the PLDA [158] distance, or the distance
(cid:88) based on RRNNs as proposed in [159]. The later method based
x(p) = σ(W L x(p−1)), (27)
i i, j j on memory networks has shown consistent improvements in
j
performance.
where L represents a normalized aﬃnity matrix added by self-
3.1.3. Deep learning-based post processing
connection, W is a trainable weight matrix for the p-th layer,
10ited by the number of element of the output layer.
As a diﬀerent approach, Horiguchi et al. proposed to apply
the EEND model (detailed in Section 3.2.4) to reﬁne the result
of a clustering-based speaker diarization [162]. A clustering-
based speaker diarization method can handle a large number of
speakers while it is not good at handling the overlapped speech.
On the other hand, EEND has the opposite characteristics. To
Fig. 8: Continuous speaker identiﬁcation system based on RMC. The speech
complementary use two methods, they ﬁrst apply a conven-
signal is segmented uniformly and each segment xt is compared against all the
available speaker proﬁles according to a distance metric d(·, ·). A speaker label tional clustering method. Then, the two-speaker EEND model
st,j is assigned to each xt minimizing this metric. is iteratively applied for each pair of detected speakers to reﬁne
the time boundary of overlapped regions.
3.2. Joint optimization for speaker diarization
3.2.1. Joint segmentation and clustering
A model called Unbounded Interleaved-State Recurrent Neu-
ral Networks (UIS-RNN) was proposed that replaces the seg-
mentation and clustering procedure into a trainable model [55].
Given the input sequence of embeddings X = (x ∈ Rd|t =
t
1, . . . , T ), UIS-RNN generates the diarization result Y = (y ∈
t
N|t = 1, . . . , T ) as a sequence of speaker index for each time
frame. The joint probability of X and Y can be decomposed by
the chain rule as follows.
(cid:89)T
P(X, Y) =P(x , y ) P(x , y |x , y ). (28)
1 1 t t 1:t−1 1:t−1
t=2
To model the distribution of speaker change, UIS-RNN then in-
troduce a latent variable Z = (z ∈ {0, 1}|t = 2, . . . , T ), where z
t t
Fig. 9: Target Speaker Voice Activity Detector becomes 1 if the speaker indices at time t − 1 and t are diﬀer-
ent, and 0 otherwise. The joint probability including Z is then
decomposed as follows.
There are a few recent studies to train a neural network that
is applied on top of the result of a clustering-based speaker di- (cid:89)T
P(X, Y, Z) =P(x , y ) P(x , y , z |x , y , z ) (29)
arization. These method can be categorized as an extension of 1 1 t t t 1:t−1 1:t−1 1:t−1
t=2
the post processing.
Medennikov et al. proposed the Target-Speaker Voice Ac- Finally, the term P(x , y , z |x , y , z ) is further decom-
t t t 1:t−1 1:t−1 1:t−1
tivity Detection (TS-VAD) to achieve accurate speaker diariza- posed into three components.
tion even with many speaker overlaps noisy conditions [91, 66].
As shown in Fig. 9, TS-VAD takes the input of acoustic fea- P(xt, yt, zt|x1:t−1, y1:t−1, z1:t−1)
ture (MFCC) as well as the i-vector of all target speakers. The = P(x |x , y )P(y |z , y )P(z |z ) (30)
t 1:t−1 1:t t t 1:t−1 t 1:t−1
model has an output layer where i-th element becomes 1 at time
frame t if i-th speaker is speaking at the time frame, and 0 oth- Here, P(x |x , y ) represents the sequence generation proba-
t 1:t−1 1:t
erwise. To convert the raw output into a sequence of segment, bility, and modeled by gated recurrent unit (GRU)-based recur-
a further post-processing based on heuristics (median ﬁltering, rent neural network. P(y |z , y ) represents the speaker as-
t t 1:t−1
binarization with the threshold, etc.) or HMM-based decod- signment probability, and modeled by a distant dependent Chi-
ing with states representing silence, non-overlapping speech of nese restaurant process [163], which can model the distribution
each speaker, and overlapping speech from all possible pairs of of unbounded number of speakers. Finally, P(z |z ) repre-
t 1:t−1
speakers is used. Prior to inference, TS-VAD requires the i- sents the speaker change probability, and modeled by Bernoulli
vector of all target speakers. The i-vectors are initialized based distribution. Since all models are represented by trainable mod-
on the conventional clustering-based speaker diarization result. els, the UIS-RNN can be trained in a supervised way by ﬁnding
After initializing the i-vector, the inference by TS-VAD and parameters that maximizes log P(X, Y, Z) over training data.
reﬁnement of i-vector based on the TS-VAD result can be re- The inference can be conducted by ﬁnding Y that maximizes
peated until it converges. TS-VAD showed a signiﬁcantly bet- log P(X, Y) given X based on the beam search in an online fash-
ter DER compared with the conventional clustering based ap- ion. While UIS-RNN works in an online fashion, UIS-RNN
proach [91, 88]. On the other hand, it has a constraint that the showed better DER than that of the oﬄine system based on the
maximum number of speakers that the model can handle is lim- spectral clustering.
11DSAepcteeticveticitoyhn EEmSxptbreeaadcktdieoinrng ReRfiengeimonent Remove Overlap
...
Neural Network
ch
feature map anchor freq Clustering
time
STFT feature freq RPN
time ...
audio input Neural Network Neural Network
(a) Region proposal network (RPN) (b) Diarization by RPN
Fig. 10: (a) RPN for speaker diarization, (b) diarization procedure based on
RPN. ...
Neural Network Neural Network
3.2.2. Joint segmentation, embedding extraction, and re-
segmentation
A speaker diarization method based on the Region Proposal
...
Networks (RPN) was proposed to jointly perform segmenta-
tion, speaker embedding extraction, and re-segmentation proce- audio block 1 audio block 2
dures by a single neural network [75]. The RPN was originally
proposed to detect multiple objects from a 2-d image [164], and Fig. 11: Joint speech separation, speaker counting, and speaker diarization
1-d variant of the RPN is used for speaker diarization along with model.
the time-axis. RPN works on the Short-Term Fourier Transform
(STFT) features. A neural network converts the STFT feature
speech separation based on the spatial covariance model with
into the feature map (Fig. 10 (a)). Then, for each candidates
non-negative matrix factorization. They derived the EM algo-
of time region of speech activity, called an anchor, the neural
rithm to estimate separated speech and speech activity of each
network jointly perform three tasks to (i) estimate whether the
speaker from the multi-channel overlapped speech. While their
anchor includes speech activity or not, (ii) extract a speaker em-
method jointly perform speaker diarization and speech separa-
bedding corresponding to the anchor, and (iii) estimate the dif-
tion, their method is based on a statistical modeling, and estima-
ference of the duration and center position of the anchor and the
tion was conducted solely based on the observation, i.e. without
reference speech activity. The ﬁrst, second, and third tasks cor-
any model training.
responds to the segmentation, speaker embedding extraction,
Neumann et al. [76, 167] later proposed a trainable model,
and re-segmentation, respectively.
called online Recurrent Selective Attention Network (online
The inference procedure by RPN is depected in Fig. 10 (b).
RSAN), for joint speech separation, speaker counting, and
The RPN is ﬁrstly applied to every anchors on the test audio,
speaker diarization based on a single neural network (Fig. 11).
and the regions with speech activity probability higher than a
Their neural network takes the input of spectrogram X ∈ RT×F,
pre-determined threshold are listed as a candidate time regions.
a speaker embedding e ∈ Rd, and a residual mask R ∈ RT×F,
Estimated regions are then clustered by using a conventional
where T and F is the maximum time index and the maximum
clustering method (e.g., k-means) based on the speaker embed-
frequency bin of the spectrogram, respectively. It output the
dings corresponding to each region. Finally, a procedure called
speech mask M ∈ RT×F and an updated speaker embedding
non-maximum suppression is applied to remove highly over-
for the speaker corresponding to e. The neural network is
lapped segments.
ﬁrstly applied with R whose element is all 1, and e whose el-
The RPN-based speaker diarization has the advantage that
ement is all 0. After the ﬁrst inference of M, R is updated as
it can handle overlapped speech with possibly any number of
R ← max(R − M, 0), and the neural network is again applied
speakers. Also, it is much simpler than the conventional speaker
with the updated R. This procedure is repeated until sum of
diarization system. It was shown in multiple dataset that the
R becomes less than a threshold. A separated speech can be
RPN-based speaker diarization system achieved signiﬁcantly
obtained by M (cid:12) X where (cid:12) is the element-wise multiplica-
better DER than the conventional clustering-based speaker di-
tion. The speaker embedding is used to keep track the speaker
arization system [75, 88].
of adjacent blocks. Thanks to the iterative approach, this neu-
ral network can cope with variable number of speakers while
3.2.3. Joint speech separation and diarization jointly performing speech separation and speaker diarization.
There are also recent researches to jointly perform speech
separation and speaker diarization. Kounades-Bastian et al.
3.2.4. Fully end-to-end neural diarization
[165, 166] proposed to incorporate a speech activity model into
12diarization result attractor existing probability
1 1 1 1 0
Sigmoid Linear + Sigmoid
attractors
LSTM encoder LSTM decoder
embedding
EEND
audio input
Fig. 13: EEND with encoder-decoder-based attractor (EDA).
forward for the prior works. On the other hand, several limi-
tations are also known for EEND. Firstly, the model architec-
ture constrains the maximum number of speakers that the model
can cope with. Secondly, EEND consists of BLSTM or self-
attention neural networks, which makes it diﬃcult to do online
Fig. 12: Two-speaker end-to-end neural diarization model processing. Thirdly, it was empirically suggested that EEND
tends to overﬁt to the distribution of the training data [56].
To cope with an unbounded number of speakers, several
Recently, the framework called End-to-End Neural Diariza-
extensions of EEND have been investigated. Horiguchi et
tion (EEND) was proposed [56, 57], which performs all speaker
al. [169] proposed an extension of EEND with the encoder-
diarization procedure based on a single neural network. The
decoder-based attractor (EDA) (Fig. 13). This method ap-
architecture of EEND is shown in Fig. 12. An input to the
plies an LSTM-based encoder-decoder on the output of EEND
EEND model is a T -length sequence of acoustic features (e.g.,
to generate multiple attractors. Attractors are generated until
log mel ﬁlterbank), X = (x ∈ RF|t = 1, . . . , T ). A neural
t the attractor existing probability becomes less than a thresh-
network then outputs the corresponding speaker label sequence
old. Then, each attractor is multiplied with the embeddings
Y = (y |t = 1, . . . , T ) where y = [y ∈ {0, 1}|k = 1, . . . , K].
t t t,k generated from EEND to calculate the speech activity for each
Here, y = 1 represents the speech activity of the speaker k
t,k speaker. On the other hand, Fujita et al. [170] proposed an-
at the time frame t, and K is the maximum number of speak-
other approach to output the speech activity one after another
ers that the neural network can output. Importantly, y and
t,k by using a conditional speaker chain rule. In this method, a
y can be both 1 for diﬀerent speakers k and k(cid:48), which repre-
t,k(cid:48) neural network is trained to produce a posterior probability
sents that two speakers k and k(cid:48) is speaking simultaneously (i.e. P(y |y , . . . , y , X), where y = (y ∈ {0, 1}|t = 1, . . . , T )
k 1 k−1 k t,k
overlapping speech). The neural network is trained to maxi-
(cid:80) (cid:80) is the speech activity for k-th speaker. Then, the joint speech
mize log P(Y|X) ∼ log P(y |X) over the training data by
t k t,k activity probability of all speakers can be estimated from the
assuming the conditional independence of the output y . Be-
t,k following speaker-wise conditional chain rule as:
cause there can be multiple candidates of the reference label
Y by swapping the speaker index k, the loss function is calcu- (cid:89)K
lated for all possible reference labels and the reference label that P(y1, . . . , yK|X) = P(yk|y1, . . . , yk−1, X). (31)
has the minimum loss is used for the error back-propagation, k=1
which is inspired by the permutation free objective used in During inference, the neural network is repeatedly applied until
speech separation [59]. EEND was initially proposed with a the speech activity y for the last estimated speaker approaches
k
bidirectional long short-term memory (BLSTM) network [56], zero. Kinoshita et al. [171] proposed a diﬀerent approach that
and was soon extended to the self-attention-based network [57] combines EEND and speaker clustering. In their method, a neu-
by showing the state-of-the-art DER for CALLHOME dataset ral network is trained to generate speaker embeddings as well as
(LDC2001S97) and Corpus of Spontaneous Japanese [168]. the speech activity probability. Speaker clustering constrained
There are multiple advantages of EEND. Firstly, it can han- by the estimated speech activity by EEND is applied to align
dle overlapping speech in a sound way. Secondly, the network the estimated speakers among diﬀerent processing blocks.
is directly optimized towards maximizing diarization accuracy, There are also a few recent trials to extend the EEND for
by which we can expect a high accuracy. Thirdly, it can be online processing. Xue et al. [172] proposed a method with a
retrained by a real data (i.e. not synthetic data) just by feed- speaker tracing buﬀer to better align the speaker labels of adja-
ing a reference diarization label while it is often not strait- cent processing blocks. Han et al. [173] proposed a block on-
13line version of EDA-EEND [169] by carrying the hidden state
of the LSTM-encoder to generate attractors block by block.
4. Speaker Diarization in the context of ASR
From a conventional perspective, speaker diarization is con-
sidered a pre-processing step for ASR. In the traditional system
structure for speaker diarization as depicted in Fig. 1, speech
Fig. 14: Integration of lexical information and acoustic information.
inputs are processed sequentially across the diarization com-
ponents without considering the ASR objective, which corre-
sponds to minimize word error rate (WER). One issue is that
the tight boundaries of speech segments as the outcomes of
speaker diarization have a high chance of causing unexpected
word truncation or deletion errors in ASR decoding. In this
section we discuss how speaker diarization systems have been
developed in the context of ASR, not only resulting in better
WER by preventing speaker diarization from hurting ASR per-
formance, but also beneﬁting from ASR artifacts to enhance
diarization performance. More recently, there have been a few
pioneering proposals made for joint modeling of speaker di-
arization and ASR, which we will introduce in the section as
well.
4.1. Early Works
The lexical information from ASR output has been employed
for speaker diarization system in a few diﬀerent ways. First,
the earliest approach was RT03 evaluation [1] which used word
boundary information for segmentation purpose. In [1], a gen- Fig. 15: Integration of lexical information and acoustic information.
eral ASR system for broadcast news data was built where the
basic components are segmentation, speaker clustering, speaker
adaptation and system combination after ASR decoding from “This is [name]” indicates who was the speaker of the broadcast
the two sub-systems with the diﬀerent adaptation methods. To news section. Although the early speaker diarization studies did
understand the impact of the word boundary information, they not fully leverage the lexical information to drastically improve
used ASR outputs to replace the segmentation part and com- DER, the idea of integrating the information from ASR output
pared the diarization performance of the each system. In ad- has been employed by many studies to reﬁne or improve the
dition, ASR result was also used for reﬁning SAD in IBM’s speaker diarizatiohn output.
submission [174] for RT07 evaluation. The system appeared in
[174] incorporates word alignments from speaker independent
4.2. Using lexical information from ASR
ASR module and reﬁnes SAD result to reduce false alarms so
that the speaker diarization system can have better clustering The more recent speaker diarization systems that take ad-
quality. The segmentation system in [175] also takes advantage vantage of the ASR transcript have employed a DNN model
of word alignments from ASR. The authors in [175] focused to capture the linguistic pattern in the given ASR output to en-
on the word-breakage problem where the words from ASR out- hance the speaker diarization result. The authors in [177] pro-
put are truncated by segmentation results since segmentation posed a way of using the linguistic information for the speaker
result and decoded word sequence are not aligned. Therefore, diarization task where participants have distinct roles that are
word-breakage (WB) ratio was proposed to measure the rate of known to the speaker diarization system. Fig. 14 shows the
change-points that are detected inside intervals corresponding diagram of speaker diarization system appeared in [177]. In
to words. The DER and WB were reported together to mea- this system, a neural text-based speaker change detector and a
sure the inﬂuence of word truncation problem. While the fore- text-based role recognizer are employed. By employing both
mentioned early works of speaker diarization systems that are linguistic and acoustic information, DER was signiﬁcantly im-
leveraging ASR output are focusing on the word alignment in- proved compared to the acoustic only system.
formation to reﬁne the SAD or segmentation resutl, the speaker Lexical information from ASR output was also utilized for
diarization system in [176] created a dictionary for the phrases speaker segmentation [178] by employing a sequence to se-
that commonly appear in broadcast news. The phrases in this quence model that outputs speaker turn tokens. Based on the
dictionary provide identity of who is speaking, who will speak estimated speaker turn, the input utterance is segmented accord-
and who spoke in the broadcase news scenario. For example, ingly. The experimental results in [178] show that using both
14word1 spk1 word2 word3 spk2 word4 spk1
word hypothesis  speaker
(with silence boundary) embeddings
(*) colored block represents non-silence hypothesis 
while white block represents silence hypothesis
End-to-End ASR
Estimation
and Diarization
audio input
audio input Fig. 17: Joint decoding framework for ASR and speaker diarization.
diarization in their experiments. On the other hand, the speaker
roles or speaker identity tags needs to be determined and ﬁxed
Fig. 16: Joint ASR and diarization by inserting a speaker tag in the transcrip- during training, so it is diﬃcult to cope with an arbitrary num-
tion.
ber of speakers with this approach.
A second approach is a MAP-based joint decoding frame-
acoustic and lexical information can get an extra advantage ow- work. Kanda et al. [79] formulated the joint decoding of
ing to the word boundaries we get from the ASR output. ASR and speaker diarization as followings (see also Fig. 17).
[179] presented follow-up research within the above thread. Assume that a sequence of observations is represented by
Unlike the system in [178], lexical information from the ASR X = {X , . . . , X }, where U is the number of segments (e.g.,
1 U
module was integrated with the speech segment clustering pro- generated by applying VAD on a long audio) and X is the
u
cess by employing an integrated adjacency matrix. The adja- acoustic feature sequence of the u-th segment. Further as-
cency matrix is obtained from max operation between acoustic sume that word hypotheses with time boundary information is
information created from aﬃnities among audio segments and represented by W = {W , . . . , W } where W is the speech
1 U u
lexical information matrix created by segmenting the word se- recognition hypotheses corresponding to the segment u. Here,
quence into word chunks that are likely to be spoken by the W = (W , ..., W ) contains all speakers’ hypotheses in the
u 1,u K,u
same speaker. Fig. 15 shows a diagram that explains how lex- segment u where K is the number of speakers, and W rep-
k,u
ical information is integrated in an aﬃnity matrix with acous- resents the speech recognition hypothesis of the k-th speaker
tic information. The integrated adjacency matrix leads to an of the segment u. Finally, a tuple of speaker embeddings
improved speaker diarization performance for CALLHOME E = (e , . . . , e ), where e ∈ Rd is d-dimensional speaker em-
1 K j
American English dataset. beddings of k-th speaker, is also assumed. With all these nota-
tions, the joint decoding framework of multi-speaker ASR and
4.3. Joint ASR and speaker diarization with deep learning diarization can be formulated as a problem to ﬁnd most likely
Wˆ as,
Motivated by the recent success of deep learning and end-
to-end modeling, several models have been proposed to jointly Wˆ = argmax P(W|X) (32)
perform ASR and speaker diarization. As with the previous
W
section, ASR results contain a strong cue to improve speaker (cid:88)
= argmax{ P(W, E|X)} (33)
diarization. On the other hand, speaker diarization results can
W E
be used to improve the ASR accuracy, for example, by adapting
≈ argmax{max P(W, E|X)}, (34)
the ASR model towards each estimated speaker. Joint modeling
W E
can leverage such inter-dependency to improve both ASR and
speaker diarization. In the evaluation, a word error rate (WER) where we use the Viterbi approximation to obtain the ﬁnal equa-
metric that is aﬀected by both ASR errors and speaker attri- tion. This maximization problem is further decomposed into
bution errors, such as speaker-attributed WER [180] or cpWER two iterative problems as,
[89], is often used. ASR-speciﬁc metrics (e.g., speaker-agnostic
WER) or diarization-speciﬁc metrics (e.g., DER) is also used Wˆ (i) = argmax P(W|Eˆ(i−1), X), (35)
W
complementary.
Eˆ(i) = argmax P(E|Wˆ (i), X), (36)
A ﬁrst line of approaches is introducing a speaker tag in the
E
transcription of end-to-end ASR models (Fig. 16). Shafey et
al. [77] proposed to insert a speaker role tag (e.g., (cid:104)doctor(cid:105) where i is the iteration index of the procedure. In [79], Eq. (35)
and (cid:104)patient(cid:105)) in the output of a recurrent neural network- is modeled by the target speaker ASR [181, 182, 183, 71] and
transducer (RNN-T)-based ASR system. Similarly, Mao et al. Eq. (36) is modeled by the overlap-aware speaker embedding
[78] proposed to insert a speaker identity tag in the output of an estimation. This method shows a similar speaker-attributed
attention-based encoder-decoder ASR system. These method WER compared to that of the target speaker ASR with oracle
have been shown to be able to perform both ASR and speaker speaker embeddings. On the other hand, it requires an iterative
15label prediction speaker prediction arization in the recent papers. CALLHOME dataset contains
500 sessions of multilingual telephonic speech. Each session
has 2 to 7 speakers while there are two dominant speakers in
InventoryAttention
each conversation.
DecoderRNN speaker query
5.1.2. AMI
DecoderOut SpeakerQueryRNN The AMI database [187] includes 100 hours of meeting
Attention recordings from multiple sites in 171 meeting sessions. AMI
database provides audio source recorded with lapel micro-
phones which are separately recoreded and ampliﬁed for each
speaker. Another audio source is recorded with omnidirectional
microphone arrays that are mounted on the table while meet-
AsrEncoder SpeakerEncoder
ing. AMI database is a suitable dataset for evaluating speaker
diarization system integrated with ASR module since AMI pro-
vides forced alignment data which contains word and phoneme
1 2 3 4 5 level timings along with the transcript and speaker label. Each
audio input speaker profiles meeting session contains 3 to 5 speakers.
Fig. 18: End-to-end speaker-attributed ASR
5.1.3. ICSI meeting Corpus
The ICSI meeting corpus [188] contains 75 meting corpus
application of the target-speaker ASR and speaker embedding with 4 meeting types. ICSI meeting corpus provides word level
extraction, which makes it challenging to apply the method in timing along with the transcript and speaker label. The au-
online mode. dio source is recorded with close-talking individual microphone
As a third line of approaches, End-to-End (E2E) Speaker- and six tabletop microphones to provide speaker-speciﬁc chan-
Attributed ASR (SA-ASR) model was recently proposed to nel and multi-channel recording. Each meeting has 3 to 10 par-
jointly perform speaker counting, multi-speaker ASR, and ticipants.
speaker identiﬁcation [184, 185]. Diﬀerent from the ﬁrst two
approaches, the E2E SA-ASR model takes the additional in- 5.1.4. DIHARD Challenge dataset
put of speaker proﬁles and identiﬁes the index of speaker pro- DIHARD challenge dataset is created for DIHARD chal-
ﬁles based on the attention mechanism (Fig. 18). Thanks to lenge 1, 2 and 3 [189, 85, 190] while focusing on very challeng-
the attention mechanism for speaker identiﬁcation and multi- ing domains. DIHARD challenge development set and eval-
talker ASR capability based on serialized output training [186], uation set include clinical interviews, web videos, speech in
there is no limitation of a maximum number of speakers that the wild (e.g., recordings in restaurants). DIHARD challenge
the model can cope with. In case relevant speaker proﬁles are dataset also includes relatively less challenging datasets such
supplied in the inference, the E2E SA-ASR model can automat- as conversational telephonic speech (CTS) and audio books to
ically transcribe the utterance while identifying the speaker of diversify the domains in development set and evaluation set.
each utterance based on the supplied speaker proﬁles. On the Contrary to other speaker diarization datasets, domains such
other hand, in case the relevant speaker proﬁles cannot be used as restaurant conversation and web videos contain signiﬁcantly
prior to the inference, the E2E SA-ASR model can still be ap- lower signal to noise ratio (SNR) that makes DER way higher.
plied with example proﬁles, and speaker clustering on the inter- The ﬁrst DIHARD challenge, DIHARD 1, started with track
nal speaker embeddings of the E2E SA-ASR model (“speaker 1 for diarization beginning from oracle SAD and track 2 di-
query” in Fig. 18) is used to diarize the speaker [80]. arization from scratch using system SAD. Unlike DIHARD 1,
DIHARD 2 included multichannel speaker diarization task in
track 3 (oracle SAD) and track 4 (system SAD) adding the
5. Evaluation of Speaker Diarization
recordings drawn from CHIME-5 corpus [191]. In the latest
This section describes the evaluation scheme for speaker di- DIHARD challenge, DIHARD 3, CTS dataset was added to
arization. The dataset that is widely used for the evaluation DIHARD 3 dev set and eval set and DIHARD 3 removed track
of speaker diarization is ﬁrst introduced in Section 5.1. Then, 3 and track 4 while keeping only track 1 (oracle SAD) and track
the evaluation metric for speaker diarization is introduced in 2 (system SAD).
Section 5.2. Finally, international eﬀorts to evaluate diariza-
tion systems are introduced in Section 5.3. The summary of the 5.1.5. CHiME-5/6 challenge corpus
dataset is shown in Table 2. The CHiME-5 corpus [191] includes 50 hours of multi-party
real conversations in the every-day home environment. It con-
5.1. Diarization Evaluation Datasets tains speaker labels, segmentation, and corresponding tran-
5.1.1. CALLHOME: NIST SRE 2000 (LDC2001S97) scriptions. All of them are manually annotated. The audio
NIST SRE 2000 (Disk-8), often referred to as CALLHOME source is recorded by multiple 4-channel microphone arrays lo-
dataset, has been the most widely used dataset for speaker di- cated in the kitchen and dining/living rooms in a house, and also
16Table 2: Diarization Evaluation Datasets
Size (hr) Style # speakers
CALLHOME 20 Conversation 2–7
AMI 100 Meeting 3–5
ICSI meeting 72 Meeting 3–10
DIHARD I Track 1,2 19(dev), 21(eval) Miscellaneous 1–7
DIHARD II Track 1,2 24(dev), 22(eval) Miscellaneous 1–8
DIHARD II Track 3,4 262(dev), 31(eval) Miscellaneous 4
DIHARD III Track 1,2 34(dev), 33(eval) Miscellaneous 1–7
CHiME-5/6 50 Conversation 4
VoxConverse 74 YouTube video 1–21
LibriCSS 10 Read speech 8
recorded by binaural microphones worn by participants. The diﬀerent error types: False alarm (FA) of speech, missed detec-
number of participants is ﬁxed as four. The CHiME-6 chal- tion of speech and confusion between speaker labels.
lenge uses the same CHiME-5 corpus, but track 2 includes the
FA + Missed + Speaker-Confusion
speaker diarization problem in the challenge (i.e., no speaker DER = (37)
labels and segmentation are given). The CHiME-5 corpus was Total Duration of Time
also used as one track in the DIHARD 2 challenge.
To establish a one-to-one mapping between the hypothesis out-
puts and the reference transcript, Hungarian algorithm [195] is
5.1.6. VoxConverse employed. In Rich Transcription 2006 evaluation [194], 0.25
The VoxConverse dataset [192] contains 74 hours of human second of “no score” collar is set around every boundary of
conversation extracted from YouTube video. The dataset is di- reference segment to mitigate the eﬀect of inconsistent annota-
vided into development set (20.3 hours, 216 recordings), and tion and human errors in reference transcript and this evaluation
test set (53.5 hours, 310 recordings). The number of speakers scheme has been most widely used in speaker diarization stud-
in each recording has a wide range of variety from 1 speaker to ies.
21 speakers. The audio includes various types of noises such as
background music, laughter etc. It also contains noticeable por- 5.2.2. JER
tion of overlapping speech from 0% to 30.1% dependent on the Jaccard Error Rate (JER) was ﬁrst introduced in DIHARD II
recording. While the dataset contains the visual information as evaluation. The goal of JER is to evaluate each speaker with
well as audio, as of January 2021, only the audio of the devel- equal weight. Unlike DER, JER does not use speaker error to
opment set is released under a Creative Commons Attribution obtain the error value.
4.0 International License for research purpose. The audio of the
evaluation set was used at the track 4 of the VoxCeleb Speaker 1 (cid:88)Nref FA + MISS
Recognition Challenge 2020 (Section 5.3) as a blind test set. JER = i i (38)
N TOTAL
i i
5.1.7. LibriCSS In Eq. (38), TOTAL is union of i-th speaker’s speaking time in
The LibriCSS corpus [87] is 10 hours of multi-channel reference transcript and i-th speaker’s speaking time in the hy-
recordings designed for the research of speech separation, potheses. The sum of FA and MISS divided by TOTAL value is
speech recognition, and speaker diarization. It was made by then averaged over Nre f -speakers in the reference script. Since
playing back the audio in the LibriSpeech corpus [193] in a real JER is using union operation between reference and the hy-
meeting room, and recorded by a 7-ch microphone array. It con- potheses, JER never exceeds 100% while DER can sometimes
sists of 10 sessions, each of which is further decomposed to six reach way over 100%. DER and JER are highly correlated but if
10-min mini-sessions. Each mini-session was made by audio a subset of speakers are dominant in the given audio recording,
of 8 speakers and designed to have diﬀerent overlap ratio from JER tends to get higher than ordinary case.
0% to 40%. To facilitate the research, the baseline system for
speech separation and ASR [87] and the baseline system that 5.2.3. WDER
integrates speech separation, speaker diarization and ASR [88] While DER is based on the duration of speaking time of each
has been developed and released. speaker, Word-level DER (WDER) is designed to measure the
error that is caused in the lexical(output transcription) side. The
motivation of WDER is the discrepency between DER and the
5.2. Diarization Evaluation Metrics
accuracy of ﬁnal transcript output since DER relies on the du-
5.2.1. DER ration of speaking time that is not always aligned with the word
The accuracy of speaker diarization system is measured by boundaries. The concept of word-breakage was proposed in
Diarization Error Rate (DER) [194] where DER is sum of three Silovsky et al. [175] where WB shares the similar idea with
17WDER. Unlike WDER, WB measures the number of speaker [192] was used for evaluation with DER as a primary metric
change point occurs inside a word boundary. The work in Park to determine the ranking of submitted systems. JER was also
and Georgiou [196] suggested the term WDER, evaluating the measured as a secondary metric.
diarization output with ground-truth transcription. More re-
cently, the joint ASR and speaker diarization system was evalu-
6. Applications
ated in WDER format in Shafey et al. [77]. Although the way of
calculating WDER would diﬀer over the studies but the under- 6.1. Meeting Transcription
lying idea is that the diarization error is calculated by counting
The goal of meeting transcription is to automatically generate
the correctly or incorrectly labeled words.
speaker-attributed transcripts during real-life meetings based on
their audio and optionally video recordings. Accurate meeting
5.3. Diarization Evaluation Series
transcriptions are the one of the processing steps in a pipeline
The Rich Transcription (RT) Evaluation [20] is the pio-
for several tasks like summarization, topic extraction, etc. Sim-
neering evaluation series of initiating deeper investigation on
ilarly, the same transcription system can be used in other do-
speaker diarization in relation with ASR. The main purpose of
mains such as healthcare [198]. Although this task was in-
this eﬀort was to create ASR technologies that would produce
troduced by NIST in the Rich Transcription Evaluation series
transcriptions with descriptive metadata, like who said when,
back in 2003 [180, 188, 199], the initial systems had very poor
where speaker diarization plays in. Thus the main tasks in the
performance, and consequently commercialization of the tech-
evaluation were naturally ASR and speaker diarization. The
nology was not possible. However, recent advances in the ar-
domains of the data of interest were broadcast news, CTS and
eas of Speech Recognition [200, 201], far-ﬁeld speech process-
meeting recordings with multiple participants. Throughout the
ing [202, 203, 204], Speaker ID and diarization [205, 206, 113],
period 2002 to 2009, the RT evaluation series promoted and
have greatly improved the speaker-attributed transcription ac-
gauged advances in speaker diarization as well as ASR tech-
curacy, enabling such commercialization. Bi-modal process-
nology.
ing combining cameras with microphone arrays has further im-
DIHARD challenge [189, 85] is the most recent evaluation
proved the overall performance [207, 208]. As such, these latest
that focuses on challenging diarization tasks. DIHARD chal-
trends motivated us to include an end-to-end audio-visual meet-
lenge data contains many diﬀerent challenging and diverse do-
ing transcription system overview in this paper.
mains including the recordings from restaurants, meetings, in-
Reﬂecting the variety of application scenarios, customer
terview videos and court room. DIHARD evaluation focuses on needs, and business scope, diﬀerent constraints may be imposed
the performance gap of state-of-the-art diarization systems on
on meeting transcription systems. For example, it is most of-
challenging domains (e.g. recordings from outdoors) and rel-
ten required to provide the resulting transcriptions in low la-
atively clean speech (e.g. telephonic speech). DIHARD chal-
tency, making the diarization and recognition even more chal-
lenge employs a stricter evaluation scheme where the scoring
lenging. On the other hand, the architecture of the transcription
rule does not have “no score” collar and also evaluates over-
system can substantially improve the overall performance, e.g.,
lapped regions. In addition, DIHARD challenge also employed
employing microphone arrays of known geometry as the input
JER.
device. Also, in the case where the expected meeting attendees
The CHiME-6 challenge [89] track 2 revisits the previous
are known beforehand, the transcription system can further im-
CHiME-5 challenge [191] and further considers the problem of
prove speaker attribution, all while providing the exact name of
distant multi-microphone conversational speech diarization and
the speaker, instead of a randomly generated discrete speaker
recognition in everyday home environments. Although the ﬁnal
labels.
evaluation criterion is ranked with the WER, the challenge par- Two diﬀerent scenarios in this space are presented: ﬁrst, a
ticipants in this track also need to submit the diarization result.
ﬁx-geometry microphone array combined with a ﬁsh-eye cam-
The evaluation metrics of the diarization follow the DIHARD
era system, and second, an ad-hoc geometry microphone array
challenge, i.e.,“no score” collar and it also evaluates overlapped
system without a camera. In both scenarios, a “non-binding”
regions when computing the DER and JER.
list of participants and their corresponding speaker proﬁles are
The VoxCeleb Speaker Recognition Challenge (VoxSRC) is
considered known. In more detail, the transcription system has
the recent evaluation series for speaker recognition systems
access to the invitees’ names and proﬁles, however the actual at-
[197, 105]. The goal of VoxSRC is to probe how well the cur-
tendees may not accurately match those invited. As such, there
rent technology can cope with the speech “in the wild”. The
is an option to either include “unannounced” participants. Also,
evaluation data is obtained from YouTube videos of various do-
some of the invitees may not have proﬁles. In both scenarios,
mains, such as celebrity interviews, news shows, talk shows,
there is a constraint of low-latency transcriptions, where initial
and debates. The audio includes various types of background
results need to be shown with low latency. The ﬁnalized results
noises, laughter as well as noticeable portion of overlapping can be updated later in an oﬄine fashion.
speech, all of which make the task very challenging. This eval-
Some of the technical challenges to overcome are [209]:
uation series initially started with a pure speaker veriﬁcation
task [197], and the diarization task was added as the track 4 1. Although ASR on overlapping speech is one of the main
at the latest evaluation at the VoxCeleb Speaker Recognition challenges in meeting transcription, limited progress has
Challenge 2020 (VoxSRC-20) [105]. The VoxConverse dataset been made over the years. Numerous multi-channel
18speech separation methods have been proposed based on Speech and spoken language are central to conversational in-
Independent Component Analysis(ICA) or Spatial Clus- teractions and carry crucial information about a speaker’s in-
tering [210, 211, 212, 213, 214, 215], but applying them tent, emotions, identity, age and other individual and interper-
to a meeting setup had limited success. In addition, neu- sonal trait and state variables including health state, and compu-
ral network-based separation methods like Permutation In- tational advances are increasingly allowing for accessing such
variant Training (PIT) [59] or deep clustering (DC) [58] rich information [219, 220]. For example, knowing how much,
cannot adequately address reverberation and background and how, a child speaks in an interaction reveals critical infor-
noise [216]. mation about the developmental state, and oﬀers clues to clini-
cians in diagnosing disorders such as Autism [221]. Such anal-
2. Flexible framework: It is desirable that the transcription
yses are made possible by capturing and processing the audio
system can process all the available information, such as
recordings of the interactions, often involving two or more peo-
the multi-channel audio and the visual cues. The system
ple. An important foundational step is identifying and associ-
needs to process a dynamically changing number of au-
ating the speech portions belonging to speciﬁc individuals in-
dio channels without loss of performance. As such, the
volved in the conversation. The technologies that provide this
architecture needs to be modular enough to encompass the
capability are speech activity detection (SAD) and speaker di-
diﬀerent settings.
arization. Speech portions segmented with speaker-speciﬁc in-
formation provided by speaker diarization, by itself without any
3. Speaker-Attributed ASR of natural meetings requires on-
line/streaming ASR, audio pre-processing such as dere- explicit lexical transcription, can oﬀer important information to
domain experts who can take advantage of speaker diarization
verberation, and accurate diarization and speaker identi-
results for quantitative turn-taking analysis.
ﬁcation. These multiple processing steps are usually op-
A domain that is the most relevant such analyses of spoken
timized separately and thus, the overall pipeline is most
frequently ineﬃcient. conversational interactions relates to behavioral signal process-
ing (BSP) [222, 219] which refers to the technology and algo-
4. Using multiple, not-synchronized audio streams, e.g., au- rithms for modeling and understanding human communicative,
dio capturing with mobile devices, adds complexity to the aﬀective and social behavior. For example, these may include
meeting setup and processing. In return, we gain poten- analyzing how positive or negative a person is, how empathic
tially better spatial coverage since the devices are usually an individual toward another, what does the behavior patterns
distributed around the room and near the speakers. As reveal about the relationship status, and health condition of an
part of the application scenario, the meeting participants individual [220]. BSP involves addressing all the complexi-
bring their personal devices, which can be re-purposed ties of spontaneous interactions in conversations with additional
to improve the overall meeting transcription quality. On challenges involved in handling and understanding emotional,
the other hand, while there are several pioneering stud- social and interpersonal behavioral dynamics revealed through
ies [217], it is unclear what the best strategies are for vocal verbal and nonverbal cues of the interaction participants.
consolidating multiple asynchronous audio streams and to Therefore, the knowledge of speaker speciﬁc vocal informa-
what extent they work for natural meetings in online and tion plays a signiﬁcant role in BSP, requiring highly accurate
oﬄine setups. speaker diarization performance. For example, speaker diariza-
tion module is employed as a pre-processing module for analyz-
Based on these considerations, an architecture of meeting
ing psychotherapy mechanisms and quality [223], and suicide
transcription system with asynchronous distant microphones
risk assessment [224].
have been proposed in [161]. In this work, various fusion
Another popular application of speaker diarization for con-
strategies have been investigating: from early fusion beam-
versation interaction analysis is the medical doctor-patient in-
forming the audio signals, to mid-fusion combining senones
teractions. In the system described in [225], the nature of mem-
per channel, to late fusion combining the diarization and ASR
ory problem of a patient is detected from the conversations be-
results [147]. The resulting system performance was bench-
tween neurologists and patients. Speech and language features
marked on real-world meeting recordings against ﬁx-geometry
extracted from ASR transcripts combined with speaker diariza-
systems. As mentioned above, the requirement of speaker-
tion results are used to predict the type of disorder. An au-
attributed transcriptions with low latency was adhered, as well.
tomated assistant system for medical domain transcription is
In addition to the end-to-end system analysis, the paper [161]
proposed in [226] which includes speaker diarization module,
proposed the idea of “leave-one-out beamforming” in the asyn-
ASR module and natural language generation (NLG) module.
chronous multi-microphone setup, enriching the “diversity” of
The automated assistant module accepts the audio clip and out-
the resulting signals, as proposed in [218]. Finally, it is de-
puts grammatically correct sentences that describe the topic of
scribed how an online, incremental version of ROVER can pro-
the conversation, subject and subject’s symptom.
cess both the ASR and diarization outputs, enhancing the over-
all speaker-attributed ASR performance.
6.3. Audio indexing
Content-based audio indexing is a well known application
6.2. Conversational Interaction Analysis and Behavioral Mod-
domain for speaker diarization. It can provide meta information
eling
such as the content or data type of a given audio data to make
19information retrieval eﬃcient since search query by machines more and more advancements have been made for speaker di-
would be limited by such metadata. The more diverse infor- arization, from a method that replaces a single module into a
mation were available, the better eﬃciency we could achieve in deep-learning-based one, to a fully end-to-end neural diariza-
retrieving audio contents from a database. tion. Furthermore, as the speech recognition technology be-
One useful piece of information for the audio indexing would comes more accessible, a trend to tightly integrate speaker di-
be ASR transcripts to understand the content of speech por- arization and ASR systems has emerged, such as beneﬁting
tions in the audio data. Speaker diarization can augment those from the ASR output to improve speaker diarization accuracy.
transcripts in terms of “who spoke when”, which was the main As of late, joint modeling for speaker diarization and speech
purpose of the Rich Transcription evaluation series [20] as we recognition is investigated in an attempt to enhance the over-
discussed in Sections 4.1 and 5.3. The aggregated spoken ut- all performance. Thanks to these great achievement, speaker
terances from speakers by a speaker diarization system also diarization systems have already been deployed in many appli-
enable per-speaker summary or keyword list-up, which can be cations, including meeting transcription, conversational inter-
used for another query values to retrieve relevant contents from action analysis, audio indexing, and conversational AI systems.
the database. In [227], we can peek a view of how speaker As we have seen, tremendous progress has been made for
diarization outputs can be linked for information searching in speaker diarization systems. Nevertheless, there are still much
consumer facing applications. room for improvement. As the ﬁnal remark, we conclude this
paper by listing up the remaining challenges for speaker diariza-
6.4. Conversational AI tion towards future research and development.
Thanks to the advance of ASR technology, the applications
Online processing of speaker diarization. Most speaker di-
of ASR are evolved from simple voice command recognition
arization methods assume that an entire recording can be ob-
systems to conversational AI systems. Conversational AI sys-
served to execute speaker diarization. However, many applica-
tems, as opposed to voice command recognition systems, have
tions such as meeting transcription systems or smart agents re-
features that voice command recognition systems are lack of.
quire only short latency for assigning the speaker. While there
The fundamental idea of conversational AI is making a ma-
have been several attempts to make online speaker diarization
chine that humans can talk to and interact with the system. In
system both for clustering-based systems (e.g., [205]) and neu-
this sense, focusing on an interested speaker in multi-party set-
ral network-based diarization systems (e.g., [55, 172, 173]), it’s
ting is one of the most important feature of conversational AI
still remaining as a challenging problem.
and speaker diarization becomes essential feature for conver-
sational AI. For example, conversational AI equipped in a car
Domain mismatch. A model that is trained on a data in a spe-
can pay attention to a speciﬁc speaker that is demanding a piece
ciﬁc domain often works poorly on a data in another domain.
of information from the navigation system by applying speaker
For example, it is experimentally known that the EEND model
diarization along with ASR.
tends to overﬁt to the distribution of the speaker overlaps of the
Smart speakers and voice assistants are the most popular
training data [56]. Such domain mismatch issue is universal
products where speaker diarization plays a signiﬁcant role for
for any training-based method. Given the growing interest for
conversational AI. Since response time and online processing
trainable speaker diarization systems, it will become more im-
are the crucial factors in real-life settings, the demand for end-
portant to assess the ability for handling the variety of inputs.
to-end speaker diarization system integrated into ASR pipeline
The international evaluation eﬀorts for speaker diarization such
is growing. The performance of incremental (online) ASR and
as the DIHARD challenge [189, 85, 190] or VoxSRC [197, 105]
speaker diarization of the commercial ASR services are eval-
will also have great importance for that direction.
uated and compared in [228]. It is expected that the real-time
and low latency aspect of speaker diarization will be more em-
Speaker overlap. Overlap of multi-talker speech is inevitable
phasized in the speaker diarization systems in the future since
nature of conversation. For example, average 12% to 15%
the performance of online diarization and online ASR still have
of speaker overlap was observed for meeting recordings [229,
much room for improvement.
102], and it can become higher for daily conversations [230,
191, 89]. Nevertheless, many conventional speaker diarization
7. Challenges and the Future of Speaker Diarization systems, especially clustering-based systems, treated only non-
overlapped region of recordings sometimes even for the evalua-
This paper has provided a comprehensive overview of tion metric. While the topic has been studied for long years (e.g.
speaker diarization techniques, highlighting the recent devel- early works [231, 232]), there is a growing interest for handling
opment of deep learning-based diarization approaches. In the the speaker overlaps towards better speaker diarization, includ-
early days, a speaker diarization system was developed as a ing the application of speech separation [104], post-processing
pipeline of sub-modules including front-end processing, speech [233, 162], and joint modeling of speech separation and speaker
activity detection, segmentation, speaker embedding extraction, diarization [76, 184].
clustering, and post-processing, leading to a standalone sys-
tem without much connection to other components in a given Integration with ASR. Not all but many applications require
speech application. As the rise of the deep learning technology, ASR results along with speaker diarization results. In the line of
20the modular combination of speaker diarization and ASR, some [10] M. A. Siegler, U. Jain, B. Raj, R. M. Stern, Automatic segmentation,
systems put a speaker diarization system before ASR [91] while classiﬁcation and clustering of broadcast news audio, in: Proceedings of
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
some systems put a diarization system after ASR [209]. Both
[11] H. Jin, F. Kubala, R. Schwartz, Automatic speaker clustering, in: Pro-
types of systems showed a strong performance for a speciﬁc
ceedings of Speech Recognition Workshop, 1997.
task, and it is still an open problem that what kind of system ar- [12] H. S. Beigi, S. H. Maes, Speaker, channel and environment change
chitecture is the best for the speaker diarization and ASR tasks detection, in: Proceedings of World Congress of Automation, 1998.
[13] S. S. Chen, P. S. Gopalakrishnan, Speaker, environment and channel
[88]. Furthermore, there is another line of research to jointly
change detection and clustering via the Bayesian Information Criterion,
perform speaker diarization and ASR [77, 78, 79, 184] as intro-
in: Tech. Rep., IBM T. J. Watson Research Center, 1998, pp. 127–132.
duced in Section 4. The joint modeling approach could leverage [14] A. Solomonoﬀ, A. Mielke, M. Schmidt, H. Gish, Clustering speakers
the inter-dependency between speaker diarization and ASR to by their voices, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, 1998, pp. 757–760.
better perform both tasks. However, it has not yet fully inves-
[15] J.-L. Gauvain, G. Adda, L. Lamel, M. Adda-Decker, Transcription of
tigated whether such joint frameworks perform better than the broadcast news: The LIMSI Nov 96 Hub4 system, in: Proceedings of
well-tuned modular systems. Overall, the integration of speaker ARPA Speech Recognition Workshop, 1997, pp. 56–63.
diarization and ASR is one of the hottest topics that has still [16] J.-L. Gauvain, L. Lamel, G. Adda, The LIMSI 1997 Hub-4E transcrip-
tion system, in: Proceedings of DARPA News Transcription and Under-
been pursued.
standing Workshop, 1998, pp. 75–79.
[17] J.-L. Gauvain, L. Lamel, G. Adda, Partitioning and transcription of
Audio visual modeling. Visual information contains a strong broadcast news data, in: Proceedings of the International Conference
clue to identify speakers. For example, the video captured by on Spoken Language Processing, 1998, pp. 1335–1338.
[18] D. Liu, F. Kubala, Fast speaker change detection for broadcast news
a ﬁsheye camera was used to improve the speaker diarization
transcription and indexing, in: Proceedings of the International Confer-
accuracy in a meeting transcription task [209]. The visual in- ence on Spoken Language Processing, 1999, pp. 1031–1034.
formation was also used to signiﬁcantly improve the speaker [19] AMI Consortium. http://www.amiproject.org/index.html.
diarization accuracy for speaker diarization on YouTube video [20] NIST, Rich Transcription Evaluation. https://www.nist.gov/itl/
[192]. While these studies showed the eﬀectiveness of visual iad/mig/rich-transcription-evaluation.
[21] J. Ajmera, C. Wooters, A robust speaker clustering algorithm, in: Pro-
information, the audio-visual speaker diarization has yet been ceedings of IEEE Workshop on Automatic Speech Recognition and Un-
rarely investigated compared with audio-only speaker diariza- derstanding, 2003, pp. 411–416.
tion, and there will be many rooms for the improvement. [22] S. E. Tranter, D. A. Reynolds, Speaker diarisation for broadcast news,
in: Proceedings of Odyssey Speaker and Language Recognition Work-
shop, 2004, pp. 337–344.
[23] C. Wooters, J. Fung, B. Peskin, X. Anguera, Toward robust speaker seg-
References
mentation: The ICSI-SRI Fall 2004 diarization system, in: Proceedings
of Fall 2004 Rich Transcription Workshop, 2004, pp. 402–414.
[1] S. E. Tranter, K. Yu, D. A. Reynolds, G. Evermann, D. Y. Kim, P. C. [24] D. A. Reynolds, P. Torres-Carrasquillo, The MIT Lincoln Laboratory
Woodland, An investigation into the the interactions between speaker RT-04F diarization systems: Applications to broadcast audio and tele-
diarisation systems and automatic speech transcription, CUED/F- phone conversations, in: Proceedings of Fall 2004 Rich Transcription
INFENG/TR-464 (2003). Workshop, 2004.
[2] S. E. Tranter, D. A. Reynolds, An overview of automatic speaker di- [25] D. A. Reynolds, P. Torres-Carrasquillo, Approaches and applications of
arization systems, IEEE Transactions on Audio, Speech, and Language audio diarization, in: Proceedings of IEEE International Conference on
Processing 14 (2006) 1557–1565. Acoustics, Speech and Signal Processing, 2005, pp. 953–956.
[3] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, [26] X. Zhu, C. Barras, S. Meignier, J.-L. Gauvain, Combining speaker iden-
O. Vinyals, Speaker diarization: A review of recent research, IEEE tiﬁcation and BIC for speaker diarization, in: Proceedings of the An-
Transactions on Audio, Speech, and Language Processing 20 (2012) nual Conference of the International Speech Communication Associa-
356–370. tion, 2005, pp. 2441–2444.
[4] H. Gish, M. . Siu, R. Rohlicek, Segregation of speakers for speech [27] C. Barras, Xuan Zhu, S. Meignier, J.-L. Gauvain, Multistage speaker
recognition and speaker identiﬁcation, in: Proceedings of IEEE Inter- diarization of broadcast news, IEEE Transactions on Audio, Speech,
national Conference on Acoustics, Speech and Signal Processing, 1991, and Language Processing 14 (2006) 1505–1512.
pp. 873–876. [28] N. Mirghafori, C. Wooters, Nuts and ﬂakes: A study of data character-
[5] M.-H. Siu, Y. George, H. Gish, An unsupervised, sequential learning al- istics in speaker diarization, in: Proceedings of IEEE International Con-
gorithm for segmentation for speech waveforms with multiple speakers, ference on Acoustics, Speech and Signal Processing, 2006, pp. 1017–
in: Proceedings of IEEE International Conference on Acoustics, Speech 1020.
and Signal Processing, 1992, pp. 189–192. [29] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step-
[6] J. R. Rohlicek, D. Ayuso, M. Bates, R. Bobrow, A. Boulanger, H. Gish, by-step and integrated approaches in broadcast news speaker diarization,
P. Jeanrenaud, M. Meteer, M. Siu, Gisting conversational speech, in: Computer, Speech & Language 20 (2006) 303–330.
Proceedings of IEEE International Conference on Acoustics, Speech and [30] A. E. Rosenberg, A. Gorin, Z. Liu, P. Parthasarathy, Unsupervised
Signal Processing, 1992, pp. 113–116. speaker segmentation of telephone conversations, in: Proceedings of
[7] M. Sugiyama, J. Murakami, H. Watanabe, Speech segmentation and the International Conference on Spoken Language Processing, 2002, pp.
clustering based on speaker features, in: Proceedings of IEEE Interna- 565–568.
tional Conference on Acoustics, Speech and Signal Processing, 1993, [31] D. Liu, F. Kubala, A cross-channel modeling approach for automatic
pp. 395–398. segmentation of conversational telephone speech, in: Proceedings of
[8] U. Jain, M. A. Siegler, S.-J. Doh, E. Gouvea, J. Huerta, P. J. Moreno, IEEE Workshop on Automatic Speech Recognition and Understanding,
B. Raj, R. M. Stern, Recognition of continuous broadcast news with 2003, pp. 333–338.
multiple unknown speakers and environments, in: Proceedings of ARPA [32] S. E. Tranter, K. Yu, G. Evermann, P. C. Woodland, Generating and
Spoken Language Technology Workshop, 1996, pp. 61–66. evaluating for automatic speech recognition of conversational telephone
[9] M. Padmanabhan, L. R. Bahl, D. Nahamoo, M. A. Picheny, Speaker speech, in: Proceedings of IEEE International Conference on Acoustics,
clustering and transformation for speaker adaptation in large-vocabulary Speech and Signal Processing, 2004, pp. 753–756.
speech recognition systems, in: Proceedings of IEEE International Con- [33] D. A. Reynolds, P. Kenny, F. Castaldo, A study of new approaches
ference on Acoustics, Speech and Signal Processing, 1996, pp. 701–704.
21to speaker diarization, in: Proceedings of the Annual Conference of Annual Conference of the International Speech Communication Associ-
the International Speech Communication Association, 2009, pp. 1047– ation, 2012, pp. 2174–2177.
1050. [54] S. H. Shum, N. Dehak, R. Dehak, J. R. Glass, Unsupervised methods for
[34] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa- speaker diarization: An integrated and iterative approach, IEEE Trans-
tions using factor analysis, IEEE Journal of Selected Topics in Signal actions on Audio, Speech, and Language Processing 21 (2013).
Processing 4 (2010) 1059–1070. [55] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, C. Wang, Fully supervised
[35] T. Pfau, D. Ellis, A. Stolcke, Multispeaker speech activity detection speaker diarization, in: Proceedings of IEEE International Conference
for the ICSI meeting recorder, in: Proceedings of IEEE Workshop on on Acoustics, Speech and Signal Processing, 2019, pp. 6301–6305.
Automatic Speech Recognition and Understanding, 2001, pp. 107–110. [56] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, S. Watanabe, End-
[36] J. Ajmera, G. Lathoud, L. McCowan, Clustering and segmenting speak- to-end neural speaker diarization with permutation-free objectives, Pro-
ers and their locations in meetings, in: Proceedings of IEEE Interna- ceedings of the Annual Conference of the International Speech Commu-
tional Conference on Acoustics, Speech and Signal Processing, 2004, nication Association (2019) 4300–4304.
pp. 605–608. [57] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, S. Watanabe,
[37] Q. Jin, K. Laskowski, T. Schultz, A. Waibel, Speaker segmentation and End-to-end neural speaker diarization with self-attention, in: Proceed-
clustering in meetings, in: Proceedings of the International Conference ings of IEEE Workshop on Automatic Speech Recognition and Under-
on Spoken Language Processing, 2004, pp. 597–600. standing, IEEE, 2019, pp. 296–303.
[38] X. Anguera, C. Wooters, B. Peskin, M. Aguilo, Robust speaker seg- [58] J. R. Hershey, Z. Chen, J. Le Roux, S. Watanabe, Deep clustering: Dis-
mentation for meetings: The ICSI-SRI Spring 2005 diarization system, criminative embeddings for segmentation and separation, in: Proceed-
in: Proceedings of Machine Learning for Multimodal Interaction Work- ings of IEEE International Conference on Acoustics, Speech and Signal
shop, 2005, pp. 402–414. Processing, IEEE, 2016, pp. 31–35.
[39] X. Anguera, C. Wooters, J. Hernando, Purity algorithms for speaker di- [59] M. Kolbæk, D. Yu, Z.-H. Tan, J. Jensen, Multitalker speech separa-
arization of meetings data, in: Proceedings of IEEE International Con- tion with utterance-level permutation invariant training of deep recur-
ference on Acoustics, Speech and Signal Processing, volume I, 2006, rent neural networks, IEEE/ACM Transactions on Audio, Speech, and
pp. 1025–1028. Language Processing 25 (2017) 1901–1913.
[40] D. Istrate, C. Fredouille, S. Meignier, L. Besacier, J.-F. Bonastre, NIST [60] Y. Luo, N. Mesgarani, Conv-tasnet: Surpassing ideal time–frequency
RT05S evaluation: Pre-processing techniques and speaker diarization on magnitude masking for speech separation, IEEE/ACM Transactions on
multiple microphone meetings, in: Proceedings of Machine Learning for Audio, Speech, and Language Processing 27 (2019) 1256–1266.
Multimodal Interaction Workshop, 2006. [61] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. Gonzalez-
[41] D. A. V. Leeuwen, M. Konecny, Progress in the AMIDA speaker diariza- Dominguez, Deep neural networks for small footprint text-dependent
tion system for meeting data, in: Proceedings of International Evaluation speaker veriﬁcation, in: Proceedings of IEEE International Conference
Workshops CLEAR 2007 and RT 2007, 2007, pp. 475–483. on Acoustics, Speech and Signal Processing, IEEE, 2014, pp. 4052–
[42] X. Anguera, C. Wooters, J. Hernando, Acoustic beamforming for 4056.
speaker diarization of meetings, IEEE Transactions on Audio, Speech, [62] D. Snyder, D. Garcia-Romero, D. Povey, S. Khudanpur, Deep neural
and Language Processing 15 (2007) 2011–2023. network embeddings for text-independent speaker veriﬁcation., in: Pro-
[43] X. Zhu, C. Barras, L. Lamel, J.-L. Gauvain, Multi-stage speaker di- ceedings of the Annual Conference of the International Speech Commu-
arization for conference and lecture meetings, in: Proceedings of Inter- nication Association, 2017, pp. 999–1003.
national Evaluation Workshops CLEAR 2007 and RT 2007, 2007, pp. [63] T. Drugman, Y. Stylianou, Y. Kida, M. Akamine, Voice activity detec-
533–542. tion: Merging source and ﬁlter-based information, IEEE Signal Process-
[44] D. Vijayasenan, F. Valente, H. Bourlard, An information theoretic ap- ing Letters 23 (2015) 252–256.
proach to speaker diarization of meeting data, IEEE Transactions on [64] X. Guo, L. Gao, X. Liu, J. Yin, Improved deep embedded clustering
Audio, Speech, and Language Processing 17 (2009) 1382–1393. with local structure preservation, in: Proceedings of International Joint
[45] F. Valente, P. Motlicek, D. Vijayasenan, Variational Bayesian speaker Conference on Artiﬁcial Intelligence, 2017, pp. 1753–1759.
diarization of meeting recordings, in: Proceedings of IEEE Interna- [65] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, M. Brudno,
tional Conference on Acoustics, Speech and Signal Processing, 2010, Speaker diarization with session-level speaker embedding reﬁnement
pp. 4954–4957. using graph neural networks, in: Proceedings of IEEE International
[46] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Joint factor analy- Conference on Acoustics, Speech and Signal Processing, IEEE, 2020,
sis versus eigenchannels in speaker recognition, IEEE Transactions on pp. 7109–7113.
Audio, Speech, and Language Processing 15 (2007) 1435–1447. [66] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
[47] E. Variani, X. Lei, E. McDermott, I. L. Moreno, J. G-Dominguez, Deep renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
neural networks for small footprint text-dependent speaker veriﬁcation, I. Podluzhny, A. Laptev, A. Romanenko, Target-speaker voice activ-
in: Proceedings of IEEE International Conference on Acoustics, Speech ity detection: a novel approach for multi-speaker diarization in a dinner
and Signal Processing, 2014, pp. 4052–4056. party scenario, in: Proceedings of the Annual Conference of the Inter-
[48] G. Heigold, I. Moreno, S. Bengio, N. Shazeer, End-to-end text- national Speech Communication Association, 2020, pp. 274–278.
dependent speaker veriﬁcation, in: Proceedings of IEEE International [67] D. Yu, X. Chang, Y. Qian, Recognizing multi-talker speech with permu-
Conference on Acoustics, Speech and Signal Processing, 2016, pp. tation invariant training, Proceedings of the Annual Conference of the
5115–5119. International Speech Communication Association (2017) 2456–2460.
[49] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, I. L. Moreno, Speaker di- [68] H. Seki, T. Hori, S. Watanabe, J. Le Roux, J. R. Hershey, A purely end-
arization with LSTM, in: Proceedings of IEEE International Conference to-end system for multi-speaker speech recognition, 2018, pp. 2620–
on Acoustics, Speech and Signal Processing, 2018, pp. 5239–5243. 2630.
[50] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X- [69] X. Chang, Y. Qian, K. Yu, S. Watanabe, End-to-end monaural multi-
vectors: Robust DNN embeddings for speaker recognition, in: Proceed- speaker ASR system without pretraining, in: Proceedings of IEEE Inter-
ings of IEEE International Conference on Acoustics, Speech and Signal national Conference on Acoustics, Speech and Signal Processing, 2019,
Processing, 2018, pp. 5329–5333. pp. 6256–6260.
[51] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouellet, Front-end [70] N. Kanda, Y. Fujita, S. Horiguchi, R. Ikeshita, K. Nagamatsu, S. Watan-
factor analysis for speaker veriﬁcation, IEEE Transactions on Audio, abe, Acoustic modeling for distant multi-talker speech recognition with
Speech, and Language Processing 19 (2011). single-and multi-channel branches, in: Proceedings of IEEE Interna-
[52] S. Shum, N. Dehak, J. Glass, On the use of spectral and iterative methods tional Conference on Acoustics, Speech and Signal Processing, 2019,
for speaker diarization, in: Proceedings of the Annual Conference of the pp. 6630–6634.
International Speech Communication Association, 2012, pp. 482–485. [71] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu,
[53] G. Dupuy, M. Rouvier, S. Meignier, Y. Esteve, i-Vectors and ILP clus- S. Watanabe, Auxiliary interference speaker loss for target-speaker
tering adapted to cross-show speaker diarization, in: Proceedings of the speech recognition, in: Proceedings of the Annual Conference of the
22International Speech Communication Association, 2019, pp. 236–240. Proceedings of IEEE Spoken Language Technology Workshop, 2021.
[72] X. Wang, N. Kanda, Y. Gaur, Z. Chen, Z. Meng, T. Yoshioka, Ex- [89] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
ploring end-to-end multi-channel asr with bias information for meeting S. Khudanpur, V. Manohar, D. Povey, D. Raj, et al., CHiME-6 challenge:
transcription, in: Proceedings of IEEE Workshop on Automatic Speech Tackling multispeaker speech recognition for unsegmented recordings,
Recognition and Understanding, 2021. in: 6th International Workshop on Speech Processing in Everyday Envi-
[73] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, J. Li, ronments (CHiME 2020), 2020.
Speech separation using speaker inventory, in: Proceedings of IEEE [90] A. Arora, D. Raj, A. S. Subramanian, K. Li, B. Ben-Yair, M. Maciejew-
Workshop on Automatic Speech Recognition and Understanding, 2019, ski, P. Z˙ elasko, P. Garcia, S. Watanabe, S. Khudanpur, The JHU multi-
pp. 230–236. microphone multi-speaker asr system for the CHiME-6 challenge, arXiv
[74] C. Han, Y. Luo, C. Li, T. Zhou, K. Kinoshita, S. Watanabe, M. Delcroix, preprint arXiv:2006.07898 (2020).
H. Erdogan, J. R. Hershey, N. Mesgarani, et al., Continuous speech [91] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Ko-
separation using speaker inventory for long multi-talker recording, arXiv renevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko,
preprint arXiv:2012.09727 (2020). I. Podluzhny, et al., The STC system for the CHiME-6 challenge, in:
[75] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, S. Khu- CHiME 2020 Workshop on Speech Processing in Everyday Environ-
danpur, Speaker diarization with region proposal network, in: Proceed- ments, 2020.
ings of IEEE International Conference on Acoustics, Speech and Signal [92] X. Lu, Y. Tsao, S. Matsuda, C. Hori, Speech enhancement based on
Processing, IEEE, 2020, pp. 6514–6518. deep denoising autoencoder., in: Proceedings of the Annual Conference
[76] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, of the International Speech Communication Association, 2013, pp. 436–
R. Haeb-Umbach, All-neural online source separation, counting, and 440.
diarization for meeting analysis, in: Proceedings of IEEE International [93] Y. Xu, J. Du, L.-R. Dai, C.-H. Lee, A regression approach to speech
Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, enhancement based on deep neural networks, IEEE/ACM Transactions
pp. 91–95. on Audio, Speech, and Language Processing 23 (2014) 7–19.
[77] L. E. Shafey, H. Soltau, I. Shafran, Joint Speech Recognition and [94] H. Erdogan, J. R. Hershey, S. Watanabe, J. Le Roux, Phase-sensitive and
Speaker Diarization via Sequence Transduction, in: Proceedings of the recognition-boosted speech separation using deep recurrent neural net-
Annual Conference of the International Speech Communication Associ- works, in: Proceedings of IEEE International Conference on Acoustics,
ation, ISCA, 2019, pp. 396–400. Speech and Signal Processing, IEEE, 2015, pp. 708–712.
[78] H. H. Mao, S. Li, J. McAuley, G. Cottrell, Speech recognition and [95] P. C. Loizou, Speech enhancement: theory and practice, CRC press,
multi-speaker diarization of long conversations, in: Proceedings of the 2013.
Annual Conference of the International Speech Communication Associ- [96] T. Gao, J. Du, L.-R. Dai, C.-H. Lee, Densely connected progressive
ation, 2020, pp. 691–695. learning for lstm-based speech enhancement, in: Proceedings of IEEE
[79] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, S. Watanabe, International Conference on Acoustics, Speech and Signal Processing,
Simultaneous speech recognition and speaker diarization for monaural IEEE, 2018, pp. 5054–5058.
dialogue recordings with target-speaker acoustic models, in: Proceed- [97] J. Heymann, L. Drude, R. Haeb-Umbach, Neural network based spectral
ings of IEEE Workshop on Automatic Speech Recognition and Under- mask estimation for acoustic beamforming, in: Proceedings of IEEE
standing, 2019, pp. 31–38. International Conference on Acoustics, Speech and Signal Processing,
[80] N. Kanda, X. Chang, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Yosh- IEEE, 2016, pp. 196–200.
ioka, Investigation of end-to-end speaker-attributed ASR for continu- [98] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, J. Le Roux,
ous multi-talker recordings, in: Proceedings of IEEE Spoken Language Improved MVDR beamforming using single-channel mask prediction
Technology Workshop, 2021. networks, Proceedings of the Annual Conference of the International
[81] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, Speech Communication Association (2016) 1981–1985.
B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for [99] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B.-H. Juang,
digital home assistants: Combining signal processing with deep-learning Speech dereverberation based on variance-normalized delayed linear
techniques, IEEE Signal Processing Magazine 36 (2019) 111–124. prediction, IEEE Transactions on Audio, Speech, and Language Pro-
[82] E. Vincent, T. Virtanen, S. Gannot, Audio source separation and speech cessing 18 (2010) 1717–1731.
enhancement, John Wiley & Sons, 2018. [100] T. Yoshioka, T. Nakatani, Generalization of multi-channel linear predic-
[83] D. Wang, J. Chen, Supervised speech separation based on deep learning: tion methods for blind mimo impulse response shortening, IEEE Trans-
An overview, IEEE/ACM Transactions on Audio, Speech, and Language actions on Audio, Speech, and Language Processing 20 (2012) 2707–
Processing 26 (2018) 1702–1726. 2720.
[84] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [101] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, et al., Diariza- WPE: A python package for weighted prediction error dereverberation
tion is hard: Some experiences and lessons learned for the JHU team in in numpy and tensorﬂow for online and oﬄine processing, in: Speech
the inaugural DIHARD challenge., in: Proceedings of the Annual Con- Communication; 13th ITG-Symposium, VDE, 2018, pp. 1–5.
ference of the International Speech Communication Association, 2018, [102] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, F. Alleva, Recognizing over-
pp. 2808–2812. lapped speech in meetings: A multichannel separation approach using
[85] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- neural networks, in: Proceedings of the Annual Conference of the Inter-
man, The second DIHARD diarization challenge: Dataset, task, and national Speech Communication Association, 2018, pp. 3038–3042.
baselines, Proceedings of the Annual Conference of the International [103] C. Boeddecker, J. Heitkaemper, J. Schmalenstroeer, L. Drude, J. Hey-
Speech Communication Association (2019) 978–982. mann, R. Haeb-Umbach, Front-end processing for the CHiME-5 dinner
[86] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmol´ıkova´, party scenario, in: Proceedings of CHiME 2018 Workshop on Speech
O. Novotny`, K. Vesely`, O. Glembek, O. Plchot, et al., BUT system for Processing in Everyday Environments, 2018, pp. 35–40.
DIHARD speech diarization challenge 2018., in: Proceedings of the [104] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu,
Annual Conference of the International Speech Communication Associ- J. Wu, J. Li, Y. Gong, Microsoft speaker diarization system for
ation, 2018, pp. 2798–2802. the voxceleb speaker recognition challenge 2020, arXiv preprint
[87] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, arXiv:2010.11458 (2020).
J. Li, Continuous speech separation: Dataset and analysis, in: Proceed- [105] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie,
ings of IEEE International Conference on Acoustics, Speech and Signal M. McLaren, D. A. Reynolds, A. Zisserman, VoxSRC 2020: The
Processing, IEEE, 2020, pp. 7284–7288. second VoxCeleb speaker recognition challenge, arXiv preprint
[88] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, arXiv:2012.06867 (2020).
J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, J. R. Hershey, [106] T. Ng, B. Zhang, L. Nguyen, S. Matsoukas, X. Zhou, N. Mesgarani,
Integration of speech separation, diarization, and recognition for multi- K. Vesely`, P. Mateˇjka, Developing a speech activity detection system
speaker meetings: System description, comparison, and analysis, in: for the darpa rats program, in: Proceedings of the Annual Conference of
23the International Speech Communication Association, 2012, pp. 1969– [127] M. Senoussaoui, P. Kenny, P. Dumouchel, T. Stafylakis, Eﬃcient iter-
1972. ative mean shift based cosine dissimilarity for multi-recording speaker
[107] R. Sarikaya, J. H. Hansen, Robust detection of speech activity in the clustering, in: Proceedings of IEEE International Conference on Acous-
presence of noise, in: Proceedings of the International Conference on tics, Speech and Signal Processing, IEEE, 2013, pp. 7712–7715.
Spoken Language Processing, volume 4, Citeseer, 1998, pp. 1455–8. [128] I. Salmun, I. Shapiro, I. Opher, I. Lapidot, Plda-based mean shift speak-
[108] D. Haws, D. Dimitriadis, G. Saon, S. Thomas, M. Picheny, On the im- ers’ short segments clustering, Computer Speech and Language 45
portance of event detection for asr, in: Proceedings of IEEE International (2017) 411–436.
Conference on Acoustics, Speech and Signal Processing, 2016. [129] K. J. Han, S. S. Narayanan, A robust stopping criterion for agglomera-
[109] S. Meignier, D. Moraru, C. Fredouille, J.-F. Bonastre, L. Besacier, Step- tive hierarchical clustering in a speaker diarization system, in: Proceed-
by-step and integrated approaches in broadcast news speaker diarization, ings of the Annual Conference of the International Speech Communica-
Computer Speech and Language 20 (2006) 303–330. tion Association, 2007.
[110] S. Chen, P. Gopalakrishnan, et al., Speaker, environment and channel [130] S. Novoselov, A. Gusev, A. Ivanov, T. Pekhovsky, A. Shulipa,
change detection and clustering via the bayesian information criterion, A. Avdeeva, A. Gorlanov, A. Kozlov, Speaker diarization with deep
in: Proceedings DARPA broadcast news transcription and understanding speaker embeddings for dihard challenge ii., in: Proceedings of the
workshop, volume 8, Virginia, USA, 1998, pp. 127–132. Annual Conference of the International Speech Communication Associ-
[111] P. Delacourt, C. J. Wellekens, Distbic: A speaker-based segmentation ation, 2019, pp. 1003–1007.
for audio data indexing, Speech Communication 32 (2000) 111–126. [131] U. Von Luxburg, A tutorial on spectral clustering, Statist. and Comput.
[112] M. Senoussaoui, P. Kenny, T. Stafylakis, P. Dumouchel, A study of 17 (2007) 395–416.
the cosine distance-based mean shift for telephone speech diarization, [132] A. Ng, M. Jordan, Y. Weiss, On spectral clustering: Analysis and an al-
IEEE/ACM Transactions on Audio, Speech, and Language Processing gorithm, Advances in neural information processing systems 14 (2001)
22 (2013) 217–227. 849–856.
[113] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Ma- [133] H. Ning, M. Liu, H. Tang, T. S. Huang, A spectral clustering approach
ciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, S. Khudan- to speaker diarization, in: Proceedings of the International Conference
pur, Diarization is hard: some experiences and lessons learned for the on Spoken Language Processing, 2006, pp. 2178–2181.
JHU team in the inaugural DIHARD challenge, in: Proceedings of the [134] J. Luque, J. Hernando, On the use of agglomerative and spectral cluster-
Annual Conference of the International Speech Communication Associ- ing in speaker diarization of meetings, in: Proceedings of Odyssey: The
ation, 2018, pp. 2808–2812. Speaker and Language Recognition Workshop, 2012, pp. 130–137.
[114] W.-H. Tsai, S.-S. Cheng, H.-M. Wang, Speaker clustering of speech [135] Q. Lin, R. Yin, M. Li, H. Bredin, C. Barras, LSTM based similarity
utterances using a voice characteristic reference space, in: Proceedings measurement with spectral clustering for speaker diarization, in: Pro-
of the International Conference on Spoken Language Processing, 2004. ceedings of the Annual Conference of the International Speech Commu-
[115] J. E. Rougui, M. Rziza, D. Aboutajdine, M. Gelgon, J. Martinez, Fast in- nication Association, 2019, pp. 366–370.
cremental clustering of gaussian mixture speaker models for scaling up [136] T. J. Park, K. J. Han, M. Kumar, S. Narayanan, Auto-tuning spectral
retrieval in on-line broadcast, in: Proceedings of IEEE International clustering for speaker diarization using normalized maximum eigengap,
Conference on Acoustics, Speech and Signal Processing, volume 5, IEEE Signal Processing Letters 27 (2019) 381–385.
IEEE, 2006, pp. V–V. [137] P. Kenny, D. Reynolds, F. Castaldo, Diarization of telephone conversa-
[116] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriﬁcation using tions using factor analysis, IEEE Journal of Selected Topics in Signal
adapted gaussian mixture models, Digital signal processing 10 (2000) Processing 4 (2010) 1059–1070.
19–41. [138] M. Diez, L. Burget, P. Matejka, Speaker diarization based on bayesian
[117] P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session hmm with eigenvoice priors., in: Proceedings of Odyssey: The Speaker
variability in gmm-based speaker veriﬁcation, IEEE Transactions on and Language Recognition Workshop, 2018, pp. 147–154.
Audio, Speech, and Language Processing 15 (2007) 1448–1460. [139] M. Diez, L. Burget, F. Landini, J. Cˇ ernocky`, Analysis of speaker diariza-
[118] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, P. Dumouchel, A study of tion based on bayesian hmm with eigenvoice priors, IEEE/ACM Trans-
interspeaker variability in speaker veriﬁcation, IEEE Transactions on actions on Audio, Speech, and Language Processing 28 (2019) 355–368.
Audio, Speech, and Language Processing 16 (2008) 980–988. [140] M. Diez, L. Burget, S. Wang, J. Rohdin, J. Cernocky`, Bayesian hmm
[119] P. Kenny, G. Boulianne, P. Dumouchel, Eigenvoice modeling with based x-vector clustering for speaker diarization., in: Proceedings of the
sparse training data, IEEE Transactions on Speech and Audio Process- Annual Conference of the International Speech Communication Associ-
ing 13 (2005) 345–354. ation, 2019, pp. 346–350.
[120] G. Sell, D. Garcia-Romero, Speaker diarization with plda i-vector scor- [141] F. Landini, J. Profant, M. Diez, L. Burget, Bayesian hmm clustering of
ing and unsupervised calibration, in: Proceedings of IEEE Spoken Lan- x-vector sequences (vbx) in speaker diarization: theory, implementation
guage Technology Workshop, IEEE, 2014, pp. 413–417. and analysis on standard tasks, arXiv preprint arXiv:2006.07898 (2020).
[121] W. Zhu, J. Pelecanos, Online speaker diarization using adapted i- [142] G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor
vector transforms, in: Proceedings of IEEE International Conference on analysis subspace, in: Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, 2016, pp. 5045–5049. Acoustics, Speech and Signal Processing, IEEE, 2015, pp. 4794–4798.
[122] Y. Sun, X. Wang, X. Tang, Deep learning face representation from pre- [143] J. G. Fiscus, A post-processing system to yield reduced word error rates:
dicting 10,000 classes, in: Proceedings of the IEEE Conference on Com- Recognizer output voting error reduction (ROVER), in: Proceedings of
puter Vision and Pattern Recognition, 2014, pp. 1891–1898. IEEE Workshop on Automatic Speech Recognition and Understanding,
[123] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, Deepface: Closing the gap IEEE, 1997, pp. 347–354.
to human-level performance in face veriﬁcation, in: Proceedings of the [144] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat,
IEEE conference on computer vision and pattern recognition, 2014, pp. D. A. van Leeuwen, P. Matejka, P. Schwarz, A. Strasheim, Fusion of
1701–1708. heterogeneous speaker recognition systems in the STBU submission for
[124] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, the NIST speaker recognition evaluation 2006, IEEE Transactions on
J. Borgstrom, F. Richardson, S. Shon, F. Grondin, et al., State-of-the- Audio, Speech, and Language Processing 15 (2007) 2072–2084.
art speaker recognition for telephone and video speech: The JHU-MIT [145] M. Huijbregts, D. van Leeuwen, F. Jong, The majority wins: a method
submission for NIST SRE18., in: Proceedings of the Annual Confer- for combining speaker diarization systems, in: Proceedings of the An-
ence of the International Speech Communication Association, 2019, pp. nual Conference of the International Speech Communication Associa-
1488–1492. tion, ISCA, 2009, pp. 924–927.
[125] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature [146] S. Bozonnet, N. Evans, X. Anguera, O. Vinyals, G. Friedland, C. Fre-
space analysis, IEEE Transactions on pattern analysis and machine in- douille, System output combination for improved speaker diarization,
telligence 24 (2002) 603–619. in: Proceedings of the Annual Conference of the International Speech
[126] T. Stafylakis, V. Katsouros, G. Carayannis, Speaker clustering via the Communication Association, ISCA, 2010, pp. 2642–2645.
mean shift algorithm, Recall 2 (2010) 7. [147] A. Stolcke, T. Yoshioka, DOVER: A method for combining diariza-
24tion outputs, in: Proceedings of IEEE Workshop on Automatic Speech speaker diarization for an unknown number of speakers with encoder-
Recognition and Understanding, IEEE, 2019, pp. 757–763. decoder based attractors, in: Proceedings of the Annual Conference of
[148] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stol- the International Speech Communication Association, 2020, pp. 269–
cke, S. Khudanpur, DOVER-Lap: A method for combining overlap- 273.
aware diarization outputs, in: Proceedings of IEEE Spoken Language [170] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, K. Nagamatsu,
Technology Workshop, 2021. Neural speaker diarization with speaker-wise chain rule, arXiv preprint
[149] D. Dimitriadis, Enhancements for Audio-only Diarization Systems, arXiv:2006.01796 (2020).
arXiv preprint arXiv:1909.00082 (2019). [171] K. Kinoshita, M. Delcroix, N. Tawara, Integrating end-to-end neural
[150] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clus- and clustering-based diarization: Getting the best of both worlds, arXiv
tering analysis, in: Proceedings ofInternational Conference on Machine preprint arXiv:2010.13366 (2020).
Learning, 2016, pp. 478–487. [172] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, K. Nagamatsu, Online
[151] E. Ustinova, V. Lempitsky, Learning deep embeddings with histogram end-to-end neural diarization with speaker-tracing buﬀer, arXiv preprint
loss, Proceedings of Advances in Neural Information Processing Sys- arXiv:2006.02616 (2020).
tems 29 (2016) 4170–4178. [173] E. Han, C. Lee, A. Stolcke, BW-EDA-EEND: Streaming end-to-end
[152] Q. Lin, Y. Hou, M. Li, Self-attentive similarity measurement strategies neural speaker diarization for a variable number of speakers, arXiv
in speaker diarization, Proceedings of the Annual Conference of the preprint arXiv:2011.02678 (2020).
International Speech Communication Association (2020) 284–288. [174] J. Huang, E. Marcheret, K. Visweswariah, G. Potamianos, The ibm
[153] T. J. Park, M. Kumar, S. Narayanan, Multi-scale speaker diarization with rt07 evaluation systems for speaker diarization on lecture meetings, in:
neural aﬃnity score fusion, arXiv preprint arXiv:2011.10527 (2020). Multimodal Technologies for Perception of Humans, Springer, 2007, pp.
[154] Y. LeCun, Y. Bengio, G. Hinton, Deep Learning, Nature 521 (2015) 497–508.
436. [175] J. Silovsky, J. Zdansky, J. Nouza, P. Cerva, J. Prazak, Incorporation of
[155] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, the asr output in speaker segmentation and clustering within the task of
D. Wierstra, O. Vinyals, R. Pascanu, T. Lillicrap, Relational Recurrent speaker diarization of broadcast streams, in: International Workshop on
Neural Networks, in: Proceedings of Advances in Neural Information Multimedia Signal Processing, IEEE, 2012, pp. 118–123.
Processing Systems, 2018, pp. 7299–7310. [176] L. Canseco-Rodriguez, L. Lamel, J.-L. Gauvain, Speaker diarization
[156] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta- from speech transcripts, in: Proceedings of the International Conference
learning with Memory-Augmented Neural Networks, in: Proceedings on Spoken Language Processing, volume 4, 2004, pp. 3–7.
ofInternational Conference on Machine Learning, 2016, pp. 1842—- [177] N. Flemotomos, P. Georgiou, S. Narayanan, Linguistically aided speaker
1850. diarization using speaker role information, arXiv (2019) arXiv–1911.
[157] S. Sukhbaatar, J. Weston, R. Fergus, et al., End-to-End Memory Net- [178] T. J. Park, P. Georgiou, Multimodal speaker segmentation and diariza-
works, in: Proceedings of Advances in Neural Information Processing tion using lexical and acoustic cues via sequence to sequence neural
Systems, 2015, pp. 2440–2448. networks, Proceedings of the Annual Conference of the International
[158] D. Garcia-Romero, C. Y. Espy-Wilson, Analysis of i-vector Length Nor- Speech Communication Association (2018) 1373–1377.
malization in Speaker Recognition Systems, in: Proceedings of the An- [179] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou,
nual Conference of the International Speech Communication Associa- S. Narayanan, Speaker diarization with lexical information, Proceed-
tion, 2011, pp. 249–252. ings of the Annual Conference of the International Speech Communica-
[159] N. Flemotomos, D. Dimitriadis, A Memory Augmented Architec- tion Association (2019) 391–395.
ture for Continuous Speaker Identiﬁcation in Meetings, arXiv preprint [180] J. Fiscus, J. Ajot, J. Garofolo, The Rich Transcription 2007 meeting
arXiv:2001.05118 (2020). recognition evaluation, 2007, pp. 373–389.
[160] Z. Zaj´ıc, M. Kunesˇova´, V. Radova´, Investigation of Segmentation in i- [181] K. Zmolikova, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,
vector Based Speaker Diarization of Telephone Speech, in: International T. Nakatani, Speaker-aware neural network based beamformer for
Conference on Speech and Computer, 2016, pp. 411–418. speaker extraction in speech mixtures., in: Proceedings of the An-
[161] T. Yoshioka, D. Dimitriadis, A. Stolcke, W. Hinthorn, Z. Chen, M. Zeng, nual Conference of the International Speech Communication Associa-
H. Xuedong, Meeting Transcription Using Asynchronous Distant Mi- tion, 2017, pp. 2655–2659.
crophones, in: Proceedings of the Annual Conference of the Interna- [182] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Sin-
tional Speech Communication Association, 2019, pp. 2968–2972. gle channel target speaker extraction and recognition with speaker beam,
[162] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, K. Nagamatsu, in: Proceedings of IEEE International Conference on Acoustics, Speech
End-to-end speaker diarization as post-processing, arXiv preprint and Signal Processing, IEEE, 2018, pp. 5554–5558.
arXiv:2012.10055 (2020). [183] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa,
[163] D. M. Blei, P. I. Frazier, Distance dependent chinese restaurant pro- T. Nakatani, End-to-end SpeakerBeam for single channel target speech
cesses., Journal of Machine Learning Research 12 (2011). recognition., in: Proceedings of the Annual Conference of the Interna-
[164] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time tional Speech Communication Association, 2019, pp. 451–455.
object detection with region proposal networks, IEEE Transactions on [184] N. Kanda, Y. Gaur, X. Wang, Z. Meng, Z. Chen, T. Zhou, T. Yoshioka,
Pattern Analysis and Machine Intelligence 39 (2016) 1137–1149. Joint speaker counting, speech recognition, and speaker identiﬁcation
[165] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, S. Gannot, R. Ho- for overlapped speech of any number of speakers, in: Proceedings of the
raud, An EM algorithm for joint source separation and diarisation of Annual Conference of the International Speech Communication Associ-
multichannel convolutive speech mixtures, in: Proceedings of IEEE ation, 2020, pp. 36–40.
International Conference on Acoustics, Speech and Signal Processing, [185] N. Kanda, Z. Meng, L. Lu, Y. Gaur, X. Wang, Z. Chen, T. Yosh-
IEEE, 2017, pp. 16–20. ioka, Minimum bayes risk training for end-to-end speaker-attributed
[166] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda, R. Horaud, S. Gan- asr, arXiv preprint arXiv:2011.02921 (2020).
not, Exploiting the intermittency of speech for joint separation and di- [186] N. Kanda, Y. Gaur, X. Wang, Z. Meng, T. Yoshioka, Serialized output
arization, in: Proceedings of IEEE Workshop on Applications of Signal training for end-to-end overlapped speech recognition, in: Proceedings
Processing to Audio and Acoustics, IEEE, 2017, pp. 41–45. of the Annual Conference of the International Speech Communication
[167] K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, Tackling real noisy Association, 2020, pp. 2797–2801.
reverberant meetings with all-neural source separation, counting, and [187] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain,
diarization system, in: Proceedings of IEEE International Conference J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al., The ami
on Acoustics, Speech and Signal Processing, IEEE, 2020, pp. 381–385. meeting corpus: A pre-announcement, in: International workshop on
[168] K. Maekawa, Corpus of spontaneous japanese: Its design and evalua- machine learning for multimodal interaction, Springer, 2005, pp. 28–39.
tion, in: ISCA & IEEE Workshop on Spontaneous Speech Processing [188] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Pe-
and Recognition, 2003, pp. 7–12. skin, T. Pfau, E. Shriberg, A. Stolcke, C. Wooters, The ICSI meeting
[169] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, K. Nagamatsu, End-to-end corpus, in: Proceedings of IEEE International Conference on Acoustics,
25Speech and Signal Processing, 2003, pp. I–364–I–367. tem, in: Proceedings of the Annual Conference of the International
[189] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, M. Liber- Speech Communication Association, 2017, pp. 2739–2743.
man, The ﬁrst dihard speech diarization challenge, in: Proceedings of [206] A. Zhang, Q. Wan, Z. Zhu, J. Paisley, C. Wang, Fully supervised speaker
the Annual Conference of the International Speech Communication As- diarization, arXiv preprint arXiv:1810.04719 (2018).
sociation, 2018. [207] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
[190] N. Ryant, K. Church, C. Cieri, J. Du, S. Ganapathy, M. Liberman, Third nition, in: IEEE Conf. Computer Vision, Pattern Recognition, 2016, pp.
dihard challenge evaluation plan, arXiv preprint arXiv:2006.05815 770–778. doi:10.1109/CVPR.2016.90.
(2020). [208] K. He, G. Gkioxari, P. Dolla´r, R. Girshick, Mask R-CNN,
[191] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth ’chime’ speech CoRR abs/1703.06870 (2017). URL: http://arxiv.org/abs/1703.
separation and recognition challenge: Dataset, task and baselines, Pro- 06870. arXiv:1703.06870.
ceedings of the Annual Conference of the International Speech Commu- [209] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimi-
nication Association (2018) 1561–1565. triadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang,
[192] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, A. Zisserman, Spot the S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov,
conversation: Speaker diarisation in the wild, in: Proceedings of the L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao,
Annual Conference of the International Speech Communication Associ- T. Zhou, Advances in Online Audio-Visual Meeting Transcription, in:
ation, 2020, pp. 299–303. Proceedings of IEEE Workshop on Automatic Speech Recognition and
[193] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, LibriSpeech: an ASR Understanding, 2019, pp. 276–283.
corpus based on public domain audio books, in: Proceedings of IEEE [210] H. Buchner, R. Aichner, W. Kellermann, A generalization of blind
International Conference on Acoustics, Speech and Signal Processing, source separation algorithms for convolutive mixtures based on second-
IEEE, 2015, pp. 5206–5210. order statistics, IEEE Transactions on Speech and Audio Processing 13
[194] J. G. Fiscus, J. Ajot, M. Michel, J. S. Garofolo, The rich transcription (2005) 120–134.
2006 spring meeting recognition evaluation, in: Proceedings of Interna- [211] H. Sawada, S. Araki, S. Makino, Measuring dependence of bin-wise
tional Workshop on Machine Learning and Multimodal Interaction, May separated signals for permutation alignment in frequency-domain BSS,
2006, pp. 309–322. in: Int. Symp. Circ., Syst., 2007, pp. 3247–3250.
[195] P. E. Black, Hungarian algorithm, 2019. [212] F. Nesta, P. Svaizer, M. Omologo, Convolutive bss of short mixtures
Https://xlinux.nist.gov/dads/HTML/HungarianAlgorithm.html. by ica recursively regularized across frequencies, IEEE Transactions on
[196] T. J. Park, P. Georgiou, Multimodal speaker segmentation and di- Audio, Speech, and Language Processing 19 (2011) 624–639.
arization using lexical and acoustic cues via sequence to sequence neu- [213] H. Sawada, S. Araki, S. Makino, Underdetermined convolutive blind
ral networks, in: Proceedings of the Annual Conference of the In- source separation via frequency bin-wise clustering and permutation
ternational Speech Communication Association, 2018, pp. 1373–1377. alignment, IEEE Transactions on Audio, Speech, and Language Pro-
URL: http://dx.doi.org/10.21437/Interspeech.2018-1364. cessing 19 (2011) 516–527.
doi:10.21437/Interspeech.2018-1364. [214] N. Ito, S. Araki, T. Yoshioka, T. Nakatani, Relaxed disjointness based
[197] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, clustering for joint blind source separation and dereverberation, in: Pro-
A. Zisserman, VoxSRC 2019: The ﬁrst VoxCeleb speaker recognition ceedings of International Workshop on Acoustic Echo and Noise Con-
challenge, arXiv preprint arXiv:1912.02522 (2019). trol, 2014, pp. 268–272.
[198] C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare, [215] L. Drude, R. Haeb-Umbach, Tight integration of spatial and spectral
A. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan, features for BSS with deep clustering embeddings, in: Proceedings of
Y. Wu, X. Zhang, Speech recognition for medical conversations, the Annual Conference of the International Speech Communication As-
CoRR abs/1711.07274 (2017). URL: http://arxiv.org/abs/1711. sociation, 2017, pp. 2650–2654.
07274. arXiv:1711.07274. [216] M. Maciejewski, G. Sell, L. P. Garcia-Perera, S. Watanabe, S. Khudan-
[199] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, pur, Building corpora for single-channel speech separation across mul-
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lin- tiple domains, CoRR abs/1811.02641 (2018). URL: http://arxiv.
coln, A. Lisowska, I. McCowan, W. P. andD. Reidsma, P. Wellner, The org/abs/1811.02641. arXiv:1811.02641.
AMI meeting corpus: a pre-announcement, in: Proceedings of Int. [217] S. Araki, N. Ono, K. Kinoshita, M. Delcroix, Meeting recognition with
Worksh. Machine Learning for Multimodal Interaction, 2006, pp. 28– asynchronous distributed microphone array using block-wise reﬁnement
39. of mask-based MVDR beamformer, in: Proceedings of IEEE Interna-
[200] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, tional Conference on Acoustics, Speech and Signal Processing, 2018,
G. Zweig, Achieving human parity in conversational speech recognition, pp. 5694–5698.
CoRR abs/1610.05256 (2016). URL: http://arxiv.org/abs/1610. [218] A. Stolcke, Making the most from multiple microphones in meeting
05256. arXiv:1610.05256. recordings, in: Proceedings of IEEE International Conference on Acous-
[201] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, tics, Speech and Signal Processing, 2011, pp. 4992–4995.
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, P. Hall, English [219] S. Narayanan, P. G. Georgiou, Behavioral signal processing: Deriving
conversational telephone speech recognition by humans and machines, human behavioral informatics from speech and language, Proceedings
CoRR abs/1703.02136 (2017). URL: http://arxiv.org/abs/1703. of the IEEE 101 (2013) 1203–1233.
02136. arXiv:1703.02136. [220] D. Bone, C.-C. Lee, T. Chaspari, J. Gibson, S. Narayanan, Signal pro-
[202] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, cessing and machine learning for mental health research and clinical ap-
C. Yu, W. Fabian, M. Espi, T. Higuchi, S. Araki, T. Nakatani, The NTT plications, IEEE Signal Processing Magazine 34 (2017) 189–196.
CHiME-3 system: advances in speech enhancement and recognition for [221] M. Kumar, S. H. Kim, C. Lord, S. Narayanan, Speaker diarization for
mobile multi-microphone devices, in: Proceedings of IEEE Workshop naturalistic child-adult conversational interactions using contextual in-
on Automatic Speech Recognition and Understanding, 2015, pp. 436– formation., Journal of the Acoustical Society of America 147 (2020)
443. EL196–EL200. doi:10.1121/10.0000736.
[203] J. Du, Y. Tu, L. Sun, F. Ma, H. Wang, J. Pan, C. Liu, J. Chen, C. Lee, [222] P. G. Georgiou, M. P. Black, S. S. Narayanan, Behavioral signal pro-
The USTC-iFlytek system for CHiME-4 challenge, in: Proceedings of cessing for understanding (distressed) dyadic interactions: some recent
CHiME-4 Workshop, 2016, pp. 36–38. developments, in: Proceedings of the joint ACM workshop on Human
[204] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, gesture and behavior understanding, 2011, pp. 7–12.
I. Shafran, H. Sak, G. Punduk, K. Chin, K. C. Sim, R. J. Weiss, K. W. [223] B. Xiao, C. Huang, Z. E. Imel, D. C. Atkins, P. Georgiou, S. S.
Wilson, E. Variani, C. Kim, O. Siohan, M. Weintrauba, E. McDermott, Narayanan, A technology prototype system for rating therapist empathy
R. Rose, M. Shannon, Acoustic modeling for Google Home, in: Pro- from audio recordings in addiction counseling, PeerJ Computer Science
ceedings of the Annual Conference of the International Speech Commu- 2 (2016) e59.
nication Association, 2017, pp. 399–403. [224] S. N. Chakravarthula, M. Nasir, S.-Y. Tseng, H. Li, T. J. Park, B. Bau-
[205] D. Dimitriadis, P. Fousek, Developing on-line speaker diarization sys- com, C. J. Bryan, S. Narayanan, P. Georgiou, Automatic prediction
26of suicidal risk in military couples using multimodal interaction cues
from couples conversations, in: Proceedings of IEEE International Con-
ference on Acoustics, Speech and Signal Processing, IEEE, 2020, pp.
6539–6543.
[225] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri,
M. Reuber, H. Christensen, Toward the automation of diagnostic con-
versation analysis in patients with memory complaints, Journal of
Alzheimer’s Disease 58 (2017) 373–387.
[226] G. P. Finley, E. Edwards, A. Robinson, N. Sadoughi, J. Fone, M. Miller,
D. Suendermann-Oeft, M. Brenndoerfer, N. Axtmann, An automated
assistant for medical scribes., in: Proceedings of the Annual Confer-
ence of the International Speech Communication Association, 2018, pp.
3212–3213.
[227] A. Guo, A. Faria, J. Riedhammer, Remeeting – Deep insights to conver-
sations, in: Proceedings of the Annual Conference of the International
Speech Communication Association, 2016, pp. 1964–1965.
[228] A. Addlesee, Y. Yu, A. Eshghi, A comprehensive evaluation of incre-
mental speech recognition and diarization for conversational ai, in: Pro-
ceedings of the International Conference on Computational Linguistics,
2020, pp. 3492–3503.
[229] O. Cetin, E. Shriberg, Speaker overlaps and ASR errors in meetings:
Eﬀects before, during, and after the overlap, in: Proceedings of IEEE
International Conference on Acoustics, Speech and Signal Processing,
volume 1, IEEE, 2006, pp. 357–360.
[230] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi,
K. Nagamatsu, R. Haeb-Umbach, Guided source separation meets a
strong ASR backend: Hitachi/Paderborn University joint investigation
for dinner party ASR, Proceedings of the Annual Conference of the
International Speech Communication Association (2019) 1248–1252.
[231] S. Otterson, M. Ostendorf, Eﬃcient use of overlap information in
speaker diarization, in: Proceedings of IEEE Workshop on Automatic
Speech Recognition and Understanding, IEEE, 2007, pp. 683–686.
[232] K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, Overlapped
speech detection for improved speaker diarization in multiparty meet-
ings, in: Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, IEEE, 2008, pp. 4353–4356.
[233] L. Bullock, H. Bredin, L. P. Garcia-Perera, Overlap-aware diarization:
Resegmentation using neural end-to-end overlapped speech detection,
in: Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing, IEEE, 2020, pp. 7114–7118.
27