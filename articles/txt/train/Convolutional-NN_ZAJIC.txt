INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566INTERSPEECH 2017
August 20–24, 2017, Stockholm, Sweden
Speaker Diarization Using Convolutional Neural Network for Statistics
Accumulation Reﬁnement
Zbyneˇk Zaj´ıc1, Marek Hru´ z1, Ludeˇk Mu¨ ller1,2
University of West Bohemia
Faculty of Applied Sciences
1NTIS - New Technologies for the Information Society and 2Dept. of Cybernetics,
Univerzitn´ı 8, 306 14 Plzenˇ, Czech Republic
zzajic@ntis.zcu.cz, mhruz@ntis.zcu.cz, muller@ntis.zcu.cz
Abstract process and use a simple constant length window segmentation
of speech [3, 5].
The aim of this paper is to investigate the beneﬁt of information
The success of DNNs in the speech recognition task [13]
from a speaker change detection system based on Convolutional
leads in recent times to their exploitation in SD systems. DNNs
Neural Network (CNN) when applied to the process of accumu-
are utilized in the task of the segmentation [11, 14] or in the
lation of statistics for an i-vector generation. The investigation
clustering process [15, 16]. In [17] DNNs are used to replace
is carried out on the problem of diarization. In our system, the
unsupervised Universal Background Model (UBM) for the ac-
output of the CNN is a probability value of a speaker change
cumulation of statistics in the i-vector generation. DNN was
in a conversation for a given time segment. According to this
also applied to the representation of the speaker in [18, 19] or
probability, we cut the conversation into short segments that are
very recently in [20] and in [21], where the triplet loss paradigm
then represented by the i-vector (to describe a speaker in it). We
was used for training the DNN descriptor with extremely short
propose a technique to utilize the information from the CNN
speech turn.
for the weighting of the acoustic data in a segment to reﬁne
the statistics accumulation process. This technique enables us In our previous papers [14, 22] we applied a CNN to the
to represent the speaker better in the ﬁnal i-vector. The experi- problem of SCD. The main difference between our approach
ments on the English part of the CallHome corpus show that our and the one in others works lies in the fact that we introduce a
proposed reﬁnement of the statistics accumulation is beneﬁcial spectrogram to a CNN and let the net compute its own features.
with the relative improvement of Diarization Error Rate almost CNNs were introduced in [23] to cope with the prob-
by 16 % when compared to the speaker diarization system with- lem of image classiﬁcation. They were popularized by
out statistics reﬁnement. Krizhevsky et al. [24] with updated design blocks such as Rec-
Index Terms: convolutional neural network, speaker change tiﬁed Linear Units (ReLU) or max pooling instead of average
detection, speaker diarization, i-vector, statistics accumulation pooling. When a CNN is trained on large scale datasets one
can observe its capability to learn discriminative features on its
1. Introduction own. Furthermore, the net is able to learn a semantic represen-
tation of the data. Our experiments with the CNN in the task
The problem of Speaker Diarization (SD) is crucial for many
of SCD exhibited better results than classical approaches based
speech applications dealing with real data, where only one
on BIC. The input of the network is a spectrogram of a segment
speaker occurrence in a recording cannot be ensured. The SD
of the original waveform and the output is a probability that
problem is deﬁned as a task of categorizing speakers in an un-
there is a speaker change in the middle of the segment. When
labeled conversation, without any prior information regarding
the CNN is applied to the whole recording in a sliding window
the number and identities of the speakers. Different approaches
fashion a probability signal of the speaker change is obtained.
were proposed to solve this task [1]. The most common ap-
Further processing of this signal is needed to determine where
proach to the SD consists of the segmentation of an input sig-
a change occurs. In our previous work, we detected peaks using
nal, followed by the merging of the segments into clusters cor-
non-maximum suppression.
responding to individual speakers [2, 3]. Alternatively, the seg-
In this paper, our goal is to determine whether the CNN
mentation and the clustering step can be combined into a single
also offers any useful information about the homogeneity of a
iterative process [4]. In this paper, we investigate the state-of-
speaker in a segment. For this purpose, we propose a reﬁnement
the-art off-line SD system based on the i-vector representation
of accumulation of statistics for i-vector generation and apply it
of the speech segments [3, 5] (other approaches utilize e.g. Hid-
to our SD system [14].
den Markov Models [6, 7]).
The speaker change detection (SCD) is often applied to the
audio signal to obtain segments which ideally contain a speech 2. Speaker Diarization System
of a single speaker [2]. Commonly used approaches to the SCD
include the Bayesian Information Criterion (BIC), Generalized Our SD system [14] is based on the i-vectors [25] that repre-
Likelihood Ratio (GLR), Kullback-Leibler divergence [8, 9], sent speech segments, as introduced in [26]. These segments
Support Vector Machine (SVM) [10] and Deep Neural Net- are obtained from the previous step using SCD based on CNN.
works (DNNs) [11, 12]. However, in a spontaneous telephone The resulting i-vectors are clustered in order to determine which
conversation containing very short speaker turns and frequent parts of the signal were produced by the same speaker. A dia-
overlapping speech, diarization systems often omit the SCD gram of our diarization system can be seen in Figure 1.
Copyright © 2017 ISCA 3562 http://dx.doi.org/10.21437/Interspeech.2017-51The speaker’s supervector ψ [28] for given data O is a con-
catenation of the zeroth and ﬁrst statistical moments of O. Our
proposed reﬁnement of this process of statistics accumulation is
described in Section 3.
Next, we extract the i-vectors from the supervectors. Su-
Figure 1: Diagram of the diarization process.
pervectors have usually a high dimension D = M ∗ (D + 1)
f
that is given by the number of mixtures M in the UBM and
the D dimensionality of the feature vectors o . The i-vectors
2.1. Segmentation f t
are a compact representation of the information encoded in the
For the segmentation step, we use the SCD approach based on supervectors, mostly the information about the identity of the
CNN [14]. The CNN as a regressor is trained supervised on speaker. Factor Analysis (FA) [29] (or extended Joint Factor
spectrograms of the acoustic signal with a reference information Analysis (JFA) [30] to handle more sessions of each speaker) is
L about the existing speaker changes. The value of the function used for dimensionality reduction of the supervector of statis-
L in time t is computed via the formula in Equation 1. We call tics. The generative i-vector model has the form
this labeling a fuzzy labeling. It has a shape of a triangle and
the main idea behind it is to model the uncertainty of human ψ = m0 + T w + (cid:15), w ∼ N (0, I), (cid:15) ∼ N (0, Σ), (5)
labeling.
where T (of size D × D ) is called the total variability space
(cid:18) (cid:19) w
L(t) = max 0, 1 − mini (|t − si|) , (1) matrix, w is the segment’s i-vector of dimension Dw having
τ standard Gaussian distribution, m is the mean vector of ψ,
0
however often approximated by the UBM’s mean supervector,
where s is the time of ith speaker change and τ = 0.6 is the
i and (cid:15) is residual noise with a diagonal covariance matrix Σ with
tolerance which models the level of uncertainty of the man-
covariance matrices C , . . . , C of the UBM ordered on the
1 M
ual labeling. Figure 2 depicts an example of a spectrogram,
diagonal. The i-vectors are also length-normalized [31]. De-
the values of the labeling and the CNN output as a probability
tails about the training of total variability space matrix T can
of speaker change P (a number between zero and one). The
be found in [32, 33].
speaker changes are identiﬁed as peaks in the signal P using
Because of the differences between each conversation (and
non-maximum suppression with a suitable window size. The
the similarity in one conversation), we also compute a conver-
detected peaks are then thresholded to remove insigniﬁcant lo-
sation dependent Principal Component Analysis (PCA) trans-
cal maxima. The signal between two detected speaker changes
formation [26], which further reduces the dimensionality of the
is considered as one segment. The minimum duration of one
i-vector. The beneﬁt of using PCA instead of FA approach is
segment is limited to one second, smaller segments are joined to
the additional information about the importance of each compo-
the adjacent one in order to obtain sufﬁcient information about
nent given by the eigenvalue of the corresponding eigenvector.
the speaker.
The reduced dimension in the PCA latent space can be found
for each conversation separately depending only on the ratio of
2.2. Segment description
eigenvalue mass.
To describe a segment we ﬁrst construct a supervector of accu-
mulated statistics. Supervectors have been used in the process 2.3. Clustering and Resegmentation
of speaker adaptation [27] where they serve as a descriptor of
Given i-vector representations of the extracted segments, we
a new speaker. They contain the zeroth and ﬁrst statistical mo-
perform a clustering into sets of i-vectors describing different
ments of speakers’ data related to a UBM. The UBM is mod-
speakers. This is a coarse clustering on the level of the segmen-
eled as a Gaussian Mixture Model (GMM) from a huge amount
tation given by SCD. To make the ﬁnal diarization more pre-
of speech data form different speakers. The parameters of the
cise we reﬁne it by resegmentation. We compute GMMs over
model are λ = {ω , µ , C }M , where M is the num-
UBM m m m m=1 the feature vectors o , one GMM per speaker cluster. Then the
ber of mixtures in the UBM, ω , µ , C are the weight, mean t
m m m whole conversation is redistributed frame by frame according to
and covariance of the mth mixture, respectively. We consider
the likelihoods of the GMMs.
only diagonal covariance matrices.
Let O = {o }T be the set of T feature vectors o of a
t t=1 t 3. Statistics Reﬁnement
dimension D of one segment of conversation, and
ω N (o ; µ , C ) Because of the uncertainty about the assumption that there is
γm(ot) = (cid:80)M m ω Nt (om; µ m, C ) (2) a speech of only one speaker in a segment, not all data from
m=1 m t m m the segment can contribute to the supervector equally. In a tele-
be the posterior probability of mth mixture given a feature vec- phone conversation, crosstalk is frequent around the place of
tor o . The soft count of the mth mixture (zeroth statistical mo- speaker change and also rapid changes of the speakers are com-
t
ment of feature vectors) is mon.
In Subsection 2.2, all statistics are accumulated into the su-
(cid:88)T pervector with the weight ωm obtained only from the UBM.
n = γ (o ) (3)
m m t This weight ω in Equation (2) informs about the relevance of
m
t=1 the acoustic data to ”the universal speaker”, in other words, how
and the sum of the ﬁrst statistical moments of feature vectors likely it is to be a part of a speech. This weight tells us nothing
with respect to the mth mixture is about the homogeneity of the speaker in the segment. Super-
vector accumulation, originally used in the speaker adaptation
T
(cid:88) task, does not have to consider the homogeneity of the speaker
b = γ (o )o . (4)
m m t t
in data.
t=1
3563Figure 2: The input speech as spectrogram is processed by the CNN into the output function P (a probability of change in time). The
L-function (the reference speaker change) for the CNN training is depicted on top. Note: the output of CNN in time t is only a number.
For this purpose, we are exploring the output of the CNN-
based SCD as a probability of the speaker change in the signal.
Although the audio signal is cut into segments according to the
maxima peaks in the function P (the CNN output), the shape of
the function can also indicate a suspicious part of the segment.
The part of the audio segment in time t with a high probabil-
ity of a speaker change P is less appropriate to represent the
t
speaker than a part with a small probability P . Thus, we use
t
the value of 1 − P as a weighting factor of the signal in the
t Figure 3: Two speech segments with the probability of speaker
accumulation process. The reﬁnement of Equation (2) is repre-
change P , the ﬁrst one with crosstalk on the end of the segment
sented by the formula
and the second one with noise disturbance in the middle of the
segment.
(1 − P )ω N (o ; µ , C )
γ (o ) = t m t m m . (6)
m t (cid:80)M ω N (o ; µ , C )
m=1 m t m m
The equations (3) and (4) stay the same because they both de-
pend on the reﬁned γ (o ) from the Equation (6). The amount
m t
of data for the statistics accumulation stay the same only the
importance of each data is changed.
4. Discussion
The limitation of the segmentation step in the SD system is Figure 4: Short speech segment with the probability of speaker
a minimal length of the segment from which the identity of change P containing two speakers. In this example, the SCD
the speaker can be extracted. In telephone conversations, the system fails and the P weight of statistics does not help to reﬁne
speaker change can occur arbitrarily often in time. In these the accumulation process.
conditions, the segments should be long enough to allow the
extraction of speaker identifying information while limiting the
risk of a speaker change being present within the segment. Still,
The other SCD approaches (e.g. GLR used in [14]) have
only one speaker in the whole segment can not be always gua-
analogical output as the likelihood function of a speaker change.
ranteed. A high probability value of a speaker change from the
But for the purpose of weighting, the information from other
CNN represents the instability of homogeneity of a speaker in
SCD systems is inappropriate because usually the value of the
the segment. This instability leads to the propagation of faulty
change is not in the interval (cid:104)0, 1(cid:105) and the interval is changed
features into the supervector accumulation process. Such faulty
for every conversation.
features usually occur on the boundaries of the segment, where
a high risk of crosstalk is common or anywhere in the segment if
5. Experiments
some disturbance in the acoustic signal is present, see Figure 3.
When using the CNN output for the reﬁnement of the statistics
The experiment was designed to investigate our proposed ap-
accumulation we suppress the effect of these faulty features by
proach to reﬁnement of the accumulation of statistics represent-
weighting them down.
ing the speaker in the segment of conversation.
Nevertheless, there are still known limitations of our pro-
posed approach. In rare situations, when the speaker change
5.1. Corpus
is missed by the SCD as seen in Figure 4, we will only penal-
ize the features corresponding to boundaries and to the missed The experiment was carried out on telephone conversations
speaker change. Thus the segment will be described by features from the English part of CallHome corpus [34]. The original
from two different speakers, resulting into inaccurate i-vector two channels have been mixed into one. Only two speaker con-
representation. versations were selected so that the clustering can be limited to
3564two clusters. This is 109 conversations in total each with about Table 1: DER [%] of the SD systems with the i-vector speaker
10 min duration in a single telephone channel sampled at 8 kHz. representation with constant length window segegmentation
For training of the CNN, only 35 conversations were used, the and SCD based on CNN (with and without reﬁned statistics ac-
rest was used for testing the SD system. cumulation).
5.2. System system DER [%]
Constant length window seg. 9.23
The SD system presented in our papers [14, 35] uses the fea- CNN-SCD without reﬁnement 9.31
ture extraction based on Linear Frequency Cepstral Coefﬁcients CNN-SCD with reﬁnement 7.84
(LFCCs), Hamming window of length 25 ms with 10 ms shift
of the window. There are 25 triangular ﬁlter banks which are
spread linearly across the frequency spectrum, and 20 LFCCs
window segmentation is small because of the resegmentation
are extracted. Delta coefﬁcients were added leading to a 40-
step, which repairs the inaccurate segmentation produced by
dimensional feature vector (D = 40). Instead of the voice
f the constant length window [14]. The effect of resegmentation
activity detector, the reference annotation about missed speech
is strong because there is sufﬁcient amount of data available in
was used.
each conversation for efﬁcient training of GMM. However, our
For segmentation, CNN described in [14] was used. The
proposed approach to reﬁned statistics accumulation using the
input of the net is a spectrogram of speech of length 1.4 sec-
output from the CNN-based SCD brings a more precise infor-
onds and the shift is 0.1 seconds. The CNN consists of three
mation to the speaker description. This improvement can be
convolutional layers with ReLU activation functions. There is
seen on the ﬁnal DER of the system even after resegmentation
a max-pooling layer after each convolutional layer. Batch nor-
step.
malization [36] is used for layer output normalization. There
are two fully connected layers with sigmoid activation func-
6. Conclusions
tion at the end. In the ﬁrst convolutional layer, there are ﬁlters
with rectangular shapes that serve as feature extractors. The Most of the DNN based SD systems introduced in Section 1
two intermediate convolutional layers learn a higher level rep- use DNN to describe a speaker in a relatively short segment
resentation of these features. The output layer consists of just of conversation and then compare two representations of adja-
one neuron with sigmoid activation function. Thus the output is cent segments (e.g. so called d-vectors [12]) to decide if the
limited between zero and one. It represents the probability of a speaker change occurred. On the contrary, our approach using
speaker change in the middle of the observed spectrogram. For the CNN-based SCD ﬁnds the possible speaker changes in spec-
the training of the CNN, we use a Binary Cross Entropy loss togram and additionally uses the information for the reﬁnement
function. It is optimized by Stochastic Gradient Descent with of accumulation process of statistics. These reﬁned statistics
a batch size of 64. The learning rate is changed after a ﬁxed represent the speaker information in the segment better than the
number of iterations by a factor of 0.1. When the loss function classical approach to the statistics accumulation, so the com-
is stabilized we use RMSProp algorithm for ﬁne tuning of the puted i-vector is more precise and the ﬁnal diarization error of
network’s weights. the whole SD system is reduced. Our next goal is to train the
For the purpose of training the i-vector we have used the CNN to represent the probability of the speaker homogeneity
following corpora: NIST SRE 2004, NIST SRE 2005, NIST in the acoustics signal instead of the probability of the speaker
SRE 2006 speaker recognition evaluations [37, 38, 39] and the change. Also, we want to replace the i-vector with a DNN-
Switchboard 1 Release 2 and Switchboard 2 Phase 3 [40, 41]. based vector and use the CNN probability of the speaker change
We model the UBM as a GMM with M = 1024 components. as a prior when constructing this vector.
We have set the dimension of the i-vector to D = 400 and
w
we have used the conversational dependent PCA to reduce the 7. Acknowledgements
dimension further. We use eigenvectors with the ratio of their
eigenvalue mass p = 0.5. We have used K-means clustering The work was supported by the Ministry of Education, Youth
with cosine distance to obtain the speaker clusters. and Sports of the Czech Republic project No. LO1506. Access
to computing and storage facilities (CESNET LM2015042) is
5.3. Results greatly appreciated.
We use the Diarization Error Rate (DER) for the evaluation
8. References
of our approach. It has been described and used by NIST in
the RT evaluations [42]. We use the standard 250 ms tolerance [1] X. A. Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
around the reference boundaries. DER is a combination of sev- and O. Vinyals, “Speaker Diarization: A Review of Recent Re-
eral types of errors (missed speech, mislabeled non-speech, in- search,” Audio, Speech, and Language Processing, vol. 20, no. 2,
pp. 356–370, 2012.
correct speaker cluster). We assume the information about the
silence in all testing audios is available and correct. That means [2] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An Open-source State-of-the-art Toolbox for Broad-
that our results represent only the error of incorrect speaker
cast News Diarization,” in Interspeech, Lyon, 2013, p. 5.
clusters. The results of the examined systems are shown in Ta-
ble 1. For comparison, the result of segmentation using only [3] G. Sell and D. Garcia-Romero, “Speaker Diarization with PLDA
I-vector Scoring and Unsupervised Calibration,” in IEEE Spoken
constant length window is also shown. Using this approach a
Language Technology Workshop, South Lake Tahoe, 2014, pp.
conversation is divided into short segments and the system then
413–417.
relies on the clustering and further resegmentation to reﬁne the
[4] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised
boundaries.
Methods for Speaker Diarization: An Integrated and Iterative
The difference in the results of the system using CNN-SCD Approach,” Audio, Speech, and Language Processing, vol. 21,
without reﬁnement and system using only the constant length no. 10, pp. 2015–2028, 2013.
3565[5] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A [26] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
Study of the Cosine Distance-Based Mean Shift for Telephone J. Glass, “Exploiting Intra-Conversation Variability for Speaker
Speech Diarization,” Audio, Speech and Language Processing, Diarization,” in Interspeech, Florence, 2011, pp. 945–948.
vol. 22, no. 1, pp. 217–227, 2014.
[27] Z. Zaj´ıc, L. Machlica, and L. Mu¨ller, “Robust Adaptation Tech-
[6] C. Fredouille, S. Bozonnet, and N. Evans, “The LIA-EURECOM niques Dealing with Small Amount of Data,” in TSD 2012. Lec-
RT 09 Speaker Diarization System,” in NIST Rich Transcription ture Notes in Computer Science, vol. 7499, Brno, 2012, pp. 418–
Workshop (RT09), Melbourne, USA, 2009. 487.
[7] O. Ben-Harush, O. Ben-Harush, I. Lapidot, and H. Guterman, [28] ——, “Robust Statistic Estimates for Adaptation in the Task of
“Initialization of Iterative-Based Speaker Diarization Systems for Speech Recognition,” in TSD 2010. Lecture Notes in Computer
Telephone Conversations,” IEEE Transactions on Audio, Speech, Science, vol. 6231. Brno: Springer, Berlin, Heidelberg, 2010,
and Language Processing, vol. 20, no. 2, pp. 414–425, 2012. pp. 464–471.
[8] A. G. Adami, S. S. Kajarekar, and H. Hermansky, “A New [29] P. Kenny and P. Dumouchel, “Experiments in Speaker Veriﬁcation
Speaker Change Detection Method for Two-Speaker Segmenta- Using Factor Analysis Likelihood Ratios,” in Odyssey - Speaker
tion,” in ICASSP, vol. 4, 2002, pp. 3908–3911. and Language Recognition Workshop, Toledo, 2004, pp. 219–226.
[9] J. Ajmera, I. McCowan, and H. Bourlard, “Robust Speaker [30] P. Kenny, “Joint Factor Analysis of Speaker and Session Variabil-
Change Detection,” Signal Processing Letters, IEEE, vol. 11, pp. ity: Theory and Algorithms,” Tech. Rep., 2006.
649–651, 2004.
[31] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of I-vector
[10] B. Fergani, M. Davy, and A. Houacine, “Speaker Diarization Us- Length Normalization in Speaker Recognition Systems,” in Inter-
ing One-Class Support Vector Machines,” Speech Communica- speech, Florence, 2011, pp. 249–252.
tion, vol. 50, no. 5, pp. 355–365, 2008.
[32] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel,
[11] V. Gupta, “Speaker Change Point Detection Using Deep Neural “A Study of Interspeaker Variability in Speaker Veriﬁcation,”
Nets,” in ICASSP, Brisbane, 2015, pp. 4420–4424. IEEE Transactions on Audio, Speech, and Language Processing,
vol. 16, no. 5, pp. 980–988, 2008.
[12] R. Wang, M. Gu, L. Li, M. Xu, and T. F. Zheng, “Speaker Seg-
mentation Using Deep Speaker Vectors for Fast Speaker Change [33] L. Machlica and Z. Zaj´ıc, “Factor Analysis and Nuisance Attribute
Scenarios,” in ICASSP, New Orleans, 2017, pp. 5420–5424. Projection Revisited,” in Interspeech, Portland, 2012, pp. 1570–
1573.
[13] S. Furui and D. Itoh, “Neural-Network-Based HMM Adaptation
for Noisy Speech,” in ICASSP, Salt Lake City, 2001, pp. 365–368. [34] A. Canavan, D. Graff, and G. Zipperlen, “CALLHOME American
English Speech, LDC97S42,” in LDC Catalog. Philadelphia:
[14] M. Hru´z and Z. Zaj´ıc, “Convolutional Neural Network for
Linguistic Data Consortium, 1997.
Speaker Change Detection in Telephone Speaker Diarization Sys-
tem,” in ICASSP, New Orleans, 2017, pp. 4945–4949. [35] Z. Zaj´ıc, M. Kunesˇova´, and V. Radova´, “Investigation of Seg-
mentation in i-Vector Based Speaker Diarization of Telephone
[15] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep Clus-
Speech,” in Specom. Budapest: Springer International Publish-
tering: Discriminative Embeddings for Segmentation and Separa-
ing, 2016, pp. 411–418.
tion,” in ICASSP, Shanghai, 2016, pp. 31–35.
[36] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating
[16] R. Milner and T. Hain, “DNN-Based Speaker Clustering for
Deep Network Training by Reducing Internal Covariate Shift,”
Speaker Diarisation,” in Interspeech, vol. 08-12-Sept, San Fran-
Arxiv, vol. abs/1502.0, 2015.
cisco, 2016, pp. 2185–2189.
[37] A. Martin and M. Przybocki, “2004 NIST Speaker Recognition
[17] G. Sell, D. Garcia-Romero, and A. Mccree, “Speaker Diariza-
Evaluation, LDC2006S44,” in LDC Catalog. Philadelphia: Lin-
tion with I-Vectors from DNN Senone Posteriors,” in Interspeech,
guistic Data Consortium, 2011.
Dresden, 2015, pp. 3096–3099.
[38] NIST Multimodal Information Group, “2005 NIST Speaker
[18] S. H. Yells, A. Stolcke, and M. Slaney, “Artiﬁcial Neural Net-
Recognition Evaluation Training Data, LDC2011S01,” in LDC
work Features for Speaker Diarization,” in Proc. IEEE Spoken
Catalog. Philadelphia: Linguistic Data Consortium, 2011.
Language Technology Workshop. IEEE, 2014, pp. 402–406.
[39] ——, “2006 NIST Speaker Recognition Evaluation Training Set,
[19] N. Dawalatabad, S. Madikeri, C. C. Sekhar, and H. A. Murthy,
LDC2011S09,” in LDC Catalog, 2011.
“Two-Pass IB Based Speaker Diarization System Using Meeting-
Speciﬁc ANN Based Features,” in Interspeech, San Francisco, [40] D. Graff, D. Miller, and K. Walker, “Switchboard-2 Phase III Au-
2016, pp. 2199–2203. dio,” in LDC Catalog. Philadelphia: Linguistic Data Consortium,
1999.
[20] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
“Speaker Diarization Using Deep Neural Network Embedings,” [41] D. Graff, K. Walker, and A. Canavan, “Switchboard-2 Phase II,
in ICASSP, New Orleans, 2017, pp. 4930 – 4934. LDC99S79,” in LDC Catalog. Philadelphia: Linguistic Data
Consortium, 2002.
[21] H. Bredin, “TristouNet: Triplet Loss for Speaker Turn Embed-
ding,” in ICASSP, New Orleans, 2017, pp. 5430–5434. [42] J. G. Fiscus, N. Radde, J. S. Garofolo, A. Le, J. Ajot, and
C. Laprun, “The Rich Transcription 2006 Spring Meeting Recog-
[22] M. Hru´z and M. Kunesˇova´, “Convolutional Neural Network in
nition Evaluation,” Machine Learning for Multimodal Interaction,
the Task of Speaker Change Detection,” in Specom. Budapest:
vol. 4299, pp. 309–322, 2006.
Springer International Publishing, 2016, pp. 191–198.
[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, 1989.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classi-
ﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems, 2012, pp. 1106–1114.
[25] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
let, “Front-End Factor Analysis for Speaker Veriﬁcation,” IEEE
Transactions on Audio, Speech, and Language Processing,
vol. 19, no. 4, pp. 788–798, 2011.
3566