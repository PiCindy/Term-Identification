INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.INTEGRATING END-TO-END NEURAL AND CLUSTERING-BASED DIARIZATION:
GETTING THE BEST OF BOTH WORLDS
Keisuke Kinoshita, Marc Delcroix, Naohiro Tawara
NTT Corporation, Japan
ABSTRACT that they cannot handle overlapped speech, i.e., time segments where
0 more than one person is speaking, because of the way of extracting
2 Recent diarization technologies can be categorized into two ap- speaker embeddings. Perhaps surprisingly, even in professional
0 proaches, i.e., clustering and end-to-end neural approaches, which
meetings, the percentage of overlapped speech is in the order of 5 to
2 have different pros and cons. The clustering-based approaches
10%, while in informal get-togethers it can easily exceed 20% [10].
  assign speaker labels to speech regions by clustering speaker em-
t End-to-End Neural Diarization (EEND) has been recently de-
c beddings such as x-vectors. While it can be seen as a current state-
veloped [11–13] to address the overlapped speech problem. Simi-
O of-the-art approach that works for various challenging data with
larly to the neural source separation algorithms [14, 15], in EEND, a
6  rtheaastoitnacbanlenorot bhuasntdnleessovaenrdlapacpceudraspcye,eciht hthaast aiscirniteivciatlabdliesaidnvnaanttuargael Neural Network (NN) receives standard frame-level spectral features
2 and directly outputs a frame-level speaker activity for each speaker,
conversational data. In contrast, the end-to-end neural diarization
  no matter whether the input signal contains overlapped speech or
  (EEND), which directly predicts diarization labels using a neural
] not. While the system is simple and has started outperforming the
S network, was devised to handle the overlapped speech. While the conventional clustering-based algorithms [12, 13], it is difﬁcult to
EEND, which can easily incorporate emerging deep-learning tech-
A directly apply the EEND systems to long recordings (e.g., record-
nologies, has started outperforming the x-vector clustering approach
s. in some realistic database, it is difﬁcult to make it work for long ings longer than 10 minutes). The system is designed to operate in
a batch processing mode and thus requires a very large computer
s recordings (e.g., recordings longer than 10 minutes) because of, e.g.,
e memory when performing inference with long recordings. Besides,
its huge memory consumption. Block-wise independent process-
e aside from the memory issue, the NNs in EEND has difﬁculty to
ing is also difﬁcult because it poses an inter-block label permutation
[ generalize to unseen very long sequential data, which also ham-
  problem, i.e., an ambiguity of the speaker label assignments between
  pers its application to the long recordings. Note that, if we segment
1 blocks. In this paper, we propose a simple but effective hybrid di-
the long recordings into small chunks and apply the original EEND
v arization framework that works with overlapped speech and for long
model to each chunk independently, the model inevitably suffers
6 recordings containing an arbitrary number of speakers. It modiﬁes
from the inter-block label permutation problem, i.e., an ambiguity
6 the conventional EEND framework to output global speaker embed-
of the speaker label assignments between chunks. To address this
3 dings so that speaker clustering can be performed across blocks to
3 solve the permutation problem. With experiments based on simu- problem (and simultaneously seek for a low-latency solution), [16]
1 lated noisy reverberant 2-speaker meeting-like data, we show that proposed an NN-based extension of the EEND to block-online pro-
0. the proposed framework works signiﬁcantly better than the original cessing. The method in [16] ﬁrst tries to ﬁnd single speaker regions,
and use them as a guide to assign the speaker labels to the diariza-
1 EEND especially when the input data is long.
tion results of future blocks. However, their performance typically
0
Index Terms— Speaker diarization, neural networks, does not reach that of the original EEND. Also, more importantly,
2
the method cannot handle an arbitrary number of speakers.
:
v 1. INTRODUCTION In this paper, we propose a simple but effective hybrid diariza-
i tion approach, called EEND-vector clustering, by combining the
X
Automatic meeting/conversation analysis is one of the essential tech- best of the clustering-based diarization and the EEND. A central
r nologies required for realizing futuristic speech applications such as component of the proposed approach is a modiﬁed EEND network
a
communication agents that can follow, respond to, and facilitate our that outputs, in each chunk, not only the diarization results but also
conversation. As an important central task for the meeting analysis, global speaker embeddings associated with the diarization results.
speaker diarization has been extensively studied [1–3]. The inter-block permutation ambiguity problem can thus be sim-
Current state-of-the-art diarization systems that achieve reli- ply solved by clustering the block-level speaker embedding vectors.
able performance in many challenges [1, 2] is based on clustering This extension thus naturally allows us to combine the advantages of
of speaker embeddings (i.e., speaker identity features) such as i- both clustering and the EEND based methods, i.e. it can work with
vectors [4] and x-vectors [5]. Such clustering-based approaches overlapped speech and deal with long recordings including an arbi-
ﬁrst segment a recording into short homogeneous blocks and com- trary number of speakers. In particular, we conﬁrm experimentally
pute speaker embeddings for each block assuming that only one that the proposed EEND-vector clustering signiﬁcantly outperforms
speaker is active in each block. Then, speaker embedding vectors the original EEND system especially when the recordings are long,
are clustered to regroup segments belonging to the same speakers e.g., more than 5 minutes, while maintaining the same performance
and obtain the diarization results. Various speaker embeddings and as the original EEND system when the recording is short.
clustering techniques have been explored in [6–9]. While these The remainder of this paper is organized as follows. We ﬁrst in-
methods can cope with very challenging scenarios [6, 7] and work troduce the proposed framework in section 2 in detail. Then, in sec-
with an arbitrary number of speakers, there is a clear disadvantage tion 3, we evaluate its performance in comparison with the originalLinearD (s = 1, 2), where s is the speaker index within a chunk.
s
Since it is not always guaranteed that the diarization results of a cer-
tain speaker are estimated at the same output node, we may have the
inter-block label permutation problem in the diarization outputs. As
an example, in Fig. 1, the network LinearD estimates the diarization
1
result of ‘speaker A’ in the ﬁrst chunk, and that of ‘speaker B’ in the
second chunk. This means that we cannot obtain an optimal diariza-
tion result simply by stitching the diarization results of a speciﬁc
output node across all the chunks.
To solve this permutation problem, we simultaneously estimate
a speaker embedding corresponding to each diarization result in each
chunk. The network to estimate the speaker embeddings are denoted
as LinearS (s = 1, 2) in Fig. 1. The speaker embedding extraction
s
network is optimized through the NN training such that the vectors
of the same speaker stay close to each other, while the vectors of
different speakers lie far away from each other. This can be seen in
the ﬁgure by examining how the embeddings are organized in the
speaker embedding space. Therefore, after obtaining diarization re-
sults for all chunks, by clustering the speaker embeddings given the
total number of speakers in the input recording (3 in this case), we
can estimate the correct association of the diarization results among
chunks. Then, ﬁnally, the overall diarization results are obtained by
stitching them together based on the embedding clustering result.
Note that while the proposed framework estimates the diarization
results of the ﬁxed number of speakers in a chunk, it can handle a
Fig. 1: Schematic diagram of the proposed diarization framework.
meeting with an arbitrary number of speakers.
The input contains 3 speakers in total (red, green, and blue speakers
For the clustering, we can use any clustering algorithms. How-
shown in the waveform in the bottom), but only at most 2 speakers
ever, it may be preferable if the clustering algorithm is aware of the
are actively speaking in each chunk.
characteristic of this framework and work with a constraint that the
speaker embeddings from a chunk should not belong to the same
speaker cluster. In this paper, to incorporate the constraint into the
EEND to clarify the advantages of the proposed framework. Finally,
clustering stage, we use a constrained k-means clustering algorithm
we conclude the paper in section 4.
called COP-k-means [18], which allows us to set cannot-link con-
straints between a given pair of embeddings to prevent the pair from
2. PROPOSED DIARIZATION FRAMEWORK: being assigned to the same speaker cluster.
EEND-VECTOR CLUSTERING
2.2. Neural diarization with speaker embedding estimation
2.1. Overall framework
This subsection details the NN model in EEND-vector clustering to
Figure 1 shows a schematic diagram of the proposed EEND-vector estimate the diarization results and the speaker embeddings.
clustering framework.
Let us denote the ground-truth diarization label sequence as
It ﬁrst segments the input recording into chunks and calculates Y = (y | t = 1, · · · , T ) that corresponds to X . Here, the
i t,i i
a sequence of the input frame features within each chunk, as Xi = diarization label y = [y ∈ {0, 1} | s = 1, · · · , S ] rep-
t,i t,i,s Local
(xt,i | t = 1, · · · , T ) where i,t and T are the chunk index, the resents a joint activity for S speakers. For example, y =
frame index in the chunk and the chunk size1. xt,i ∈ RK is the K- yt,i,s(cid:48) = 1(s (cid:54)= s(cid:48)) indicateLsocbaloth speakers s and s(cid:48) spoket,ia,ts the
dimensional input frame feature at the time frame t. In the example time frame t in the chunk i.
shown in Fig 1, the input recording consists of 2 chunks and contains
In the EEND framework, the diarization task is formulated as
3 speakers in total. In the following, we assume that we can ﬁx
a multi-label classiﬁcation problem. Speciﬁcally, we estimate the
the maximum number of active speakers in a chunk, S , to 2,
Local dirarization result of the s-th speaker at each time frame, yˆ , as,
t,i,s
although the method could be generalized to more speakers or an
unknown number of speakers [13] 2. (cid:2)h , . . . , h (cid:3) = Encoder(X ) ∈ RD×T ,
Based on the hyper-parameter S = 2, the network estimates 1,i T,i i
Local
diarization results for 2 speakers in each chunk. In Fig. 1, the pro- yˆt,i,s = sigmoid(LinearDs (ht,i)) ∈ (0, 1)
cessing for the 1st speaker is drawn with black lines and put in the (s = 1, . . . , S ), (1)
Local
foreground, while that of the 2nd speaker is drawn with grey lines
and put in the background. The diarization results are estimated in- where Encoder(·) is an encoder such as a multi-head self-attention
dependently in each chunk through NNs denoted as Encoder and NN [12], which utilizes all the input features X for inference. h
i t,i
is a D-dimensional internal representation in the NN, LinearD(·) :
1The chunk size T for estimating speaker embeddings can be advanta- RD → R1 is a fully-connected layer to estimate the diarizatiosn re-
geously much longer than the homogeneous blocks used in x-vector cluster-
sult, and sigmoid(·) is the element-wise sigmoid function.
ing since we can handle heterogeneous chunks including more than 1 speaker.
2If we select the chunk size carefully, it is not too difﬁcult to set an ap- Now, after estimating the diarization results, for the purpose
propriate maximum number of speakers even for practical use cases [17]. of solving the inter-block permutation problem, we estimate thespeaker embedding, eˆ , corresponding to the diarization result of 2.3.2. Speaker embedding loss
i,s
the s-th speaker as follows.
For the speaker embedding training, we use a loss function that en-
zt,i,s = LinearSs(ht,i) ∈ RC, courages the embeddings to have small intra-speaker and large inter-
T speaker distances. Speciﬁcally, we utilize the loss proposed recently
z¯ = (cid:88) yˆ z , ∈ RC (2) in [19] , which was shown to be very effective for the speech separa-
i,s t,i,s t,i,s
t=1 tion task. For this loss function, we assume that the training data is
z¯ annotated with speaker identity labels, i.e., indices, based on a ﬁnite
eˆ = i,s ∈ RC (s = 1, . . . , S ), (3)
i,s (cid:107)z¯ (cid:107) Local set of M training speakers. Note, however, that the speaker identity
i,s
is not required at test time, and that training and test speakers can
wRhDer→e CRiCs itshea dfuimllyen-csoionnneocftetdhelasypeeratkoeresetmimbaetdedtihneg,sL-thinsepaeraSsk(e·)r’s: dbiefftehre(ia.bes.,oolupteenssppeeaakkeerricdoenndtiittyioinnsd)i.ceLsetthσai(cid:63)t c=or(cid:2)reσsi(cid:63)p,1o,n.d. .to, σtih(cid:63),eSLpocealr(cid:3)-
embedding ei,s, and (cid:107)·(cid:107) is a vector norm. Here we chose to estimate mutation of the labels that gives minimum value to Eq. (5), i.e., φ(cid:63).
the speaker embeddings as weighted sum of frame-level embeddings σ(cid:63) is a subset of the M speaker identity indices. Then, the speaker
i
zt,i,s with weights determined by the diarization results yˆt,i,s, as in embedding loss for chunk i, Lspeaker,i, is formulated as follows.
Eq. (2). With these operations, we can estimate diarization results
aonudt tshpeeaspkeearkeemr beemdbdeindgdsinfgoresatlilmSaLtoocralisspeesaseknetrisa.llyThthise msaomdeelaws itthhe- L = 1 S(cid:88)Local l (cid:0)σ(cid:63) , eˆ (cid:1) , (6)
conventional EEND [11]. speaker,i SLocal speaker i,s i,s
s=1
2.3. Training objectives where
Now, we will explain a way to train the model to realize the behav-  (cid:16) (cid:16) (cid:17)(cid:17) 
idoirareizxaptliaoinnerdesiunltSs eacntdiosnpe2a.1k.erSeimncbeedthdeinngestwsiomruklteasntiemouastelys,booutrhntahte- lspeaker (cid:0)σi(cid:63),s, eˆi,s(cid:1) = − ln  (cid:80)eMmxp=1 e−xdp (−Eσdi(cid:63)(,sE,meˆi,,seˆi,s))  , (7)
ural choice is to use the following multi-task loss.
d (E , eˆ ) = α(cid:107)E − eˆ (cid:107)2 + β, (8)
m i,s m i,s
L = (1 − λ)L + λL , (4)
diarization speaker
where E is a learnable global speaker embedding dictionary, and E
where L is the total loss function to be minimized, L is the m
diarization is a learnable global speaker embedding associated with the m-th
diarization error loss, L is speaker embedding loss, and λ is a
speaker training speaker. Eq. (8) is the squared Euclidean distance between
hyper-parameter to weight the two loss functions.
the learnable global speaker embedding and the estimated speaker
embedding, which is rescaled with learnable scalar parameters α >
2.3.1. Diarization loss
0 and β. Eq. (7) is the log softmax over the distances between the es-
Following [11], the diariation loss in each chunk is formulated as: timated embedding and the global embeddings, which can be derived
from the categorical cross-entropy loss. The loss function L is
speaker
L , φ(cid:63) = 1 min (cid:88)T BCE (cid:16)lφ , yˆ (cid:17) ,(5) formed by collecting B chunks, similarly to Ldiarization.
diarization,i T SLocal φ∈perm(SLocal) t=1 t,i t,i arizaBtiyonmriensiumltisziancgcuthreasteelyloesvsefnunifcttihoenrse, iws eoveexrplaepctpetod espsteiemcaht,eadnid-
where perm(S ) is the set of all the possible permutations of simultaneously estimate speaker embeddings that are suitable for the
Local
(1, . . . , SLocal), yˆt,i = [yˆt,i,1, . . . , yˆt,i,SLocal] ∈ RSLocal, lφt,i is the φ- subsequent clustering process.
th permutation of the reference speaker labels, and BCE(·, ·) is the
binary cross-entropy function between the labels and the estimated
diarization outputs. φ(cid:63) is the permutation that minimizes the right 3. EXPERIMENTS
hand side of the Eq. (5). This training scheme called permutation-
In this section, we evaluate the effectiveness of the proposed method
invariant training has shown to be effective for the neural diarization
in comparison with the conventional EEND [12], based on test data
[11], but at the same time, it incurs another problem, i.e., the inter-
including long recordings with a signiﬁcant amount of overlapped
block label permutation problem since it clearly allows the speaker
speech. Comparison with the x-vector clustering is omitted since it
labels to permute from chunk to chunk. The diarization loss func-
was already shown in [12] that the conventional EEND works better
tion L is formed by collecting B chunks, i.e., L =
diarization diarization
(cid:80)B L , where B is the size of the mini-batch. in case the data contains overlapped speech.
i=1 diarization,i
Here, as it was mentioned earlier, S is a hyper-parameter that
Local
has to be appropriately chosen to satisfy (1) SLocal ≤ Stotal where 3.1. Data
S is the total number of speakers in the recording, and (2) S
total Local
is always greater than or equal to the maximum number of speakers The training, development, and test data are based on the 16 kHz
speaking in a chunk. With an assumption that S is chosen in such Librispeech database [20]. To simulate a conversation-like mixture
Local
a way, the diarization labels in the chunk i, Y , should be formed as of two speakers, we picked up utterances from randomly selected
i
a subset of all S speaker’s labels Ytotal, i.e., Y ⊆ Ytotal. The two speakers, and generated a noisy reverberant mixture contain-
total i i i
subset should be chosen appropriately for each chunk such that it ing many utterances per speaker with reasonable silence intervals
covers all speakers speaking in the chunk i. If the number of speak- between utterances. For the simulation, we used the algorithm pro-
ers speaking in the chunk is smaller than S , we ﬁll Y with di- posed in [11], and set the average silence interval between utterances
Local i
arization label(s) of a virtual (S + 1)-th always-silent speaker, at 2 seconds. Noise data was obtained from MUSAN noise data [21].
total
i.e., (y ∈ {0} | t = 1, . . . , T ). The signal-to-noise ratio was sampled randomly for each mixture
t,i,Stotal+1Table 1: DERs (%) of the conventional EEND and the proposed
models for each test set that differs in the duration.
Model Chunking Clustering Test data duration (minutes)
3 5 10 20
1. EEND - N/A 7.9 8.8 9.2 N/A
2. EEND (cid:88) N/A 9.9 9.9 10.2 9.9
3. Proposed - - 8.0 8.7 9.1 N/A
4. Proposed (cid:88) - 10.6 10.5 10.9 10.8
5. Proposed (cid:88) (cid:88) 9.1 8.2 7.9 7.7
Table 2: DERs (%) of the conventional EEND and the proposed
EEND-vector clustering for each overlap condition.
Model Chunking Clustering Overlap ratio (%)
0 - 30 30 - 60 60 - 90
Fig. 2: t-SNE plot of the test speaker’s embeddings vector
EEND - - 10.5 9.4 7.1
Proposed (cid:88) (cid:88) 5.4 8.3 6.6
from 5, 10, 15, and 20 dBs. For reverberation, we used 20000 im- 3.3. Results
pulse response data in [22], which simulates various rooms. Con-
sequently, we obtained a set of training, development, and test data Table 1 shows the results of the conventional EEND (1st row) and
that contains various overlapping ratios ranging from 10 to 90 %. the proposed method (5th row). The table contains some variants of
For the training and development data, we randomly selected these methods to clarify the effectiveness of each component in the
utterances from 460-hour clean speech training data containing 1172 proposed model.
speakers (M =1172) and generated 40000 and 500 mixtures that
amount to 2774 and 23 hours, respectively. For the test data, we First, by comparing the 1st row (conventional EEND applied to
generated 4 different sets of data that differ in duration. Each test set the entire sequence without chunking) and 5th row (the proposed
contains 500 utterances. The average duration of mixtures in each model that processes chunks and performs clustering, i.e., EEND-
set is 3, 5, 10, and 20 minutes, respectively. All the test data were vector clustering), we can see that, as the duration of the test data
generated based on the Librispeech test set containing 26 speakers gets longer, the proposed method becomes increasingly advanta-
that were not included in the training and development data. geous. While the conventional EEND cannot well handle 10- and
20-minute data because of poor generalization to the long data and
the CPU memory constraint, EEND-vector clustering can achieve
3.2. NN training and hyper-parameters
stable diarization performance for such data. Interestingly, it tends
For the input frame feature, we extracted 23-dimensional log-Mel- to work better (at least for this data) especially when the duration of
ﬁlterbank features with 25 ms frame length and 10 ms frame shift. the data is long. It is probably because the number of embeddings
For both the proposed method and the conventional EEND, the available for the clustering becomes larger as the data gets longer,
chunk size T at the training stage was set at 500 (= 50 seconds) as in which helps the clustering algorithm ﬁnd better cluster centroids.
[12]. Therefore, when the training data is longer than 50 seconds, we
Now, let us compare the 1st row (EEND without chunking) and
split the input audio into non-overlapping 50-second chunks. At the
3rd row (the proposed model applied to the entire sequence without
inference stage, the conventional EEND uses an entire sequence for
chunking). The performance of the proposed model turned out to be
inference without chunking. On the other hand, the proposed method
almost equal to that of the conventional method in all cases, which
segments the input data into 50-second non-overlapping chunks, and
indicates that the additional speaker loss did not negatively affect the
perform diarization as explained in Section 2.1.
diarization capability of the model. The results show that the addi-
For both methods, we used the same network architecture as
tional speaker loss did not negatively affect the diarization capability
[12]. For Encoder, we used two multi-head attention blocks with
of the model.
256 attention units containing four heads (D = 256).We used the
Adam optimizer with the learning rate scheduler introduced in [23].
Next, let us focus on the comparison between 1st/3rd rows
The number of warm-up steps used in the learning rate scheduler was
(models without chunking) and 2nd/4th rows (models with chunk-
25000. The batch size B was 64. The number of training epochs
ing but without clustering). The performance degradation when
was 70. The ﬁnal models were obtained by averaging the model
using chunking reveals the inter-block label permutation problem.
parameters of the last 10 epochs.
We assume this problem may become even more severe when deal-
For the proposed method, λ was set at 0.01. With an assumption
ing with more speakers. With this comparison, we could conﬁrm the
that the maximum number of speakers speaking in each chunk is 2,
effectiveness of the clustering-based diarization result stitching.
we set S at 2. The dimension of the speaker embedding, C, was
Local
set at 256. Since the performance of the proposed method slightly Overall, we found that, if the test data is shorter than 5 minutes,
changes due to the initialization of the COP-k-means algorithm, we we can apply either the conventional EEND or the proposed model to
ran the test inference 10 times with random initialization and ob- the entire sequence (without chunking) to obtain a good diarization
tained the averaged results. The standard deviation of the obtained performance. On the other hand, if the data is longer than that, it is
diarization error rate (DER) was less than 0.2%. signiﬁcantly better to use the proposed framework.3.4. Detailed analysis DIHARD challenge,” in Proc. Interspeech 2018, 2018, pp.
2808–2812.
3.4.1. Evaluation in terms of overlapping ratio
[7] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, K. Zmo-
Table 2 shows the DERs in each overlap condition. The results were
likova, O. Novotny´, K. Vesely´, O. Glembek, O. Plchot,
obtained from the test set of 10-minute mixtures. Since each mixture
L. Mosˇner, and P. Mateˇjka, “BUT system for DIHARD speech
in the test set differs in the amount of overlapped speech, i.e., overlap
diarization challenge 2018,” in Proc. Interspeech 2018, 2018,
ratio, we categorized the mixtures into several overlap ratio ranges
pp. 2798–2802.
and obtained DER in each condition. , to better understand the model
behavior. The proposed method is shown to largely outperform the [8] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully
conventional EEND in all conditions. supervised speaker diarization,” in Proc. 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 6301–6305.
3.4.2. Speaker embedding estimation accuracy
[9] X. Li, Y. Zhao, C. Luo, and W. Zeng, “Online speaker diariza-
Here we also examine whether the speaker embeddings of the test tion with relation network,” 2020, arXiv:2009.08162.
data is estimated accurately such that they have large inter-speaker
[10] T. von Neumann and S. Araki T. Nakatani R. Haeb-Umbach
and small intra-speaker distances. Figure 2 shows the t-SNE visual-
K. Kinoshita, M. Delcroix, “All-neural online source separa-
ization of the speaker embeddings of the 26 test speakers. It clearly
tion, counting, and diarization for meeting analysis,” in Proc.
shows distinguished clusters for each speaker, which proves that we
2018 IEEE International Conference on Acoustics, Speech and
can estimate the global speaker embeddings accurately even if the
Signal Processing (ICASSP), May 2019, pp. 91–95.
input data contains a signiﬁcant amount of overlapped speech.
[11] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and
S. Watanabe, “End-to-end neural speaker diarization with
4. CONCLUSIONS
permutation-free objectives,” in Proc. Interspeech 2019, 2019,
We proposed a simple but effective diarization framework, EEND- pp. 4300–4304.
vector clustering, that estimates both diarization results and speaker
[12] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and
embeddings. By utilizing the speaker embeddings, we solved the
S. Watanabe, “End-to-end neural speaker diarization with self-
inter-block label permutation problem. Experimental results showed
attention,” in Proc. IEEE ASRU, 2019, pp. 296–303.
that EEND-vector clustering works signiﬁcantly better than the orig-
inal EEND especially when the input data is long. Future work in- [13] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Naga-
cludes application of the proposed framework to more challenging matsu, “End-to-end speaker diarization for an unknown num-
conditions as well as an extension to a scheme that can handle an ber of speakers with encoder-decoder based attractors,” 2020,
arbitrary number of speakers within a chunk, e.g., [13]. arXiv:2005.09921.
[14] M. Kolbæk, D. Yu, Z. Tan, and J. Jensen, “Multitalker speech
5. REFERENCES separation with utterance-level permutation invariant training
of deep recurrent neural networks,” IEEE/ACM Transactions
[1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- on Audio, Speech, and Language Processing, vol. 25, no. 10,
land, and O. Vinyals, “Speaker diarization: A review of recent pp. 1901–1913, Oct 2017.
research,” IEEE Transactions on Audio, Speech, and Language [15] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Lis-
Processing, vol. 20, no. 2, pp. 356–370, Feb 2012. tening to each speaker one by one with recurrent selective hear-
[2] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, ing networks,” in Proc. 2018 IEEE International Conference
and M. Liberman, First DIHARD Challenge Evaluation Plan, on Acoustics, Speech and Signal Processing (ICASSP), April
2018, https://zenodo.org/record/1199638. 2018, pp. 5064–5068.
[3] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, [16] Y. Xue, S. Horiguchi, Y. Fujita, S. Watanabe, and K. Naga-
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, matsu, “Online end-to-end neural diarization with speaker-
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, tracing buffer,” 2020, arXiv:2006.02616.
D. Reidsma, , and P. Wellner, “The AMI meeting corpus: [17] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and
A pre-announcement,” in The Second International Confer- D. Dimitriadis, “Low-latency speaker-independent continuous
ence on Machine Learning for Multimodal Interaction, ser. speech separation,” in Proc. 2019 IEEE International Confer-
MLMI’05, 2006, pp. 28–39. ence on Acoustics, Speech and Signal Processing (ICASSP),
[4] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, , and P. Ouel- May 2019, pp. 6980–6984.
let, “Front-end factor analysis for speaker veriﬁcation,” IEEE [18] K. Wagstaff, C. Cardie, S. Rogers, and S S. Schroedl, “Con-
Trans. Audio, Speech, and Language Processing, vol. 19(4), strained k-means clustering with background knowledge,” in
pp. 788–798, 2011. Proc. 18th International Conference on Machine Learning
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, (ICML), 2001.
Y. Carmiel, , and S. Khudanpur, “Deep neural network-based [19] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech
speaker embeddings for end-to-end speaker veriﬁcation,” in separation by speaker clustering,” 2020, arXiv:2002.08933.
Proc. IEEE Spoken Language Technology Workshop, 2016.
[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
[6] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, rispeech: An asr corpus based on public domain audio books,”
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- in Proc. 2015 IEEE International Conference on Acoustics,
abe, and S. Khudanpur, “Diarization is hard: Some experi- Speech and Signal Processing (ICASSP), 2015, pp. 5206–
ences and lessons learned for the JHU team in the inaugural 5210.[21] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music,
speech, and noise corpus,,” 2015, arXiv:1510.08484.
[22] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
March 2017, pp. 5220––5224.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. The Thirty-ﬁrst Annual Conference on Neu-
ral Information Processing Systems (NIPS), 2017, pp. 5998–
–6008.