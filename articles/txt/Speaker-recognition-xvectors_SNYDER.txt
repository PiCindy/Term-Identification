SPEAKERRECOGNITIONFORMULTI-SPEAKERCONVERSATIONSUSINGX-VECTORS
DavidSnyder,DanielGarcia-Romero,GregorySell,AlanMcCree,DanielPovey,SanjeevKhudanpur
CenterforLanguageandSpeechProcessing&HumanLanguageTechnologyCenterofExcellence
TheJohnsHopkinsUniversity,Baltimore,MD21218,USA
ABSTRACT Early work using discriminatively trained neural networks to
capture speaker characteristics focused on extracting frame-level
Recently, deep neural networks that map utterances to ﬁxed-
features to be used as input to Gaussian speaker models [6, 7].
dimensional embeddings have emerged as the state-of-the-art in
Heigold et al., introduced an end-to-end system, trained on the
speaker recognition. Our prior work introduced x-vectors, an em-
phrase“OKGoogle,”thatjointlylearnsanembeddingalongwitha
bedding that is very effective for both speaker recognition and
similaritymetrictocomparepairsofembeddings[8]. Snyderetal.,
diarization. Thispapercombinesourpreviousworkandappliesit
generalizedthisframeworktotext-independentspeakerrecognition
to the problem of speaker recognition on multi-speaker conversa-
and inserted a temporal pooling layer into the network to handle
tions. WemeasureperformanceonSpeakersintheWildandreport
variable-lengthsegments[9]. Theworkin[1,10]splittheend-to-
what we believe are the best published error rates on this dataset.
endapproachintotwoparts: aDNNtoproduceembeddingscalled
Moreover, we ﬁnd that diarization substantially reduces error rate
x-vectors,andaseparatelytrainedclassiﬁertocomparethem. This
whentherearemultiplespeakers, whilemaintainingexcellentper-
facilitatesuseofalltheaccumulatedbackendtechnologydeveloped
formance on single-speaker recordings. Finally, we introduce an
overtheyearsfori-vectors,suchaslength-normalizationandPLDA
easilyimplementedmethodtoremovethedomain-sensitivethresh-
scoring.Thex-vectorframeworkisdescribedinSection3.
old typically used in the clustering stage of a diarization system.
Theproposedmethodismorerobusttodomainshifts,andachieves
similarresultstothoseobtainedusingawell-tunedthreshold.
2.2. Speakerdiarization
Index Terms— speaker recognition, speaker diarization, deep
neuralnetworks,x-vectors Soon after their development for speaker recognition, Shum et al.,
adapted i-vectors to the task of speaker diarization [11, 12]. Mir-
roringprogressinspeakerrecognition,recentsystemshavereplaced
1. INTRODUCTION
i-vectorswithDNN-basedembeddingsforcapturingspeakerchar-
acteristics[13,14,15].
Most research in speaker recognition assumes that there is only
one speaker per recording and the majority of standard evaluation Apopulardiarizationframeworkinvolvesextractingrepresenta-
datasets reﬂect this assumption. However, speech data collected tions(i-vectorsorDNNembeddings)fromshortspeechsegments,
from many real-world environments violate this single-speaker as- andclusteringthem,todiscovertheindividualspeakersinarecord-
sumption,andthereforebeneﬁtfromspeakerdiarizationasaprepro- ing. EarlyworkusedK-meansorspectralclustering[11,12]. Al-
cessingstep.Speakerdiarizationistheprocessofgroupingsegments ternatively, a score matrix can be computed between pairs of rep-
ofspeechaccordingtothespeaker,andissometimesreferredtoas resentationsusingcosinedistance[16]orPLDAlog-likelihoodra-
the“whospokewhen”task. Recently,bothspeakerrecognitionand tios[17], andclusteredusingagglomerativehierarchicalclustering
diarizationhaveadvancedsigniﬁcantlyduetotheadoptionofdeep (AHC) [18]. Clustering provides a coarse segmentation, which is
neural network (DNN) embeddings to capture speaker character- often reﬁned at the frame-level, using a process called Variational
istics. These embeddings are now replacing i-vectors, which have Bayesresegmentation[19].
beenthestate-of-the-artinbothtasksforalmosttenyears.Ourwork
isbasedonx-vectors,atypeofDNNembeddingwedevelopedfor
speakerrecognition[1]. Thispaperstudiestheproblemofspeaker 2.3. Multi-speakerconversations
recognition for multi-speaker conversations using a modern DNN
embedding-basedsystem. Capturingspeakercharacteristicsinﬁxed-dimensionalembeddings
assumesthattheinputspeechwasgeneratedfromasinglespeaker,
and violating this assumption reduces the effectiveness of the rep-
2. BACKGROUND
resentation[18,20]. Interestinthetopicofspeakerrecognitionon
multi-speakerconversationshasincreasedwiththe2016Speakersin
2.1. Speakerrecognition
theWild(SITW)challenge[21]andtherecentNIST2018Speaker
Until recently, most state-of-the-art speaker recognition systems RecognitionEvaluation[22]duetothepresenceofmulti-speakeren-
werebasedoni-vectors[2]. ThestandardapproachusesGaussian rollmentandtestrecordings. Thisencouragesdiarizationtobeper-
mixture models (GMMs) and factor analysis to compress multi- formedinconjunctionwithspeakerrecognition. Participantsinthe
ple sources of variability into a low-dimensional representation, SITW challenge showed that diarization can signiﬁcantly improve
known as an i-vector. A probabilistic linear discriminant analy- speakerrecognitionrates[23,24]. Ourstudyunderscoresthevalue
sis (PLDA) [3] classiﬁer is used to compare i-vectors, and enable ofdiarizationforspeakerrecognitioninthemulti-speakerenviron-
same-or-differentspeakerdecisions[4,5]. ment.DNN for 6 epochs (instead of 3) and use a minibatch size of 128
Table1.X-vectorDNNarchitecture
(insteadof64).
Layer LayerType Context Size
1 TDNN-ReLU t-2:t+2 512 3.4. Embeddingextraction
2 Dense-ReLU t 512
3 TDNN-ReLU t-2,t,t+2 512 Oncethenetworkistrained,x-vectorsareextractedfromtheafﬁne
4 Dense-ReLU t 512 componentoflayer12. Thex-vectorsareusedasfeaturesfortwo
5 TDNN-ReLU t-3,t,t+3 512 differentPLDAbackends(oneforthediarizationsystemdescribed
6 Dense-ReLU t 512 inSection4andoneforthespeakerrecognitionsystemdescribedin
7 TDNN-ReLU t-4,t,t+4 512 Section5).
8 Dense-ReLU t 512
9 Dense-ReLU t 512 4. SPEAKERDIARIZATION
10 Dense-ReLU t 1500
11 Pooling(mean+stddev) Full-seq 2x1500 The diarization system is based on a system we devel-
12 Dense(Embedding)-ReLU 512 oped for the 2018 DIHARD speaker recognition challenge
13 Dense-ReLU 512 [14, 27]. A similar recipe (for narrowband telephone speech)
14 Dense-Softmax 7185(#spkrs) can be found in the main branch of the Kaldi toolkit:
https://github.com/kaldi-asr/kaldi/tree/
master/egs/callhome_diarization/v2. The system
usesx-vectorsextractedfromtheDNNinSection3withPLDA,and
3. X-VECTORDNN
agglomerative hierarchical clustering (AHC). The PLDA backend
This section describes the x-vector DNN. The architecture is consistsofcentering,whiteningandlengthnormalization,followed
based on the DNN embedding system described in [1, 10]. byscoring. Allcomponentsofthebackendaretrainedon3second
Our software framework has been made available in the Kaldi segmentsextractedfromtheaugmentedVoxCelebdatadescribedin
toolkit [25]. An example recipe is in the main branch Section6.1.
of Kaldi at https://github.com/kaldi-asr/kaldi/ Foreitheranenrollmentrecordingoratestrecording,x-vectors
tree/master/egs/sitw/v2 and several pretrained x-vector areextractedfrom1.5secondsegmentswitha0.75secondoverlap.
systems can be downloaded from http://kaldi-asr.org/ PLDAscoresarecomputedbetweenallpairsofx-vectors. Thisis
models.html. We plan on updating the recipe and pretrained followedbyAHCwithaveragelinkageclustering. Inourprimary
modelswiththeimprovedsystemdescribedinthiswork. system,thenumberofclustersiscontrolledbyastoppingthreshold
whichwastunedontheheld-outSITWDEV set. Themostsimilar
clusters are repeatedly merged, until the average PLDA scores be-
3.1. Architecture
tween clusters is less than the threshold. Diarization results in N
Table1summarizesthearchitectureusedinthiswork. Theﬁrst10 clusters(which,ideallycorrespondtospeakers).
layersofthex-vectorDNNconsistsoflayersthatoperateonspeech
frames, with a small temporal context centered around the current 4.1. RemovingtheAHCthreshold
framet. Thepoolinglayerreceivestheoutputoflayer10asinput,
aggregatesovertheinputsegment,andcomputesitsmeanandstan- AHC-baseddiarizationtypicallyrequiresawell-chosenclusterstop-
darddeviation. Thesesegment-levelstatisticsareconcatenatedto- pingthresholdtoachievegoodperformance. Thisthresholdissen-
getherandpassedthroughtheremaininglayersofthenetwork. The sitivetothedomainofthedata,andapoorlychosenthresholdwill
outputlayercomputesposteriorprobabilitiesforthetrainingspeak- resultinbadperformance. Thisisaparticularlyconcerningpossi-
ers.Comparedtothearchitecturedescribedin[1],weuseaslightly bilitywhenareliabledevelopmentsetisnotavailable.
wider temporal context in the TDNN layers, and interleave dense Toimproverobustness,weproposeasimplealternativetoelim-
layers between the TDNN layers. We found that this architecture inatetheneedfortheAHCthreshold. Insteadofrelyingonatuned
greatlyoutperformsthebaselinearchitectureavailableintheKaldi AHCthreshold,webeginwithanestimateofthemaximumnumber
recipes. ofspeakersK thatmightappearintherecordings. Weassumethat
therearenevermorethanK speakersinanutterance,andperform
clustering K times, with exactly k ∈ {1,2,...,K} clusters each
3.2. Features
timeweperformclustering.Takingtheunionofeachoftheindivid-
Thefeaturesare30dimensionalMFCCswithaframe-lengthof25 ualdiarizationsresultsinasetofN = K(K+1) waystopartition
2
ms,mean-normalizedoveraslidingwindowofupto3seconds.Au- a recording that has at most K speakers. The N potential speak-
dioﬁlesaresampledat16kHz. TheKaldienergySADisusedto ersarethentreatedexactlythesameasthespeakersdiscoveredby
ﬁlteroutnonspeechframes. clusteringwithanAHCthreshold,asdescribedinSection5.
LookingattheSITWDEV set,wefoundthattheperformance
isn’tverysensitivetodifferentvaluesofK ≥3.WeuseK =5for
3.3. Training
theexperimentsintheresultssection.
The DNN is trained to classify the 7,185 speakers in the training
datausingamulti-classcrossentropyobjectivefunction.Atraining
4.2. Diarizingenrollmentrecordings
exampleconsistsofa2–4secondspeechsegment(about3seconds
average), alongwiththecorrespondingspeakerlabel. Followinga Ifweareprocessinganenrollmentrecording,thenthegoalistouse
studybyMcLarenetal. in[26],weusemuchmoreaggressivedata anassistsegmenttoidentifyanyotherspeechintherecordingwhich
augmentation than in previous studies (see Section 6.1), train the belongstothespeakerwewishtoenroll,whileremovinganyspeechbelongingtootherspeakers. AsdescribedinSection6.2,anassist DNN was trained on 7.2 million segments, comprised of the 1.2
segmentisabout5secondsofspeechinalongerrecording,whichis million “raw” segments extracted directly from VoxCeleb, plus an
knowntocontainthespeakerwewishtoenroll. additional6millionsegmentsobtainedbydataaugmentation. The
Thespeechcorrespondingtotheassistsegmentistreatedasan PLDA backend for speaker recognition (Section 5) was trained on
“auxiliaryenrollment”andtheentirerecordingistreatedasan“aux- thefull-lengthrecordingsofVoxCeleb,butweonlykeepthespeech
iliarytest”recording. Afterclustering,weobtainN speakersinthe belongingto thespeakersof interest(asprovided bythesegments
auxiliarytest.WethenperformtheproceduredescribedinSection5, that are distributed with the corpora). We apply augmentation to
which involves computing PLDA scores between the auxiliary en- doubletheamountoftrainingdata,whichincreasesthenumberof
rollmentandeachoftheN speakersdiscoveredintheauxiliarytest. recordingsfromabout150,000to300,000. Finally,thediarization
All the speech segments belonging to the speaker in the auxiliary backend(Section4)wastrainedon256,000threesecondsegments
testthatmaximizesthePLDAscore(asinEquation1)areidentiﬁed, extractedrandomlyfromthefull-lengthaugmentedrecordings.
andusedbythespeakerrecognitionsystemtoextractanenrollment
x-vector.
6.2. SpeakersintheWild
4.3. Diarizingtestrecordings WeperformexperimentsontheSpeakersintheWild(SITW)dataset
developedbySRIInternational[21]. Thedatasetconsistsofchal-
Handling the test recordings is straightforward once AHC is per- lengingaudiocollectedfromdiverseconditionsinthevideoaudio
formed.ThespeechsegmentsaregroupedaccordingtotheNspeak- domain. Oneofthechallengesisthepresenceofmultiplespeakers
ers discovered in the conversation, and are passed directly to the insomeoftheutterances. Therecordingsvaryinlength,from6to
speakerrecognitionsystem,wheretheyareusedtoperformrecog- 240seconds.
nitionasdescribedinthenextsection. The datasetis divided intoa development setDEV (which we
useonlyfortuning)andanevaluationsetEVAL.TheEVALsetcon-
5. SPEAKERRECOGNITION tains 180 speakers divided into 4,170 models and a total of 2,883
audioﬁles.
Recognitionisperformedusingx-vectorsextractedfromtheDNNin Enrollmentconditions
Section3andaPLDAbackend. Thex-vectorsarecentered,dimen- • CORE: Enrollmentrecordingscontainexactlyonespeaker.
sionalityreducedto225usingLDA,andarelength-normalized.All • ASSIST: Oneormorespeakersinenroll,alongwithan“as-
parametersinthebackendareestimatedontheaugmentedVoxCeleb sist”mark,whichisashortsegment(typically5seconds)of
data,asdescribedinSection6.1. therecordingthatisknowntocontainthespeakerofinterest.
Ifdiarizationwasperformedonatestrecording,then,insteadof
Testconditions
extractingasinglex-vectorfortheentiretestrecording,weextractN
x-vectors,oneforeachoftheN speakersidentiﬁedintherecording. • CORE: Testrecordingscontainexactlyonespeaker.
Suppose R(,) is the PLDA log-likelihood ratio score, u is the x- • MULTI: Oneormorespeakersinthetestrecordings.
vectorfortheenrolledspeakerandv ,v ,...,v arethex-vectors
1 2 N
foreachoftheN speakersinthetestrecording.Toperformspeaker
7. EXPERIMENTALRESULTS
recognition,wecomputethePLDAscoreasinEquation1,whichis
themaximumofthePLDAscoresbetweentheenrollmentx-vector InTable2wereportresultsontheEVALportionoftheSpeakersin
andallN testx-vectors. theWild(SITW)dataset.Thefourevaluationconditionsareformed
bypairinganenrollmentconditionwithatestconditiondescribedin
R(enroll,test)=max{R(u,v ),...,R(u,v )} (1)
1 N Section 6.2. Performance on these conditions is examined in Sec-
Handlingadiarizedenrollmentrecordingissimpler,sincethere tions 7.1–7.4. The results are further broken down by whether or
canonlybeonespeakerofinterestatatime. Wesimplyextractthe nottheenrollortestrecordingsarediarized.Thediarizationsystem
enrollmentx-vectorfromallspeechframesidentiﬁedasbelonging anditsinteractionwithspeakerrecognitionisthesubjectofSections
to the speaker of interest (as described in Section 4.2), and ignore 4–5. We report results in terms of equal error rate (EER) and the
theremainingframes. minimum of the normalized detection cost function (DCF). DCF1
usesP =10−2andDCF2usesP =10−3.
Target Target
TheThresholdsystemusesanAHCthresholdtunedontheDEV
6. EXPERIMENTALSETUP
settocontrolthenumberofspeakers,whereasNothresholdusesthe
alternativemethoddescribedinSection4.1toeliminatethethresh-
6.1. Trainingdata
old. InSection7.5,wediscussperformanceusingtheproposedal-
ThesystemistrainedonalargesubsetofthecombinedVoxCeleb1 ternativesystemthateliminatestheAHCthreshold.
[28]andVoxCeleb2[29]corporasampledat16kHz. Thetestpor-
tion of VoxCeleb 2 as well as 60 speakers from VoxCeleb 1 over-
7.1. CORE-CORE
lap with the evaluation dataset, and so we removed them before
training. Seehttp://www.openslr.org/resources/49/ In the simplest SITW evaluation condition, there is exactly one
voxceleb1_sitw_overlap.txt for a list of speakers from speaker present in both the test and enrollment recordings. In the
VoxCeleb1whichareknowntooverlapwithSITW.Thisleavesa ﬁrst row of results in Table 2 (NO DIAR), we do not apply any
totalofover150,000recordingsfrom7,185speakers.Usingthetar- diarization and achieve very low error rates. In the next row of
getspeakermarksprovidedinthecorpora, therecordingsaresplit results (TEST), we apply diarization to the test recordings. Using
intoover1.2millionsegments. thestandardapproach,diarizingsingle-speakerrecordingsdegrades
Weapplyadataaugmentationstrategybasedon[1]thatconsists performancebyaverysmallamount–lessthanhalfapercentrelative
of adding noises, music, babble, and reverberation. The x-vector onallperformancemetrics.Table2.ResultsontheSITWevaluationset.
EVALCORE-CORE EVALCORE-MULTI EVALASSIST-CORE EVALASSIST-MULTI
Diarization EER DCF1 DCF2 EER DCF1 DCF2 EER DCF1 DCF2 EER DCF1 DCF2
NODIAR 1.7 0.20 0.34 3.5 0.28 0.44 3.2 0.24 0.38 4.3 0.28 0.43
ENROLL 1.6 0.20 0.35 3.0 0.26 0.41
Threshold TEST 1.8 0.21 0.35 2.1 0.22 0.41 3.3 0.24 0.39 3.8 0.26 0.41
BOTH 1.7 0.21 0.36 2.1 0.21 0.37
ENROLL 1.6 0.20 0.36 3.0 0.26 0.42
Nothreshold TEST 1.8 0.23 0.36 2.0 0.22 0.40 3.8 0.26 0.40 3.9 0.26 0.41
BOTH 2.2 0.23 0.38 2.2 0.22 0.38
CORE-COREisthemostcommonlyusedconditionfromSITW. Diarizing either enroll or test recordings individually (but not
OurbestperformanceonthisconditionisEER=1.7%DCF1=0.20, together)resultsinmoderateimprovementsinEER,andsmallerim-
which comfortably outperforms the best previously reported num- provements in DCF1 and DCF2. Fortunately, the beneﬁt of com-
bers in [30], which are EER=2.7% and DCF1=0.33. The x-vector biningenrollandtestdiarizationresultsinmuchmoredramaticim-
DNNarchitectureinthispaperissimilartothatofthepreviouswork, provements. Looking at the Threshold system, we observe a 50%
sotheimprovementsaremostlyduetoabettertrainingrecipe,which EERreductionovernodiarizationanda14–23%reductioninDCF.
consistsofmoreaggressivedataaugmentationthanpreviouslyused,
andtheadditionofasubstantialamountofin-domaindatafromthe
7.5. Removingthethreshold
VoxCeleb2Corpus[29].
The previous sections showed that the Threshold system achieves
excellent results. It relies on an AHC threshold tuned on labeled
7.2. CORE-MULTI
in-domaindata. Althoughthisisnotanobstacleforthispaper, as
CORE-MULTIextendsthepreviousconditionwithtestrecordings weareabletotuneonthewell-matchedDEV set, itcannotbeas-
thatcontainoneormorespeakers. Westillusesingle-speakeren- sumedthatanin-domaindevelopmentsetisalwaysavailable. The
rollmentrecordingsinthiscondition. NothresholdsystemusesthemethoddescribedinSection4.1toad-
Diarizingthemulti-speakertestconversations(TEST)resultsin dresstheproblemofperformingdiarizingwhennodevelopmentset
a clear improvement over performing no diarization (NO DIAR). isavailabletotuneon.
UsingatunedAHCthreshold,diarizationreducesEERby38%,and InTable2weseethat,undermostconditions,thealternativeNo
by 20% in DCF1 and 8% in DCF2. The results that eliminate the threshold systemperformssimilarlytoThreshold. Whendiarizing
AHCthresholdareevenslightlybetter.Notethatwedonotconsider is required for multi-speaker conversations, the results of this sys-
the effect of diarizing the enrollment recordings yet, as we do not temareverysimilartothestandardapproach. Thesystemperforms
considerthatmeaningfulunlesstheassistsegmentsareprovided. worstonASSIST-COREwhenweneedlesslydiarizethetestrecord-
ings. However, the BOTH results are nonetheless better than the
resultswithoutdiarization.
7.3. ASSIST-CORE
Thisconditionintroducesoursystemstotheassistsegments. These
8. CONCLUSIONS
segmentsprovideafewsecondsofspeechofthespeakerwewish
to enroll. As described in Section 4.2, we use the assist marks to
This paper investigated speaker recognition with multi-speaker
discoveradditionalspeech(intheenrollmentrecording)thatbelongs
recordings.Weusedadiarizationsystembasedonx-vectors,PLDA,
to the speaker of interest, while discarding any speech from other
andagglomerativehierarchicalclustering(AHC)asafront-endfor
speakers. Although the enrollment recordings may have multiple
a speaker recognition system. We evaluated performance on the
speakers,thetestrecordingsaresingle-speakerinthiscondition.
Speakers in the Wild dataset, and found that diarization signiﬁ-
Diarizing the enrollment recordings (ENROLL) reduces EER
cantlyimprovedspeakerrecognitionperformanceonmulti-speaker
by50%relativetoNODIAR.TheDCFnumbersalsoimprove,but
conversations, and retained strong performance on single-speaker
byasmalleramount. Asexpected,unnecessarilydiarizingthetest
recordings as well. Finally, we showed that the AHC threshold,
recordings (but not enrollment) results in the worst performance.
which controls the number of clusters, can be replaced with an
Nonetheless, theThreshold resultsarenotsigniﬁcantlyworsethan
alternative method that achieves similar performance under most
theresultswithoutdiarization. Inthelastrow(BOTH),wediarize
conditions,buteliminatestheneedforain-domaindevelopmentset
boththeenrollmentandthetestrecordings. FortheThresholdsys-
fortuning.
tem, this degrades performance by 2–8% relative to the ENROLL
results,butstillmaintainsanimprovementoverNODIAR.
9. ACKNOWLEDGMENTS
7.4. ASSIST-MULTI
This material is based upon work supported by the National Sci-
Thisconditioncombinesthechallengeofpotentialmulti-speakeren- ence Foundation Graduate Research Fellowship under Grant No.
rollment recordings with multi-speaker test recordings. As in the 1232825. Anyopinion, ﬁndings, andconclusionsorrecommenda-
previoussection, diarizingtheenrollmentrecordingsisenabledby tionsexpressedinthismaterialarethoseoftheauthors(s)anddonot
theassistsegments. necessarilyreﬂecttheviewsoftheNationalScienceFoundation.10. REFERENCES [15] Q. Wang, C. Downey, L. Wan, P. Mansﬁeld, and I. Lopez
Moreno, “Speaker diarization with lstm,” in 2018 IEEE In-
[1] D.Snyder,D.Garcia-Romero,G.Sell,D.Povey,andS.Khu- ternationalConferenceonAcoustics,SpeechandSignalPro-
danpur, “X-vectors: Robust dnn embeddings for speaker cessing(ICASSP).IEEE,2018,pp.5239–5243.
recognition,” in 2018 IEEE International Conference on [16] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel,
Acoustics, Speech and Signal Processing (ICASSP). IEEE, “Astudyofthecosinedistance-basedmeanshiftfortelephone
2018. speechdiarization,”IEEE/ACMTransactionsonAudio,Speech
[2] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouel- and Language Processing (TASLP), vol. 22, no. 1, pp. 217–
let, “Front-endfactoranalysisforspeakerveriﬁcation,” IEEE 227,2014.
TransactionsonAudio,Speech,andLanguageProcessing,vol. [17] G.SellandD.Garcia-Romero, “Speakerdiarizationwithplda
19,no.4,pp.788–798,2011. i-vectorscoringandunsupervisedcalibration,” inSpokenLan-
guage Technology Workshop (SLT), 2014 IEEE. IEEE, 2014,
[3] S.Ioffe,“Probabilisticlineardiscriminantanalysis,”Computer
pp.413–417.
Vision–ECCV2006,pp.531–542,2006.
[18] P.Kenny,D.Reynolds,andF.Castaldo, “Diarizationoftele-
[4] P.Kenny,“Bayesianspeakerveriﬁcationwithheavy-tailedpri-
phone conversations using factor analysis,” IEEE Journal of
ors.,” inOdyssey,2010,p.14.
SelectedTopicsinSignalProcessing, vol.4, no.6, pp.1059,
[5] N. Bru¨mmer and E. De Villiers, “The speaker partitioning 2010.
problem.,” inOdyssey,2010,p.34. [19] M.Diez,L.Burget,andP.Matejka,“Speakerdiarizationbased
on bayesian hmm with eigenvoice priors,” in Proc. Odyssey
[6] L. Heck, Y. Konig, K. Sonmez, and M. Weintraub, “Ro-
2018TheSpeakerandLanguageRecognitionWorkshop,2018,
bustnesstotelephonehandsetdistortioninspeakerrecognition
pp.147–154.
bydiscriminativefeaturedesign,” inSpeechCommunication,
2000,vol.31,pp.181–192. [20] A.MartinandM.Przybocki, “Speakerrecognitioninamulti-
speaker environment,” in Seventh European Conference on
[7] A. Salman, Learning speaker-speciﬁc characteristics with
SpeechCommunicationandTechnology,2001.
deepneuralarchitecture, Ph.D.thesis,UniversityofManch-
[21] M.McLaren,L.Ferrer,D.Castan,andA.Lawson, “The2016
ester,2012.
speakersinthewildspeakerrecognitionevaluation.,” inInter-
[8] G.Heigold,I.Moreno,S.Bengio,andN.Shazeer,“End-to-end speech,2016,pp.823–827.
text-dependent speaker veriﬁcation,” in 2016 IEEE Interna-
[22] “NIST speaker recognition evaluation 2018,” https:
tionalConferenceonAcoustics,SpeechandSignalProcessing
//www.nist.gov/sites/default/files/
(ICASSP).IEEE,2016,pp.5115–5119.
documents/2018/08/17/sre18_eval_plan_
[9] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, 2018-05-31_v6.pdf,2018.
Y. Carmiel, and S. Khudanpur, “Deep neural network-based [23] O. Novotny`, P. Matejka, O. Plchot, O. Glembek, L. Burget,
speaker embeddings for end-to-end speaker veriﬁcation,” in andJ.Cernocky`, “Analysisofspeakerrecognitionsystemsin
SpokenLanguageTechnologyWorkshop(SLT).IEEE,2016. realisticscenariosofthesitw2016challenge.,”inInterspeech,
2016,pp.828–832.
[10] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudan-
pur, “Deep neural network embeddings for text-independent [24] Y. Liu, Y. Tian, L. He, and J. Liu, “Investigating various
speakerveriﬁcation,” Proc.Interspeech,pp.999–1003,2017. diarization algorithms for speaker in the wild (sitw) speaker
recognitionchallenge.,” inInterspeech,2016,pp.853–857.
[11] S. Shum, N. Dehak, E. Chuangsuwanich, D. Reynolds, and
J.Glass, “Exploitingintra-conversationvariabilityforspeaker [25] D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
diarization,”inTwelfthAnnualConferenceoftheInternational N. Goel, M. Hannemann, P. Motl´ıcˇek, Y. Qian, P. Schwarz,
SpeechCommunicationAssociation,2011. etal., “TheKaldispeechrecognitiontoolkit,” inProceedings
oftheAutomaticSpeechRecognition&Understanding(ASRU)
[12] S.Shum,N.Dehak,andJ.Glass, “Ontheuseofspectraland Workshop,2011.
iterative methods for speaker diarization,” in Thirteenth An-
[26] M. McLaren, D. Castan, M. Nandwana, L. Ferrer, and
nual Conference of the International Speech Communication
E. Yılmaz, “How to train your speaker embeddings extrac-
Association,2012.
tor,” in Odyssey: The Speaker and Language Recognition
[13] D.Garcia-Romero,D.Snyder,G.Sell,D.Povey,andA.Mc- Workshop,LesSablesdOlonne,2018.
Cree, “Speaker diarization using deep neural network em-
[27] N.Ryant,K.Church,C.Cieri,A.Cristia,J.Du,S.Ganapathy,
beddings,” in2017IEEEInternationalConferenceonAcous-
and M. Liberman, “First dihard challenge evaluation plan,”
tics,SpeechandSignalProcessing(ICASSP).IEEE,2017,pp.
2018.
4930–4934.
[28] A.Nagrani,J.S.Chung,andA.Zisserman,“Voxceleb:alarge-
[14] G.Sell,D.Snyder,A.Mccree,D.Garcia-Romero,J.Villalba, scalespeakeridentiﬁcationdataset,” inInterspeech,2017.
M.Maciejewski,V.Manohar,N.Dehak,D.Povey,S.Watan-
[29] J.S.Chung,A.Nagrani,andA.Zisserman,“Voxceleb2:Deep
abe, and S. Khudanpur, “Diarization is Hard: Some Experi-
speakerrecognition,” inINTERSPEECH,2018.
encesandLessonsLearnedfortheJHUTeamintheInaugural
[30] A. Silnova, N. Bru¨mmer, D. Garcia-Romero, D. Snyder, and
DIHARDChallenge,” inProceedingsofthe19thAnnualCon-
L.Burget,“FastVariationalBayesforHeavy-tailedPLDAAp-
ference of the International Speech Communication Associa-
pliedtoi-vectorsandx-vectors,” inInterspeech2018,Hyder-
tion, INTERSPEECH 2018, Hyderabad, India, sep 2018, pp.
abad,India,2018,pp.72–76.
2808—-2812.