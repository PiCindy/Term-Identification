NIST 2019 Speaker Recognition Evaluation Plan
August16,2019
1 Introduction
The 2019 speaker recognition evaluation (SRE19) is the next in an ongoing series of speaker recognition
evaluations conducted by the US National Institute of Standards and Technology (NIST) since 1996. The
objectives of the evaluation series are (1) for NIST to effectively measure system-calibrated performance
of the current state of technology, (2) to provide a common test bed that enables the research community
to explore promising new ideas in speaker recognition, and (3) to support the community in their devel-
opment of advanced technology incorporating these ideas. The evaluations are intended to be of interest
to all researchers working on the general problem of text-independent speaker recognition. To this end,
the evaluations are designed to focus on core technology issues and to be simple and accessible to those
wishingtoparticipate.
SRE19willconsistoftwoseparateactivities: 1)aleaderboard-stylechallengeusingconversationaltele-
phone speech (CTS) extracted from the unexposed portions of the Call My Net 2 (CMN2) corpus, and 2)
aregularevaluationusingaudio-visual(AV)materialextractedfromtheunexposedportionsoftheVideo
AnnotationforSpeechTechnology(VAST)corpus.Thisdocumentdescribesthetask,theperformancemet-
ric,data,andtheevaluationprotocolaswellasrules/requirementsfortheregularevaluation(referredto
asSRE19hereafter). TheevaluationplanfortheSRE19CTSChallengecanbefoundontheSRE19website1.
Notethatinordertoparticipateintheregularevaluation(i.e.,Part2),onemustﬁrstcompletePart1.
TheSRE19willbeorganizedinasimilarmannertotheSRE18,exceptthatforthisyear’sevaluationonly
theopentrainingconditionwillbeoffered(seeSection2.2). Moreover,inadditiontotheregularaudio-only
track,theSRE19willalsointroduceaudio-visualandvisual-onlytracks. Systemsubmissionisrequiredfor
theaudioandaudio-visualtracks,andoptionalforthevisualtrack. Table1summarizesthetracksforthe
SRE19.
Track Input Core
Audio AudiofromVideo Yes
Audio-Visual AudioandFramesfromVideo Yes
Visual FramesfromVideo No
Table1: TheSRE19tracks
Participation in the SRE19 is open to all who ﬁnd the evaluation of interest and are able to comply
with the evaluation rules set forth in this plan. Although there is no cost to participate in SRE19 (i.e., the
evaluationdata,webplatform,andscoringsoftwarewillbeavailablefreeofcharge),participatingteams
must be represented at the post-evaluation workshop2 to be co-located with IEEE ASRU workshop in
Sentosa, Singapore, on December 12-13, 2019. Information about evaluation registration can be found on
theSRE19website1.
1https://www.nist.gov/itl/iad/mig/nist-2019-speaker-recognition-evaluation
2Workshopregistrationisrequired.
1NIST2019SpeakerRecognitionEvaluationPlan
2 Task Description
2.1 TaskDeﬁnition
The task for the SRE19 is individual/person detection: given a test video segment and a target individual’s
enrollment video, automatically determine whether the target individual is present in the test segment.
Thetestsegmentalongwiththeenrollmentsegmentfromadesignatedtargetindividualconstituteatrial.
Thesystemisrequiredtoprocesseachtrialindependentlyandtooutputalog-likelihoodratio(LLR),using
natural(base e)logarithm, forthattrial. TheLLRforagiventrialincludingatestsegment s isdeﬁnedas
follows
(cid:18)P(s|H )(cid:19)
LLR(s)=log 0 . (1)
P(s|H )
1
where P(·) denotestheprobabilitydistributionfunction(pdf), and H and H representthenull(i.e., the
0 1
targetindividualispresentin s)andalternative(i.e., thetargetindividualisnotpresentin s)hypotheses,
respectively.
2.2 TrainingCondition
The training condition is deﬁned as the amount of data/resources used to build an individual/person
recognitionsystem. UnlikeSRE16andSRE18,thisyear’sevaluationonlyofferstheopentrainingcondition
thatallowstheuseofanypubliclyavailableand/orproprietarydataforsystemtraininganddevelopment.
Themotivationbehindthisdecisionistwofold. First, resultsfromthemostrecentNISTSREs(i.e., SRE16
andSRE18)indicatelimitedperformanceimprovements,ifany,fromunconstrainedtrainingcomparedto
ﬁxedtraining.Wenote,however,thatparticipantscitedlackoftimeand/orresourcesduringtheevaluation
period for not demonstrating signiﬁcant improvement with open vs ﬁxed training. Second, the number of
publiclyavailablelarge-scaledataresourcesthatcanbeusedforspeakerandindividual/personrecognition
has dramatically increased over the past few years (e.g., see VoxCeleb3 and SITW4). Therefore, removing
theﬁxedtrainingconditionwillallowmorein-depthexplorationintothegainsthatcanbeachievedwiththe
availabilityofunconstrainedresourcesgiventhesuccessofdata-hungryNeuralNetworkbasedapproaches
inthemostrecentevaluation(i.e. SRE18).
For the sake of convenience, in particular for the audio-visual and visual-only tracks, NIST will also
providetwoDevelopmentsetsthatcanbeusedforsystemtraininganddevelopmentpurposes:
• JANUSMultimediaDataset(LDC2019E55)
• 2019NISTSpeakerRecognitionEvaluationAudio-VisualDevelopmentSet(LDC2019E56)
The LDC2019E55, which has been extracted from the IARPA JANUS Benchmark-C datatset, is available
from the Linguistic Data Consortium (LDC), subject to approval of the LDC data license agreement. The
LDC2019E56containstheoriginalvideosfromwhichtheVASTportionofSRE18Dev/Testsetswerecom-
piled. Participants can obtain this dataset through the evaluation web platform (https://sre.nist.gov)
aftertheyhavesignedtheLDCdatalicenseagreement.
AlthoughSRE19allowsunconstrainedsystemtraininganddevelopment,participatingteamsmustpro-
vide a sufﬁcient description of speech and non-speech data resources as well as pre-trained models used
duringthetraininganddevelopmentoftheirsystems(seeSection6.4.2).
2.3 EnrollmentConditions
Theenrollmentconditionisdeﬁnedasthenumberofvideosegmentsprovidedtocreateatargetspeaker
model. ThereisonlyoneenrollmentconditionfortheSRE19:
3http://www.robots.ox.ac.uk/~vgg/data/voxceleb/
4http://www.speech.sri.com/projects/sitw/
Page2 of9NIST2019SpeakerRecognitionEvaluationPlan
• One-segment–inwhichthesystemisgivenonlyonevideosegment,thatcanvaryindurationfrom
afewsecondstoseveralminutes,tobuildthemodelofthetargetindividual.
Notethatfortheaudiotrack,speechextractedfromtheenrollmentvideoservesasenrollmentdata,while
forthevisualtrack,faceframe(s)(i.e.,framesinwhichthefaceofthetargetindividualisvisible)extracted
fromthevideoservethatpurpose. SinceNISTwillonlybereleasingvideoﬁlesforSRE19,participantsare
responsibleforextractingtherelevantdata(i.e.,speechorfaceframes)forsubsequentprocessing.
Asinthemostrecentevaluations,genderlabelswillnotbeprovidedfortheenrollmentsegmentsinthe
testset.
2.4 TestConditions
ThetestconditionsfortheSRE19areasfollows:
• Thetestsegmentvideodurationmayvaryfromafewsecondstoseveralminutes.
• Thetestvideocancontainaudio-visualdatafrompotentiallymultipleindividuals.
• Therewillbebothsame-genderandcross-gendertrials.
3 Performance Measurement
3.1 PrimaryMetric
A basic cost model is used to measure the individual/person detection performance in SRE19, which is
deﬁned as a weighted sum of false-reject (missed detection) and false-alarm error probabilities for some
decisionthresholdθasfollows
C (θ)=C ×P ×P (θ)+
Det Miss Target Miss
C ×(1−P )×P (θ), (2)
FalseAlarm Target FalseAlarm
where the parameters of the cost function are C (cost of a missed detection) and C (cost of a
Miss FalseAlarm
spurious detection), and P (a priori probability of the speciﬁed target individual) and are deﬁned to
Target
havethefollowingvalues:
SourceType ParameterID C C P
Miss FalseAlarm Target
AV 1 1 1 0.05
Table2: TheSRE19detectioncostparameters
ToimprovetheinterpretabilityofthecostfunctionC in(2), itwillbenormalizedbyC which
Det Default
isdeﬁnedasthebestcostthatcouldbeobtainedwithoutprocessingtheinputdata(i.e., byeitheralways
acceptingoralwaysrejectingthesegmentindividual(s)asmatchingthetargetindividual,whichevergives
thelowercost),asfollows
C (θ)
C (θ) = Det , (3)
Norm
C
Default
whereC isdeﬁnedas
Default
(cid:40)
C ×P ,
C = min Miss Target (4)
Default
C ×(1−P ).
FalseAlarm Target
Page3 of9NIST2019SpeakerRecognitionEvaluationPlan
SubstitutingthesetofparametervaluesfromTable2into(4)yields
C =C ×P . (5)
Default Miss Target
SubstitutingC andC in(3)with(2)and(5),respectively,alongwithsomealgebraicmanipulations
Det Default
yields
C (θ) = P (θ)+β×P (θ), (6)
Norm Miss FalseAlarm
whereβisdeﬁnedas
β = CFalseAlarm × 1−PTarget. (7)
C P
Miss Target
The actual detection cost will be computed from the trial scores by applying a detection threshold of
log(β), where log denotes the natural logarithm. The detection threshold will be computed for β with
1
P =0.05. TheprimarycostmeasurefortheSRE19isthendeﬁnedas
Target1
C =C (log(β )). (8)
Primary Norm 1
Inadditionto C , aminimumdetectioncostwillalsobecomputedbyusingthedetectionthreshold
Primary
that minimizes the detection cost. NIST will make available the script that calculates the primary metric,
ontheevaluationwebplatform.
4 Data Description
The data collected by the LDC as part of the Video Annotation for Speech Technology (VAST) corpus to
supportspeakerrecognitionresearchwillbeusedtocompiletheSRE19developmentandtestsets.
The VAST corpus contains amateur video recordings (such as video blogs) collected by the LDC from
variousonlinemediahostingservices. Thevideosvaryindurationfromafewsecondstoseveralminutes
andincludespeechspokeninEnglish. Eachvideomaycontainaudio-visualdatafrompotentiallymultiple
individualswhomayormaynotbevisibleintherecording,thereforemanuallyproduceddiarizationlabels
(i.e., speakertimemarks), aswellaskeyfaceframes5 andboundingboxes(thatmarkanindividual’sface
inthevideo)willbeprovidedforboththedevsetandtestsetenrollmentvideos(butnotforthetestvideos
ineitherset). AllvideodatawillbeencodedasMPEG4.
The VAST Development and Test sets will be distributed by NIST via the online evaluation platform
(https://sre.nist.gov),whiletheJANUSMultimediaDatasetwillbereleasedbytheLDC.
4.1 DataOrganization
TheDevelopmentandTestsetsfollowasimilardirectorystructure:
<base directory>/
README.txt
data/
enrollment/
test/
docs/
4.2 TrialFile
Thetrialﬁle,namedsre19 av {dev|eval} trials.tsvandlocatedinthedocsdirectory,iscomposedofa
headerandasetofrecordswhereeachrecorddescribesagiventrial. Eachrecordisasinglelinecontaining
5Notethatonlyafew(outofpotentiallymany)targetfaceframesperenrollmentvideohavebeenmanuallyannotated.
Page4 of9NIST2019SpeakerRecognitionEvaluationPlan
threeﬁeldsseparatedbyatabcharacterandinthefollowingformat:
modelid<TAB>segmentid<TAB>side<NEWLINE>
where
modelid-Theenrollmentidentiﬁer
segmentid-Thetestsegmentidentiﬁer
side-Thechannel6
Forexample:
modelid segmentid side
1001 sre19 dtadhlw sre19 a
1001 sre19 dtaekaz sre19 a
1001 sre19 dtaekbb sre19 a
4.3 DevelopmentSet
Participants in the SRE19 will receive data for development experiments that will mirror the evaluation
conditions,andwillinclude:
• videosfrom52individualsfromtheVASTportionofSRE18
• Associatedmetadatawhichwillbelocatedinthedocsdirectoryasoutlinedinsection4.1:
– sre19 av dev segment key.tsv contains information about the video segments as well as the
individualswithinthem,andincludesthefollowingﬁelds:
∗ segmentid(segmentidentiﬁer)
∗ subjectid(LDCspeakerid)
∗ gender(maleorfemale)
∗ partition(enrollmentortest)
– sre19 av dev enrollment diarization.tsvcontainsmanuallyproducedtimemarksfortarget
speakers,andincludesthefollowingﬁelds:
∗ segmentid(segmentidentiﬁer)
∗ speaker type(speakertype,always“target”)
∗ start(startoftargetspeakersegmenttimemarkinseconds)
∗ end(endoftargetspeakersegmenttimemarkinseconds)
– sre19 av dev enrollment boundingbox.tsvcontainsmanuallyproducedinformationabouttar-
getindividuals’faces(e.g.,coordinates)invideos,andincludesthefollowingﬁelds:
∗ segmentid(segmentidentiﬁer)
∗ speaker type(speakertype,always“target”)
∗ face frame sec(theframeinwhichatargetindividual’sfaceisvisibleinseconds)
∗ bounding box(coordinatesforatargetindividual’sfaceinaspeciﬁedframeas[x1,y1,x2,y2])
∗ face covered(whethertheatargetindividual’sfaceiscovered)
∗ eyewear(whethertheatargetindividualiswearingglasses)
∗ facial hair(whethertheatargetindividualhasfacialhair)
6SRE19segmentswillbeassumedsinglechannel,thereforethisﬁeldisalways”a”
Page5 of9NIST2019SpeakerRecognitionEvaluationPlan
In addition to the data noted above, LDC will also release selected data resources from the IARPA
JANUSBenchmark-C,namelytheJANUSMultimediaDataset7(LDC2019E55).
Thesedevelopmentdatamaybeusedforanypurpose.
4.4 TrainingSet
Section 2.2 describes the training condition for the SRE19 (i.e., open training condition). Participants are
allowed to use any publicly available and/or proprietary data they have available for system training
anddevelopmentpurposes. TheSRE19participantswillalsoreceivetwoDevsets(i.e., LDC2019E55and
LDC2019E56)thattheycanuseforsystemtraining. ToobtaintheseDevelopmentdata,participantsmust
signtheLDCdatalicenseagreementwhichoutlinesthetermsofthedatausage.
5 Evaluation Rules and Requirements
The SRE19 is conducted as an open evaluation where the test data is sent to the participants to process
locallyandsubmittheoutputoftheirsystemstoNISTforscoring. Assuch,theparticipantshaveagreedto
processthedatainaccordancewiththefollowingrules:
• Theparticipantsagreetomakeatleastonevalidsubmissionfortheopentrainingcondition.
• The participants agree to process each trial independently. That is, each decision for a trial is to be
basedonlyuponthespeciﬁedtestsegmentandtargetspeakerenrollmentdata. Theuseofinforma-
tionaboutothertestsegmentsand/orothertargetspeakerdataisnotallowed.
• Theparticipantsagreenottoprobetheenrollmentortestsegmentsviamanual/humanmeanssuch
aslisteningtoorwatchingthedata,orproducingthemanualtranscriptofthespeech,orproducing
themanualfacecoordinates.
• Theparticipantsareallowedtouseanyautomaticallyderivedinformationfortraining,development,
enrollment,ortestsegments.
• Theparticipantsareallowedtouseinformationavailableintheheaderofthevideoﬁles.
• Theparticipantscanregisteruptothreesystemsforeachtrack(i.e.,audio,audio-visual,andvisual)
of the open training condition, one of which under each track should be designated as the primary
system. Bug-ﬁxdoesnotcounttowardthislimit. Teamscanmakeunlimitednumberofsubmissions
foreachofthethreesystemsuntiltheevaluationperiodisover.
Inadditiontotheabovedataprocessingrules,participantsagreetocomplywiththefollowinggeneral
requirements:
• The participants agree to submit reports to NIST that describe in sufﬁcient length details of their
systemsandsubmissions. Thesystemdescriptionreportsshouldcomplywithguidelinesdescribed
inSection6.4.2.
• Theparticipantsagreetohaveoneormorerepresentativesatthepost-evaluationworkshop,topresent
ameaningfuldescriptionoftheirsystem(s). Evaluationparticipantsfailingtodosowillbeexcluded
fromfutureevaluationparticipation.
• Theparticipantsagreetotheguidelinesgoverningthepublicationoftheresults:
7Thedataisdescribedindetailinthefollowingpaper: G.Sell, K.Duh, D.Snyder, D.Etter, D.Garcia-Romero, “Audio-Visual
PersonRecognitioninMultimediaDataFromtheIARPAJanusProgram,”inProc.IEEEICASSP,pp.3031-3035,2018.
Page6 of9NIST2019SpeakerRecognitionEvaluationPlan
– Participantsarefreetopublishresultsfortheirownsystembutmustnotpubliclycomparetheir
resultswithotherparticipants(ranking,scoredifferences,etc.) withoutexplicitwrittenconsent
fromtheotherparticipants.
– Whileparticipantsmayreporttheirownresults,participantsmaynotmakeadvertisingclaims
abouttheirstandingintheevaluation,regardlessofrank,orwinningtheevaluation,orclaim
NISTendorsementoftheirsystem(s). ThefollowinglanguageintheU.S.CodeofFederalReg-
ulations (15 C.F.R. § 200.113) shall be respected8: NIST does not approve, recommend, or endorse
anyproprietaryproductorproprietarymaterial. NoreferenceshallbemadetoNIST,ortoreportsorre-
sultsfurnishedbyNISTinanyadvertisingorsalespromotionwhichwouldindicateorimplythatNIST
approves, recommends, or endorses any proprietary product or proprietary material, or which has as its
purposeanintenttocausedirectlyorindirectlytheadvertisedproducttobeusedorpurchasedbecauseof
NISTtestreportsorresults.
– At the conclusion of the evaluation NIST generates a report summarizing the system results
for conditions of interest, but these results/charts do not contain the participant names of the
systemsinvolved. Participantsmustnotpubliclypublishorotherwisedisseminatethesecharts.
– The report that NIST creates should not be construed or represented as endorsements for any
participant’ssystemorcommercialproduct,orasofﬁcialﬁndingsonthepartofNISTortheU.S.
Government.
Sitesfailingtomeettheabovenotedrulesandrequirements,willbeexcludedfromfutureevaluationpar-
ticipation,andtheirfutureregistrationswillnotbeaccepteduntiltheycommittofullycomplywiththe
rules.
6 Evaluation Protocol
To facilitate information exchange between the participants and NIST, all evaluation activities are con-
ductedoveraweb-interface.
6.1 EvaluationAccount
Participants must sign up for an evaluation account where they can perform various activities such as
registeringfortheevaluation,signingthedatalicenseagreement,aswellasuploadingthesubmissionand
system description. To sign up for an evaluation account, go to https://sre.nist.gov. The password
must be at least 12 characters long and must contain a mix of upper and lowercase letters, numbers, and
symbols. After the evaluation account is conﬁrmed, the participant is asked to join a site or create one if
it does not exist. The participant is also asked to associate his site to a team or create one if it does not
exist. Thisallowsmultiplememberswiththeirindividualaccountstoperformactivitiesonbehalfoftheir
siteand/orteam(e.g.,makeasubmission)inadditiontoperformingtheirownactivities(e.g.,requesting
workshopinvitationletter).
• Aparticipantisdeﬁnedasamemberorrepresentativeofasitewhotakespartintheevaluation(e.g.,
JohnDoe)
• Asiteisdeﬁnedasasingleorganization(e.g.,NIST)
• Ateamisdeﬁnedasagroupoforganizationscollaboratingonatask(e.g.,Team1consistingofNIST
andLDC)
8Seehttp://www.ecfr.gov/cgi-bin/ECFR?page=browse
Page7 of9NIST2019SpeakerRecognitionEvaluationPlan
6.2 EvaluationRegistration
Oneparticipantfromasitemustformallyregisterhissitetoparticipateintheevaluationbyagreeingtothe
termsofparticipation. Formoreinformationaboutthetermsofparticipation,seeSection5.
6.3 DataLicenseAgreement
OneparticipantfromeachsitemustsigntheLDCdatalicenseagreementtoobtainthedevelopment/training
datafortheSRE19.
6.4 SubmissionRequirements
Eachteammustmakeatleastonevalidsubmissionfortheaudio-onlyandtheaudio-visualtracks,process-
ingalltestsegments. Submissionswithmissingtestsegmentswillnotpassthevalidationstep,andhence
willberejected.Submissionforthevisual-onlytrackisoptionalbuthighlyencouragedtogaininsightsinto
howthefacerecognitiontechnologycompareswiththespeakerrecognitiontechnologyonthesamedata.
Eachteamisrequiredtosubmitasystemdescriptionatthedesignatedtime(seeSection7). Theevalua-
tionresultsaremadeavailableonlyafterthesystemdescriptionreportisreceivedandconﬁrmedtocomply
withguidelinesdescribedinSection6.4.2.
6.4.1 SystemOutputFormat
Thesystemoutputﬁleiscomposedofaheaderandasetofrecordswhereeachrecordcontainsatrialgiven
in the trial ﬁle (see Section 4.2) and a log likelihood ratio output by the system for the trial. The order of
thetrialsinthesystemoutputﬁlemustfollowthesameorderasthetriallist. Eachrecordisasingleline
containing4ﬁeldsseparatedbytabcharacterinthefollowingformat:
modelid<TAB>segment<TAB>side<TAB>LLR<NEWLINE>
where
modelid-Theenrollmentidentiﬁer
segmentid-Thetestsegmentidentiﬁer
side-Thechannel(always”a”forSRE19sincethedataisassumedsinglechannel)
LLR-Thelog-likelihoodratio
Forexample:
modelid segmentid side LLR
1001 sre19 dtadhlw sre19 a 0.79402
1001 sre19 dtaekaz sre19 a 0.24256
1001 sre19 dtaekbb sre19 a 0.01038
There should be one output ﬁle for each track for each system. NIST will make available the script that
validatesthesystemoutput.
6.4.2 SystemDescriptionFormat
Eachteamisrequiredtosubmitasystemdescription. Thesystemdescriptionmustincludethefollowing
items:
• acompletedescriptionofthesystemcomponents,includingfront-end(e.g.,speechactivitydetection,
diarization, face detection, face tracking, features, normalization) and back-end (e.g., background
models, speaker/face embedding extractor, LDA/PLDA) modules along with their conﬁgurations
Page8 of9NIST2019SpeakerRecognitionEvaluationPlan
(i.e., ﬁlterbankconﬁguration, dimensionalityandtypeoftheacousticfeatureparameters, aswellas
theacousticmodelandthebackendmodelconﬁgurations),
• acompletedescriptionofthedatapartitionsusedtotrainthevariousmodels(asmentionedabove).
Teams are encouraged to report how having access to the Development set (labeled and unlabeled)
impactedtheperformance,
• acompletedescriptionofthesystemcombinationstrategy(e.g., scorenormalization/calibrationfor
fusion)usedforaudio-visualindividual/personrecognition,
• performanceofthesubmissionsystems(primaryandsecondary)ontheSRE19Developmentset(or
a derivative/custom dev set), using the scoring software provided via the web platform (https://
sre.nist.gov).Teamsareencouragedtoquantifythecontributionoftheirmajorsystemcomponents
thattheybelieveresultedinsigniﬁcantperformancegains,
• areportoftheCPU(singlethreaded)andGPUexecutiontimesaswellastheamountofmemoryused
toprocessasingletrial(i.e.,thetimeandmemoryusedforcreatingaspeakermodelfromenrollment
dataaswellasprocessingatestsegmenttocomputetheLLR).
ThesystemdescriptionshouldfollowthelatestIEEEICASSPconferenceproceedingtemplate.
7 Schedule
Milestone Date
Evaluationplanpublished August14,2019
Registrationperiod August15-September16,2019
Trainingdataavailable August15,2019
Evaluationdataavailabletoparticipants August15,2019
SystemoutputandsystemdescriptionduetoNIST October21,2019
Finalofﬁcialresultsreleased October28,2019
Post-evaluationworkshop December12–13,2019
Page9 of9