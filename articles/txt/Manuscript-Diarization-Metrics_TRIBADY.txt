Noname manuscript No.
(will be inserted by the editor)
A tutorial on evaluation metrics for speaker
diarization systems
Supratim Tribady · Shefali Waldekar ·
A Kishore Kumar · Goutam Saha · Md
Sahidullah ·
Received:DDMonthYEAR/Accepted:DDMonthYEAR
Abstract Inthisarticle,wepresentacomprehensivereviewoftheevaluation
metricsfortheSDsystems.Wedemonstratehowtheycalculatetheevaluation
metrics from the ground truth and the system-generated output. Here, diﬀer-
ent errors are considered in each evaluation metrics, such as speaker error,
false alarm, and missed speech with the help of ground truth and system out-
put. We explain the importance of diﬀerent error terms for computing these
evaluation metrics for SD with the help of case studies. The limitations of
diﬀerent evaluation metrics are brieﬂy explained in this article. Finally, we
discuss the formulation of new or diﬀerent evaluation metric for evaluation of
SD systems.
Keywords Diarization error rate (DER) · Jaccard error rate (JER) · Rich
transcription time marked (RTTM) · Speaker diarization (SD) · Speaker
indexing · Un-partitioned evaluation map (UEM)
SupratimTribady
DepartmentofElectronics&ElectricalCommunicationEngineering,IITKharagpur
E-mail:supratimtribedy96@gmail.com
ShefaliWaldekar
DepartmentofElectronics&ElectricalCommunicationEngineering,IITKharagpur
E-mail:shefaliw@ece.iitkgp.ernet.in
AKishoreKumar
DepartmentofElectronics&ElectricalCommunicationEngineering,IITKharagpur
E-mail:kishore@iitkgp.ac.in
GoutamSaha
DepartmentofElectronics&ElectricalCommunicationEngineering,IITKharagpur
E-mail:gsaha@ece.iitkgp.ac.in
MdSahidullah
Universit´edeLorraine,CNRS,Inria,LORIA,F-54000,Nancy,France
E-mail:md.sahidullah@inria.fr2 SupratimTribadyetal.
1 Introduction
Speaker diarization (SD) (also known as speaker indexing (Wilcox and Kim-
ber, 1997)) aims to solve the problem “Who spoke When” for a given speech
signal (Anguera et al., 2012). It mainly involves dividing a speech signal into
segmentsfollowedbygroupingofthehomogeneoussegmentsbasedonspeaker
similarity indexing. SD has many practical applications, such as automatic
video captioning (Song et al., 2018), automatic transcript generation for spo-
ken conversations (Bentley et al., 2018), smart speaker technology (Bentley
et al., 2018), etc. In the present period with a growing number of broadcast-
ing and online meeting, SD could play a key role in creating transcripts for
content summarization andsentiment analysis innaturallanguageprocessing
application (Tiwary and Siddiqui, 2008). Most of thestudies conducted in SD
research focus on three kinds of audio-data: (i) broadcast news audio where
speechdataareusuallycollectedfromradioandTVprogramscontainingcom-
mercial breaks and music (Wachob, 1992), (ii) meeting audio where multiple
people are involved in a conversation (Mieczakowski et al.), and (iii) audio-
data from telephone conversation (Elvins et al., 2003). However, studies on
SDarealsoconductedwithDIHARDcorporawhichconsistsofawidevariety
of audio-data collected from a number of real-world conditions (Ryant et al.,
2018).
ThemainchallengeinSDsystemarisesduetodiﬀerentpracticalproblems,
whichmainlyincludesdomainmismatchbecauseofdiﬀerentacousticenviron-
ment (Himawan et al., 2018), incorrect detection of speakers in multi-speaker
speech recognition from unsegmented recordings (Watanabe et al., 2020), and
improper evaluation of the metrics during overlapping of speakers in a con-
versation (Vipperla et al., 2012). The system should be strong enough to deal
with multiple speakers during overlapping.
The main aim of this work is to review the evaluation metrics for SD sys-
tem. Evaluation metrics play a very important role in determining the best
system, based on the various shortcomings of the diarization process. The se-
lectionofanevaluationmetricdecidesthesystemperformanceindiﬀerentad-
versarialconditions.Animportantaspectoftheevaluationmetricisthecapa-
bility to distinguish among various systems. Several metrics are used to check
the performance of the speech processing systems, like for automatic speaker
veriﬁcationEqual error rate (EER)isused(Jyh-MinChengandHsiao-Chuan
Wang,2004),foracousticsceneclassiﬁcationthestandardisaccuracy (Valenti
et al., 2016), F1 score is used for sound event detection (Kong et al., 2019),
metrics such as Unweighted average recall (UAR) is used for emotion recog-
nition evaluation (Gamage et al., 2017), and min t-DCF is used for detecting
spooﬁng countermeasures (Kinnunen et al., 2020),Word error rate (WER) is
the primary evaluation metric for automatic speech recognition (Galibert,
2013), etc. These are some evaluation metrics used for checking the perfor-
mance of the systems for the respective domain. Similarly, Diarization error
rate and Jaccard error rate are the two widely used evaluation metric, used
to check the performance of the SD system. Diarization error rate, remainsAtutorialonevaluationmetricsforspeakerdiarizationsystems 3
the principal evaluation metric in this area which was introduced by National
InstituteofStandardsandTechnology(NIST)intheRichTranscriptions(RT)
evaluations1 in the year 2000. Jaccard error rate, a metric introduced for Sec-
ond DIHARD Diarization Challenge, 201923 that is based on the Jaccard
index (Ryant et al., 2019).
In this article, we review diﬀerent metrics used for the evaluation of SD
system. We mainly analyse two widely used evaluation metric known as DER
and JER, for synthetically prepared ground-truth and predicted output. We
discuss the limitations of the currently used evaluation metrics and brieﬂy
discusshowtodevelopanewreliablemetricfortheevaluationofSDsystems.
The rest of the paper is organized as follows. In Section 2, we present
a brief overview of state-of-the-art SD system. In Section 3, we present the
case studies with two evaluation metrics mainly DER and JER along with
some other clustering metrics. In Section 4, we prepared synthetic data in the
formofreferencegroundtruthandsystempredictedlabelsandcalculatedthe
DER and JER. And lastly, in Section 5, and Section 6 we will discuss the
limitations of DER and JER, and also give overview regarding development
of new evaluation metrics.
2 An overview of the state-of-the-art speaker diarization system
SD is one of the diﬀerent ways of processing done on audio signals (Anguera
et al., 2012). A SD system usually consists of several components. The ﬁrst
importantcomponentisavoiceactivitydetector (VAD)(MoattarandHomay-
ounpour, 2009), which separates the speech segments from the non-speech
segments in an audio-data. Then it applies a speech based segmentation tech-
nique to split the speech regions into diﬀerent small segments (Tritschler and
Gopinath, 1999). After segmentation speaker embeddings4 are extracted. The
state-of-the-artspeakerdiarizationsystemsrelyonspeakerembeddings(Cyrta
et al., 2017) for speaker similarity measure (Sell and Garcia-Romero, 2014).
Inthefollowingstep,itusesaclustering techniqueforclusteringthesegments
into disjoint speakers. Finally, re-segmentation is used for further frame-level
reﬁnementofspeakerdiarizationoutput(SellandGarcia-Romero,2015).Fig1
illustrates the diﬀerent components of the SD system.4 SupratimTribadyetal.
Input Speech
Voice Activity Detection
Input speech without silence
Segmentation
Overlapped or non-overlapped segments
Speaker Embeddings
x-vector or i-vector
Speaker Similarity Measure
Similarity matrix computed with PLDA
Clustering
A B C
AHC with threshold=1.5
Re-Segmentation
Diarization Output
A B C
Fig. 1 This ﬁgure tells about the standard SD structure with multiple modules. A raw
audio recording of a conversation is given as an input, after that speech part of the signal
is extracted or separated from the non-speech part of the audio signal. The speech part
of the audio signal is segmented into small segments from which speaker embeddings (i-
vectororx-vector)areextracted.Thespeakersimilarityismeasuredandcomputedfromthe
speakerembeddingsandﬁnally,basedonthesimilaritymeasurethespeakerembeddingsare
clusteredusingAgglomerativeHierarchicalclusteringwithathresholdof1.5whichassigns
similar speaker segments to a global speaker ID. After clustering, again it is re-segmented
andﬁnallyatimelineshowingdiarizationoutputaudioisfound.Atutorialonevaluationmetricsforspeakerdiarizationsystems 5
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec 9 sec
A A A A
B B B BB B
Ground Truth
C C C
D D D D D
P P P
Q Q Q Q Q
System Output
R R R R
S S S
P
Q Q Q
Speaker Error
R R R
S S
P
False Alarm Q
R
S
P
Missed Speech Q Q
S S S
Fig. 2 Synthetic ground truth and system predicted labels to illustrate diﬀerent types of
error.Heregreenspeakerlabelsindicatethereferencegroundtruth,yellowcolorspeakerla-
belsindicatessystempredictedspeakerlabelsandbluecolorspeakerlabelsindicatesspeaker
error,falsealarmandmissedspeech
3 Evaluation metrics for speaker diarization
DER and JER, are used to measure the performance of a SD system. For
speech regions, the diarization system speciﬁes the locations of speaker labels
toeachhomogeneoussegmentofspeech.DERandJERprovidesaconvenient
way to compare diﬀerent diarization approaches. The diﬀerence or error gen-
erated by a system in diarizing a speech, that is by comparing the error from
1 http://www.xavieranguera.com/phdthesis/node147.html#NIS rt eval plan 2006
2 https://signalprocessingsociety.org/publications-resources/data-challenges/second-
dihard-speech-diarization-challenge
3 DIHARDIIisthesecondinaseriesofdiarizationchallengesfocusingon”hard”diariza-
tion; that is, SD for challenging recordings where there is an expectation that the current
state-of-the-artwillfarepoorlyhttps://coml.lscp.ens.fr/dihard/index.html
4 Speaker embeddings are representation of speech segments created with deep neural
network.6 SupratimTribadyetal.
thegroundtruth(referencetruth)RichTranscriptionTimeMarked(RTTM),
and system predicted or system-generated RTTM using an Unpartioned For-
mat Evaluation (UEM) ﬁle, which is used to specify the scoring within each
recording. So, the motivation will be to decrease the DER and JER, in case
of output RTTM which will help to improve the SD system and to match
the relative speaker labels and location of speaker boundaries from reference
RTTMcomparedtooutputpredictedRTTM.Theevaluationmetricsaregen-
eratedwiththehelpofsomeﬁlessuchasreferenceRTTM,systemRTTMand
UEM ﬁles. The DER of the system can be over 100 %, whereas the JER of
the system cannot exceed over 100% (Anguera et al., 2005). The RTTM ﬁles
arespace-delimitedtextﬁlescontainingoneturnperline,eachlinecontaining
ten ﬁelds whereas UEM ﬁles are used to specify the scoring regions within
each audio recording. The UEM ﬁle contains a line with four space-delimited
ﬁelds for each scoring region. For speech regions, the diarization system spec-
iﬁes the locations of speaker labels, to each homogeneous segment of speech.
Computation of an error rate requires describing what are the errors present.
The various types of error in the evaluation metrics of the SD system are:
Speaker error: Speaker error corresponds to the percentage of scored time
thatareferencespeakerisassignedtoawrongspeakerintheoutputreference
speakerlabels.Speakererrorismainlyadiarizationsystemerror.Speakererror
is assigned within a speech region, and it does not account for speaker errors
in overlapping regions, or any other error coming from non-speech frames.
False alarm speech: False alarm speech corresponds to the percentage of a
scored time, that a non-speech part is incorrectly labelled as a speech region
in system-generated output.
Missed speech: Missed speech corresponds to the percentage of a scored
time,thataspeechpartisincorrectlylabelledasanon-speechpartinsystem-
generated output.
3.1 Diarization error rate
It is the most commonly used metric in the SD system. To compute DER, an
optimal one-to-one mapping of reference speakers to system output speakers
is determined. The DER is then the sum of the per speakers false alarm time,
miss time and speaker error time that is not matched to the reference speaker
divided by total speech time in an audio ﬁle. It is measured as the fraction of
time that is not attributed correctly to a speaker or non-speech.
ERROR+FA+MISS
DER= (1)
TOTAL
Here TOTAL refers to the duration of the union of reference and system
speaker segments and if the reference speaker was not paired with a system
speaker, it is the duration of all reference speaker segments.Atutorialonevaluationmetricsforspeakerdiarizationsystems 7
In Fig. 2, a synthetic speaker label has been generated to show the diﬀer-
ent errors generated in a SD system. In order to check the speaker mapping
between the reference speaker and system speaker output, DER uses Hun-
garian algorithm and Weighted-Bipartite graph matching algorithm. Using
Hungarian algorithm and Weighted-Bipartite graph matching we have found
thereferencespeakerAismappedwithsystemspeakerR,referencespeakerB
is mapped with system speaker P, reference speaker C is mapped with system
speakerSandreferencespeakerDismappedwithsystemspeakerQ.InTable
1, diﬀerent types of error that are generated in the synthetic speaker labels
are shown in Fig.2.
TimeFrame ReferenceSpeaker SystemOutput Error
0-1second A,C Q,S 1SpeakerError
1-2second A,B,D P,R 1MissedSpeech
2-3second A P,R 1FalseAlarm
3-4second B Q 1SpeakerError
4-5second B,C,D R 1SpeakerError,2MissedSpeech
5-6second B,D Q 1MissedSpeech
6-7second A,D Q,R NoError
7-8second B,C P,S NoError
8-9second D Q,S 1FalseAlarm
Table 1 Demonstration of diﬀerent types of error present in diﬀerent time frames for the
syntheticallyprepareddatainFig.2
3.2 Goodman-Kruskal tau (GKT)
GKT (Zarghami et al., 2009) is an unbalanced measure which was discovered
by Goodman and Kruskal in 1954. For a reference speaker label ’ref’ and a
system speaker label ’sys’, GKT(ref, sys) correlates to the fraction of change
in sys that can be explained by ref. Therefore, GKT(sys,ref) is 1 when ref is
exactlypredictivecomparedtosysandis0whenrefisnotpredictivecompared
to sys in system-output.
3.3 Conditional entropy
Anotherevaluationmetric,whichreportsfourinformationtheoreticmeasures.
– H(X—Y) : conditional entropy in bits of the reference speaker label when
system speaker label is present.
– H(Y—X) : conditional entropy in bits of the system speaker label when
reference speaker label is present.
– MI : mutual information in bits between reference and system speaker
labels.8 SupratimTribadyetal.
– NMI : normalized mutual information between the reference and system
speaker labels.
Here, X refers to the sequence of true frame-wise speaker labels whereas Y
referstothesequenceofhypothesizedspeakerlabels.NMIisbasicallyderived
from MI after being normalized in the interval between 0 to 1.
3.4 Purity, coverage and clustering metrics
ApartfromDERandJER,purity(Cettolo,2000)andcoverage(Gauvainetal.,
1998) also provide a convenient way to compare between systems of diﬀerent
diarization approaches. It is usually not suﬃcient to understand the type of
error executed by the system. To understand the type of error performed by
the system, purity and coverage play a key role to judge the behaviour of the
system. A fourth approach or evaluation metrics to check the performance of
the system uses both the reference and system output labels. Each recording
is converted to a sequence of 10 msec out of which is a single speaker label is
assigned to the following cases:
– frame containing no speech
– frame containing speech from a single speaker
– frame containing overlapping speech
B-cubedprecision,recall,andF1:TheB-cubedprecisionforasingleframe
assignedspeakerSinthereferencediarizationandCinthesystemdiarization
is the proportion of frames assigned C that are also assigned S. Similarly, the
B-cubed recall for a frame is the proportion of all frames assigned S that are
alsoassignedC.Theoverallprecisionandrecall,then,arejustthemeanofthe
frame-level precision and recall measures and the overall F-1 their harmonic
mean.
3.5 Speaker error rate
Whenspeechornon-speechsegmentsdonotplayanimportantroleintheex-
periment, then the standard Speaker error rate (SER) comes into play, which
does not include speech or non-speech errors. Speaker error rate (SER) corre-
sponds to the amount of scored time when a reference speaker in the ground
truth is mapped to a wrong speaker in the output speaker labels (Aronowitz,
2010). Speaker error rate (SER) is only assigned for speech regions, and it
does not account for speaker errors in the non-speech part. For the evalua-
tion of two-speaker segmentation task, Speaker error rate (SER) is computed
according to the standard NIST protocol 5.
5 http://www.itl.nist.gov/iad/mig/tests/sre/2002/SpkrSegEval-v07.plAtutorialonevaluationmetricsforspeakerdiarizationsystems 9
3.6 Jaccard error rate
In addition to the principal metric, the JER is based on the Jaccard Index,
whichisasimilaritymeasureusedtoevaluatetheoutputofspeakersegmenta-
tion.JERwasnewlyintroducedinthesecondDIHARDDiarizationChallenge
as another evaluation metric along with DER (Ryant et al., 2019). The JER
calculatesthemissedspeechandfalsealarmspeechforeachindividualspeaker.
An optimal mapping between reference speakers and system output speakers
is determined. The Jaccard index is computed, for each such speaker pairs.
The JER is deﬁned as 1 minus the average of these speaker pair scores.
More speciﬁcally, “N“ reference speakers and “M“ system speakers are as-
sumed from the ground truth and system predicted output. An optimal map-
pingbetweenspeakersisdeterminedusingtheHungarianalgorithm7 (Jonker
andVolgenant,1986)sothateachreferencespeakerispairedwithatmostone
system speaker and each system speaker with at most one reference speaker
(BellandDee,2016).Then,foreachreferencespeaker“ref“thespeaker-speciﬁc
Jaccard error rate is “(FA + MISS)/TOTAL“, where “TOTAL“ denotes the
durationoftheunionofreferenceandsystemspeakersegments;ifthereference
speakerwasnotpairedwithasystemspeaker,itisthedurationofallreference
speaker segments - “FA“ is the total system speaker time not attributed to
the reference speaker; if the reference speaker was not paired with a system
speaker, it is 0 - “MISS“ is the total reference speaker time not attributed
to the system speaker; if the reference speaker was not paired with a system
speaker, it is equal to “TOTAL“.The Jaccard error rate is the average of the
speaker-speciﬁc Jaccard error rate.
JER and DER are highly correlated with the JER typically being higher,
especially in recordings where one or more speakers is particularly dominant.
When a ith speaker from reference output corresponds to the jth speaker in
FA +Miss
JER = i i . (2)
i Union of ref +system
i j
1 N
X
Overall = JER (3)
JER N i
i=1
Here N refers to number of speakers present in the conversation.
4 Examples demonstrating the computation of evaluation metrics
In this section, we demonstrate with examples how evaluation metrics are
computed from the ground-truth and system predicted output. For better
understanding of the computation process, we show each intermediate steps.
We considered ﬁve diﬀerent examples as summarized in Table. All the speech
recordings are nine seconds in length.10 SupratimTribadyetal.
{#refspk,#sysspk}
Example1 {1,1}
Example2 {2,2}
Example3 {4,4}
Example4 {4,3}
Example5 {3,4}
Table 2 SummaryoftheﬁveexamplesforthecomputationofDERandJER.Hererefspk
denotesthenumberspeakersinreference(orground-truth)andsys denotesthenumber
spk
ofspeakersinsystemoutput.
4.1 Example 1
Inthisexample(asshowninFig.3),weshowhowtheDERandJERarecom-
puted for single speaker in both ground-truth and system predicted output.
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec 9sec
Ground Truth A A A A A A
System Output P P P P
Speaker Error
False Alarm P P
Missed Speech P P P P
Fig. 3 Syntheticgroundtruthandsystempredictedlabelsandillustrationdiﬀerenttypes
oferrorforExample 1.Thegreenboxesindicatethereferencegroundtruth,yellowboxes
indicatesystempredictedspeakerlabelsandcyanboxesindicatesspeakererror,falsealarm
andmissedspeech.Atutorialonevaluationmetricsforspeakerdiarizationsystems 11
Example 1
InordertocomputetheDER,weﬁrstneedtocomputethethreebasic
errors: speaker error, false alarm and missed speech as shown in Eq. 1.
In this case, we have no speaker error as the single speaker in ground-
truth (i.e., Speaker A) is paired with the single speaker in predicted
output(i.e.,SpeakerP).Weobservetwosecondsoffalsealarmdueand
four seconds missed speech as shown in Fig. 3. In this case, the total
amount of speech for ground-truth speaker is six seconds. Therefore,
the DER for Example 1 will be,
0+2+4
DER = ×100%=100%.
ex1 6
In JER computation, ﬁrst speaker correspondence between each of the
reference speakers and system output is computed with Hungarian al-
gorithm.ThenwecomputeindividualJERsforeachreferencespeakers
as shown in Eq. 3. Finally, overall JER is computed by taking average
oftheindividualJERs.Inthisexample,wehavesinglespeakerinboth
reference and system output. Therefore, the overall JER is computed
as,
FA +Miss 2+4
JER =JER = A A = ×100%=75%
ex1 A ∪(A,P) 812 SupratimTribadyetal.
4.2 Example 2
InFig.4, wedemonstratetheevaluationmetriccomputationfortwospeakers
inbothground-truthandsystempredictedoutputinarecordingof9seconds.
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec 9 sec
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec
A A A A A A
Ground Truth B B B B B B
P P P P P P
System Output
Q Q Q Q Q
Speaker Error
P
False Alarm
Q
P
Missed Speech
Q Q
Fig. 4 Syntheticgroundtruthandsystempredictedlabelsandillustrationdiﬀerenttypes
oferrorforExample 2.Thegreenboxesindicatethereferencegroundtruth,yellowboxes
indicatesystempredictedspeakerlabelsandcyanboxesindicatesspeakererror,falsealarm
andmissedspeech.Atutorialonevaluationmetricsforspeakerdiarizationsystems 13
Example 2
For Example 2, we observe two seconds of false alarm, two seconds of
missedspeech,andnospeakererrorasshowninFig.4.Wealsocompute
the total amount of speech spoken by two speakers in reference is 12
seconds. Therefore, we can compute DER as,
0+2+3
DER = ×100%=41.66%.
ex2 12
Now to compute the JER, we ﬁrst need to ﬁnd the speaker corre-
spondence. Using Hungarian algorithm, we have found that reference
speakerApairswithsystempredictedSpeakerPandreferencespeaker
B pairs with system predicted Speaker Q. Then, we can compute the
JERs of individual referene speakers as,
FA +Miss 1+1
JER = A A = ×100%=28.57%.
A ∪(A,P) 7
FA +Miss 2+1
JER = B B = ×100%=42.86%.
B ∪(B,Q) 7
Therefore, the overall JER will be,
1h i
JER = 28.57+42.86 ×100%=35.71%.
ex2 2
4.3 Example 3
InFig.5,wedemonstratetheevaluationmetriccomputationforfourspeakers
inbothground-truthandsystempredictedoutputinarecordingof9seconds.14 SupratimTribadyetal.
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec 9 sec
A A A A
B B
Ground Truth
C C
D
P
System Output Q Q Q
R R
S S S
Q
Speaker Error
R
S
R
False Alarm
S
P
Missed Speech
R
Fig. 5 Syntheticgroundtruthandsystempredictedlabelsandillustrationdiﬀerenttypes
oferrorforExample 3.Thegreenboxesindicatethereferencegroundtruth,yellowboxes
indicatesystempredictedspeakerlabelsandcyanboxesindicatesspeakererror,falsealarm
andmissedspeech.
Example 3
For Example 3, we observe two seconds of false alarm, two seconds of
missed speech, and three seconds of speaker error as shown in Fig. 5.
We also compute the total amount of speech spoken by four speakers
in reference is nine seconds. Therefore, we can compute DER as,
3+2+2
DER = ×100%=77.77%.
ex3 9
Now to compute the JER, we ﬁrst need to ﬁnd the speaker corre-
spondence. Using Hungarian algorithm, we have found that reference
speaker A pairs with system predicted Speaker P, reference speaker B
pairs with system predicted Speaker R, reference speaker C pairs with
systempredictedSpeakerQandreferencespeakerDpairswithsystem
predicted Speaker S. Then, we can compute the JERs of individual
reference speakers as,
FA +Miss 3+0 3
JER = A A = = ×100%=75.00%.
A ∪(A,P) 4 4
FA +Miss 2+2 4
JER = B B = = ×100%=100.00%.
B ∪(B,Q) 4 4
FA +Miss 0+1 1
JER = C C = = ×100%=33.33%.
C ∪(C,R) 3 3
FA +Miss 0+2 2
JER = D D = = ×100%=66.66%.
D ∪(D,S) 3 3
Therefore, the overall JER will be,
1
JER = [JER +JER +JER +JER ]. (4)
ex3 N A B C D
1
JER = [75.00+100.00+33.33+66.66]×100%=68.74%..
ex3 4Atutorialonevaluationmetricsforspeakerdiarizationsystems 15
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec 9 sec
A A A A
Ground Truth CB C B
D
P P
System Output Q Q
R R R
P
Speaker Error
False Alarm
R
P
Missed Speech
R
D
Fig. 6 Syntheticgroundtruthandsystempredictedlabelsandillustrationdiﬀerenttypes
oferrorforExample 4.Thegreenboxesindicatethereferencegroundtruth,yellowboxes
indicatesystempredictedspeakerlabelsandcyanboxesindicatesspeakererror,falsealarm
andmissedspeech
4.4 Example 4:
InFig.6,wedemonstratetheevaluationmetriccomputationforfourspeakers
inbothground-truthandsystempredictedoutputinarecordingof9seconds.16 SupratimTribadyetal.
Example 4
In Example 4, we observe one second of false alarm, three seconds of
missed speech and one second of speaker error as shown in Fig. 6. We
also compute the total amount of speech spoken by four speakers in
reference is 9 seconds. Therefore, we can compute the DER as,
3+1+1
DER = ×100%=55.56%.
ex4 9
Now to compute the JERs, we ﬁrst need to ﬁnd the speaker corre-
spondence. Using Hungarian algorithm, we have found that reference
speaker A pairs with system predicted speaker R, reference speaker B
pairs with system predicted speaker P, reference speaker C pairs with
system predicted speaker Q. Then, we can compute the JERs of indi-
vidual reference speakers as,
FA +Miss 2+1 3
JER = A A = = ×100%=60.00%.
A ∪(A,Q) 5 5
FA +Miss 1+1 2
JER = B B = = ×100%=66.667%.
B ∪(B,R) 3 3
FA +Miss 0+0 0
JER = C C = = ×100%=0.00%.
C ∪(C,P) 5 5
FA +Miss 0+1 1
JER = D D = = ×100%=100.00%.
D ∪(D,D) 1 1
So, the overall JER will be,
1
JER = [JER +JER +JER +JER ]. (5)
ex4 N A B C D
1
JER = [60.00+66.667+0.00+100.00]×100%=56.67%..
ex4 4
4.5 Example 5:
InFig.7,wedemonstratetheevaluationmetriccomputationforfourspeakers
inbothground-truthandsystempredictedoutputinarecordingof8seconds.Atutorialonevaluationmetricsforspeakerdiarizationsystems 17
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec
0 sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 sec
A A A
Ground Truth B
C C
P P
Q Q Q
System Output
R
S
P
Q
SpeakerError
False Alarm R
S
S
Missed Speech
Fig. 7 Syntheticgroundtruthandsystempredictedlabelsandillustrationdiﬀerenttypes
oferrorforExample 5.Thegreenboxesindicatethereferencegroundtruth,yellowboxes
indicatesystempredictedspeakerlabelsandcyanboxesindicatesspeakererror,falsealarm
andmissedspeech
Example 5
In Example 5, we observe one second of false alarm, two seconds of
missed speech and two seconds of speaker error as shown in Fig. 6. We
also compute the total amount of speech spoken by four speakers in
reference is 6 seconds. Therefore, we can compute the DER as,
2+2+1
DER = ×100%=83.33%.
ex5 6
Now to compute the JERs, we ﬁrst need to ﬁnd the speaker corre-
spondence. Using Hungarian algorithm, we have found that reference
speaker A pairs with system predicted speaker Q, reference speaker B
pairs with system predicted speaker S and reference speaker C pairs
with system predicted speaker P. Then, we can compute the JERs of
individual reference speakers as,
FA +Miss 1+1 2
JER = A A = = ×100%=50.00%.
A ∪(A,R) 4 4
FA +Miss 1+3 4
JER = B B = = ×100%=100.00%.
B ∪(B,Q) 4 4
FA +Miss 1+1 2
JER = C C = = ×100%=67.66%.
C ∪(C,P) 3 3
Here,SpeakerDisonlypresentinthesystemgeneratedspeakerlabels.
So, it will not be considered for calculation of JER in case of speaker
D. So, the overall JER will be,
1
JER = [JER +JER +JER ]. (6)
ex5 N A B C
1
JER = [50.00+100.00+67.66]×100%=72.22%..
ex5 418 SupratimTribadyetal.
5 Limitations of the existing evaluation metrics
DER and JER one of the current existing evaluation metrics in the ﬁeld of
SD.Therearevariousdrawbacksoftheexistingevaluationmetrics.According
to the formulae of the DER, the denominator part ”TOTAL” is the duration
of all reference speaker segments, but it does not include the system-speaker
segmentsforcalculationoftheerrorrate.Ifthereisadataimbalanceoftwoor
more speakers in terms of duration of the active speaker in a conversation in
referencespeakerlevel,thenirrespectiveofthesystem-generatedspeakerlevels
it will produce a good result which in turn will give less DER, which is not
correct will respect to the ground scenario. DER has no upper limit, as it can
exceed100%.AfterthatJERwasintroducedintheDIHARDIIchallenge,2019
whichalsohassomedrawbacks.ThedrawbackforupperlimitinDERissolved
intheJER,asitcannotexceedmorethan100%.TheJERisusedtocalculate
the error rate from the weighted average of each individual speaker present
in a conversation. It performs speaker correspondence using the Hungarian
algorithm.Butfromourexperiments,weseethatthesespeakercorrespondence
does not reﬂect actual speaker mapping for the calculation of JER during the
overlapping of more than two active speakers. DER and JER gives the overall
error rate of an audio ﬁle, but it does not provide the error rate of each
segment-wise speaker boundaries of each speaker, which might be helpful to
analyse and reduce the overall DER and JER of the entire audio ﬁle. Systems
which do not consider overlapping will always acknowledge the considerable
amountoferror.IgnoringtheoverlappingsdecreasesoverallJaccarderrorrate,
but it does not portray the actual scenario of the number of speakers present
in the conversation, and the actual identity of the speaker error, false alarm
and missed speech. In case of synthetically prepared data from Fig. 5, there
is an overlapping of speaker B and speaker C and in the system output only
speaker Q is present, but the system considered it as speaker R instead of
speaker Q. So, to get the minimum JER, the system is considering speaker
R instead of speaker Q. The main challenge of implementing the metric is
establishing the mapping between reference speaker ID and system-generated
speaker ID. These are the major limitations of the DER and JER.
6 Proposal for new evaluation metrics
One of the new ﬁndings from our experiments is the importance of using a
correctevaluationmetricforSDsystem.Theexistingevaluationmetricsorthe
evaluation tools are reaching the limits under certain conditions, so there is a
needtobuildandgeneralizenewmetricforevaluationandrebuilditbasedon
theirapplicationtomakethemusableunderthenewchallengesandconditions
along with making it comparable with the previous results. Though, it is very
diﬃcult to deﬁne a precise point in time boundaries about when a speaker
starts or stop, especially when overlapping speech is present, it is better to
buildanewstate-of-the-artevaluationmetricwhichwilldetectapropererrorAtutorialonevaluationmetricsforspeakerdiarizationsystems 19
duringthespeakeroverlappings.Anewevaluationmetricsshouldbedeveloped
to give segment-wise errors between speaker labels of reference ground truth
andsystem-generatedspeakerlabelstherebyproperlydetectingspeakererrors.
The evaluation metric should also detect errors during speaker overlappings,
such as speaker error, missed speech and false alarm and denoting it for the
respective speakers. The new evaluation metrics should calculate the DER
and JER with respect to the system generated speaker labels irrespective of
calculating it concerning to generate optimum DER and JER.
7 Conclusions
Following on from the previous study, we draw diﬀerent conclusions on the
evaluationmetricsforSDsystem.Duetotheincreasedusedofonlinemeetings,
and smart speakers, SD has become very important. The DER and JER is
stillarelevantevaluationmetricusedtomeasurethestandardofadiarization
system, with much more composite setup including:
– Cross-showdiarization,wherere-occurringspeakersinmultipleshowshave
to be acknowledged.
– Speaker overlappings, where multiple speakers speak concurrently.
More eminently, we described the implementation method along with the al-
gorithms to ensure a better understanding of the evaluation metric. These
evaluation metrics serve as an important parameter for checking the overall
performance of the system.
Appendix A: Speaker correspondence
HungarianAlgorithm:TheHungarianmethodisacombinationaloptimization
algorithm that solves the speaker correspondence assignment issue. In this
section, we explore the working of the Hungarian algorithm which is used to
computethebestmatchesbetweenground-truthspeakersequenceandsystem
output sequence of speakers. Here, we will demonstrate the calculation of
JER for speaker correspondence with the help of Hungarian algorithm for
Example 3 in the Fig. 5. The JER is calculated for all possible cases or
conditions. In our case, we calculate for all possible combinations thereby
mappingfromonespeakerinreferencespeakersequencetoanotherspeakerin
system-speaker sequence. After calculating the JER, we put the values in the
matrix corresponding to the speaker levels and then we go for the calculation
of the optimal value of JER.
In Example 3, there are four speakers in the ground-truth (i.e., Speaker
A,SpeakerB,SpeakerCandSpeakerD)andfoursystemspeakeroutput(i.e.
Speaker P, Speaker Q, Speaker R, and Speaker S). So, using the Hungarian
algorithmwe will explain the speaker correspondence in case of JER calcula-
tion. The matrix below shows the cost of assigning a speaker from reference20 SupratimTribadyetal.
Referencespeakersequence
SpeakerA
SpeakerB
SpeakerC
SpeakerD
Systemspeakersequence
SpeakerP
SpeakerQ
SpeakerR
SpeakerS
speaker level to a speaker in the system speaker level. The main objective is
to minimize the total JER in the system.
Speaker P Q R S
A 0.750 0.833 0.800 1.000
B 1.000 0.750 1.000 0.750
C 1.000 0.330 0.660 1.000
D 1.000 1.000 1.000 0.660
Step 1: Substraction of row minima from each row.
Speaker P Q R S
A 0.000 0.083 0.050 0.250
B 0.250 0.000 0.250 0.000
C 0.670 0.000 0.330 0.670
D 0.340 0.340 0.340 0.000
Step 2: Substraction of column minima from each column
Speaker P Q R S
A 0.000 0.083 0.000 0.250
B 0.250 0.000 0.2000 0.000
C 0.670 0.000 0.280 0.670
D 0.340 0.340 0.290 0.000
Step 3: Covering all the zero rows and columns with minimum number of
lines.
Speaker P Q R S
A 0.000 0.083 0.000 0.250
B 0.250 0.000 0.200 0.000
C 0.670 0.000 0.280 0.670
D 0.340 0.340 0.290 0.000
Step 4: Creating additional zeros in the matrix. For example, we ﬁnd the
smallest number from all uncovered rows and columns and substract it from
all uncovered elements and add it to all elements that are covered by boxes
twice.Atutorialonevaluationmetricsforspeakerdiarizationsystems 21
Speaker P Q R S
A 0.000 0.283 0.000 0.450
B 0.050 0.000 0.000 0.000
C 0.470 0.000 0.080 0.670
D 0.140 0.340 0.090 0.000
Now in order to cover all the minimum number of zero rows and columns
, we return to step 3.
Step 3: Again, covering all the rows and columns with minimum number
of zeros.
Speaker P Q R S
A 0.000 0.283 0.000 0.480
B 0.050 0.000 0.000 0.000
C 0.470 0.000 0.080 0.670
D 0.140 0.340 0.090 0.000
Now in order to cover all the minimum number of zero rows and columns
, we return to step 3.
Step 5: Therefore, the zeros in each row shows optimal assignment.
Speaker P Q R S
A 0.000 0.283 0.000 0.450
B 0.050 0.000 0.000 0.000
C 0.470 0.000 0.080 0.670
D 0.140 0.340 0.090 0.000
Step 6: Now, corresponding to the optimal matrix for cost function.
Speaker P Q R S
A 0.750 0.833 0.800 1.000
B 1.000 0.750 1.000 0.750
C 1.000 0.330 0.660 1.000
D 1.000 1.000 1.000 0.660
Hence, the optimal Jaccard error rate value will be:
1
JER = [0.750+1.000+0.330+0.660]×100%=68.73%.
min 4
(7)
Hence, using Hungarian algorithm we found the speaker correspondence
between reference speaker sequence and system-speaker sequence.
Speaker A →Speaker P
Speaker B →Speaker R
Speaker C →Speaker Q
Speaker D →Speaker S22 SupratimTribadyetal.
References
X. Anguera, C. Woofers, J. Hernando, Speaker diarization for multi-party
meetings using acoustic fusion, in IEEE Workshop on Automatic Speech
Recognition and Understanding, 2005., IEEE, 2005, pp. 426–431. IEEE
X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, O. Vinyals,
Speakerdiarization:Areviewofrecentresearch.IEEETransactionsonAu-
dio, Speech, and Language Processing 20(2), 356–370 (2012)
H. Aronowitz, Unsupervised Compensation of Intra-Session Intra-Speaker
Variability for Speaker Diarization., in Odyssey, 2010, p. 25
J. Bell, H.M. Dee, The subset-matched jaccard index for evaluation of seg-
mentation for plant images. arXiv preprint arXiv:1611.06880 (2016)
F. Bentley, C. Luvogt, M. Silverman, R. Wirasinghe, B. White, D. Lottridge,
Understanding the long-term use of smart speaker assistants. Proceedings
of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
2(3), 1–24 (2018)
M.Cettolo,Segmentation,classiﬁcationandclusteringofanItalianbroadcast
news corpus, in Proc. of RIAO, Citeseer, 2000. Citeseer
P. Cyrta, T. Trzcin´ski, W. Stokowiec, Speaker diarization using deep recur-
rentconvolutionalneuralnetworksforspeakerembeddings,inInternational
Conference on Information Systems Architecture and Technology, Springer,
2017, pp. 107–117. Springer
T.T.Elvins,R.T.Fassett,P.Shinn,System and method for gathering, person-
alized rendering, and secure telephonic transmission of audio data (Google
Patents, 2003). US Patent 6,529,586
O.Galibert,Methodologiesfortheevaluationofspeakerdiarizationandauto-
matic speech recognition in the presence of overlapping speech., in INTER-
SPEECH, 2013, pp. 1131–1134
K.W. Gamage, V. Sethu, E. Ambikairajah, Salience based lexical features for
emotion recognition, in 2017 IEEE international conference on acoustics,
speech and signal processing (ICASSP), IEEE, 2017, pp. 5830–5834. IEEE
J.-L. Gauvain, L.F. Lamel, G. Adda, Partitioning and transcription of broad-
castnews data, in Fifth International Conference on Spoken Language Pro-
cessing, 1998
I.Himawan,M.H.Rahman,S.Sridharan,C.Fookes,A.Kanagasundaram,In-
vestigating deep neural networks for speaker diarization in the dihard chal-
lenge, in 2018 IEEE Spoken Language Technology Workshop (SLT), IEEE,
2018, pp. 1029–1035. IEEE
R. Jonker, T. Volgenant, Improving the hungarian assignment algorithm. Op-
erations Research Letters 5(4), 171–175 (1986)
Jyh-Min Cheng, Hsiao-Chuan Wang, A method of estimating the equal error
rateforautomaticspeakerveriﬁcation,in2004 International Symposium on
Chinese Spoken Language Processing, 2004, pp. 285–288
T. Kinnunen, H. Delgado, N. Evans, K.A. Lee, V. Vestman, A. Nautsch, M.
Todisco, X. Wang, M. Sahidullah, J. Yamagishi, et al., Tandem assessment
ofspooﬁngcountermeasuresandautomaticspeakerveriﬁcation:Fundamen-Atutorialonevaluationmetricsforspeakerdiarizationsystems 23
tals. IEEE/ACM Transactions on Audio, Speech, and Language Processing
(2020)
Q.Kong,Y.Xu,I.Sobieraj,W.Wang,M.D.Plumbley,Soundeventdetection
and time–frequency segmentation from weakly labelled data. IEEE/ACM
Transactions on Audio, Speech, and Language Processing 27(4), 777–787
(2019)
A.Mieczakowski,J.Goodman-Deane,J.Patmore,J.Clarkson,Conversations,
conferencing and collaboration
M.H. Moattar, M.M. Homayounpour, A simple but eﬃcient real-time voice
activity detection algorithm, in 2009 17th European Signal Processing Con-
ference, IEEE, 2009, pp. 2549–2553. IEEE
N.Ryant,K.Church,C.Cieri,A.Cristia,J.Du,S.Ganapathy,M.Liberman,
First dihard challenge evaluation plan. 2018, tech. Rep. (2018)
N.Ryant,K.Church,C.Cieri,A.Cristia,J.Du,S.Ganapathy,M.Liberman,
Theseconddiharddiarizationchallenge:Dataset,task,andbaselines.arXiv
preprint arXiv:1906.07839 (2019)
G. Sell, D. Garcia-Romero, Speaker diarization with PLDA i-vector scoring
and unsupervised calibration, in 2014 IEEE Spoken Language Technology
Workshop (SLT), IEEE, 2014, pp. 413–417. IEEE
G. Sell, D. Garcia-Romero, Diarization resegmentation in the factor analysis
subspace, in 2015 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), IEEE, 2015, pp. 4794–4798. IEEE
J.Song,Y.Guo,L.Gao,X.Li,A.Hanjalic,H.T.Shen,Fromdeterministicto
generative: Multimodal stochastic rnns for video captioning. IEEE transac-
tions on neural networks and learning systems 30(10), 3047–3058 (2018)
U. Tiwary, T. Siddiqui, Natural language processing and information retrieval
(Oxford University Press, Inc., ???, 2008)
A. Tritschler, R.A. Gopinath, Improved speaker segmentation and segments
clustering using the bayesian information criterion, in Sixth European Con-
ference on Speech Communication and Technology, 1999
M. Valenti, A. Diment, G. Parascandolo, S. Squartini, T. Virtanen, DCASE
2016 acoustic scene classiﬁcation using convolutional neural networks, in
Proc. Workshop Detection Classif. Acoust. Scenes Events, 2016, pp. 95–99
R. Vipperla, J.T. Geiger, S. Bozonnet, D. Wang, N. Evans, B. Schuller, G.
Rigoll, Speech overlap detection and attribution using convolutive non-
negative sparse coding, in 2012 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), IEEE, 2012, pp. 4181–4184.
IEEE
D.E. Wachob, Method and apparatus for providing demographically targeted
television commercials (Google Patents, 1992). US Patent 5,155,591
S. Watanabe, M. Mandel, J. Barker, E. Vincent, Chime-6 challenge: Tackling
multispeakerspeechrecognitionforunsegmentedrecordings.arXivpreprint
arXiv:2004.09249 (2020)
L.D. Wilcox, D.G. Kimber, Unsupervised speaker clustering for automatic
speaker indexing of recorded audio data (Google Patents, 1997). US Patent
5,659,66224 SupratimTribadyetal.
A. Zarghami, S. Fazeli, N. Dokoohaki, M. Matskin, Social trust-aware recom-
mendation system: A t-index approach, in 2009 IEEE/WIC/ACM Interna-
tional Joint Conference on Web Intelligence and Intelligent Agent Technol-
ogy, vol. 3, IEEE, 2009, pp. 85–90. IEEE