The Second DIHARD Diarization Challenge: Dataset, task, and baselines
Neville Ryant1, Kenneth Church2, Christopher Cieri1, Alejandrina Cristia3, Jun Du4, Sriram
Ganapathy5, Mark Liberman1
1Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA, USA
2Baidu Research, Sunnyvale, CA, USA
3Laboratoire de Sciences Cognitives et de Psycholinguistique, De´pt d’e´tudes cognitives, ENS,
EHESS, CNRS, PSL University, Paris, France
4University of Science and Technology of China, Hefei, China
5Electrical Engineering Department, Indian Institute of Science, Bangalore, India
nryant@ldc.upenn.edu
9
1
0 Abstract backchannels and overlapping speech are both common in con-
2
versation, this may have resulted in an over-optimistic assess-
n  This paper introduces the second DIHARD challenge, the sec- ment of performance even within these domains1 [11].
ond in a series of speaker diarization challenges intended to
u It is against this backdrop that the JSALT-2017 workshop
improve the robustness of diarization systems to variation in
J [12] and DIHARD challenges2 emerged. The DIHARD series
  recording equipment, noise conditions, and conversational do-
8 of challenges introduce a new common task for diarization that
main. The challenge comprises four tracks evaluating diariza-
1 is intended both to facilitate comparison of current and future
tion performance under two input conditions (single channel
  systems through standardized data, tasks, and metrics and pro-
  vs. multi-channel) and two segmentation conditions (diariza-
] mote work on robust diarization systems; that is systems, that
S tion from a reference speech segmentation vs. diarization from
are able to accurately handle highly interactive and overlapping
A scratch). In order to prevent participants from overtuning to a speech from a range of conversational domains, while being re-
particular combination of recording conditions and conversa-
. silient to variation in recording equipment, recording environ-
s tional domain, recordings are drawn from a variety of sources
ment, reverberation, ambient noise, number of speakers, and
s ranging from read audiobooks to meeting speech, to child lan-
e speaker demographics. As with the NIST RT evaluations, DER
guage acquisition recordings, to dinner parties, to web video.
e is adopted as the primary evaluation metric, but without use of
We describe the task and metrics, challenge design, datasets,
[ collars or exclusion of overlapping speech. There are no con-
  and baseline systems for speech enhancement, speech activity
  straints on training data, with participants allowed to use any
1 detection, and diarization.
combination of public/proprietary data for system development.
v Index Terms: speaker diarization, speaker recognition, robust
The initial DIHARD challenge (DIHARD I) [13] ran during
9 ASR, noise, conversational speech, DIHARD challenge
the spring of 2018 and attracted registrations from 20 teams, of
3
8 1. Introduction which 13 submitted systems. As expected, state-of-the-art sys-
7 tems performed poorly, with ﬁnal DER on the evaluation set for
0 Speaker diarization, often referred to as “who spoke when”, the top systems ranging from 23.73% [14] when provided with
. is the task of determining how many speakers are present in reference speech activity detection (SAD) marks to 35.51% [15]
6
when forced to perform diarization from scratch. These error
a conversation and correctly identifying all segments for each
0
rates rates are more than double the state-of-the-art for CALL-
9 speaker. In addition to being an interesting technical chal-
1 lenge, it forms an important part of the pre-processing pipeline HOME [16] at the time [4, 5]. For some domains, error rates
: for speech-to-text and is essential for making objective mea- for the best systems exceeded 49% when using reference SAD
v surements of turn-taking behavior. Early work in this area and 75% when performing diarization from scratch!
Xi was driven by the NIST Rich Transcription (RT) evaluations The second DIHARD Challenge (DIHARD II) [17], like
[1], which ran between 2002 and 2009. In addition to driving its predecessor, examines diarization system performance un-
ar substantial performance improvements, especially for meeting der two SAD conditions: diarization from a supplied refer-
speech, the RT evaluations introduced the diarization error rate ence SAD and diarization from scratch. As with DIHARD I,
(DER) metric, which remains the principal evaluation metric in it includes a single channel input condition utilizing wideband
this area. Since the RT evaluation series ended in 2009, diariza- speech sampled from 11 demanding domains, ranging from
tion performance has continued to improve, though the lack clean, nearﬁeld recordings of read audiobooks to extremely
of a common task has resulted in fragmentation with individ- noisy, highly interactive, farﬁeld recordings of speech in restau-
ual research groups focusing on different datasets or domains rants to child language data recorded in the home using LENA
(e.g., conversational telephone speech [2, 3, 4, 5, 6], broadcast vests. Unlike DIHARD I, it additionally offers a multichan-
[7, 8], or meeting [9, 10]). At best, this has made comparing nel input condition requiring participants to perform diarization
performance difﬁcult, while at worst it may have engendered from farﬁeld microphone arrays of dinner party speech drawn
overﬁtting to individual domains/datasets resulting in systems
1See, for instance, the release of IBM’s diarization API in 2017.
that do not generalize. Moreover, the majority of this work has
The feature worked well for simple cases, but when run by users on
evaluated systems using a modiﬁed version of DER in which
real inputs, the performance was found to be lacking, especially for
speech within 250 ms of reference boundaries and overlapped overlaps, back-channels, and short turns.
speech are excluded from scoring. As short segments such as 2https://coml.lscp.ens.fr/dihard/index.htmlfrom the CHiME-5 corpus [18]. For the ﬁrst time, we also Table 1: Overview of DIHARD II datasets. For the CHiME-
provide participants with baseline systems for speech enhance- 5 (multichannel) data, each Kinect is treated as a separate
ment, SAD, and diarization, as well as results obtained with recording.
these systems for all tracks.
Input condition Set Duration (hours) # Recordings
2. Tracks single channel dev 23.81 192
eval 22.49 194
The challenge features two audio input conditions: dev 262.41 105
multichannel
eval 31.24 12
• Single channel – Systems are provided with a single
channel of audio for each recording. Depending on the
recording source, this channel may be taken from a sin-
gle distant microphone, a single channel from a distant speaker is paired with a system speaker with an identical seg-
microphone array, a mix of head-mounted or array mi- mentation to 100% in the case where none of the system speak-
crophones, or a mix of binaural microphones. ers overlap any of the reference speakers.
• Multichannel – Each recording session contains output FA + MISS
from one or more distant microphone arrays, each con- JERref = TOTAL (1)
taining multiple channels. Participants are instructed to
treat the arrays separately, producing one output per ar- All metrics are computed using version 1.0.1 of the dscore
ray. They are free to use as few or as many of the chan- tool3 without the use of forgiveness collars and with scoring of
nels on each array as they wish to perform diarization. overlapped speech.
As system performance is strongly tied to the quality of the
4. Datasets
SAD component, we also include two SAD conditions:
• Reference SAD – Systems are provided with a refer- 4.1. Overview
ence speech segmentation that is generated by merging
The DIHARD II development and evaluation sets draw from
speaker turns in the reference diarization.
a diverse set of sources exhibiting wide variation in recording
• System SAD – Systems are provided with just the raw equipment, recording environment, ambient noise, number of
audio input for each recording session and are responsi- speakers, and speaker demographics. The single channel input
ble for producing their own speech segmentation. condition (tracks 1 and 2) dataset is a superset of that used in
DIHARD I, though 6 hours of additional material have been
Together, this yields the following four evaluation tracks:
added to ensure that all domains are represented in both the de-
• Track 1 – single channel audio using reference SAD velopment and evaluation set. Additionally, two domains where
• Track 2 – single channel audio using system SAD the DIHARD I annotation was deemed suspect (child language
and web video) have been entirely resegmented. For the multi-
• Track 3 – multichannel audio using reference SAD
channel input condition (tracks 3 and 4) we use the multi-party
• Track 4 – multichannel audio using system SAD dinner recordings originally collected for and exposed during
the CHiME-5 challenge [18]. The development and evaluation
All teams are required to register for at least one of track 1 or
sets are summarized in Table 1.
track 3.
The development set includes reference diarization and
speech segmentation and may be used for any purpose includ-
3. Performance Metrics
ing system development or training. As with DIHARD I, there
As in DIHARD I, the primary metric is DER [1], which is the is no training set, with participants free to train their systems on
sum of missed speech, false alarm speech, and speaker mis- any proprietary and/or public data. Both the development and
classiﬁcation error rates. Because systems are provided with evaluation sets will be submitted for publication via LDC at the
the reference speech segmentation for tracks 1 and 3, for these end of the evaluation.
tracks, it exclusively measures speaker misclassiﬁcation error.
This is the metric used to rank systems on the leaderboard. 4.2. Single channel data (tracks 1 and 2)
For each system we also compute a secondary metric, Jac-
The single channel input condition development and evaluation
card error rate (JER), which is newly developed for DIHARD sets consist of selections of 5-10 minute duration samples drawn
II. JER is based on the Jaccard similarity index [19, 20], a met- from 11 conversational domains, each including approximately
ric commonly used to evaluate the output of image segmenta- 2 hours of audio. The full set of domains is described below
tion systems, which is deﬁned as the ratio between the sizes of
with LDC Catalog numbers where appropriate. Unless other-
the intersections and unions of two sets of segments. An opti- wise speciﬁed, all speech is English, though not necessarily by
mal mapping between speakers in the reference diarization and native or even ﬂuent speakers. All audio is distributed via LDC
speakers in the system diarization is determined and for each as 16 kHz, monochannel FLAC ﬁles.
pair the Jaccard index of their segmentations is computed. JER
is deﬁned as 1 minus the average of these scores, expressed as • audiobooks – amateur recordings of public domain En-
a percentage. That is, it is the mean of Eq. 1 across all refer- glish works drawn from LibriVox; care was taken to
ence speakers ref, where TOTAL is the duration of the union of avoid overlap with LibriSpeech [21] (unpublished)
reference and system speaker segments, FA is the total system • broadcast interview – student produced interviews with
speaker time not attributed to the reference speaker, and MISS newsmakers of the day taken from a late 1970s college
is the total reference speaker time not attributed to the system
speaker. It ranges from 0% in the case where each reference 3https://github.com/nryant/dscoreradio show; recorded on open reel tapes before being dinner parties from 18 homes. The evaluation set is identical
digitized and contributed to LDC (unpublished) to the CHiME-5 evaluation set and consists of 5 hours of din-
• child language – day-long recordings of 6-18 month ner parties from 2 homes. Each party was recorded using 6
old vocalizations collected at home by University of Microsoft Kinect devices (4 channel linear arrays) distributed
Rochester researchers for the SEEDLingS corpus [22] throughout the home in such a way that the conversation was al-
ways present on each array. Due to a combination of clock drift
• clinical – interviews with 12-16 year old children in-
and random frame dropping, the Kinects within each record-
tended to determine whether or not they ﬁt the clinical di-
ing session exhibit massive desynchronization, both with each
agnosis for autism; all recordings conducted at the Cen-
other and with the binaural recording devices worn by partici-
ter for Autism Research (CAR) of the Childrens Hospital
pants. For this reason, each Kinect device is treated separately
of Philadelphia (CHOP) using a mixture of cameras and
with the resulting development and evaluation sets having du-
ceiling mounted microphones (unpublished)
rations of 262.4 hours and 31.2 hours respectively. All audio is
• courtroom – oral arguments from the 2001 term of the distributed via the University of Shefﬁeld as 16 kHz WAV ﬁles.
U.S. Supreme Court that were digitized for the OYEZ
project; recordings are summed from individual table- 4.4. Processing
mounted microphones, one per speaker (unpublished)
A limited number of recordings contained regions carrying per-
• map task – recordings of map tasks in which one par-
sonal identifying information (PII), which were removed prior
ticipant, the leader, describes a route drawn on a map
to publication. For the clinical and restaurant domains, this
to the other participant, the follower, who attempts to
was done at LDC by low-pass ﬁltering using a 10th order But-
draw the same route on a copy of the map lacking the
terworth ﬁlter with a passband of 0 to 400 Hz. To avoid abrupt
route and optionally lacking some landmarks; audio was
transitions in the resulting waveform, the effect of the ﬁlter was
recorded via close-talking microphones under quiet con-
gradually faded in and out at the beginning and end of the re-
ditions (previously released as LDC96S38)
gions using a ramp of 40 ms. In the case of the sociolinguis-
• meeting – meetings with between 3 and 7 participants, tic ﬁeld recordings domain and the CHiME-5 data, PII was re-
each recorded with a variety of close-talking and distant moved by the original creators of the corpora. In the former
microphones, from which a single, centrally located dis- case, PII was replaced by tones of matched duration, while in
tant microphone was selected; the development set draws the latter case it was zeroed out. PII containing regions are ig-
from the NIST Spring 2004 Rich Transcription Evalua- nored during scoring.
tion (LDC2007S11 and LDC2007S12) while the evalu-
ation set draws from previously upublished recordings
4.5. Annotation
conducted for the DARPA Robust Omnipresent Auto-
matic Recognition (ROAR) project at LDC in 2001 Reference segmentation and speaker labeling was produced by
• restaurant – ≈1 hour sessions involving 3-6 diners annotators at LDC using a tool equipped with playback, wave-
form and spectrogram display. Annotators were instructed to
recorded on a binaural microphone worn by one partici-
split on pauses > 200 ms, where a pause was deﬁned as any
pant in restaurants with varying room acoustics and noise
stretch of time during which the speaker was not producing vo-
levels; inspired by the NSF Hearables Challenge and ex-
calization (e.g., backchannels, ﬁlled pauses, singing, speech er-
tended by LDC for DIHARD (unpublished)
rors and disﬂuencies, infant babbling or vocalizations, laughter,
• sociolinguistic ﬁeld recordings – sociolinguistic inter-
coughs, breaths, lipsmacks, and humming) of any kind. Bound-
views recorded under ﬁeld conditions during the 1960s
aries were placed within 10 ms of the true boundary, taking care
and 1970s; recorded under diverse locations and condi-
not to truncate sounds at edges of words (e.g., utterance-ﬁnal
tions with subjects ranging from 15 to 81 years of age
fricatives). Where individual close talking microphones were
and representing diverse ethnicities, backgrounds, and
available for speakers, annotation was performed separately for
dialects of world English; the development set draws
each speaker using their individual microphone. Due to time
from SLX (LDC2003T15) and the evaluation set from
constraints, this manual segmentation process could not be im-
DASS (LDC2012S03 & LDC2016S05)
plemented for the multichannel development data; for this data,
• sociolinguistic lab recordings – sociolinguistic inter- segmentation was taken from the turn boundaries established
views recorded as part of MIXER6 (LDC2013S03) un- during the original CHiME-5 transcription.
der quiet conditions in a controlled environment; ses- An additional post-processing step was necessary for the
sions were recorded with a variety of close-talking and CHiME-5 annotation to correct for the lack of synchroniza-
distant microphones from which a single, centrally lo- tion between binaural recording devices and Kinects. For each
cated distant microphone was selected Kinect, the lag between that array and the binaural recording de-
• web video – English and Mandarin amateur videos col- vices was estimated at regular intervals using normalized cross-
lected from online sharing sites (e.g., YouTube and correlation. The speech boundaries etablished by annotation on
Vimeo) as part of the Video Annotation for Speech Tech- the binaural devices were then corrected for each Kinect using
nologies (VAST) [23] collection (mostly unpublished) these estimated lags.
4.3. Multichannel data (tracks 3 and 4) 5. Baseline system
The multichannel input condition development and evaluation
5.1. Speech enhancement
sets are drawn from the CHiME-5 dinner party corpus [18], a
corpus of conversational speech collected during dinner parties For speech enhancement we use a densely-connected LSTM
held in real homes. The development set combines the CHiME- architecture [24, 25, 26] trained to predict the ideal ratio
5 training and development sets and encompasses 45 hours of masks (IRM) [27] of speech from log-power spectra (LPS) fea-tures. The model is trained via progressive multi-target learning Table 2: Baseline performance (measured by DER and JER)
[24, 28] using 400 hours of noisy speech produced by corrupt- on dev and eval sets for all tracks. The Enh. column indicates
ing clean utterances from WSJ0 [29] and a 50 hour Chinese whether or not speech enhancement was applied prior to SAD.
speech corpus from the 863 Program [30]. Utterances were cor-
rupted using 115 noise types [24] at 3 SNR levels (-5dB, 0dB, Track Enh. DER (%) JER (%)
and 5dB). The trained models as well as scripts for applying Dev Eval Dev Eval
them, are distributed through GitHub4. Track 1 no 23.70 25.99 56.20 59.51
Track 2 no 46.33 50.12 69.26 72.1
Track 2 yes 38.26 40.86 62.59 66.60
5.2. Beamforming
Track 3 no 59.73 50.85 68.00 65.91
For the multichannel tracks, we use weighted delay-and-sum Track 4 no 87.55 83.41 88.08 85.12
beamforming as implemented in BeamformIt [31]. Beamform- Track 4 yes 82.49 77.34 83.6 80.42
ing is applied independently for each Kinect in each session
using all four channels following the CHiME-5 recipe [18].
5.5. Baseline results
5.3. Speech activity detection
DER and JER of the baseline system on both the development
The baselines for tracks 2 and 4 use WebRTC’s5 SAD as imple- and evaluation sets for each track are presented in Table 2. The
mented in the py-webrtc Python package6. Scripts for perform- speech enhancement module is used only for tracks 2 and 4 as
a pre-processing front-end for the SAD pipeline as the diariza-
ing SAD using the same settings used to obtain the baseline
results are distributed through GitHub4. tion system did not show improvements using the enhanced au-
dio. The scores obtained by the challenge baseline are quite
high, with track 1 DER roughly in line with the performance of
5.4. Diarization
the best DIHARD I systems [14, 15, 25] and track 2 DER 5%
higher than for DIHARD I (15% without enhancement), which
The diarization baseline is based on the previously published
Kaldi [32] recipe7 for JHU’s submission to DIHARD I [14]. At we suspect reﬂects a combination of superior SAD components
in those systems and the more careful segmentation for the child
a high level, the system performs diarization by dividing each
language and web video domains in DIHARD II. Error rates
recording into short overlapping segments, extracting x-vectors
are noticeably higher for tracks 3 and 4, reaching 50.85% and
[33, 34], scoring with probabilistic linear discriminant analysis
77.34% respectively, though, again, these rates are roughly in
(PLDA) [35], and clustering using agglomerative hierarchical
line with those observed for the best DIHARD I systems on
clustering (AHC) [36]. In contrast to the original JHU system,
the two most difﬁcult domains in that challenge: restaurant and
we omit the Variational Bayes resegmentation step [37]. The
trained models are distributed through GitHub8. child language.
The x-vector extractor conﬁguration is identical to that used
6. Conclusion
in previous speaker recognition and diarization systems [34, 14]
with two exceptions: i) 30 dimensional mel frequency cepstral The ﬁeld of speaker diarization has changed drastically in the
coefﬁcient (MFCC) features are used instead of mel ﬁlterbank two short years we have been running this challenge. In the lead
features; ii) the embedding layer uses 512 dimensions. MFCCs up to DIHARD I, the research community was fragmented and
are extracted every 10 ms using a 25 ms window and mean- most research concentrated on relatively easy datasets using for-
normalized using a 3 second sliding window. For training we giving evaluation metrics. This both made comparison of sys-
use a combination of VoxCeleb 1 and VoxCeleb 2 [38, 39] aug- tems difﬁcult and led some to believe that diarization was rela-
mented with additive noise and reverberation according to the tively solved and uninteresting. However, we were pleased by
recipe from [33]. Segments under 4 seconds duration are dis- the response to DIHARD I, both during the evaluation and after,
carded, resulting in a training set with 7,323 speakers. Rever- demonstrating that there is interest in robust diarization. This
beration is added by convolution with room responses from the renewed energy is on display in DIHARD II, which attracted
RIR dataset [40], while additive noises are drawn from the MU- 48 registered teams from 17 countries, more than doubling the
SAN dataset [41]. At test time, x-vectors are extracted from 1.5 number of teams registered for DIHARD I. It is also evident in
second segments with 0.75 second overlap. the recent announcement of the Fearless Steps challenge, which
Following extraction, x-vectors are pre-processed to per- includes diarization among its tasks. We hope that this year’s
form domain adaptation to the DIHARD II dataset. This is done contributions lead to marked progress toward the goal of truly
by normalizing with a global mean and whitening transform robust diarization.
learned from the DIHARD II development set. The whitened
x-vectors are then length normalized [42] and used to train a 7. Acknowledgements
Gaussian PLDA model [35] using a subset of VoxCeleb consist-
ing of segments of at least 3 seconds duration. Following PLDA We would like to thank Harshah Vardhan MA, Prachi Singh,
and Lei Sun for their help in preparing the baseline sytems
scoring, clustering is performed using AHC with the threshold
and results. We would also like to acknowledge the gener-
set by minimizing DER on the development data.
ous support of Agence Nationale de la Recherche (ANR-16-
DATA-0004 ACLEW, ANR-14-CE30-0003 MechELex, ANR-
4https://github.com/staplesinLA/denoising_DIHARD18 17-EURE-0017), the J. S. McDonnell Foundation, and the Lin-
5https://webrtc.org/ guistic Data Consortium as well as the CHiME-5 challenge for
6https://github.com/wiseman/py-webrtcvad allowing us use of their data.
7https://github.com/kaldi-asr/kaldi/tree/master/egs/dihard_2018/v2
8https://github.com/iiscleap/DIHARD_2019_baseline_alltracks8. References
[21] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
riSpeech: an ASR corpus based on public domain audio books,”
[1] J. G. Fiscus, J. Ajot, M. Michel, and J. S. Garofolo, “The Rich
in Proc. ICASSP, 2015, pp. 5206–5210.
Transcription 2006 Spring Meeting Recognition Evaluation,” in
International Workshop on Machine Learning for Multimodal In- [22] E. Bergelson, “Bergelson Seedlings HomeBank Corpus,” 2016,
teraction. Springer, 2006, pp. 309–322. doi:10.21415/T5PK6D.
[23] J. Tracey and S. Strassel, “VAST: A corpus of video annotation
[2] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA
for speech technologies,” in Proc. LREC, 2018.
i-vector scoring and unsupervised calibration,” in Proc. IEEE Spo-
ken Language Technology Workshop (SLT), 2014, pp. 413–417. [24] T. Gao, J. Du, L.-R. Dai, and C.-H. Lee, “Densely connected pro-
gressive learning for LSTM-based speech enhancement,” in Proc.
[3] W. Zhu and J. Pelecanos, “Online speaker diarization using
ICASSP, 2018, pp. 5054–5058.
adapted i-vector transforms,” in Proc. ICASSP, 2016.
[25] L. Sun, J. Du, C. Jiang, X. Zhang, S. He, B. Yin, and C.-H.
[4] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree, Lee, “Speaker diarization with enhancing speech for the First DI-
“Speaker diarization using deep neural network embeddings,” in HARD Challenge,” Proc. Interspeech, pp. 2793–2797, 2018.
Proc. ICASSP, 2017, pp. 4930–4934.
[26] L. Sun, J. Du, T. Gao, Y.-D. Lu, Y. Tsao, C.-H. Lee, and N. Ryant,
[5] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, and I. L. Moreno, “A novel LSTM-based speech preprocessor for speaker diariza-
“Speaker diarization with LSTM,” in Proc. ICASSP, 2018, pp. tion in realistic mismatch conditions,” in Proc. ICASSP, 2018, pp.
5239–5243. 5234–5238.
[6] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully su- [27] S. Srinivasan, N. Roman, and D. Wang, “Binary and ratio time-
pervised speaker diarization,” Proc. ICASSP, 2019. frequency masks for robust speech recognition,” Speech Commu-
nication, vol. 48, no. 11, pp. 1486–1501, 2006.
[7] M. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and
S. Meignier, “An open-source state-of-the-art toolbox for broad- [28] L. Sun, J. Du, L.-R. Dai, and C.-H. Lee, “Multiple-target deep
cast news diarization,” in Proc. Interspeech, 2013, pp. 1477–1481. learning for LSTM-RNN based speech enhancement,” in Proc.
HSCMA, 2017, pp. 136–140.
[8] I. Vin˜als, A. Ortega, J. A. V. Lo´pez, A. Miguel, and E. Lleida,
“Domain adaptation of PLDA models in broadcast diarization by [29] J. S. Garofolo et al., CSR-I (WSJ0) Complete LDC93S6A.
means of unsupervised speaker clustering.” in Proc. Interspeech, Philadelphia: Linguistic Data Consortium, 1993.
2017, pp. 2829–2833. [30] Y. L. Qian, S. X. Lin, Y. D. Zhang, Y. Liu, H. Liu, and Q. Liu,
“An introduction to corpora resources of 863 program for Chinese
[9] S. H. Yella and H. Bourlard, “Improved overlap speech diarization
language processing and human-machine interaction,” Proc. ALR,
of meeting recordings using long-term conversational features,” in
2004.
Proc. ICASSP, 2013, pp. 7746–7750.
[31] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamform-
[10] S. H. Yella, A. Stolcke, and M. Slaney, “Artiﬁcial neural network
ing for speaker diarization of meetings,” IEEE Trans. Audio,
features for speaker diarization,” in Proc. IEEE Spoken Language
Speech, Language Process, vol. 15, no. 7, pp. 2011–2022, 2007.
Technology Workshop, 2014, pp. 402–406.
[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
[11] R. Milner and T. Hain, “Segment-oriented evaluation of speaker
N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al.,
diarisation performance,” in Proc. ICASSP, 2016, pp. 5460–5464.
“The Kaldi speech recognition toolkit,” IEEE Signal Processing
[12] N. Ryant, E. Bergelson, K. Church, A. Cristia, J. Du, S. Ganap- Society, Tech. Rep., 2011.
athy, S. Khudanpur, D. Kowalski, M. Krishnamoorthy, R. Kul- [33] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
shreshta et al., “Enhancement and analysis of conversational Y. Carmiel, and S. Khudanpur, “Deep neural network-based
speech: JSALT 2017,” in Proc. ICASSP, 2018, pp. 5154–5158. speaker embeddings for end-to-end speaker veriﬁcation,” in 2016
[13] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, IEEE Spoken Language Technology Workshop, 2016, pp. 165–
S. Ganapathy, and M. Liberman, “First DIHARD chal- 170.
lenge evaluation plan,” Tech. Rep., 2018. [Online]. Available: [34] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-
https://zenodo.org/record/1199638 pur, “X-vectors: Robust DNN embeddings for speaker recogni-
[14] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, tion,” in Proc. ICASSP, 2018, pp. 5329–5333.
M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe [35] S. J. Prince and J. H. Elder, “Probabilistic linear discriminant anal-
et al., “Diarization is hard: Some experiences and lessons learned ysis for inferences about identity,” in 2007 IEEE 11th Interna-
for the JHU team in the inaugural DIHARD Challenge,” in Proc. tional Conference on Computer Vision, 2007, pp. 1–8.
Interspeech, 2018, pp. 2808–2812. [36] K. J. Han, S. Kim, and S. S. Narayanan, “Strategies to im-
[15] M. Diez, F. Landini, L. Burget, J. Rohdin, A. Silnova, prove the robustness of agglomerative hierarchical clustering un-
K. Zmolıkova´, O. Novotny`, K. Vesely`, O. Glembek, O. Plchot der data source variation for speaker diarization,” IEEE Trans. Au-
et al., “BUT system for DIHARD Speech Diarization Challenge dio, Speech, Language Process, vol. 16, no. 8, pp. 1590–1601,
2018,” in Proc. Interspeech, 2018, pp. 2798–2802. 2008.
[16] C. Cieri, D. Miller, and K. Walker, “From Switchboard to Fisher: [37] M. Diez, L. Burget, and P. Matejka, “Speaker diarization based on
Telephone collection protocols, their uses and yields,” in Proc. Bayesian HMM with eigenvoice priors,” in Proc. Odyssey, 2018,
EUROSPEECH, 2003. pp. 147–154.
[38] A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb:
[17] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du,
a large-scale speaker identiﬁcation dataset,” arXiv preprint
S. Ganapathy, and M. Liberman, “Second DIHARD chal-
arXiv:1706.08612, 2017.
lenge evaluation plan,” Tech. Rep., 2019. [Online]. Available:
https://coml.lscp.ens.fr/dihard/2019/second dihard eval plan v1.1.pdf [39] J. S. Chung, A. Nagrani, and A. Zisserman, “VoxCeleb2: Deep
speaker recognition,” Proc. Interspeech, pp. 1086–1090, 2018.
[18] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The Fifth
‘CHiME’ Speech Separation and Recognition Challenge: Dataset, [40] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
task and baselines,” in Proc. Interspeech, 2018, pp. 1561–1565. “A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. ICASSP, 2017, pp. 5220–5224.
[19] L. Hamers et al., “Similarity measures in scientometric research:
[41] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech,
The Jaccard index versus Salton’s cosine formula.” Information
and noise corpus,” arXiv preprint arXiv:1510.08484, 2015.
Processing and Management, vol. 25, no. 3, pp. 315–18, 1989.
[42] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of i-vector
[20] R. Real and J. M. Vargas, “The probabilistic basis of Jaccard’s
length normalization in speaker recognition systems,” in Proc. In-
index of similarity,” Systematic Biology, vol. 45, no. 3, pp. 380–
terspeech, 2011, pp. 249–252.
385, 1996.