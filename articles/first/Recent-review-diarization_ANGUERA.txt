356 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
Speaker Diarization: A Review of Recent Research
Xavier AngueraMiro,Member,IEEE, SimonBozonnet,StudentMember,IEEE, Nicholas Evans,Member,IEEE,
CorinneFredouille, GeraldFriedland, Member,IEEE,and Oriol Vinyals
Abstract—Speakerdiarizationisthetaskofdetermining“who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers.Initially, it was proposed as a research topicrelated to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, activespeaker.Thisisthecasefortelephoneconversations(for
speakerdiarizationhasbecomeanimportantkeytechnologyfor examplestemmingfromcallcenters),broadcastnews,debates,
manytasks,suchasnavigation,retrieval,orhigherlevelinference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operationsfor instance), orevenlecture orconference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferencesinthearea.Theapplicationdomains,frombroadcast
news, to lectures and meetings, vary greatly and pose different sessions.In allsuchcases, itcanbe advantageous toautomat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition(inthepresenceofmultipleorcompetingspeakers),
meetings. Finally, we present an analysis of speaker diarization tohelpinspeech-to-texttranscription(i.e.,so-calledspeakerat-
performance as reported through the NIST Rich Transcription tributedspeech-to-text),speechtranslationand,moregenerally,
evaluations on meeting data and identify important areas for
RichTranscription(RT),acommunitywithinwhichthecurrent
futureresearch.
state-of-the-art technology has been developed. The most sig-
IndexTerms—Meetings,richtranscription,speakerdiarization.
niﬁcanteffortintheRichTranscriptiondomaincomesdirectly
fromtheinternationallycompetitiveRTevaluations,sponsored
bythe National Institute of Standards and Technology (NIST)
I. INTRODUCTION
intheUnitesStates[1].Initiatedoriginallywithinthetelephony
S PEAKER diarization has emerged as an increasingly im-
domain,andsubsequentlyinthatofbroadcastnews,todayitis
portantanddedicateddomainofspeechresearch.Whereas
in the domain of conference meetings that speaker diarization
speakerandspeechrecognitioninvolve,respectively,therecog-
receives the most attention. Speaker diarization is thus an
nitionofaperson’sidentityorthetranscriptionoftheirspeech,
extremelyimportantareaofspeechprocessingresearch.
speakerdiarizationrelatestotheproblemofdetermining“who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sentedin[2],althoughitpredominantlyfocusesitsattentionto
identiﬁcation of each speaker within an audio stream and the
speakerdiarizationforbroadcastnews.Coupledwiththetran-
intervalsduringwhicheachspeakerisactive.
sitiontoconferencemeetings,however,thestate-of-the-arthas
advancedsigniﬁcantlysincethen.Thispaperpresentsanup-to-
datereviewofpresentstate-of-the-artsystemsandreviewsthe
ManuscriptreceivedAugust19,2010;revisedDecember03,2010;accepted
progressmadeintheﬁeldofspeakerdiarizationsince2005up
February13,2011.DateofcurrentversionJanuary13,2012.Thisworkwas
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS)projectfundedthroughtheEuropeanAmbientAssistedLiving(AAL) washeldin2009.Ofﬁcialevaluationsareanimportantvehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
forpushing thestate-of-the-artforward asitisonlywithstan-
Collaborativepourl’AccessibilitéVidéo”(ACAV)projectfundedbytheFrench
MinistryofIndustry(InnovativeWebcall)underContract09.2.93.0966.The dardexperimentalprotocolsanddatabasesthatitispossibleto
workofX.AngueraMirowassupportedinpartbytheTorresQuevedoSpanish meaningfullycomparedifferentapproaches.Whilewealsoad-
program.Theassociateeditorcoordinatingthereviewofthismanuscriptand
dressemergingnewresearchinspeakerdiarization,inthispaper
approvingitforpublicationwasProf.SadaokiFurui.
X.AngueraMiroiswiththeMultimediaResearchGroup,TelefonicaRe- special emphasis is placed on established technologies within
search,08021Barcelona,Spain(e-mail:xanguera@tid.es). thecontextoftheNISTRTbenchmarkevaluations,whichhas
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C.FredouilleiswiththeUniversityofAvignon,CERI/LIA,F-84911Avignon ence overview of established approaches, both for the general
Cedex9,France(e-mail:corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedlandand O.Vinyals are withthe International ComputerScience
Institute(ICSI),Berkeley,CA94704USA(e-mail:fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Colorversionsofoneormoreoftheﬁguresinthispaperareavailableonline
tion.Thereareoutstandingopportunitiesforcontributionsand
athttp://ieeexplore.ieee.org.
DigitalObjectIdentiﬁer10.1109/TASL.2011.2125954 wehopethatthispaperservestoencourageotherstoparticipate.
1558-7916/$31.00©2012IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 357
Section II presents a brief history of speaker diarization linguisticcontentandothermetadatacanbeadded(suchasthe
research and the transition to the conference meeting domain. dominantspeakers,thelevelofinteractions,oremotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
presentamoredetaileddescriptionofthemainalgorithmsthat ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. SectionIVpresents someof the most recent workin data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researcherstogetherandtostimulatingnewideastoadvancethe
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetingsand coffeebreaks havealso beenconsidered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
presentanoverviewofthecurrentstatusinspeakerdiarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusionsarepresentedinSectionVI. diarizationthattypicallywerelessrelevantinearlierresearch.
II. SPEAKERDIARIZATION A. BroadcastNewsVersusConferenceMeetings
Over recent years, the scientiﬁc community has developed
WiththechangeoffocusoftheNISTRTevaluationsfromBN
research on speaker diarization in a number of different do-
tomeetingsdiarizationalgorithmshadtobeadaptedaccording
mains,withthefocususuallybeingdictatedbyfundedresearch
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
dataisusuallyacquiredusingboomorlapelmicrophoneswith
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld.Conversely,meetingsareusuallyrecordedusingdesktop
tion was aimed at automatically annotating TV and radio
orfar-ﬁeldmicrophones(singlemicrophonesormicrophonear-
transmissions that are broadcast daily all over the world. An-
rays)whicharemoreconvenientforusersthanhead-mountedor
notations included automatic speech transcription and meta
lapelmicrophones.2Asaresult,thesignal-to-noiseratioisgen-
data labeling, including speaker diarization. Interest in the
erallybetterforBNdatathanitisformeetingrecordings.Addi-
meeting domain grew extensively from 2002, with the launch
tionally,differencesbetweenmeetingroomconﬁgurationsand
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
includingbackgroundnoise,reverberationandvariablespeech
Swiss InteractiveMultimodal Information Management (IM2)
levels(dependingonthedistancebetweenspeakersandmicro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop(CHIL)project.Alltheseprojectsaddressedtheresearch
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter,orapplause(farlesscommonforconferencemeeting
enhancementofhuman-to-humancommunications(notablyin
data),ingeneral,thedetectionofacousticeventsandspeakers
distant access) by automatically extracting meeting content,
tendstobemorechallengingforconferencemeetingdatathan
makingtheinformationavailabletomeetingparticipants,orfor
forBN data.
archivingpurposes.
Finally, the number of speakers is usually larger in BN but
These technologies haveto meet challenging demands such
speaker turnsoccur less frequently than they doin conference
ascontentindexing,linkingand/orsummarizationofon-going
meetingdata,resultinginBNhavingalongeraveragespeaker
orarchivedmeetings,theinclusionofbothverbalandnonverbal
turn length. An extensive analysis of BN characteristics is re-
humancommunication(peoplemovements,emotions,interac-
portedin[3]andacomparisonofBNandconferencemeeting
tions with others, etc.). This is achieved by exploiting several
datacanbefoundin[4].
synchronizeddatastreams,suchasaudio,videoandtextualin-
formation(agenda,discussionpapers,slides,etc.),thatareable 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
tocapturedifferentkindsofinformationthatareusefulforthe Recognition (SR) evaluation campaigns (focusing on telephone speech) and
notwithintheRTevaluationcampaigns.
structuringandanalysisofmeetingcontent.Speakerdiarization
2Meeting databases recorded for research purposes usually contain
playsanimportantroleintheanalysisofmeetingdatasinceital-
head-mounted and lapel microphone recordings for ground-truth creation
lowsforsuchcontenttobestructuredinspeakerturns,towhich purposesonly.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
assignedtothetwoindividualclusters.Standarddistancemet-
rics,suchasthosedescribedinSectionIII-C,areusedtoiden-
tify the closest clusters. A reassignment of frames to clusters
isusuallyperformedaftereachclustermerging,viaViterbire-
alignmentforexample,andthewholeprocessisrepeateditera-
tively,untilsomestoppingcriterionisreached,uponwhichthere
shouldremainonlyoneclusterforeachdetectedspeaker.Pos-
sible stopping criteria include thresholded approaches such as
theBayesianInformationCriterion(BIC)[9],Kullback–Leibler
(KL)-basedmetrics[10],thegeneralizedlikelihoodratio(GLR)
[11]ortherecentlyproposed metric[12].Bottom-upsystems
submittedtotheNISTRTevaluations[9],[13]haveperformed
consistentlywell.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b)Generalspeakerdiarizationarchitecture.
streamwithasinglespeakermodelandsuccessivelyaddsnew
modelstoituntilthefullnumberofspeakersaredeemedtobe
B. MainApproaches
accountedfor.AsingleGMMmodelistrainedonallthespeech
Most of present state-of-the-art speaker diarization systems segmentsavailable,allofwhicharemarkedasunlabeled.Using
ﬁtintooneoftwocategories:thebottom-upandthetop-down someselectionproceduretoidentifysuitabletrainingdatafrom
approaches,asillustratedinFig.1(a).Thetop-downapproach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the addedtothemodelone-by-one,withinterleavedViterbirealign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim newmodelsaremarkedaslabeled.Stoppingcriteriasimilarto
istoiterativelyconvergetowardsanoptimumnumberofclus- thoseemployedinbottom-upsystemsmaybeusedtoterminate
ters. If the ﬁnal number is higher than the optimum then the theprocessoritcancontinueuntilnomorerelevantunlabeled
system is said to under-cluster. If it is lower it is said to over- segmentswithwhichtotrainnewspeakermodelsremain.Top-
cluster.Bothbottom-upandtop-downapproachesaregenerally downapproachesarefarlesspopularthantheirbottom-upcoun-
basedonhiddenMarkovmodels(HMMs)whereeachstateisa terparts.Someexamplesinclude[14]–[16].Whiletheyaregen-
Gaussianmixturemodel(GMM)andcorrespondstoaspeaker. erallyout-performedbythebestbottom-upsystems,top-down
Transitionsbetweenstatescorrespond tospeakerturns.Inthis approaches have performed consistently and respectably well
section,webrieﬂyoutlinethestandardbottom-upandtop-down againstthebroaderﬁeldofotherbottom-upentries.Top-down
approaches as well as two recently proposed alternatives: one approachesarealsoextremelycomputationallyefﬁcientandcan
basedoninformationtheory;andasecondonebasedonanon beimprovedthroughclusterpuriﬁcation[17].
parametricBayesianapproach.Althoughthesenewapproaches 3) OtherApproaches: Arecentalternativeapproach,though
havenotbeenreportedpreviouslyinthecontextofofﬁcialNIST alsobottom-upinnature,isinspiredfromrate-distortiontheory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
someotherworksproposesequentialsingle-passsegmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance withsigniﬁcantsavingsincomputation.Clusteringisbasedon
tendstofallshortofthestate-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-UpApproach: Thebottom-upapproachisbyfar of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up anewspaceofrelevancevariablesdeﬁned bytheGMMcom-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successivelymergingandreducingthenumberofclustersuntil informationbetweensuccessiveclusteringswhilepreservingas
onlyoneremainsforeachspeaker.Variousinitializationshave much information as possible from the original dataset. Two
beenstudiedand,whereassomehaveinvestigated -meansclus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mationbottleneck(aIB)[18]andthesequentialinformationbot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simplerapproach generally leadsto equivalent better performance than parametric approaches, results com-
performance[8].Inallcasestheaudiostreamisinitiallyover- parable to state-of-the-art GMM systems are reported and are
segmentedintoanumberofsegmentswhichexceedstheantic- achievedwithgreatsavingsincomputation.
ipatedmaximumnumberofspeakers.Thebottom-upapproach Alternatively,Bayesianmachinelearningbecamepopularby
theniterativelyselectscloselymatchingclusterstomerge,hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
ClustersaregenerallymodeledwithaGMMand,uponmerging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 359
relateddistribution(hyperparameters).Thisallowsforavoiding clustering[15],[16].Next,inFig.1(b)-iii/iv,adistancebetween
anyprematureharddecisioninthediarizationproblemandfor clusters and a split/merging mechanism (see Section III-D) is
automaticallyregulatingthesystemwiththeobservations(e.g., usedtoiterativelymergeclusters[13],[31]ortointroducenew
the complexity of the model is data dependent). However, the ones[16].Optionally,datapuriﬁcationalgorithmscanbeused
computationofposteriordistributionsoftenrequiresintractable tomakeclustersmorediscriminant[13],[17],[32].Finally,as
integralsand,asaresult,thestatisticscommunityhasdeveloped illustratedinFig.1(b)-v,stoppingcriteriaareusedtodetermine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs)wereﬁrstused[20]toprovideasystematicapproach [34].
to the computation of distributions via sampling, enabling the
deploymentofBayesianmethods.However,samplingmethods A. AcousticBeamforming
are generally slow and prohibitivewhen the amount of data is Theapplicationofspeakerdiarizationtothemeetingdomain
large,andtheyrequiretoberunseveraltimesasthechainsmay triggeredtheneedfordealingwithmultiplemicrophoneswhich
getstuckandnotconvergeinapracticalnumberofiterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can havedif-
hasbeenpopularsince1993[21],[22]andaimsatprovidinga ferentcharacteristics:wall-mountedmicrophones(intendedfor
deterministic approximation of the distributions. It enables an speakerlocalization),lapelmicrophones,desktopmicrophones
inference problem to be converted toan optimization problem positionedonthemeetingroomtableormicrophonearrays.The
by approximating the intractable distribution with a tractable useofdifferentmicrophonecombinationsaswellasdifferences
approximation obtained by minimizing the Kullback–Leibler inmicrophonequalitycalledfornewapproachestospeakerdi-
divergence between them. In [23] a Variational Bayes-EM arizationwithmultiplechannels.
algorithmisusedtolearnaGMMspeakermodelandoptimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithmshavebeenproposedtoextendmono-channeldiariza-
modeling, described in [25], for the speaker diarization of tionsystemstohandlemultiplechannels.Oneoption,proposed
telephone conversations. However, these systems still con- in[38],istoperformspeakerdiarizationoneachchannelinde-
sider classical Viterbi decoding for the classiﬁcation and pendentlyandthentomergetheindividualoutputs.Inorderto
differ from the nonparametric Bayesian systems introduced in doso,atwoaxismergingalgorithmisusedwhichconsidersthe
SectionIV-F. longestdetectedspeakersegmentsineachchannelanditerates
Finally,therecentlyproposedspeakerbinarykeys[26]have overthesegmentationoutput.Inthesameyear,alate-stagefu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keysare small bi- segmentshavethebestsignal-to-noiseratio(SNR).Subsequent
naryvectorscomputedfromtheacousticdatausingauniversal approachesinvestigatedpreprocessingtocombinetheacoustic
backgroundmodel(UBM)-likemodel.Oncetheyarecomputed signalstoobtainasinglechannelwhichcouldthenbeprocessed
all processing tasks take place in the binary domain. Other byaregularmono-channeldiarizationsystem.In[40],themul-
worksinspeakerdiarizationconcernedwithspeedinclude[28], tiple channels are combined with a simple weighted sum ac-
[29]whichachievefasterthanreal-timeprocessingthroughthe cordingtotheirSNR.Thoughstraightforwardtoimplement,it
useofseveralprocessingtricksappliedtoastandardbottom-up doesnottakeintoaccountthetimedifferenceofarrivalbetween
approach ([28]) or by parallelizing most of the processing eachmicrophonechannelandmighteasilyleadtoadecreasein
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
whenusingdiarizationasapreprocessingsteptootherspeech proach to multi-channel speaker diarization involves acoustic
algorithms. beamformingasinitiallyproposedin[41]anddescribedinde-
tailin[42].ManyRTparticipantsusethefreeandopen-source
III. MAINALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig.1(b)showsablockdiagramofthegenericmoduleswhich which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correctmisalignmentsduetothetime-delay-of-arrival(TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc.Formeetingdata,preprocessingusuallyinvolvesnoisere- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acousticbeamforming(seeSectionIII-A),theparameterization theotherchannelsareappropriatelyalignedandcombinedwith
ofspeechdataintoacousticfeatures(suchasMFCC,PLP,etc.) astandarddelay-and-sumalgorithm.Thecontributionmadeby
and the detection of speech segments with a speech activity eachsignalchanneltotheoutputisthendynamicallyweighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down andtostabilizetheTDOAvaluesbetweenchannelsbeforethe
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
aremadeavailableasoutputsandhavebeenusedsuccessfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamformingisthemostreliableonewhennoinformationon ture.Themaindrawbackofmodel-basedapproachesistheirre-
the location or nature of each microphone is known a priori. lianceonexternaldataforthetrainingofspeechandnonspeech
Amongalternativebeamformingalgorithmsweﬁndmaximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions.Hybridapproacheshavebeenproposedasapoten-
[47] which adaptively ﬁnd the optimum parameters, and min- tialsolution.Inmostcases,anenergy-baseddetectionisﬁrstap-
imum variance distortionless response (MVDR) [48] when pliedinordertolabelalimitedamountofspeechandnonspeech
prior information on ambient noise is available. All of these dataforwhichthereishighconﬁdenceintheclassiﬁcation.Ina
havehighercomputationalrequirementsand,inthecaseofthe secondstep,thelabeleddataareusedtotrainmeeting-speciﬁc
adaptivealgorithms,thereisthe dangerofconvergingtoinac- speechandnonspeechmodels,whicharesubsequentlyusedina
curateparameters,especiallywhenprocessingmicrophonesof model-baseddetectortoobtaintheﬁnalspeech/nonspeechseg-
differenttypes. mentation[9],[55]–[57].Finally,[58]combinesamodel-based
witha4-Hzmodulationenergy-baseddetector.Interestingly,in-
B. SpeechActivityDetection steadofbeing applied asa preprocessingstage, inthis system
Speech activity detection (SAD) involves the labeling of SADisincorporatedintothespeakerdiarizationprocess.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), timesusedtorefertobothsegmentationandclustering.While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process,andmorespeciﬁcallytheacousticmodelsinvolvedin speakerdiarizationandsomesystems,suchasthatreportedin
theprocess[49].Indeed,theinclusionofnon-speechsegments [6], apply distinctly independent segmentation and clustering
inspeakermodellingleadstolessdiscriminantmodelsandthus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good describedseparatelyhere.
compromisebetweenmissedandfalsealarmspeecherrorrates Speaker segmentation is core to the diarization process and
hastobefoundtoenhancethequalityofthefollowingspeaker aims at splitting the audio stream into speaker homogeneous
diarizationprocess. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech knownasspeakerturns.Theclassicalapproachtosegmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the twoslidingandpossiblyoverlapping,consecutivewindows.For
literature [50]. Initial approaches for diarization tried to solve eachconsideredchangepointtherearetwopossiblehypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrstthatbothsegmentscomefromthesamespeaker( ),and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a secondthattherearetwodifferentspeakers( ),andthusthat
dedicated speech/nonspeech detector as preprocessing step. twodifferentmodelsaremoreappropriate.Inpractice,models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teriaareusedtodeterminewhethertheyarebestaccountedfor
knocks or non-lexical noise such as breathing, coughing, and bytwoseparatemodels(andhencetwoseparatespeakers),orby
laughing, among other background noises. Therefore, highly asinglemodel(andhencethesamespeaker)byusinganempir-
variable energy levels can be observed in the nonspeech parts icallydeterminedordynamicallyadaptedthreshold[10],[59].
of the signal. Moreover, differences in microphones or room Thisisperformedacrossthewholeaudiostreamandasequence
conﬁgurations may result in variable SNRs from one meeting ofspeakerturnsisextracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
andpitchestimation)combinedwithathreshold-baseddecision during the last four years. The most common approach is that
haveproventoberelativelyineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely ona two-class detector, with models pre-trainedwith e.g.,[60]–[62].Theapproachrequiresthesettingofanexplicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penaltytermwhichcontrolsthetradeoffbetweenmissedturns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
differentmeetingsandthusnew,morerobustapproacheshave the introduction of missing speaker turns. Most of present
beendevised.Theyeitheradaptthepenaltytermautomatically, diarization systems thus perform segmentation and clustering
i.e.,themodiﬁedBICcriterion[33],[63],[64],oravoidtheuse simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
andsomesystemshavebeendevelopedinordertousetheBIC based on the current clustering hypothesis before the models
onlyinasecondpass,whileastatistical-baseddistanceisused are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usuallyperformed.InordertomaketheViterbidecodingmore
cross-BIC and introduced in [67] and [68], involves the com- stable,itiscommontouseaViterbibuffertosmooththestate,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
accordingtoamodeltunedfromthesecondsegmentandvice brief speaker turns, as in [16]. Most state-of-the-art systems
versa.In[69],differenttechniquesforlikelihoodnormalization employsomevariationsonthisparticularissue.
arepresentedandarereferredtoasbilateralscoring. Analternativeapproachtoclusteringinvolvesmajorityvoting
ApopularandalternativeapproachtoBIC-basedmeasuresis [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trasttotheBIC,theGLRisalikelihood-basedmetricandcorre- framesduringdecoding.Thistechniqueleadstosavingsincom-
spondstotheratiobetweenthetwoaforementionedhypotheses, putationbutismoresuitedtoonlineorlivespeakerdiarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
ordertotakeintoaccounttheamountoftrainingdataavailable
E. One-StepSegmentationandClustering
inthetwosegments,apenalizedGLRwasproposedin[74].
ThelastofthedominantapproachesistheKullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems,segmentationandclusteringareperformedhand-in-hand
asymmetric,andthustheKL2metric,asymmetricalternative, inoneloop.SuchamethodwasinitiallyproposedbyICSIfor
hasprovedtobemorepopularinspeakerdiarizationwhenused a bottom-up system [31] and has subsequently been adopted
tocharacterizethesimilarityoftwoaudiosegments[75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally,inthissectionweincludeanewlyintroduceddistance rithmsitwasinitiallyproposedbyLIA[14]asusedintheirlatest
metricthathasshownpromiseinaspeakerdiarizationtask.The system[16].
informationchangerate(ICR),orentropycanbeusedtochar- Inallcasesthedifferentacousticclassesarerepresentedusing
acterizethesimilarityoftwoneighboringspeechsegments.The HMM/GMMmodels.EMtrainingorMAPadaptationisusedto
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- modelassignments,andaViterbialgorithmisusedtoreassign
tionandcanthusbeusedforspeakersegmentation.Unlikethe all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessingissometimesperformedseveraltimesfortheframeas-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segmentsinaspaceofrelevancevariables,withmaximummu- ated/eliminatedsothattheresultingclassdistributionisallowed
tualinformationorminimumentropy.Onesuitablespacecomes toadapttothedata.
from GMM component parameters [18]. The ICR approach is Theone-stepsegmentationandclusteringapproach,although
computationallyefﬁcientand,in[78],ICRisshowntobemore much slower, constitutes a clear advantage versus sequential
robusttodatasourcevariationthanaBIC-baseddistance. single-passsegmentationandclusteringapproaches[5]–[7].On
theonehand,earlyerrors(mostlymissedspeakerturnsfromthe
D. Clustering segmentationstep)canbelatercorrectedbythere-segmentation
Whereasthesegmentationstepoperatesonadjacentwindows steps.Ontheotherhand,mostspeakersegmentationalgorithms
inordertodeterminewhetherornottheycorrespondtothesame useonlylocalinformationtodecideonaspeakerchangewhile
speaker, clustering aims at identifying and grouping together whenusingspeakermodelsandViterbirealignmentalldatais
same-speakersegmentswhichcanbelocalizedanywhereinthe takenintoconsideration.
audiostream.Ideally,therewillbeoneclusterforeachspeaker. WhenperformingframeassignmentusingViterbialgorithm
Theproblemofmeasuringsegmentsimilarityremainsthesame a minimum assignment duration is usually enforced to avoid
andallthedistancemetricsdescribedinSectionIII-Cmayalso an unrealistic assignment of very small consecutive segments
beusedforclustering,i.e.,theKLdistanceasin[10],amodiﬁed todifferentspeakermodels.Suchminimumdurationisusually
KL2 metric as in [61], a BIC measure as in [79] or the cross madeaccordingtotheestimatedminimumlengthofanygiven
likelihoodratio(CLR)asin[80]and[81]. speakerturn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENTRESEARCHDIRECTIONS
single speaker, and thus diarization algorithms can only work Inthissection,wereviewthoseareasofworkwhicharestill
well if the initial segmentation is of sufﬁciently high quality. notmaturebutwhichhavethepotentialtoimprovediarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
performance.WeﬁrstdiscussthetrendinrecentNISTRTeval- diarization performance and the success of these systems in
uationstousespatialinformationobtainedfrommultiplemicro- NISTRTevaluationswouldseemtosupporttheiruse.
phones, whichareusedbymanyincombinationwithMFCCs
B. UseofProsodicFeaturesinDiarization
toimproveperformance.Then,wediscusstheuseofprosodic
informationwhichhasledtopromisingspeakerdiarizationre- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
ofoverlappingspeechanditscorrectlabelingforimproveddi- and speech recognition (which requires invariance against
arizationoutputs.Wethenconsiderarecenttrendtowardsmul- speakers)[93].In[84],theauthorspresentasystematicinvesti-
timodal speaker diarization including studies of multimodal, gationofthespeakerdiscriminabilityof70long-termfeatures,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despitethedominanceofshort-termcepstralfeaturesinspeaker
weconsidergeneralcombinationstrategiesthatcanbeusedto recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowingsummarizesrecentworkinalloftheseareas. suggested in [94], the consideration of patterns derived from
largersegmentsofspeechcanrevealindividualcharacteristics
A. Time-DelayFeatures
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sumbeamformingofmultiplemicrophonechannels, frame-based cepstral analysis. The authors use Fisher LDA as
asdescribedinSectionIII-A,butalsoforspeakerlocalization. arankingmethodologyandsortthe70prosodicandlong-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
trackingalgorithmsareused,thenestimatesofspeakerlocation top-tenrankedprosodicandlong-termfeaturescombinedwith
maythusbeusedasalternativefeatures,whichhavenowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formationisnotalwaysavailable.Theﬁrstwork[88]thatdoes scheme that can be applied to standard bottom-up diarization
notrelyonmicrophonelocationsledtopromisingresults,even algorithms. The initialization method is a combination of the
iferrorrateswereconsiderablyhigherthanthatachievedwith recently proposed “adaptive seconds per Gaussian” (ASPG)
acousticfeatures.Earlyeffortstocombineacousticfeaturesand method [96] and a new pre-clustering method in addition to
estimatesofinter-channeldelayclearlydemonstratedtheirpo- a new strategy which automatically estimates an appropriate
tential,e.g.,[89],thoughthisworkagainrelieduponknownmi- numberofinitialclustersbasedonprosodicfeatures.Itoutper-
crophonelocations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. OverlapDetection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though Afundamentallimitationofmostcurrentspeakerdiarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based Thepresenceofoverlappedspeech,though,iscommoninmul-
onanentropy-basedmetricisusedforclustercomparisonina tipartymeetingsand,consequently,presentsasigniﬁcantchal-
bottom-upspeakerdiarizationsystem.Acompletefront-endfor lengetoautomaticsystems.Speciﬁcally,inregionswheremore
speakerdiarizationwithmultiplemicrophoneswasproposedin thanonespeakerisactive,missedspeecherrorswillbeincurred
[42].Hereatwo-stepTDOAViterbipost-processingalgorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lappedspeechindiarizationpertainstospeakerclusteringand
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms.Morerecently,anapproachtotheunsuperviseddis- singlespeakershouldnotbeassignedtoanyindividualspeaker
criminantanalysisofinter-channeldelayfeatureswasproposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
usingdelayfeaturesalone. matelyreducesdiarizationperformance.Approachestooverlap
InthemostrecentNISTRTevaluation,in2009,allbutone detection werethoroughly assessed in[97] and [98] and,even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are asmallnumberofsystemsactuallydetectsoverlappingspeech
rarely reported it is not possible to assess the contribution wellenoughtoimproveerrorrates[99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 363
speaker during overlap regions using a simple strategy of detectionofthemouthisnotalwaysfeasible.Therefore,other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boringsegments,aswellasbyexcludingoverlapregionsfrom visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study hasbeenrelativelylittleworkonusingglobalbodymovements
assumedground-truthoverlapdetection.In[100],arealoverlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out[82],[117]–[119]thatshowpromisinginitialresults.
that computed posterior probabilities from diarization to post However,untiltheworkpresentedin[120],approacheshave
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions.Themainbottleneckoftheachievedperformancegain vised joint optimization problem. The work in [120], though,
ismainlyduetoerrorsinoverlapdetection,andmoreworkon relies on multiple cameras. The ﬁrst paper that discusses joint
enhancingitsprecisionandrecallisreportedin[99]and[101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech),andthebestfeaturecombinationisMFCCandmodu- [121].Thealgorithmreliesonveryfewassumptionsandisable
lationspectrogramfeatures[103],althoughcomparableresults to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Mostimportantly,asaresultoftrainingacombinedaudiovisual
energy,spectralﬂatness,orharmonicenergyratio.Thereported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and canresultinspeakerlocalizationassideinformation.Thisway
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “whospokenwhenandfromwhere.”Thissolutiontothelocal-
DER improvement goes up to 37%. This way, this area has izationproblemhaspropertiesthatmaynotbeobservedeither
potentialforfutureresearchefforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. AudiovisualDiarization channel.Inaddition,incontrasttoaudio-onlyspeakerdiariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. SystemCombination
speaker localization in broadcast video. The authors of [105]
presentamulti-modalspeakerlocalizationmethodusingaspe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Thoughtheresultsseemcomparabletothestate-of-the-art,the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarizationusing a dynamic Bayesian network(DBN) mergingmultipleoutputsegmentations.Combinationstrategies
but tests were limited to discussions with two to three people havetoaccommodatedifferencesintemporalsynchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual ofspeakerlabels.Moreover,systemsinvolvedinthecombina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation whencombined.Someofthecombinationstrategiesproposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
theauthorsdemonstratethatthedifferentshapesthemouthcan tially, based onthe segmentationoutputsof the previous steps
take when speaking facilitate word recognition under tightly inorderto reﬁneboundaries(referred toas “hybridization” or
constrainedtestconditions(e.g.,frontalpositionofthesubject “piped” systems in [122]). In [123] for instance, the authors
withrespecttothecamerawhilereadingdigits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented bytwo dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferentFrenchlaboratories(LIUMandIRIT)aremergedand/or
motion from a person comes predominantly from the motion usedsequentially,whichleadstoaperformancegaincompared
of the lower half of their face. In addition, gestural or other toresultsfromindividualsystems.Anoriginalapproachispro-
nonverbal behaviors associated with natural body motion posedin[125],basedona“real”systemcombination.Here,a
during conversations are artiﬁcially suppressed, e.g., for the coupleofsystemsuniquelydifferentiatedbytheirinputfeatures
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
onlywhereshorttermsynchronyoflipmotionandspeechare phonecallsconversations.Thecombinationapproachrelieson
the basis for audiovisual localization. In a real scenario the bothsystemsidentifyingsomecommonclusterswhicharethen
subjectbehaviorisnotcontrolledand,consequently,thecorrect consideredasthemostrelevant.Allthesegmentsnotbelonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involvedinanewre-classiﬁcationstepbasedonaGMMmod- a priori knowledge available to the participants relates to the
elingofthecommonclustersandamaximumlikelihood-based recordingscenario/source (e.g., conference meetings,lectures,
decision. orcoffeebreaksforthemeetingsdomain),thelanguage(Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. AlternativeModels participants may use external or background data for building
worldmodelsand/orfornormalizationpurposesbutnoapriori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differfromthestandardHMMinsofarastheyarefullynonpara-
Thenumberofspeakersisalsonotknown.
metric(thatis,thenumberofparametersofthesystemdepends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization.Eachmeetingusedintheevaluationswasrecorded
mixturesand deﬁnesthe numberofcomponents byameasure
using multiple microphones (of different types and quality)
overdistributions.Theauthorsof[127]illustratetheuseofthe
whicharepositionedontheparticipantsorindifferentlocations
Dirichletprocessmixtures,showinganimprovementcompared
aroundthemeetingroom.Bygroupingthesemicrophonesinto
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
ontransitionmatricesovercountablyinﬁnitestatespaces,that
microphones (MDM), multiple mark III arrays (MM3A), and
is,noﬁxednumberofspeakersisassumed,norfoundthrough
alldistantmicrophones(ADM).MM3Amicrophonesarethose
eithersplitormergingapproachesusingclassicalmodelselec-
exclusivelyfoundwithinthearraysbuiltandprovidedbyNIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measureisplacedoverdistributions(calledarandommeasure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resultingHDP-HMMleadstoadata-drivenlearningalgorithm
considerthemtobethemostrepresentativeofstandardmeeting
which infers posterior distributions over the number of states.
roomrecordingequipment.Theseconditionshavealsoproven
This posterior uncertainty can be integrated out when making
tobethemostpopularamongevaluationparticipants.
predictions effectivelyaveragingovermodelsofvaryingcom-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
withspeakerlabels,whichareusedsolelytoidentifythemul-
erative HMM with GMM emissions, while requiring very
tipleinterventionsofagivenspeaker,butdonotneedtoreﬂect
littlehyperparametertuningandprovidingastatisticallysound
thespeaker’srealidentity.Thesesystemoutputsarecompared
model.Globally,thesenonparametricBayesianapproachesdid
totheground-truthreferenceinordertoobtaintheoverallDER.
not bring a major improvementcompared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech(percentageofspeechintheground-truthbutnotinthe
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesisbutnotintheground-truth)andspeakererror(per-
more, they provide a probabilistic interpretation on posterior
centageofspeechassignedtothewrongspeaker).Thespeaker
distributions(e.g.,numberofspeakers).
errorcanbefurtherclassiﬁedintoincorrectlyassignedspeakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speakerdoesnotcorrespondtothereal(ground-truth)speaker.
V. PERFORMANCEEVALUATION
Speakeroverlaperrorreferstothecasewhenthewrongnumber
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performanceasreportedduringthefourmostrecentNISTRT thesametime.Theinclusionofoverlappingspeecherrorinthe
evaluations.Theanalysisfocusessolelyonconferencemeetings evaluationwasrestrictedtoacontrastivemetricintheinitialRT
whicharethecoreevaluationcondition.Wealsopresentananal- evaluationsbuthasbeentheprimarymetricsince2006.Overlap
ysisoftheground-truthreferencesinordertounderlinethechar- errorscanbeclassiﬁedasmissedoverlap(whenfewerspeakers
acteristics of the data with respect to meeting sources and the thantherealnumberarehypothesized)andfalsealarmoverlap
differentevaluationcampaigns.Finallyweshowstate-of-the-art (whentoomanyspeakersarehypothesized).IntheNISTeval-
systemresults,collatedfromfourNISTRT’07andRT’09eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note thatasthe DERistime-weighted, itascribes littleim-
portance to the diarization quality of speakers whose overall
A. BenchmarkingEvaluations speakingtimeissmall.Additionally,anonscoringcollarof250
msisgenerallyappliedeithersideoftheground-truthsegment
Since2004,NISThasorganizedaseriesofbenchmarkeval-
boundaries to account for inevitable inconsistencies in precise
uationswithintheRichTranscription(RT)campaigns.3Oneof
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
putswiththeground-truth,andgiventhatthelabelsidentifying
3Seehttp://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 365
TABLEI
GROUND-TRUTHANALYSISFORTHEDATASETSOFTHELASTFOURSPEAKERDIARIZATIONEVALUATIONCAMPAIGNS(RT’05TO
RT’09)ANDMEETINGSOURCE.COMPARISONSAREBASEDONTHEAVERAGESPEAKERANDTURNDURATIONS(LEFT-HALFSIDE)
ANDTHEPERCENTAGEOFSILENCEANDOVERLAPPINGSPEECH(RIGHT-HALFSIDE)
ForRT’05theaveragespeakersegmentdurationis2.5s.This
valuedecreasescontinuouslyforsubsequentdatasets(2.3sfor
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leadstoincreasinglymorefrequentspeakerturnsandincreases
thechancesofmiss-classifyingaspeechsegment.Theaverage
turnsegmentdurationis2.1sforRT’05.Thisvaluefallsto1.4s
forRT’06andremainsstableforRT’07andRT’09(1.5sand1.4
Fig.2. Examplesofturnandspeakerdurationsinthepresenceofoverlapped srespectively).Theconsistentdecreaseinspeaker/turnduration
speechandsilences.
ratio highlights a general trend of increasing spontaneity and
helpstoexplainthedifferencesinresultsfromonedatasettoan-
ﬁrst computes an optimum mapping between both sets of la-
other.Therearenodistinctdifferencesacrossdifferentmeeting
belsinordertoobtaintheDER.Thisisnormallyperformedac-
sites.
cordingtoastandarddynamicprogrammingalgorithmdeﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics.ThepercentageofsilenceislowerfortheRT’05and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-TruthAnalysis
(10.3%and17.5%cf.31.5%and24.9%).However,theRT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
wereinitiallyobtainedviamanuallabelingoftheacousticdata; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
beproblematic.Therefore,morerecently,anautomaticallygen- and NIST sites) which have overlap rates over 25% (note that
eratedforcedalignmenthasbeenusedinordertoextractmore values in Table I are averaged across sites, and do not reﬂect
reliablespeakerstartandendpointsusinganautomaticspeech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
theaudiofromindividualheadmicrophones(IHM). meeting(recordedbyNIST)inwhichtheoverlapreaches31%.
Asmeetingdatacomefromavarietyofsourcessomediffer- Listeningtothismeetingweconcludedthatthereasonofsuch
encesbetweenthemareexpected.Furthermore,largechangesin overlap is that it is not a professional meeting but a social
theﬁnalDERscoresfromdifferentevaluationswouldsuggest rendezvous. Conversely, RT’05 and RT’09 have in average a
thattherearedifferencesbetweenthesetsofmeetingsusedeach lowerpercentageofsilence(10%and17%)comparedtoRT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferentparameterscomputedontheground-truthdata.InTableI, overlap might indicate that these meetings are more dynamic,
wereportfouroftheseparameters,whichwefoundmostinter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
Intheleftsideofthetablewereportaveragespeakerandturn dataset.
durations.AsexempliﬁedinFig.2,theaveragespeakerduration Overall, we see that, although all recordings belong to the
referstotheaveragetimeduringwhichaspeakerisactive(i.e.,a sametask,therearelargedifferencesbetweenthedatasetsused
singlelineintheRTTMreferenceﬁles).Conversely,theaverage for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two performwellregardlessofparticulardatasetcharacteristics.Itis
statisticsreﬂectsthedegreeofoverlapandspontaneity.Without importanttonote,however,thattheNISTRTdatasetsdiscussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
averagespeakerandturndurationswouldbeidentical.Increases ofthemcontributingtoasingleDERscore.Randomvariations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impactonaverageresults.Itisthendifﬁculttoreliablyinterpret
silenceandofoverlappingspeech. resultsandhencealsodifﬁculttodrawmeaningfulconclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
Fig.3. DERsfortheRT’07andRT’09(a)inmultipledistantmicrophone(MDM)condition,and(b)singledistantmicrophone(SDM)condition(notethat
spkr_errorinmeetingNIST_20080201–1405hasbeentrimmedtoﬁtthescreen,withaspeakererrorof31.79%andatotalDERof49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus,thereisalargevariationinperformanceacrossdifferent
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which highpercentageofoverlappingspeechforthismeetingasillus-
makesitdifﬁculttoassessnovelalgorithms,isacriticalbarrier tratedinTableI.Hence,thedetectionandappropriatetreatment
tofurtherresearchinourﬁeld.Signiﬁcantlylargerdatasetsare ofoverlappingspeechremainsanunsolvedproblem.Infact,the
needed in order to obtain more robust and meaningful perfor- overlaperrorshowninFig.3isentirelyduetomissedoverlap
mance estimates and comparisons. As a result of processing regions,asnoneofthespeakerdiarizationsystemsconsidered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
forresearchinspeakerdiarizationtobefeasiblewithstandard the general stability of speech activity detection (SAD) algo-
computingresources. rithmswhichachieveimpressivelevelsofperformanceinboth
MDMandSDMconditions(i.e.,theyarerobusttothequality
C. EvaluationResults
ofthesignal).Valuesofaround1%to2%missedspeecherror
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
forfutureresearchwepresentresultsfortheRT’07(Fig.3left ical.ThemaindifferencebetweenMDMandSDMperformance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly inthe speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDMcondition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall,thelargevariationsinDERobservedamongthedif-
andUPC)andbyselectingtheresultwithlowestDERforeach ferentmeetingsandmeetingsetsoriginatefromthelargevari-
meetingrecording.Giventhevolatilityoftheresultsdescribed ance of many important factors for speaker diarization, which
andstudiedin[3],byselectingthebestresultineachcasewehy- makes the conference meeting domain not as easily tractable
pothesizethattheseresultsareamoremeaningfulestimationof as more formalized settings such as broadcast news, lectures,
thestate-of-the-artperformanceinspeakerdiarizationforcon- or court house trials. Previous work has highlighted the difﬁ-
ference meetingdata than selectingall resultsfrom any single cultyinassessingtheperformanceofspeakerdiarizationalgo-
systemoutput.Toillustratethevariationinperformancefordif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In portedinSectionIII,currentapproachestospeakerdiarization
bothﬁgures,errorsaredecomposedintothespeakererror(Spkr involve a sequence of separate stages where each stage takes
error),overlaperror(OVLerror),falsealarmspeecherror(FA itsinputfromtheprecedingstage(s).Whencombinedinsucha
speech),andmissedspeecherror(MISSspeech). fashion,itisexceedinglydifﬁculttoassesstheperformanceof
FortheMDMcondition[Fig.3(a)]theaverageDERforthe eachsystemcomponentsinceeverysingleoneisaffectedbythe
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performanceof allprevious processingstages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 isnotguaranteedthatimprovementstoonestage,forexample
dataset whereas for the RT’09 dataset performance varies be- thatofsegmentation,willleadunequivocallytoimprovements
tween 5.3% and 22.2%. For the SDM condition the average inlaterstages,forexamplethatofclustering.Thismakestheop-
DERis11.6%and17.7%fortheRT’07andRT’09datasets,re- timizationofdifferentsystemcomponentsrathertroublesome.
spectively.PerformanceisalwayspoorerthanthatfortheMDM Onceagain,bydrawingcomparisonstothespeechandspeaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches,asisalreadyinprogresswiththenowcommonplace
4EurecomwasassociatedwiththeLIAfortheRT’09campaignonly.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “TheNISTRichTranscription2009(RT’09)Evaluation,”NIST,2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tivealgorithmsthatcandetectandcorrectlyassignoverlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S.TranterandD.Reynolds,“Anoverviewofautomaticspeakerdi-
arizationsystems,”IEEETrans.Audio,Speech,Lang.Process.,vol.
14,no.5,pp.1557–1565,Sep.2006.
[3] N.MirghaforiandC.Wooters,“Nutsandﬂakes:Astudyofdatachar-
VI. CONCLUSIONANDDIRECTIONSFORFUTURERESEARCH acteristicsinspeakerdiarization,”inProc.ICASSP,2006.
[4] X.Anguera,“Robustspeakerdiarizationformeetings,”Ph.D.disser-
Researchonspeakerdiarizationhasbeendevelopedinmany
tation,Univ.PolitecnicadeCatalunya,Barcelona,Spain,2006.
domains, from phone calls conversations within the speaker [5] M.Kotti,E.Benetos,andC.Kotropoulos,“Computationallyefﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognitionevaluations,tobroadcastnewsandmeetingrecord-
Speech,Lang.Process.,vol.16,no.5,pp.920–933,Jul.2008.
ingsintheNISTRichTranscriptionevaluations.Furthermore, [6] X.Zhu,C.Barras,L.Lamel,andJ.-L.Gauvain,“Multi-stagespeaker
it has been used in many applications such as a front-end for diarizationforconferenceandlecturemeetings,”inProc.Multimodal
Technol.PerceptionofHumans:Int.Eval.WorkshopsCLEAR2007and
speaker and speech recognition, asa meta-data extractiontool
RT2007,Baltimore,MD,May8–11,2007,RevisedSelectedPapers,
toaidnavigationinbroadcastTV,lecturerecordings,meetings, Berlin,Heidelberg:Springer-Verlag,2008,pp.533–542.
andvideoconferencesandevenforapplicationssuchasmedia [7] S.Jothilakshmi,V.Ramalingam,andS.Palanivel,“Speakerdiariza-
tionusingautoassociativeneuralnetworks,”Eng.Applicat.Artif.In-
similarityestimationforcopyrightdetection.Also,speakerdi- tell.,vol.22,no.4-5,pp.667–675,2009.
arizationresearchhasledtovariousby-products.Forexample, [8] X.Anguera,C.Wooters,andJ.Hernando,“Robustspeakerdiarization
formeetings:ICSIRT06sevaluationsystem,”inProc.ICSLP,Pitts-
withtheavailabilityofrecordingsusingmultiplemicrophones,
burgh,PA,Sep.2006.
a set of algorithms has been proposed in recent years both for [9] C.WootersandM.Huijbregts,“TheICSIRT07sspeakerdiarization
signal enhancement and to take advantage of the extra infor- system,”inMultimodalTechnologiesforPerceptionofHumans:Inter-
nationalEvaluationWorkshopsCLEAR2007andRT2007,Baltimore,
mation that these offer. In addition, the availability of other MD,USA,May8–11,2007,RevisedSelectedPapers,Berlin,Heidel-
modalities, such as video, have started to inspire multimodal berg:Springer-Verlag,2008,pp.509–519.
[10] J.Rougui,M.Rziza,D.Aboutajdine,M.Gelgon,andJ.Martinez,“Fast
diarization systems, thus merging the visual and the acoustic
incrementalclusteringofGaussianmixturespeakermodelsforscaling
domains. upretrievalinon-linebroadcast,”inProc.ICASSP,May2006,vol.5,
This paper provides an overview of the current state-of- pp.521–524.
[11] W.Tsai,S.Cheng,andH.Wang,inProc.ICSLP,2004.
the-art in speaker diarization systems and underlines several
[12] T.H.Nguyen,E.S.Chng,andH.Li,“T-testdistanceandclustering
challenges that need to be addressed in future years. For ex- criterionforspeakerdiarization,”inProc.Interspeech,Brisbane,Aus-
tralia,2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T.Nguyenetal.,“TheIIR-NTUspeakerdiarizationsystemsforRT
thatmethods canbe easilyportedacross differentdomains,as 2009,”inProc.RT’09,NISTRichTranscriptionWorkshop,Melbourne,
shown in Section V, where small differences in meeting data FL,2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learningandadaptingsoundmodelsforspeakerindexing,”inProc.
mance.Inthemeantime,largerdatasetsneedtobecompiledin Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
orderforresultstobecomemoremeaningfulandforsystemsto Jun.2001,pp.175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
bemorerobusttounseenvariations.Ofcourse,withincreasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA,May8–11,2007,RevisedSelectedPapers,Berlin,Heidelberg:
ordertoprocesssuchdatainreasonabletime.Still,thebiggest
Springer-Verlag,2008,pp.520–532.
single challenge is probably the handling of overlapping [16] C.Fredouille,S.Bozonnet,andN.W.D.Evans,“TheLIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09speakerdiarizationsystem,”inProc.RT’09,NISTRichTran-
scriptionWorkshop,Melbourne,FL,2009.
a relatively embryonic community, at least compared to the
[17] S.Bozonnet,N.W.D.Evans,andC.Fredouille,“TheLIA-EURECOM
moreestablishedﬁeldsofspeechandspeakerrecognition,there RT’09speakerdiarizationsystem:Enhancementsinspeakermodelling
andclusterpuriﬁcation,”inProc.ICASSP,Dallas,TX,Mar.14–19,
are thus outstanding opportunities for signiﬁcant advances
2010,pp.4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D.Vijayasenan, F.Valente, and H. Bourlard, “Agglomerative infor-
approachesthatcurrentlydominatetheﬁeld. mationbottleneckforspeakerdiarizationofmeetingsdata,”inProc.
ASRU,Dec.2007,pp.250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D.Vijayasenan,F.Valente,andH.Bourlard,“Aninformationtheoretic
brighter than the present, as more and more people acknowl- approachtospeakerdiarizationofmeetingdata,”IEEETrans.Audio,
Speech,Lang.Process.,vol.17,no.7,pp.1382–1393,Sep.2009.
edgetheusefulnessofaudiomethodsformanytasksthathave
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994,vol.23,pp.727–741.
[21] G.E.HintonandD.vanCamp,“Keepingtheneuralnetworkssimpleby
problems underlying virtually any task that involves acoustics
minimizingthedescriptionlengthoftheweights,”inProc.6thAnnu.
andthepresenceofmorethanoneperson. Conf.Comput.Learn.Theory,NewYork,1993,COLT’93,pp.5–13.
[22] M.J.WainwrightandM.I.Jordan,“Variationalinferenceingraphical
models:Theviewfromthemarginalpolytope,”inProc.41stAnnu.
AllertonConf.Commun.,Control,Comput.,Urbana-Champaign,IL,
ACKNOWLEDGMENT 2003.
[23] F.Valente,“VariationalBayesianmethodsforaudioindexing,”Ph.D.
TheauthorswouldliketothankthoseRTparticipatingsites dissertation,EurecomInst.,Sophia-Antipolis,France,2005.
thatlentthemtheirresultsforthisstudy,inparticularJ.Luque [24] D.Reynolds,P.Kenny,andF.Castaldo,“Astudyofnewapproaches
tospeakerdiarization,”inProc.Interspeech,2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P.Kenny,“BayesianAnalysisofSpeakerDiarizationwithEigenvoice
NTU,Singapore. Priors,”TechnicalReport. Montreal,QC,Canada:CRIM,2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
[26] X.AngueraandJ.-F.Bonastre,“Anovelspeakerbinarykeyderived [50] J.Ramirez,J.M.Girriz,andJ.C.Segura,M.GrimmandK.Kroschel,
fromanchormodels,”inProc.Interspeech,2010. Eds.,“Voiceactivitydetection.Fundamentalsandspeechrecognition
[27] X.AngueraandJ.-F.Bonastre,“Fastspeakerdiarizationbasedonbi- systemrobustness,”inProc.RobustSpeechRecognit.Understand.,Vi-
narykeys,”inProc.ICASSP,2011. enna,Austria,Jun.2007,p.460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C.FredouilleandG.Senay,“TechnicalimprovementsoftheE-HMM
C.Wooters,“Afast-matchapproachforrobust,fasterthanreal-time basedspeakerdiarizationsystemformeetingrecords,”inProc.MLMI
speakerdiarization,”inProc.IEEEWorkshopAutom.SpeechRecogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tionUnderstanding,Kyoto,Japan,Dec.2007,pp.693–698. Berlin,Heidelberg:Springer-Verlag,2006,pp.359–370.
[29] G.Friedland,J.Ching,andA.Janin,“Parallelizingspeaker-attributed [52] D.A.V.LeeuwenandM.Konecˇný,“ProgressintheAMIDAspeaker
speechrecognitionformeetingbrowsing,”inProc.IEEEInt.Symp. diarizationsystemformeetingdata,”inProc.MultimodalTechnol.for
Multimedia,Taichung,Taiwan,Dec.2010,pp.121–128. Percept.ofHumans:Int.Eval.WorkshopsCLEAR2007andRT2007,
[30] X.Anguera,C.Wooters,andJ.Hernando,“Friendsandenemies:A Baltimore,MD,May8–11,2007,RevisedSelectedPapers,Berlin,Hei-
novelinitializationforspeakerdiarization,”inProc.ICSLP,Pittsburgh, delberg:Springer-Verlag,2008,pp.475–483.
PA,Sep.2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera,“A robust speakerclustering algorithm,”in Proc.ASRU, Polymenakos,“The2006Athensinformationtechnologyspeechac-
2003,pp.411–416. tivitydetectionandspeakerdiarizationsystems,”inProc.Mach.Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France,May2006,pp.1025–1028. 2006,pp.385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A.Temko,D.Macho,andC.Nadeu,“EnhancedSVMtrainingforro-
channelchangedetectionandclusteringviathebayesianinformation bustspeechactivitydetection,”inProc.ICASSP,Honolulu,HI,2007,
criterion,”inProc.DARPABroadcastNewsTranscriptionandUnder- pp.1025–1028.
standingWorkshop,Lansdowne,VA,Feb.1998,pp.127–132. [55] X.Anguera,C.Wooters,M.Anguilo,andC.Nadeu,“Hybridspeech/
[34] H.GishandM.Schmidt,“Textindependentspeakeridentiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEESignalProcess.Mag.,vol.11,no.4,pp.18–32,Oct.1994. Proc.SpeakerOdysseyWorkshop,PuertoRico,Jun.2006.
[35] A.Janin,J.Ang,S.Bhagat,R.Dhillon,J.Edwards,J.Macias-Guarasa, [56] H.Sun,T.L.Nwe,B.Ma,andH.Li,“Speakerdiarizationformeeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. roomaudio,”inProc.Interspeech’09,Sep.2009.
Wrede,“TheICSImeetingproject:Resourcesandresearch,”inProc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSPMeetingRecognitionWorkshop,2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn,M.Guillemot,T.Hain,J.Kadlec,V.Karaiskos,M.Kronen- [58] E.El-Khoury,C.Senac,andJ.Pinquier,“Improvedspeakerdiariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tionsystemformeetings,”inProc.ICASSP,Taipei,Taiwan,2009,pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L.Lu,H.-J.Zhang,andH.Jiang,“Contentanalysisforaudioclassiﬁ-
[37] D.Mostefa,N.Moreau,K.Choukri,G.Potamianos,S.M.Chu,A. cationandsegmentation,”IEEETrans.SpeechAudioProcess.,vol.10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no.7,pp.504–516,Oct.2002.
matikakis,V.Mylonakis,F.Talantzis,S.Burger,R.Stiefelhagen,K. [60] R.Li, Q.Jin,and T.Schultz, “Improving speakersegmentation via
Bernardin,andC.Rochet,“TheCHILaudiovisualcorpusforlecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
andmeetinganalysisinsidesmartrooms,”Lang.ResourcesEval.,vol. Sep.2009,pp.3073–3076.
41,Dec.2007. [61] M.Ben,M.Betser,F.Bimbot,andG.Gravier,“Speakerdiarization
[38] C.Fredouille,D.Moraru,S.Meignier,L.Besacier,andJ.-F.Bonastre, usingbottom-upclusteringbasedonaparameter-deriveddistancebe-
“The NIST 2004 spring rich transcription evaluation: Two-axis tweenadaptedgmms,”inProc.ICSLP,JejuIsland,Korea,2004.
mergingstrategyinthecontextofmultipledistantmicrophonebased [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript.Eval.Workshop,Montreal,QC,Canada,2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q.Jin,K.Laskowski,T.Schultz,andA.Waibel,“Speakersegmen- vol.4299,LectureNotesinComputerScience,pp.371–384.
tationandclusteringinmeetings,”inProc.ICSLP,Jeju,Korea,Sep. [63] A.Vandecatseye,J.-P.Martens,J.Neto,H.Meinedo,C.Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D.Istrate,C.Fredouille,S.Meignier,L.Besacier,andJ.-F.Bonastre, Cizmar,H.Papageorgiou,andC.Alexandris,“Thecost278pan-Eu-
“NISTRT05Sevaluation:Pre-processingtechniquesandspeakerdi- ropeanbroadcastnewsdatabase,”inProc.LREC,Lisbon,Portugal,5,
arizationonmultiplemicrophonemeetings,”inProc.NIST2005Spring 2004,vol.4,pp.873–876.
RichTranscript.Eval.Workshop,Edinburgh,U.K.,Jul.2005. [64] K.MoriandS.Nakagawa,“Speakerchangedetectionandspeakerclus-
[41] X.Anguera,C.Wooters,B.Peskin,andM.Aguilo,“Robustspeaker teringusingVQdistortionforbroadcastnewsspeechrecognition,”in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc.ICASSP,2001,pp.413–416.
system,”inProc.NISTMLMIMeetingRecognitionWorkshop,Edin- [65] J.AjmeraandI.McCowan,“Robustspeakerchangedetection,”IEEE
burgh,U.K.,2005. SignalProcess.Lett.,vol.11,pp.649–651,2004.
[42] X.Anguera,C.Wooters,andJ.Hernando,“Acousticbeamformingfor [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speakerdiarizationofmeetings,”IEEETrans.Audio,Speech,Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process.,vol.15,no.7,pp.2011–2023,Sep.2007. 358–361.
[43] X.Anguera,BeamformIt(TheFastandRobustAcousticBeamformer) [67] X.AngueraandJ.Hernando,“Evolutivespeakersegmentationusinga
[Online].Available:http://www.xavieranguera.com/beamformit/ repositorysystem,”inProc.Interspeech,2004.
[44] N.Wiener,Extrapolation,Interpolation,andSmoothingofStationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
TimeSeries. NewYork:Wiley,1949. multi-partymeetingsusingacousticfusion,”inProc.ASRU,Nov.2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp.426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGIfeaturesforASR,”inProc.ICSLP,2002,vol.1,pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,”IEEESignal Process.Lett.,vol.13,no.8,pp.509–512,
[46] M.L.Seltzer,B.Raj,andR.M.Stern,“Likelihoodmaximizingbeam- Aug.2006.
formingforrobusthands-freespeechrecognition,”IEEETrans.Speech [70] M.-H.Siu,G.Yu,andH.Gish,“Segregationofspeakersforspeech
AudioProcess.,vol.12,no.5,pp.489–498,Sep.2004. recognitionandspeakeridentiﬁcation,”inProc.ICASSP’91,1991,pp.
[47] L.J.GrifﬁthsandC.W.Jim,“Analternativeapproachtolinearlycon- 873–876.
strainedadaptivebeamforming,”IEEETrans.AntennasPropagat.,vol. [71] P.DelacourtandC.Wellekens,“DISTBIC:Aspeaker-basedsegmen-
AP-30,no.1,pp.27–34,Jan.1982. tationforaudiodataindexing,”SpeechCommun.,pp.111–126,2000.
[48] M.Woelfel andJ. McDonough, DistantSpeechRecognition. New [72] S.S.HanandK.J.Narayanan,“Agglomerativehierarchicalspeaker
York:Wiley,2009. clusteringusingincrementalGaussianmixtureclustermodeling,”in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc.Interspeech’08,Brisbane,Australia,2008,pp.20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R.Gangadharaiah,B.Narayanaswamy,andN.Balakrishnan,“Anovel
inProc.Fall2004RichTranscript.Workshop(RT04),Palisades,NY, methodfortwospeakersegmentation,”inProc.ICSLP,Jeju,Korea,
Nov.2004. Sep.2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 369
[74] D.LiuandF.Kubala,“Fastspeakerchangedetectionforbroadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
newstranscriptionandindexing,”inProc.Eurospeech’99,Sep.1999, Ph.D.dissertation,Univ.ofCalifornia,Berkeley,2008.
pp.1031–1034. [102] S.OttersonandM.Ostendorf,“Efﬁcientuseofoverlapinformationin
[75] M.A.Siegler,U.Jain,B.Raj,andR.M.Stern,“Automaticsegmen- speakerdiarization,”inProc.ASRU,Kyoto,Japan,2007,pp.686–6.
tation,classiﬁcationandclusteringofbroadcastnewsaudio,”inProc. [103] B. E. D. Kingsbury, N.Morgan, and S. Greenberg, “Robust speech
DARPASpeechRecognit.Workshop,1997,pp.97–99. recognitionusingthemodulationspectrogram,”SpeechCommun.,vol.
[76] P.ZochováandV.Radová,“ModiﬁedDISTBICalgorithmforspeaker 25,no.1-3,pp.117–132,1998.
changedetection,”inProc.9thEur.Conf.SpeechCommun.Technol., [104] H.J.Nock,G.Iyengar,andC.Neti,“Speakerlocalizationusingaudio-
Bonn,Germany,2005,pp.3073–3076. visualsynchrony:Anempiricalstudy,”LectureNotesinComput.Sci.,
[77] X.Zhu,C.Barras,L.Lamel,andJ.-L.Gauvain,“Speakerdiarization: vol.2728,pp.565–570,2003.
Frombroadcastnewstolectures,”inProc.MLMI,2006,pp.396–406. [105] C.Zhang, P.Yin,Y.Rui,R.Cutler,and P.Viola,“Boosting-based
[78] K.HanandS.Narayanan,“Novelinter-clusterdistancemeasurecom- multimodalspeakerdetectionfordistributedmeetings,”inProc.IEEE
biningGLRandICRforimprovedagglomerativehierarchicalspeaker Int.WorkshopMultimediaSignalProcess.(MMSP),2006,pp.86–91.
clustering,”inProc.ICASSP,Apr.2008,pp.4373–4376. [106] A.NoulasandB.J.A.Krose,“On-linemulti-modalspeakerdiariza-
[79] D.Moraru,M.Ben,andG.Gravier,“Experimentsonspeakertracking tion,” inProc. 9th Int. Conf. Multimodal InterfacesICMI ’07, New
andsegmentationinradiobroadcastnews,”inProc.ICSLP,2005. York,2007,pp.350–357.
[80] C.Barras,X.Zhu,S.Meignier,andJ.-L.Gauvain,“Improvingspeaker [107] Z.GhahramaniandM.I.Jordan,“FactorialhiddenMarkovmodels,”
diarization,”inProc.DARPART04,2004. Mach.Learn.,vol.29,pp.245–273,Nov.1997.
[81] H.Aronowitz,“Trainablespeakerdiarization,”inProc.Interspeech, [108] A.K.Noulas,G.Englebienne,andB.J.A.Krose,“Mutimodalspeaker
Aug.2007,pp.1861–1864. diarization,”IEEETrans.PatternAnal.Mach.Intell.,2011,preprint,to
[82] H.HungandG.Friedland,“Towardsaudio-visualon-linediarizationof bepublished.
participantsingroupmeetings,”inProc.WorkshopMulti-Cameraand [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-ModalSensorFusionAlgorithmsApplicat.–M2SFA2,Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France,2008. Process.,vol.36,no.2–3,pp.117–124,2004.
[83] G.FriedlandandO.Vinyals,“Livespeakeridentiﬁcationinconversa- [110] T.ChenandR.Rao,“Cross-modalpredictioninaudio-visualcommu-
tions,”inProc.MM’08:Proc.16thACMInt.Conf.Multimedia,New nication,”inProc.ICASSP,1996,vol.4,pp.2056–2059.
York,2008,pp.1017–1018. [111] J.W.Fisher,T.Darrell,W.T.Freeman,andP.A.Viola,“Learningjoint
[84] G.Friedland,O.Vinyals,Y.Huang,andC.Muller,“Prosodicandother statistical models for audio-visual fusion and segregation,” in Proc.
long-termfeaturesforspeakerdiarization,”IEEETrans.Audio,Speech, NIPS,2000,pp.772–778.
Lang.Process.,vol.17,no.5,pp.985–993,Jul.2009. [112] J.W.FisherandT.Darrell,“Speakerassociationwithsignal-levelau-
[85] J.Luque,X.Anguera,A.Temko,andJ.Hernando,“Speakerdiariza- diovisualfusion,”IEEETrans.Multimedia,vol.6,no.3,pp.406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun.2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R.RaoandT.Chen,“Exploitingaudio-visualcorrelationincodingof
shopsCLEAR2007andRT2007,Baltimore,MD,May8–11,2007, talkingheadsequences,”inProc.Int.PictureCodingSymp.,Mar.1996.
RevisedSelectedPapers,Berlin,Heidelberg:Springer-Verlag, 2008, [114] M.SiracusaandJ.Fisher,“Dynamicdependencytestsforaudio-visual
pp.543–553. speakerassociation,”inProc.ICASSP,Apr.2007,pp.457–460.
[86] J.Pardo,X.Anguera,andC.Wooters,“Speakerdiarizationformul- [115] E.K.Patterson,S.Gurbuz,Z.Tufekci,andJ.N.Gowdy,“CUAVE:A
tipledistantmicrophonemeetings:Mixingacousticfeaturesandinter- newaudio-visualdatabaseformultimodalhuman–computerinterface
channeltimedifferences,”inProc.Interspeech,2006. research,”inProc.ICASSP,2002,pp.2017–2020.
[87] G.LathoudandI.M.Cowan,“Locationbasedspeakersegmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
inProc.ICASSP,2003,vol.1,pp.176–179. Press,2000.
[88] D.EllisandJ.C.Liu,“Speakerturndetectionbasedonbetween-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nelsdifferences,”inProc.ICASSP,2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J.Ajmera,G.Lathoud,andL.McCowan,“Clusteringandsegmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakersandtheirlocationsinmeetings,”inProc.ICASSP,2004,vol. 1150–1153.
1,pp.605–608. [118] H.Hung,Y.Huang,C.Yeo,andD.Gatica-Perez,“Associatingaudio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visualactivitycuesinadominanceestimationframework,”inProc.
multipledistantmicrophonemeetings:Mixingacousticfeaturesand IEEEComput.Soc.Conf.Comput.Vis.PatternRecognition(CVPR)
inter-channeltimedifferences,”inProc.Interspeech,2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J.Pardo,X.Anguera,andC.Wooters,“Speakerdiarizationformul- pp.1–6.
tiple-distant-microphonemeetingsusingseveralsourcesofinforma- [119] N.CampbellandN.Suzuki,“Workingwithverysparsedatatodetect
tion,”IEEETrans.Comput.,vol.56,no.9,pp.1212–1224,Sep.2007. speakerandlistenerparticipationinameetingscorpus,”inProc.Work-
[92] N.W.D.Evans,C.Fredouille,andJ.-F.Bonastre,“Speakerdiariza- shopProgramme,May2006,vol.10.
tionusingunsuperviseddiscriminantanalysisofinter-channeldelay [120] G.Friedland,H.Hung,andC.Yeo,“Multimodalspeakerdiarization
features,”inProc.ICASSP,Apr.2009,pp.4061–4064. ofreal-worldmeetingsusingcompressed-domainvideofeatures,”in
[93] M.Wölfel,Q.Yang,Q.Jin,andT.Schultz,“Speakeridentiﬁcation Proc.ICASSP,Apr.2009,pp.4069–4072.
usingwarpedMVDRcepstralfeatures,”inProc.Interspeech,2009. [121] G.Friedland,C.Yeo,andH.Hung,“Visualspeakerlocalizationaided
[94] E.Shriberg,“Higher-levelfeaturesinspeakerrecognition,”inSpeaker byacousticmodels,”inProc.17thACMInt.Conf.MultimediaMM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: ,NewYork,2009,pp.195–202.
Springer,2007,vol.4343,LectureNotesinArtiﬁcialIntelligence. [122] S.Meignier,D.Moraru,C.Fredouille,J.-F.Bonastre,andL.Besacier,
[95] D.ImsengandG.Friedland,“Tuning-robustinitializationmethodsfor “Step-by-stepandintegratedapproachesinbroadcastnewsspeakerdi-
speakerdiarization,”IEEETrans.Audio,Speech,Lang.Process.,vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18,no.8,pp.2028–2037,Nov.2010. Workshop(Odyssey’04),2006,pp.303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speechrecordings,”inProc.IEEEWorkshopAutom.SpeechRecognit. glomerativeandsequentialclusteringforspeakerdiarization,”inProc.
Understand.,Dec.2009,pp.432–437. ICASSP,LasVegas,NV,2008,pp.4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E.El-Khoury,C.Senac,andS.Meignier,“Speakerdiarization:Com-
Findings and implications for automatic processing of multi-party binationoftheLIUMandIRITsystems,”inInternalReport,2008.
conversations,”inProc.Eurospeech’01,Aalborg,Denmark,2001,pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O.ÇetinandE.Shriberg,“SpeakeroverlapsandASRerrorsinmeet- speaker diarization of telephone conversations,” in IEEE Signal
ings:Effectsbefore,during,andaftertheoverlap,”inProc.ICASSP, Process.Lett.,Dec.2007,vol.14,no.12,pp.1040–1043.
Toulouse,France,2006,pp.357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K.Boakye,B.Trueba-Hornero,O.Vinyals,andG.Friedland,“Over- lems,”Ann.Statist.,vol.1,no.2,pp.209–230,1973.
lappedspeechdetectionforimprovedspeakerdiarizationinmultiparty [127] F.Valente,“Inﬁnitemodelsforspeakerclustering,”inProc.Int.Conf.
meetings,”inProc.ICASSP,2008,pp.4353–4356. SpokenLang.Process.,2006,iDIAP-RR06–19.
[100] B.Trueba-Hornero,“Handlingoverlappedspeechinspeakerdiariza- [128] Y.W.Teh,M.I.Jordan,M.J.Beal,andD.M.Blei,“Hierarchical
tion,”M.S.thesis,Univ.PolitecnicadeCatalunya,Barcelona,Spain, Dirichletprocesses,” J.Amer. Statist. Assoc., vol. 101, no.476, pp.
2008. 1566–1581,2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics,speechenhancement,andacousticechocancellation.Histeamled
HDP-HMMforsystemswithstatepersistence,”inProc.ICML,Jul. LIA-EURECOM’sjointentrytotheNISTRichTranscriptionevaluationsin
2008. 2009.Hehasauthoredorcoauthoredinexcessof50peer-reviewedresearch
[130] M.HuijbregtsandC.Wooters,“Theblamegame:Performanceanalysis articlesandparticipatesinseveralnationalandEuropeanprojects,allinvolving
ofspeakerdiarizationsystemcomponents,”inProc.Interspeech,Aug. speechprocessing.
2007,pp.1857–60. Dr.EvansisamemberoftheIEEESignalProcessingSociety,ISCA,and
EURASIPandheservesasanAssociateEditoroftheEURASIPJournalon
XavierAngueraMiro(M’06)receivedtheTelecom- Audio,Speech,andMusicProcessing.
munications Engineering and European Masters in
LanguageandSpeech(M.S.)degreesfromtheUni-
versitatPolitècnicadeCatalunya(UPC),Barcelona,
Spain,in2001andthePh.D.degreefromUPC,witha CorinneFredouillereceivedthePh.D.degreefrom
thesison“RobustSpeakerDiarizationforMeetings.” theLaboratoireInformatiqued’Avignon(LIA),Uni-
From2001to2003,hewaswithPanasonicSpeech versityofAvignon,Avignon,France,in2000
TechnologyLab,SantaBarbara,CA.From2004to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIAin2003.Herresearchinterestsincludeacoustic
tionalComputerScienceInstitute(ICSI),Berkeley, analysis, voice quality assessment, statistical
CA,wherehepursuedresearchonspeakerdiariza- modeling, automatic speaker recognition, speaker
tionformeetings,contributingtoICSI’sparticipationintheNISTRTevalua- diarization and, more recently, speech and voice
tionsin2004(broadcastnews)and2005–2007(meetings),obtainingstate-of- disorder assessment and acoustic-based characteri-
the-artresults.HebrieﬂyjoinedLIMSI,Paris,France,in2006.Hehasbeen zation.Shehasparticipatedinseveralnationaland
withTelefonicaResearch,Barcelona,Spain,since2007,pursuingresearchin international speaker diarization system evaluation
multimedia.Hiscurrentresearchinterestsincludespeakercharacterization(in- campaignsandhaspublishedover15researchpapersinthisﬁeld.
cludingdiarization,recognition,etc.),languageidentiﬁcation(includingapar- Prof.FredouilleisamemberoftheInternationalSpeechCommunicationAs-
ticipationinNISTLRE’07evaluation)andseveraltopicsinmultimodalmulti- sociation(ISCA)andsecretaryoftheFrenchspeakingcommunicationassoci-
mediaanalysis(e.g.,videocopydetection,involvingtheparticipationinNIST ation(AFCP),SpecialInterestGroup(SIG)ofISCA.
TRECVID2009and2010evaluations).Hehasauthoredorcoauthoredover50
peer-reviewedresearcharticles.HeisthemaindeveloperoftheBeamformIt
toolkit,extensivelyusedbytheRTcommunityforprocessingmultiplemicro-
phonerecordings. GeraldFriedland(M’08)receivedthediplomand
Dr.AngueraMiroisamemberofISCA,ACM,andIEEESignalProcessing doctorate (summa cum laude) degrees in computer
SocietyandhasbeeninvolvedintheorganizationofseveralACMandIEEE science from Freie Universität Berlin, Berlin, Ger-
conferences.Hehasbeenareviewerformanyconferences,aswellasforseveral many,in2002and2006,respectively.
journalsinthemultimediadomain. HeisaSeniorResearchScientistattheInterna-
tionalComputerScienceInstitute(ICSI),Berkeley,
CA,anindependentnonproﬁtresearchlabassociated
withtheUniversityofCalifornia(UC)atBerkeley
Simon Bozonnet (S’08) received the diploma in wherehe,amongotherfunctions,iscurrentlyleading
electricalengineeringfromINSAdeLyon,France, thespeakerdiarizationresearch.Apartfromspeech,
in 2008 with specialization in signal processing hisinterestsalsoincludeimageandvideoprocessing
andtheMasterofResearchinImagesandSystems andmultimodalmachinelearning.HeisaPrincipalInvestigatoronanIARPA
from INSA. He undertook his M.S. thesis at the projectonvideoconceptdetectionandaCo-PrincipalInvestigatoronanNGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURIgrantonmultimodallocationestimation.Until2009hehadbeenasite
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is whichsponsoredtheresearchonmultimodalmeetinganalysisalgorithms.
currently pursuing the Ph.D. degree from Telecom Dr.FriedlandisamemberoftheIEEEComputerSocietyandtheIEEECom-
ParisTech,Paris,France,andjoinedtheMultimedia municationSociety,andheisinvolvedintheorganizationofvariousACMand
Communications Department as a Ph.D. candidate IEEEconferences,includingtheIEEEInternationalConferenceonSemantic
withLIA-EURECOM,Sophia-Antipolis,France. Computing(ICSC),whereheservedascochairandtheIEEEInternationalSym-
As part of his studies, he spent one year at KTH (Royal Institute of posiumonMultimedia(ISM2009),whereheservedasprogramcochair.Heis
Technology),Stockholm, Sweden. His research interests includemultimedia alsocofounderandProgramDirectoroftheIEEEInternationalSummerSchool
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- forSemanticComputingatUCBerkeley.Heistherecipientofseveralresearch
RECOMrecentsubmissiontotheNISTRT’09evaluationandcontributeshis andindustryrecognitions,amongthemtheMultimediaEntrepreneurAwardby
expertiseinspeakerdiarizationtothenational“ACAV”projectwhichaimsto theGermangovernmentandtheEuropeanAcademicSoftwareAward.Mostre-
improvewebaccessibilityforthevisuallyandhearingimpaired. cently,hewontheﬁrstprizeintheACMMultimediaGrandChallenge2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea(UWS),Swansea,U.K.,in1999and2003, thePolytechnicUniversityofCatalonia,Barcelona,
respectively. Spain,andtheM.S.degreeincomputersciencefrom
From2002and2006,hewasaLectureratUWS the University of California, San Diego, in 2009.
andwasanHonoraryLectureruntil2009.Hebrieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon UniversityofCalifornia,Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse(UAPV),Avignon,France,in2006before particularemphasisonmachinelearning,speech,and
moving to EURECOM, Sophia Antipolis, France, vision.He wasa VisitingScholarat theComputer
in2007whereheisnowanAssistantProfessor.At Science Department, Carnegie Mellon University,
EURECOM,heheadsresearchinspeechandaudioprocessingandiscurrently Pittsburgh,PA,in2006,whereheworkedincomputervisionandrobotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr.VinyalsreceivedaMicrosoftResearchPh.D.Fellowshipin2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
Speaker Diarization: A Review of Recent Research
Xavier AngueraMiro,Member,IEEE, SimonBozonnet,StudentMember,IEEE, Nicholas Evans,Member,IEEE,
CorinneFredouille, GeraldFriedland, Member,IEEE,and Oriol Vinyals
Abstract—Speakerdiarizationisthetaskofdetermining“who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers.Initially, it was proposed as a research topicrelated to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, activespeaker.Thisisthecasefortelephoneconversations(for
speakerdiarizationhasbecomeanimportantkeytechnologyfor examplestemmingfromcallcenters),broadcastnews,debates,
manytasks,suchasnavigation,retrieval,orhigherlevelinference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operationsfor instance), orevenlecture orconference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferencesinthearea.Theapplicationdomains,frombroadcast
news, to lectures and meetings, vary greatly and pose different sessions.In allsuchcases, itcanbe advantageous toautomat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition(inthepresenceofmultipleorcompetingspeakers),
meetings. Finally, we present an analysis of speaker diarization tohelpinspeech-to-texttranscription(i.e.,so-calledspeakerat-
performance as reported through the NIST Rich Transcription tributedspeech-to-text),speechtranslationand,moregenerally,
evaluations on meeting data and identify important areas for
RichTranscription(RT),acommunitywithinwhichthecurrent
futureresearch.
state-of-the-art technology has been developed. The most sig-
IndexTerms—Meetings,richtranscription,speakerdiarization.
niﬁcanteffortintheRichTranscriptiondomaincomesdirectly
fromtheinternationallycompetitiveRTevaluations,sponsored
bythe National Institute of Standards and Technology (NIST)
I. INTRODUCTION
intheUnitesStates[1].Initiatedoriginallywithinthetelephony
S PEAKER diarization has emerged as an increasingly im-
domain,andsubsequentlyinthatofbroadcastnews,todayitis
portantanddedicateddomainofspeechresearch.Whereas
in the domain of conference meetings that speaker diarization
speakerandspeechrecognitioninvolve,respectively,therecog-
receives the most attention. Speaker diarization is thus an
nitionofaperson’sidentityorthetranscriptionoftheirspeech,
extremelyimportantareaofspeechprocessingresearch.
speakerdiarizationrelatestotheproblemofdetermining“who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sentedin[2],althoughitpredominantlyfocusesitsattentionto
identiﬁcation of each speaker within an audio stream and the
speakerdiarizationforbroadcastnews.Coupledwiththetran-
intervalsduringwhicheachspeakerisactive.
sitiontoconferencemeetings,however,thestate-of-the-arthas
advancedsigniﬁcantlysincethen.Thispaperpresentsanup-to-
datereviewofpresentstate-of-the-artsystemsandreviewsthe
ManuscriptreceivedAugust19,2010;revisedDecember03,2010;accepted
progressmadeintheﬁeldofspeakerdiarizationsince2005up
February13,2011.DateofcurrentversionJanuary13,2012.Thisworkwas
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS)projectfundedthroughtheEuropeanAmbientAssistedLiving(AAL) washeldin2009.Ofﬁcialevaluationsareanimportantvehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
forpushing thestate-of-the-artforward asitisonlywithstan-
Collaborativepourl’AccessibilitéVidéo”(ACAV)projectfundedbytheFrench
MinistryofIndustry(InnovativeWebcall)underContract09.2.93.0966.The dardexperimentalprotocolsanddatabasesthatitispossibleto
workofX.AngueraMirowassupportedinpartbytheTorresQuevedoSpanish meaningfullycomparedifferentapproaches.Whilewealsoad-
program.Theassociateeditorcoordinatingthereviewofthismanuscriptand
dressemergingnewresearchinspeakerdiarization,inthispaper
approvingitforpublicationwasProf.SadaokiFurui.
X.AngueraMiroiswiththeMultimediaResearchGroup,TelefonicaRe- special emphasis is placed on established technologies within
search,08021Barcelona,Spain(e-mail:xanguera@tid.es). thecontextoftheNISTRTbenchmarkevaluations,whichhas
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C.FredouilleiswiththeUniversityofAvignon,CERI/LIA,F-84911Avignon ence overview of established approaches, both for the general
Cedex9,France(e-mail:corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedlandand O.Vinyals are withthe International ComputerScience
Institute(ICSI),Berkeley,CA94704USA(e-mail:fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Colorversionsofoneormoreoftheﬁguresinthispaperareavailableonline
tion.Thereareoutstandingopportunitiesforcontributionsand
athttp://ieeexplore.ieee.org.
DigitalObjectIdentiﬁer10.1109/TASL.2011.2125954 wehopethatthispaperservestoencourageotherstoparticipate.
1558-7916/$31.00©2012IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 357
Section II presents a brief history of speaker diarization linguisticcontentandothermetadatacanbeadded(suchasthe
research and the transition to the conference meeting domain. dominantspeakers,thelevelofinteractions,oremotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
presentamoredetaileddescriptionofthemainalgorithmsthat ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. SectionIVpresents someof the most recent workin data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researcherstogetherandtostimulatingnewideastoadvancethe
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetingsand coffeebreaks havealso beenconsidered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
presentanoverviewofthecurrentstatusinspeakerdiarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusionsarepresentedinSectionVI. diarizationthattypicallywerelessrelevantinearlierresearch.
II. SPEAKERDIARIZATION A. BroadcastNewsVersusConferenceMeetings
Over recent years, the scientiﬁc community has developed
WiththechangeoffocusoftheNISTRTevaluationsfromBN
research on speaker diarization in a number of different do-
tomeetingsdiarizationalgorithmshadtobeadaptedaccording
mains,withthefocususuallybeingdictatedbyfundedresearch
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
dataisusuallyacquiredusingboomorlapelmicrophoneswith
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld.Conversely,meetingsareusuallyrecordedusingdesktop
tion was aimed at automatically annotating TV and radio
orfar-ﬁeldmicrophones(singlemicrophonesormicrophonear-
transmissions that are broadcast daily all over the world. An-
rays)whicharemoreconvenientforusersthanhead-mountedor
notations included automatic speech transcription and meta
lapelmicrophones.2Asaresult,thesignal-to-noiseratioisgen-
data labeling, including speaker diarization. Interest in the
erallybetterforBNdatathanitisformeetingrecordings.Addi-
meeting domain grew extensively from 2002, with the launch
tionally,differencesbetweenmeetingroomconﬁgurationsand
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
includingbackgroundnoise,reverberationandvariablespeech
Swiss InteractiveMultimodal Information Management (IM2)
levels(dependingonthedistancebetweenspeakersandmicro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop(CHIL)project.Alltheseprojectsaddressedtheresearch
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter,orapplause(farlesscommonforconferencemeeting
enhancementofhuman-to-humancommunications(notablyin
data),ingeneral,thedetectionofacousticeventsandspeakers
distant access) by automatically extracting meeting content,
tendstobemorechallengingforconferencemeetingdatathan
makingtheinformationavailabletomeetingparticipants,orfor
forBN data.
archivingpurposes.
Finally, the number of speakers is usually larger in BN but
These technologies haveto meet challenging demands such
speaker turnsoccur less frequently than they doin conference
ascontentindexing,linkingand/orsummarizationofon-going
meetingdata,resultinginBNhavingalongeraveragespeaker
orarchivedmeetings,theinclusionofbothverbalandnonverbal
turn length. An extensive analysis of BN characteristics is re-
humancommunication(peoplemovements,emotions,interac-
portedin[3]andacomparisonofBNandconferencemeeting
tions with others, etc.). This is achieved by exploiting several
datacanbefoundin[4].
synchronizeddatastreams,suchasaudio,videoandtextualin-
formation(agenda,discussionpapers,slides,etc.),thatareable 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
tocapturedifferentkindsofinformationthatareusefulforthe Recognition (SR) evaluation campaigns (focusing on telephone speech) and
notwithintheRTevaluationcampaigns.
structuringandanalysisofmeetingcontent.Speakerdiarization
2Meeting databases recorded for research purposes usually contain
playsanimportantroleintheanalysisofmeetingdatasinceital-
head-mounted and lapel microphone recordings for ground-truth creation
lowsforsuchcontenttobestructuredinspeakerturns,towhich purposesonly.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
assignedtothetwoindividualclusters.Standarddistancemet-
rics,suchasthosedescribedinSectionIII-C,areusedtoiden-
tify the closest clusters. A reassignment of frames to clusters
isusuallyperformedaftereachclustermerging,viaViterbire-
alignmentforexample,andthewholeprocessisrepeateditera-
tively,untilsomestoppingcriterionisreached,uponwhichthere
shouldremainonlyoneclusterforeachdetectedspeaker.Pos-
sible stopping criteria include thresholded approaches such as
theBayesianInformationCriterion(BIC)[9],Kullback–Leibler
(KL)-basedmetrics[10],thegeneralizedlikelihoodratio(GLR)
[11]ortherecentlyproposed metric[12].Bottom-upsystems
submittedtotheNISTRTevaluations[9],[13]haveperformed
consistentlywell.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b)Generalspeakerdiarizationarchitecture.
streamwithasinglespeakermodelandsuccessivelyaddsnew
modelstoituntilthefullnumberofspeakersaredeemedtobe
B. MainApproaches
accountedfor.AsingleGMMmodelistrainedonallthespeech
Most of present state-of-the-art speaker diarization systems segmentsavailable,allofwhicharemarkedasunlabeled.Using
ﬁtintooneoftwocategories:thebottom-upandthetop-down someselectionproceduretoidentifysuitabletrainingdatafrom
approaches,asillustratedinFig.1(a).Thetop-downapproach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the addedtothemodelone-by-one,withinterleavedViterbirealign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim newmodelsaremarkedaslabeled.Stoppingcriteriasimilarto
istoiterativelyconvergetowardsanoptimumnumberofclus- thoseemployedinbottom-upsystemsmaybeusedtoterminate
ters. If the ﬁnal number is higher than the optimum then the theprocessoritcancontinueuntilnomorerelevantunlabeled
system is said to under-cluster. If it is lower it is said to over- segmentswithwhichtotrainnewspeakermodelsremain.Top-
cluster.Bothbottom-upandtop-downapproachesaregenerally downapproachesarefarlesspopularthantheirbottom-upcoun-
basedonhiddenMarkovmodels(HMMs)whereeachstateisa terparts.Someexamplesinclude[14]–[16].Whiletheyaregen-
Gaussianmixturemodel(GMM)andcorrespondstoaspeaker. erallyout-performedbythebestbottom-upsystems,top-down
Transitionsbetweenstatescorrespond tospeakerturns.Inthis approaches have performed consistently and respectably well
section,webrieﬂyoutlinethestandardbottom-upandtop-down againstthebroaderﬁeldofotherbottom-upentries.Top-down
approaches as well as two recently proposed alternatives: one approachesarealsoextremelycomputationallyefﬁcientandcan
basedoninformationtheory;andasecondonebasedonanon beimprovedthroughclusterpuriﬁcation[17].
parametricBayesianapproach.Althoughthesenewapproaches 3) OtherApproaches: Arecentalternativeapproach,though
havenotbeenreportedpreviouslyinthecontextofofﬁcialNIST alsobottom-upinnature,isinspiredfromrate-distortiontheory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
someotherworksproposesequentialsingle-passsegmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance withsigniﬁcantsavingsincomputation.Clusteringisbasedon
tendstofallshortofthestate-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-UpApproach: Thebottom-upapproachisbyfar of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up anewspaceofrelevancevariablesdeﬁned bytheGMMcom-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successivelymergingandreducingthenumberofclustersuntil informationbetweensuccessiveclusteringswhilepreservingas
onlyoneremainsforeachspeaker.Variousinitializationshave much information as possible from the original dataset. Two
beenstudiedand,whereassomehaveinvestigated -meansclus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mationbottleneck(aIB)[18]andthesequentialinformationbot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simplerapproach generally leadsto equivalent better performance than parametric approaches, results com-
performance[8].Inallcasestheaudiostreamisinitiallyover- parable to state-of-the-art GMM systems are reported and are
segmentedintoanumberofsegmentswhichexceedstheantic- achievedwithgreatsavingsincomputation.
ipatedmaximumnumberofspeakers.Thebottom-upapproach Alternatively,Bayesianmachinelearningbecamepopularby
theniterativelyselectscloselymatchingclusterstomerge,hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
ClustersaregenerallymodeledwithaGMMand,uponmerging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 359
relateddistribution(hyperparameters).Thisallowsforavoiding clustering[15],[16].Next,inFig.1(b)-iii/iv,adistancebetween
anyprematureharddecisioninthediarizationproblemandfor clusters and a split/merging mechanism (see Section III-D) is
automaticallyregulatingthesystemwiththeobservations(e.g., usedtoiterativelymergeclusters[13],[31]ortointroducenew
the complexity of the model is data dependent). However, the ones[16].Optionally,datapuriﬁcationalgorithmscanbeused
computationofposteriordistributionsoftenrequiresintractable tomakeclustersmorediscriminant[13],[17],[32].Finally,as
integralsand,asaresult,thestatisticscommunityhasdeveloped illustratedinFig.1(b)-v,stoppingcriteriaareusedtodetermine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs)wereﬁrstused[20]toprovideasystematicapproach [34].
to the computation of distributions via sampling, enabling the
deploymentofBayesianmethods.However,samplingmethods A. AcousticBeamforming
are generally slow and prohibitivewhen the amount of data is Theapplicationofspeakerdiarizationtothemeetingdomain
large,andtheyrequiretoberunseveraltimesasthechainsmay triggeredtheneedfordealingwithmultiplemicrophoneswhich
getstuckandnotconvergeinapracticalnumberofiterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can havedif-
hasbeenpopularsince1993[21],[22]andaimsatprovidinga ferentcharacteristics:wall-mountedmicrophones(intendedfor
deterministic approximation of the distributions. It enables an speakerlocalization),lapelmicrophones,desktopmicrophones
inference problem to be converted toan optimization problem positionedonthemeetingroomtableormicrophonearrays.The
by approximating the intractable distribution with a tractable useofdifferentmicrophonecombinationsaswellasdifferences
approximation obtained by minimizing the Kullback–Leibler inmicrophonequalitycalledfornewapproachestospeakerdi-
divergence between them. In [23] a Variational Bayes-EM arizationwithmultiplechannels.
algorithmisusedtolearnaGMMspeakermodelandoptimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithmshavebeenproposedtoextendmono-channeldiariza-
modeling, described in [25], for the speaker diarization of tionsystemstohandlemultiplechannels.Oneoption,proposed
telephone conversations. However, these systems still con- in[38],istoperformspeakerdiarizationoneachchannelinde-
sider classical Viterbi decoding for the classiﬁcation and pendentlyandthentomergetheindividualoutputs.Inorderto
differ from the nonparametric Bayesian systems introduced in doso,atwoaxismergingalgorithmisusedwhichconsidersthe
SectionIV-F. longestdetectedspeakersegmentsineachchannelanditerates
Finally,therecentlyproposedspeakerbinarykeys[26]have overthesegmentationoutput.Inthesameyear,alate-stagefu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keysare small bi- segmentshavethebestsignal-to-noiseratio(SNR).Subsequent
naryvectorscomputedfromtheacousticdatausingauniversal approachesinvestigatedpreprocessingtocombinetheacoustic
backgroundmodel(UBM)-likemodel.Oncetheyarecomputed signalstoobtainasinglechannelwhichcouldthenbeprocessed
all processing tasks take place in the binary domain. Other byaregularmono-channeldiarizationsystem.In[40],themul-
worksinspeakerdiarizationconcernedwithspeedinclude[28], tiple channels are combined with a simple weighted sum ac-
[29]whichachievefasterthanreal-timeprocessingthroughthe cordingtotheirSNR.Thoughstraightforwardtoimplement,it
useofseveralprocessingtricksappliedtoastandardbottom-up doesnottakeintoaccountthetimedifferenceofarrivalbetween
approach ([28]) or by parallelizing most of the processing eachmicrophonechannelandmighteasilyleadtoadecreasein
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
whenusingdiarizationasapreprocessingsteptootherspeech proach to multi-channel speaker diarization involves acoustic
algorithms. beamformingasinitiallyproposedin[41]anddescribedinde-
tailin[42].ManyRTparticipantsusethefreeandopen-source
III. MAINALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig.1(b)showsablockdiagramofthegenericmoduleswhich which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correctmisalignmentsduetothetime-delay-of-arrival(TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc.Formeetingdata,preprocessingusuallyinvolvesnoisere- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acousticbeamforming(seeSectionIII-A),theparameterization theotherchannelsareappropriatelyalignedandcombinedwith
ofspeechdataintoacousticfeatures(suchasMFCC,PLP,etc.) astandarddelay-and-sumalgorithm.Thecontributionmadeby
and the detection of speech segments with a speech activity eachsignalchanneltotheoutputisthendynamicallyweighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down andtostabilizetheTDOAvaluesbetweenchannelsbeforethe
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 360 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
signals are summed. Finally, the TDOA estimates themselves Speech and nonspeech models may optionally be adapted to
aremadeavailableasoutputsandhavebeenusedsuccessfully speciﬁc meeting conditions [15]. Discriminant classiﬁers such
to improve diarization, as explained in Section IV-A. Note as linear discriminant analysis (LDA) coupled with Mel fre-
that, although there are other algorithms that can provide quency cepstrum coefﬁcients (MFCCs) [53] or support vector
better beamforming results for some cases, delay-and-sum machines (SVMs) [54] have also been proposed in the litera-
beamformingisthemostreliableonewhennoinformationon ture.Themaindrawbackofmodel-basedapproachesistheirre-
the location or nature of each microphone is known a priori. lianceonexternaldataforthetrainingofspeechandnonspeech
Amongalternativebeamformingalgorithmsweﬁndmaximum models which makes them less robust to changes in acoustic
likelihood (ML) [46] or generalized sidelobe canceller (GSC) conditions.Hybridapproacheshavebeenproposedasapoten-
[47] which adaptively ﬁnd the optimum parameters, and min- tialsolution.Inmostcases,anenergy-baseddetectionisﬁrstap-
imum variance distortionless response (MVDR) [48] when pliedinordertolabelalimitedamountofspeechandnonspeech
prior information on ambient noise is available. All of these dataforwhichthereishighconﬁdenceintheclassiﬁcation.Ina
havehighercomputationalrequirementsand,inthecaseofthe secondstep,thelabeleddataareusedtotrainmeeting-speciﬁc
adaptivealgorithms,thereisthe dangerofconvergingtoinac- speechandnonspeechmodels,whicharesubsequentlyusedina
curateparameters,especiallywhenprocessingmicrophonesof model-baseddetectortoobtaintheﬁnalspeech/nonspeechseg-
differenttypes. mentation[9],[55]–[57].Finally,[58]combinesamodel-based
witha4-Hzmodulationenergy-baseddetector.Interestingly,in-
B. SpeechActivityDetection steadofbeing applied asa preprocessingstage, inthis system
Speech activity detection (SAD) involves the labeling of SADisincorporatedintothespeakerdiarizationprocess.
speech and nonspeech segments. SAD can have a signiﬁcant
C. Segmentation
impact on speaker diarization performance for two reasons.
The ﬁrst stems directly from the standard speaker diarization In the literature, the term “speaker segmentation” is some-
performance metric, namely the diarization error rate (DER), timesusedtorefertobothsegmentationandclustering.While
which takes into account both the false alarm and missed some systems treat each task separately many of present
speaker error rates (see Section VI-A for more details on state-of-the-art systems tackle them simultaneously, as de-
evaluation metrics); poor SAD performance will therefore scribed in Section III-E. In these cases the notion of strictly
lead to an increased DER. The second follows from the fact independent segmentation and clustering modules is less rel-
that nonspeech segments can disturb the speaker diarization evant. However, both modules are fundamental to the task of
process,andmorespeciﬁcallytheacousticmodelsinvolvedin speakerdiarizationandsomesystems,suchasthatreportedin
theprocess[49].Indeed,theinclusionofnon-speechsegments [6], apply distinctly independent segmentation and clustering
inspeakermodellingleadstolessdiscriminantmodelsandthus stages. Thus, the segmentation and clustering models are
increased difﬁculties in segmentation. Consequently, a good describedseparatelyhere.
compromisebetweenmissedandfalsealarmspeecherrorrates Speaker segmentation is core to the diarization process and
hastobefoundtoenhancethequalityofthefollowingspeaker aims at splitting the audio stream into speaker homogeneous
diarizationprocess. segments or, alternatively, to detect changes in speakers, also
SAD is a fundamental task in almost all ﬁelds of speech knownasspeakerturns.Theclassicalapproachtosegmentation
processing (coding, enhancement, and recognition) and many performs a hypothesis testing using the acoustic segments in
different approaches and studies have been reported in the twoslidingandpossiblyoverlapping,consecutivewindows.For
literature [50]. Initial approaches for diarization tried to solve eachconsideredchangepointtherearetwopossiblehypotheses:
speech activity detection on the ﬂy, i.e., by having a non- ﬁrstthatbothsegmentscomefromthesamespeaker( ),and
speech cluster be a by-product of the diarization. However, thus that they can be well represented by a single model; and
it became evident that better results are obtained using a secondthattherearetwodifferentspeakers( ),andthusthat
dedicated speech/nonspeech detector as preprocessing step. twodifferentmodelsaremoreappropriate.Inpractice,models
In the context of meetings nonspeech segments may include are estimated from each of the speech windows and some cri-
silence, but also ambient noise such as paper shufﬂing, door teriaareusedtodeterminewhethertheyarebestaccountedfor
knocks or non-lexical noise such as breathing, coughing, and bytwoseparatemodels(andhencetwoseparatespeakers),orby
laughing, among other background noises. Therefore, highly asinglemodel(andhencethesamespeaker)byusinganempir-
variable energy levels can be observed in the nonspeech parts icallydeterminedordynamicallyadaptedthreshold[10],[59].
of the signal. Moreover, differences in microphones or room Thisisperformedacrossthewholeaudiostreamandasequence
conﬁgurations may result in variable SNRs from one meeting ofspeakerturnsisextracted.
to another. Thus, SAD is far from being trivial in this context Many different distance metrics have appeared in the liter-
and typical techniques based on feature extraction (energy, ature. Next, we review the dominant approaches which have
spectrum divergence between speech and background noise, been used for the NIST RT speaker diarization evaluations
andpitchestimation)combinedwithathreshold-baseddecision during the last four years. The most common approach is that
haveproventoberelativelyineffective. of the Bayesian information criterion (BIC) and its associated
Model-based approaches tend to have better performances BIC metric [33] which has proved to be extremely popular,
and rely ona two-class detector, with models pre-trainedwith e.g.,[60]–[62].Theapproachrequiresthesettingofanexplicit
external speech and nonspeech data [6], [41], [49], [51], [52]. penaltytermwhichcontrolsthetradeoffbetweenmissedturns
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 361
and those falsely detected. It is generally difﬁcult to estimate Since this is rarely the case, alternative approaches combine
the penalty term such that it gives stable performance across clustering with iterative resegmentation, hence facilitating
differentmeetingsandthusnew,morerobustapproacheshave the introduction of missing speaker turns. Most of present
beendevised.Theyeitheradaptthepenaltytermautomatically, diarization systems thus perform segmentation and clustering
i.e.,themodiﬁedBICcriterion[33],[63],[64],oravoidtheuse simultaneously or clustering on a frame-to-cluster basis, as
of a penalty term altogether by controlling model complexity described in Section III-E. The general approach involves
[65]. BIC-based approaches are computationally demanding Viterbi realignment where the audio stream is resegmented
andsomesystemshavebeendevelopedinordertousetheBIC based on the current clustering hypothesis before the models
onlyinasecondpass,whileastatistical-baseddistanceisused are retrained on the new segmentation. Several iterations are
in a ﬁrst pass [66]. Another BIC-variant metric, referred to as usuallyperformed.InordertomaketheViterbidecodingmore
cross-BIC and introduced in [67] and [68], involves the com- stable,itiscommontouseaViterbibuffertosmooththestate,
putation of cross-likelihood: the likelihood of a ﬁrst segment cluster or speaker sequence to remove erroneously detected,
accordingtoamodeltunedfromthesecondsegmentandvice brief speaker turns, as in [16]. Most state-of-the-art systems
versa.In[69],differenttechniquesforlikelihoodnormalization employsomevariationsonthisparticularissue.
arepresentedandarereferredtoasbilateralscoring. Analternativeapproachtoclusteringinvolvesmajorityvoting
ApopularandalternativeapproachtoBIC-basedmeasuresis [82], [83] whereby short windows of frames are entirely as-
the generalized likelihood ratio (GLR), e.g.,[70], [71]. In con- signed to the closest cluster, i.e., that which attracts the most
trasttotheBIC,theGLRisalikelihood-basedmetricandcorre- framesduringdecoding.Thistechniqueleadstosavingsincom-
spondstotheratiobetweenthetwoaforementionedhypotheses, putationbutismoresuitedtoonlineorlivespeakerdiarization
as described in [39], [72], and [73]. To adapt the criterion in systems.
ordertotakeintoaccounttheamountoftrainingdataavailable
E. One-StepSegmentationandClustering
inthetwosegments,apenalizedGLRwasproposedin[74].
ThelastofthedominantapproachesistheKullback–Leibler Most state-of-the-art speaker diarization engines unify the
(KL) divergence which estimates the distance between two segmentation and clustering tasks into one step. In these sys-
random distributions [75]. However, the KL divergence is tems,segmentationandclusteringareperformedhand-in-hand
asymmetric,andthustheKL2metric,asymmetricalternative, inoneloop.SuchamethodwasinitiallyproposedbyICSIfor
hasprovedtobemorepopularinspeakerdiarizationwhenused a bottom-up system [31] and has subsequently been adopted
tocharacterizethesimilarityoftwoaudiosegments[75]–[77]. by many others [9], [41], [52], [84]–[86]. For top-down algo-
Finally,inthissectionweincludeanewlyintroduceddistance rithmsitwasinitiallyproposedbyLIA[14]asusedintheirlatest
metricthathasshownpromiseinaspeakerdiarizationtask.The system[16].
informationchangerate(ICR),orentropycanbeusedtochar- Inallcasesthedifferentacousticclassesarerepresentedusing
acterizethesimilarityoftwoneighboringspeechsegments.The HMM/GMMmodels.EMtrainingorMAPadaptationisusedto
ICR determines the change in information that would be ob- obtain the closest possible models given the current frame-to-
tained by merging any two speech segments under considera- modelassignments,andaViterbialgorithmisusedtoreassign
tionandcanthusbeusedforspeakersegmentation.Unlikethe all the data into the closest newly-created models. Such pro-
measures outlined above, the ICR similarity is not based on a cessingissometimesperformedseveraltimesfortheframeas-
model of each segment but, instead, on the distance between signments to stabilize. This step is useful when a class is cre-
segmentsinaspaceofrelevancevariables,withmaximummu- ated/eliminatedsothattheresultingclassdistributionisallowed
tualinformationorminimumentropy.Onesuitablespacecomes toadapttothedata.
from GMM component parameters [18]. The ICR approach is Theone-stepsegmentationandclusteringapproach,although
computationallyefﬁcientand,in[78],ICRisshowntobemore much slower, constitutes a clear advantage versus sequential
robusttodatasourcevariationthanaBIC-baseddistance. single-passsegmentationandclusteringapproaches[5]–[7].On
theonehand,earlyerrors(mostlymissedspeakerturnsfromthe
D. Clustering segmentationstep)canbelatercorrectedbythere-segmentation
Whereasthesegmentationstepoperatesonadjacentwindows steps.Ontheotherhand,mostspeakersegmentationalgorithms
inordertodeterminewhetherornottheycorrespondtothesame useonlylocalinformationtodecideonaspeakerchangewhile
speaker, clustering aims at identifying and grouping together whenusingspeakermodelsandViterbirealignmentalldatais
same-speakersegmentswhichcanbelocalizedanywhereinthe takenintoconsideration.
audiostream.Ideally,therewillbeoneclusterforeachspeaker. WhenperformingframeassignmentusingViterbialgorithm
Theproblemofmeasuringsegmentsimilarityremainsthesame a minimum assignment duration is usually enforced to avoid
andallthedistancemetricsdescribedinSectionIII-Cmayalso an unrealistic assignment of very small consecutive segments
beusedforclustering,i.e.,theKLdistanceasin[10],amodiﬁed todifferentspeakermodels.Suchminimumdurationisusually
KL2 metric as in [61], a BIC measure as in [79] or the cross madeaccordingtotheestimatedminimumlengthofanygiven
likelihoodratio(CLR)asin[80]and[81]. speakerturn.
However, with such an approach to diarization, there is no
provision for splitting segments which contain more than a IV. CURRENTRESEARCHDIRECTIONS
single speaker, and thus diarization algorithms can only work Inthissection,wereviewthoseareasofworkwhicharestill
well if the initial segmentation is of sufﬁciently high quality. notmaturebutwhichhavethepotentialtoimprovediarization
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 362 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
performance.WeﬁrstdiscussthetrendinrecentNISTRTeval- diarization performance and the success of these systems in
uationstousespatialinformationobtainedfrommultiplemicro- NISTRTevaluationswouldseemtosupporttheiruse.
phones, whichareusedbymanyincombinationwithMFCCs
B. UseofProsodicFeaturesinDiarization
toimproveperformance.Then,wediscusstheuseofprosodic
informationwhichhasledtopromisingspeakerdiarizationre- The use of prosodic features for both speaker detection
sults. Also addressed in this section is the “Achilles heel” of and diarization is emerging as a reaction to the theoretical
speaker diarization for meetings, which involves overlapping inconsistency derived from using MFCC features both for
speech; many researchers have started to tackle the detection speaker recognition (which requires invariance against words)
ofoverlappingspeechanditscorrectlabelingforimproveddi- and speech recognition (which requires invariance against
arizationoutputs.Wethenconsiderarecenttrendtowardsmul- speakers)[93].In[84],theauthorspresentasystematicinvesti-
timodal speaker diarization including studies of multimodal, gationofthespeakerdiscriminabilityof70long-termfeatures,
audiovisual techniques which have been successfully used for most of them prosodic features. They provide evidence that
speaker diarization, at least for laboratory conditions. Finally, despitethedominanceofshort-termcepstralfeaturesinspeaker
weconsidergeneralcombinationstrategiesthatcanbeusedto recognition, a number of long-term features can provide sig-
combine the output of different diarization systems. The fol- niﬁcant information for speaker discrimination. As already
lowingsummarizesrecentworkinalloftheseareas. suggested in [94], the consideration of patterns derived from
largersegmentsofspeechcanrevealindividualcharacteristics
A. Time-DelayFeatures
of the speakers’ voices as well as their speaking behavior,
Estimates of inter-channel delay may be used not only for information which cannot be captured using a short-term,
delay-and-sumbeamformingofmultiplemicrophonechannels, frame-based cepstral analysis. The authors use Fisher LDA as
asdescribedinSectionIII-A,butalsoforspeakerlocalization. arankingmethodologyandsortthe70prosodicandlong-term
If we assume that speakers do not move, or that appropriate features by speaker discriminability. The combination of the
trackingalgorithmsareused,thenestimatesofspeakerlocation top-tenrankedprosodicandlong-termfeaturescombinedwith
maythusbeusedasalternativefeatures,whichhavenowadays regular MFCCs leads to a 30% relative improvement in terms
become extremely popular. Much of the early work, e.g.,[87], of DER compared to the top-performing system of the NIST
requires explicit knowledge of microphone placement. How- RT evaluation in 2007. An extension of the work is provided
ever, as is the case with NIST evaluations, such a priori in- in [95]. The paper presents a novel, adaptive initialization
formationisnotalwaysavailable.Theﬁrstwork[88]thatdoes scheme that can be applied to standard bottom-up diarization
notrelyonmicrophonelocationsledtopromisingresults,even algorithms. The initialization method is a combination of the
iferrorrateswereconsiderablyhigherthanthatachievedwith recently proposed “adaptive seconds per Gaussian” (ASPG)
acousticfeatures.Earlyeffortstocombineacousticfeaturesand method [96] and a new pre-clustering method in addition to
estimatesofinter-channeldelayclearlydemonstratedtheirpo- a new strategy which automatically estimates an appropriate
tential,e.g.,[89],thoughthisworkagainrelieduponknownmi- numberofinitialclustersbasedonprosodicfeatures.Itoutper-
crophonelocations. forms previous cluster initialization algorithms by up to 67%
More recent work, and speciﬁcally in the context of NIST (relative).
evaluations, reports the successful combination of acoustic
C. OverlapDetection
and inter-channel delay features [86], [90], [91] when they
are combined at the weighted log-likelihood level, though Afundamentallimitationofmostcurrentspeakerdiarization
optimum weights were found to vary across meetings. Better systems is that only one speaker is assigned to each segment.
results are reported in [42] where automatic weighting based Thepresenceofoverlappedspeech,though,iscommoninmul-
onanentropy-basedmetricisusedforclustercomparisonina tipartymeetingsand,consequently,presentsasigniﬁcantchal-
bottom-upspeakerdiarizationsystem.Acompletefront-endfor lengetoautomaticsystems.Speciﬁcally,inregionswheremore
speakerdiarizationwithmultiplemicrophoneswasproposedin thanonespeakerisactive,missedspeecherrorswillbeincurred
[42].Hereatwo-stepTDOAViterbipost-processingalgorithm and, given the high performance of some state-of-the-art sys-
together with a dynamic output signal weighting algorithm tems, this can be a substantial fraction of the overall diariza-
were shown to greatly improve speaker diarization accuracy tion error. A less direct, but also signiﬁcant, effect of over-
and the robustness of inter-channel delay estimates to noise lappedspeechindiarizationpertainstospeakerclusteringand
and reverberation, which commonly afﬂict source localization modeling. Segments which contain speech from more than a
algorithms.Morerecently,anapproachtotheunsuperviseddis- singlespeakershouldnotbeassignedtoanyindividualspeaker
criminantanalysisofinter-channeldelayfeatureswasproposed cluster nor included in any individual speaker model. Doing
in [92] and results of approximately 20% DER were reported so adversely affects the purity of speaker models, which ulti-
usingdelayfeaturesalone. matelyreducesdiarizationperformance.Approachestooverlap
InthemostrecentNISTRTevaluation,in2009,allbutone detection werethoroughly assessed in[97] and [98] and,even
entry used estimates of inter-channel delay both for beam- while applied to ASR as opposed to speaker diarization, only
forming and as features. Since comparative experiments are asmallnumberofsystemsactuallydetectsoverlappingspeech
rarely reported it is not possible to assess the contribution wellenoughtoimproveerrorrates[99]–[101].
of delay features to diarization performance. However, those Initially, the authors in [102] demonstrated a theoretical
who do use delay features report signiﬁcant improvements in improvement in diarization performance by adding a second
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 363
speaker during overlap regions using a simple strategy of detectionofthemouthisnotalwaysfeasible.Therefore,other
assigning speaker labels according to the labels of the neigh- forms of body behavior, e.g., head gestures, which are also
boringsegments,aswellasbyexcludingoverlapregionsfrom visible manifestations of speech [116] are used. While there
the input to the diarization system. However, this initial study hasbeenrelativelylittleworkonusingglobalbodymovements
assumedground-truthoverlapdetection.In[100],arealoverlap for inferring speaking status, some studies have been carried
detection system was developed, as well as a better heuristic out[82],[117]–[119]thatshowpromisinginitialresults.
that computed posterior probabilities from diarization to post However,untiltheworkpresentedin[120],approacheshave
process the output and include a second speaker on overlap never considered audiovisual diarization as a single, unsuper-
regions.Themainbottleneckoftheachievedperformancegain vised joint optimization problem. The work in [120], though,
ismainlyduetoerrorsinoverlapdetection,andmoreworkon relies on multiple cameras. The ﬁrst paper that discusses joint
enhancingitsprecisionandrecallisreportedin[99]and[101]. audiovisual diarization using only a single, low-resolution
The main approach consists of a three-state HMM-GMM overview camera and also tests on meeting scenarios where
system (nonspeech, nonoverlapped speech, and overlapped the participants are able to move around freely in the room is
speech),andthebestfeaturecombinationisMFCCandmodu- [121].Thealgorithmreliesonveryfewassumptionsandisable
lationspectrogramfeatures[103],althoughcomparableresults to cope with an arbitrary amount of cameras and subframes.
were achieved with other features such as root mean squared Mostimportantly,asaresultoftrainingacombinedaudiovisual
energy,spectralﬂatness,orharmonicenergyratio.Thereported model, the authors found that speaker diarization algorithms
performance of the overlap detection is 82% precision and canresultinspeakerlocalizationassideinformation.Thisway
21% recall, and yielded a relative improvement of 11% DER. joint audiovisual speaker diarization can answer the question
However, assuming reference overlap detection, the relative “whospokenwhenandfromwhere.”Thissolutiontothelocal-
DER improvement goes up to 37%. This way, this area has izationproblemhaspropertiesthatmaynotbeobservedeither
potentialforfutureresearchefforts. by audio-only diarization nor by video-only localization, such
as increased robustness against various issues present in the
D. AudiovisualDiarization channel.Inaddition,incontrasttoaudio-onlyspeakerdiariza-
Reference [104] presents an empirical study to review deﬁ- tion, this solution provides a means for identifying speakers
nitions of audiovisual synchrony and examine their empirical beyond clustering numbers by associating video regions with
behavior. The results provide justiﬁcations for the application the clusters.
of audiovisual synchrony techniques to the problem of active
E. SystemCombination
speaker localization in broadcast video. The authors of [105]
presentamulti-modalspeakerlocalizationmethodusingaspe- System or component combination is often reported in the
cialized satellite microphone and an omni-directional camera. literature as an effective means for improving performance
Thoughtheresultsseemcomparabletothestate-of-the-art,the in many speech processing applications. However, very few
solution requires specialized hardware. The work presented studies related to speaker diarization have been reported in
in [106] integrates audiovisual features for online audiovisual recent years. This could be due to the inherent difﬁculty of
speaker diarizationusing a dynamic Bayesian network(DBN) mergingmultipleoutputsegmentations.Combinationstrategies
but tests were limited to discussions with two to three people havetoaccommodatedifferencesintemporalsynchronization,
on two short test scenarios. Another use of DBN, also called outputs with different number of speakers, and the matching
factorial HMMs [107], is proposed in [108] as an audiovisual ofspeakerlabels.Moreover,systemsinvolvedinthecombina-
framework. The factorial HMM arises by forming a dynamic tion have to exhibit segmentation outputs that are sufﬁciently
Bayesian belief network composed of several layers. Each of orthogonal in order to ensure signiﬁcant gains in performance
the layers has independent dynamics but the ﬁnal observation whencombined.Someofthecombinationstrategiesproposed
vector depends upon the state in each of the layers. In [109], consist of applying different algorithms/components sequen-
theauthorsdemonstratethatthedifferentshapesthemouthcan tially, based onthe segmentationoutputsof the previous steps
take when speaking facilitate word recognition under tightly inorderto reﬁneboundaries(referred toas “hybridization” or
constrainedtestconditions(e.g.,frontalpositionofthesubject “piped” systems in [122]). In [123] for instance, the authors
withrespecttothecamerawhilereadingdigits). combine two different algorithms based on the Information
Common approaches to audiovisual speaker identiﬁ- Bottleneck framework. In [124], the best components of two
cation involve identifying lip motion from frontal faces, different speaker diarization systems implemented bytwo dif-
e.g.,[110]–[114]. Therefore, the underlying assumption is that ferentFrenchlaboratories(LIUMandIRIT)aremergedand/or
motion from a person comes predominantly from the motion usedsequentially,whichleadstoaperformancegaincompared
of the lower half of their face. In addition, gestural or other toresultsfromindividualsystems.Anoriginalapproachispro-
nonverbal behaviors associated with natural body motion posedin[125],basedona“real”systemcombination.Here,a
during conversations are artiﬁcially suppressed, e.g., for the coupleofsystemsuniquelydifferentiatedbytheirinputfeatures
CUAVE database [115]. Most of the techniques involve the (parameterizations based on Gaussianized against non-Gaus-
identiﬁcation of one or two people in a single video camera sianized MFCCs) are combined for the speaker diarization of
onlywhereshorttermsynchronyoflipmotionandspeechare phonecallsconversations.Thecombinationapproachrelieson
the basis for audiovisual localization. In a real scenario the bothsystemsidentifyingsomecommonclusterswhicharethen
subjectbehaviorisnotcontrolledand,consequently,thecorrect consideredasthemostrelevant.Allthesegmentsnotbelonging
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 364 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
to these common clusters are labeled as misclassiﬁed and are A common characteristic of these evaluations is that the only
involvedinanewre-classiﬁcationstepbasedonaGMMmod- a priori knowledge available to the participants relates to the
elingofthecommonclustersandamaximumlikelihood-based recordingscenario/source (e.g., conference meetings,lectures,
decision. orcoffeebreaksforthemeetingsdomain),thelanguage(Eng-
lish), and the formats of the input and output ﬁles. Evaluation
F. AlternativeModels participants may use external or background data for building
worldmodelsand/orfornormalizationpurposesbutnoapriori
Among the clustering structures recently developed some
information relating to speakers in the recordings is available.
differfromthestandardHMMinsofarastheyarefullynonpara-
Thenumberofspeakersisalsonotknown.
metric(thatis,thenumberofparametersofthesystemdepends
In recent years, the NIST RT evaluations have focussed
on the observations). The Dirichlet process (DP) [126] allows
on the conference meeting domain, where the spontaneous
for converting the systems into Bayesian and nonparametric
speaking style presents a considerable challenge for speaker
systems. The DP mixture model produces inﬁnite Gaussian
diarization.Eachmeetingusedintheevaluationswasrecorded
mixturesand deﬁnesthe numberofcomponents byameasure
using multiple microphones (of different types and quality)
overdistributions.Theauthorsof[127]illustratetheuseofthe
whicharepositionedontheparticipantsorindifferentlocations
Dirichletprocessmixtures,showinganimprovementcompared
aroundthemeetingroom.Bygroupingthesemicrophonesinto
to other classical methods. Reference [128] proposes another
different classes, NIST created several contrastive evaluation
nonparametric Bayesian approach, in which a stochastic hier-
conditions. These include: individual headphone microphones
archical Dirichlet process (HDP) deﬁnes a prior distribution
(IHM), single distant microphones (SDM), multiple distant
ontransitionmatricesovercountablyinﬁnitestatespaces,that
microphones (MDM), multiple mark III arrays (MM3A), and
is,noﬁxednumberofspeakersisassumed,norfoundthrough
alldistantmicrophones(ADM).MM3Amicrophonesarethose
eithersplitormergingapproachesusingclassicalmodelselec-
exclusivelyfoundwithinthearraysbuiltandprovidedbyNIST.
tion approaches (such as the BIC criterion). Instead, this prior
These are usually not included within the MDM condition,
measureisplacedoverdistributions(calledarandommeasure),
they are included within the ADM condition. In this section
which is integrated out using likelihood-prior conjugacy. The
we show results for the MDM and SDM conditions since we
resultingHDP-HMMleadstoadata-drivenlearningalgorithm
considerthemtobethemostrepresentativeofstandardmeeting
which infers posterior distributions over the number of states.
roomrecordingequipment.Theseconditionshavealsoproven
This posterior uncertainty can be integrated out when making
tobethemostpopularamongevaluationparticipants.
predictions effectivelyaveragingovermodelsofvaryingcom-
Participating teams are required to submit a hypothesis of
plexity. The HDP-HMM has shown promise in diarization
speaker activity including start-stop times of speech segments
[129], yielding similar performance to the standard agglom-
withspeakerlabels,whichareusedsolelytoidentifythemul-
erative HMM with GMM emissions, while requiring very
tipleinterventionsofagivenspeaker,butdonotneedtoreﬂect
littlehyperparametertuningandprovidingastatisticallysound
thespeaker’srealidentity.Thesesystemoutputsarecompared
model.Globally,thesenonparametricBayesianapproachesdid
totheground-truthreferenceinordertoobtaintheoverallDER.
not bring a major improvementcompared to classical systems
The DER metric is the sum of three sources of error: missed
as presented in Section III. However, they may be promising
speech(percentageofspeechintheground-truthbutnotinthe
insofar as they do not necessarily need to be optimized for
hypothesis), false alarm speech (percentage of speech in the
certain data compared to methods cited in Section II. Further-
hypothesisbutnotintheground-truth)andspeakererror(per-
more, they provide a probabilistic interpretation on posterior
centageofspeechassignedtothewrongspeaker).Thespeaker
distributions(e.g.,numberofspeakers).
errorcanbefurtherclassiﬁedintoincorrectlyassignedspeakers
and speaker overlap error. In the ﬁrst case, the hypothesized
speakerdoesnotcorrespondtothereal(ground-truth)speaker.
V. PERFORMANCEEVALUATION
Speakeroverlaperrorreferstothecasewhenthewrongnumber
In this section, we report an analysis of speaker diarization of speakers is hypothesized when multiple speakers speak at
performanceasreportedduringthefourmostrecentNISTRT thesametime.Theinclusionofoverlappingspeecherrorinthe
evaluations.Theanalysisfocusessolelyonconferencemeetings evaluationwasrestrictedtoacontrastivemetricintheinitialRT
whicharethecoreevaluationcondition.Wealsopresentananal- evaluationsbuthasbeentheprimarymetricsince2006.Overlap
ysisoftheground-truthreferencesinordertounderlinethechar- errorscanbeclassiﬁedasmissedoverlap(whenfewerspeakers
acteristics of the data with respect to meeting sources and the thantherealnumberarehypothesized)andfalsealarmoverlap
differentevaluationcampaigns.Finallyweshowstate-of-the-art (whentoomanyspeakersarehypothesized).IntheNISTeval-
systemresults,collatedfromfourNISTRT’07andRT’09eval- uations up to four overlapping speakers are considered in the
uation participants, which aim at giving a baseline for future scoring.
research. Note thatasthe DERistime-weighted, itascribes littleim-
portance to the diarization quality of speakers whose overall
A. BenchmarkingEvaluations speakingtimeissmall.Additionally,anonscoringcollarof250
msisgenerallyappliedeithersideoftheground-truthsegment
Since2004,NISThasorganizedaseriesofbenchmarkeval-
boundaries to account for inevitable inconsistencies in precise
uationswithintheRichTranscription(RT)campaigns.3Oneof
start and end point labeling. When comparing the system out-
the tasks involves speaker diarization of different sets of data.
putswiththeground-truth,andgiventhatthelabelsidentifying
3Seehttp://nist.gov/speech/tests/rt. the speakers are just relative identiﬁers, the scoring algorithm
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 365
TABLEI
GROUND-TRUTHANALYSISFORTHEDATASETSOFTHELASTFOURSPEAKERDIARIZATIONEVALUATIONCAMPAIGNS(RT’05TO
RT’09)ANDMEETINGSOURCE.COMPARISONSAREBASEDONTHEAVERAGESPEAKERANDTURNDURATIONS(LEFT-HALFSIDE)
ANDTHEPERCENTAGEOFSILENCEANDOVERLAPPINGSPEECH(RIGHT-HALFSIDE)
ForRT’05theaveragespeakersegmentdurationis2.5s.This
valuedecreasescontinuouslyforsubsequentdatasets(2.3sfor
RT’06, 2.0 s for RT’07, and 1.8 s for RT’09). This tendency
leadstoincreasinglymorefrequentspeakerturnsandincreases
thechancesofmiss-classifyingaspeechsegment.Theaverage
turnsegmentdurationis2.1sforRT’05.Thisvaluefallsto1.4s
forRT’06andremainsstableforRT’07andRT’09(1.5sand1.4
Fig.2. Examplesofturnandspeakerdurationsinthepresenceofoverlapped srespectively).Theconsistentdecreaseinspeaker/turnduration
speechandsilences.
ratio highlights a general trend of increasing spontaneity and
helpstoexplainthedifferencesinresultsfromonedatasettoan-
ﬁrst computes an optimum mapping between both sets of la-
other.Therearenodistinctdifferencesacrossdifferentmeeting
belsinordertoobtaintheDER.Thisisnormallyperformedac-
sites.
cordingtoastandarddynamicprogrammingalgorithmdeﬁned
There are also noticeable differences in silence and overlap
by NIST.
statistics.ThepercentageofsilenceislowerfortheRT’05and
RT’09 datasets than it is for the RT’06 and RT’09 datasets
B. Ground-TruthAnalysis
(10.3%and17.5%cf.31.5%and24.9%).However,theRT’05
Ground-truth references for evaluating speaker diarization and RT’09 datasets have a higher overlap rate than the RT’06
wereinitiallyobtainedviamanuallabelingoftheacousticdata; and RT’07 datasets (16.0% and 13.6% cf. 7.7% and 7.6%).
however, high variations between different labelers proved to This is primarily due to three meetings (from CMU, ICSI,
beproblematic.Therefore,morerecently,anautomaticallygen- and NIST sites) which have overlap rates over 25% (note that
eratedforcedalignmenthasbeenusedinordertoextractmore values in Table I are averaged across sites, and do not reﬂect
reliablespeakerstartandendpointsusinganautomaticspeech individual meeting scores). In the case of the RT’09 dataset,
recognition (ASR) system, human-created transcriptions, and the slightly high average overlap of 13% is due to a single
theaudiofromindividualheadmicrophones(IHM). meeting(recordedbyNIST)inwhichtheoverlapreaches31%.
Asmeetingdatacomefromavarietyofsourcessomediffer- Listeningtothismeetingweconcludedthatthereasonofsuch
encesbetweenthemareexpected.Furthermore,largechangesin overlap is that it is not a professional meeting but a social
theﬁnalDERscoresfromdifferentevaluationswouldsuggest rendezvous. Conversely, RT’05 and RT’09 have in average a
thattherearedifferencesbetweenthesetsofmeetingsusedeach lowerpercentageofsilence(10%and17%)comparedtoRT’06
year. To gauge the differences we have analyzed over 20 dif- and RT’07 (31% and 25%). A lower silence rate and higher
ferentparameterscomputedontheground-truthdata.InTableI, overlap might indicate that these meetings are more dynamic,
wereportfouroftheseparameters,whichwefoundmostinter- with less idle time and more discussion, although this does
esting, and group results by meeting source and by evaluation not mean that they are more spontaneous, as their speech and
year. speaker segment lengths are still high compared to the RT’09
Intheleftsideofthetablewereportaveragespeakerandturn dataset.
durations.AsexempliﬁedinFig.2,theaveragespeakerduration Overall, we see that, although all recordings belong to the
referstotheaveragetimeduringwhichaspeakerisactive(i.e.,a sametask,therearelargedifferencesbetweenthedatasetsused
singlelineintheRTTMreferenceﬁles).Conversely,theaverage for each evaluation campaign, as well as between recordings
turn duration refers to the average time during which there is from the same source (recording site), but from different
no change in speaker activity and is thus always smaller than datasets. This emphasizes the need for robust systems which
the average speaker duration. The difference between the two performwellregardlessofparticulardatasetcharacteristics.Itis
statisticsreﬂectsthedegreeofoverlapandspontaneity.Without importanttonote,however,thattheNISTRTdatasetsdiscussed
any overlap and a pause between each speaker exchange the here typically contain around eight meetings per dataset, each
averagespeakerandturndurationswouldbeidentical.Increases ofthemcontributingtoasingleDERscore.Randomvariations
in overlap and spontaneity will result in a larger speaker/turn on any meeting from these small datasets have a signiﬁcant
ratio. In the right side of Table I we report the percentage of impactonaverageresults.Itisthendifﬁculttoreliablyinterpret
silenceandofoverlappingspeech. resultsandhencealsodifﬁculttodrawmeaningfulconclusions.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 366 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
Fig.3. DERsfortheRT’07andRT’09(a)inmultipledistantmicrophone(MDM)condition,and(b)singledistantmicrophone(SDM)condition(notethat
spkr_errorinmeetingNIST_20080201–1405hasbeentrimmedtoﬁtthescreen,withaspeakererrorof31.79%andatotalDERof49.65%).
Comparisons with the work of the speech and speaker dataset and between 7.4% and 49.7% for the RT’09 dataset.
recognition communities highlight the rapid acceleration in Thus,thereisalargevariationinperformanceacrossdifferent
research effort and progress stemming from the availability meetings and in all cases we observe signiﬁcant overlap er-
of huge datasets. Advances in sophisticated modeling and rors and their often-dominant impact upon the ﬁnal DER. Of
normalization strategies have revolutionized research in these particular note is the poor performance obtained on the single
related ﬁelds over recent years. It becomes apparent that the NIST_20080201–1405, which correlates with the particularly
fundamental lack of larger speaker diarization datasets, which highpercentageofoverlappingspeechforthismeetingasillus-
makesitdifﬁculttoassessnovelalgorithms,isacriticalbarrier tratedinTableI.Hence,thedetectionandappropriatetreatment
tofurtherresearchinourﬁeld.Signiﬁcantlylargerdatasetsare ofoverlappingspeechremainsanunsolvedproblem.Infact,the
needed in order to obtain more robust and meaningful perfor- overlaperrorshowninFig.3isentirelyduetomissedoverlap
mance estimates and comparisons. As a result of processing regions,asnoneofthespeakerdiarizationsystemsconsidered
more data, faster algorithms will also need to be investigated in this analysis included an overlap detector. Also of note is
forresearchinspeakerdiarizationtobefeasiblewithstandard the general stability of speech activity detection (SAD) algo-
computingresources. rithmswhichachieveimpressivelevelsofperformanceinboth
MDMandSDMconditions(i.e.,theyarerobusttothequality
C. EvaluationResults
ofthesignal).Valuesofaround1%to2%missedspeecherror
To assess the current state-of-the-art and provide a baseline rates and 2% to 3% false alarm error rates are currently typ-
forfutureresearchwepresentresultsfortheRT’07(Fig.3left ical.ThemaindifferencebetweenMDMandSDMperformance
half) and RT’09 (Fig. 3 right half) NIST evaluations for the rests mainly inthe speaker error. Here diarization systems are
MDM [Fig. 3(a)] and SDM [Fig. 3(b)] conditions. Both ﬁg- affected by the reduced signal quality which characterizes the
ures have been compiled from a comparison of results from SDMcondition.
four of the participating sites (LIA/Eurecom,4 I2R/NTU, ICSI Overall,thelargevariationsinDERobservedamongthedif-
andUPC)andbyselectingtheresultwithlowestDERforeach ferentmeetingsandmeetingsetsoriginatefromthelargevari-
meetingrecording.Giventhevolatilityoftheresultsdescribed ance of many important factors for speaker diarization, which
andstudiedin[3],byselectingthebestresultineachcasewehy- makes the conference meeting domain not as easily tractable
pothesizethattheseresultsareamoremeaningfulestimationof as more formalized settings such as broadcast news, lectures,
thestate-of-the-artperformanceinspeakerdiarizationforcon- or court house trials. Previous work has highlighted the difﬁ-
ference meetingdata than selectingall resultsfrom any single cultyinassessingtheperformanceofspeakerdiarizationalgo-
systemoutput.Toillustratethevariationinperformancefordif- rithms with the view of improving performance [130]. As re-
ferent meetings we provide results for individual meetings. In portedinSectionIII,currentapproachestospeakerdiarization
bothﬁgures,errorsaredecomposedintothespeakererror(Spkr involve a sequence of separate stages where each stage takes
error),overlaperror(OVLerror),falsealarmspeecherror(FA itsinputfromtheprecedingstage(s).Whencombinedinsucha
speech),andmissedspeecherror(MISSspeech). fashion,itisexceedinglydifﬁculttoassesstheperformanceof
FortheMDMcondition[Fig.3(a)]theaverageDERforthe eachsystemcomponentsinceeverysingleoneisaffectedbythe
RT’07 and RT’09 datasets is 7.5% and 10.1%, respectively. performanceof allprevious processingstages. Furthermore, it
Performance varies between 3.5% and 15.7% for the RT’07 isnotguaranteedthatimprovementstoonestage,forexample
dataset whereas for the RT’09 dataset performance varies be- thatofsegmentation,willleadunequivocallytoimprovements
tween 5.3% and 22.2%. For the SDM condition the average inlaterstages,forexamplethatofclustering.Thismakestheop-
DERis11.6%and17.7%fortheRT’07andRT’09datasets,re- timizationofdifferentsystemcomponentsrathertroublesome.
spectively.PerformanceisalwayspoorerthanthatfortheMDM Onceagain,bydrawingcomparisonstothespeechandspeaker
condition and varies between 3.7% and 19.9% for the RT’07 recognition ﬁelds, it is reasonable to foresee more uniﬁed ap-
proaches,asisalreadyinprogresswiththenowcommonplace
4EurecomwasassociatedwiththeLIAfortheRT’09campaignonly.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 367
combined approaches to segmentation and clustering. In par- REFERENCES
ticular, we believe that important decreases in DER will have
to come in the near future from systems incorporating effec- [1] “TheNISTRichTranscription2009(RT’09)Evaluation,”NIST,2009
[Online]. Available: http://www.itl.nist.gov/iad/mig/tests/rt/2009/
tivealgorithmsthatcandetectandcorrectlyassignoverlapping
docs/rt09-meeting-eval-plan-v2.pdf
speech. [2] S.TranterandD.Reynolds,“Anoverviewofautomaticspeakerdi-
arizationsystems,”IEEETrans.Audio,Speech,Lang.Process.,vol.
14,no.5,pp.1557–1565,Sep.2006.
[3] N.MirghaforiandC.Wooters,“Nutsandﬂakes:Astudyofdatachar-
VI. CONCLUSIONANDDIRECTIONSFORFUTURERESEARCH acteristicsinspeakerdiarization,”inProc.ICASSP,2006.
[4] X.Anguera,“Robustspeakerdiarizationformeetings,”Ph.D.disser-
Researchonspeakerdiarizationhasbeendevelopedinmany
tation,Univ.PolitecnicadeCatalunya,Barcelona,Spain,2006.
domains, from phone calls conversations within the speaker [5] M.Kotti,E.Benetos,andC.Kotropoulos,“Computationallyefﬁcient
and robust BIC-based speaker segmentation,” IEEE Trans. Audio,
recognitionevaluations,tobroadcastnewsandmeetingrecord-
Speech,Lang.Process.,vol.16,no.5,pp.920–933,Jul.2008.
ingsintheNISTRichTranscriptionevaluations.Furthermore, [6] X.Zhu,C.Barras,L.Lamel,andJ.-L.Gauvain,“Multi-stagespeaker
it has been used in many applications such as a front-end for diarizationforconferenceandlecturemeetings,”inProc.Multimodal
Technol.PerceptionofHumans:Int.Eval.WorkshopsCLEAR2007and
speaker and speech recognition, asa meta-data extractiontool
RT2007,Baltimore,MD,May8–11,2007,RevisedSelectedPapers,
toaidnavigationinbroadcastTV,lecturerecordings,meetings, Berlin,Heidelberg:Springer-Verlag,2008,pp.533–542.
andvideoconferencesandevenforapplicationssuchasmedia [7] S.Jothilakshmi,V.Ramalingam,andS.Palanivel,“Speakerdiariza-
tionusingautoassociativeneuralnetworks,”Eng.Applicat.Artif.In-
similarityestimationforcopyrightdetection.Also,speakerdi- tell.,vol.22,no.4-5,pp.667–675,2009.
arizationresearchhasledtovariousby-products.Forexample, [8] X.Anguera,C.Wooters,andJ.Hernando,“Robustspeakerdiarization
formeetings:ICSIRT06sevaluationsystem,”inProc.ICSLP,Pitts-
withtheavailabilityofrecordingsusingmultiplemicrophones,
burgh,PA,Sep.2006.
a set of algorithms has been proposed in recent years both for [9] C.WootersandM.Huijbregts,“TheICSIRT07sspeakerdiarization
signal enhancement and to take advantage of the extra infor- system,”inMultimodalTechnologiesforPerceptionofHumans:Inter-
nationalEvaluationWorkshopsCLEAR2007andRT2007,Baltimore,
mation that these offer. In addition, the availability of other MD,USA,May8–11,2007,RevisedSelectedPapers,Berlin,Heidel-
modalities, such as video, have started to inspire multimodal berg:Springer-Verlag,2008,pp.509–519.
[10] J.Rougui,M.Rziza,D.Aboutajdine,M.Gelgon,andJ.Martinez,“Fast
diarization systems, thus merging the visual and the acoustic
incrementalclusteringofGaussianmixturespeakermodelsforscaling
domains. upretrievalinon-linebroadcast,”inProc.ICASSP,May2006,vol.5,
This paper provides an overview of the current state-of- pp.521–524.
[11] W.Tsai,S.Cheng,andH.Wang,inProc.ICSLP,2004.
the-art in speaker diarization systems and underlines several
[12] T.H.Nguyen,E.S.Chng,andH.Li,“T-testdistanceandclustering
challenges that need to be addressed in future years. For ex- criterionforspeakerdiarization,”inProc.Interspeech,Brisbane,Aus-
tralia,2008.
ample, speaker diarization is not yet sufﬁciently mature so
[13] T.Nguyenetal.,“TheIIR-NTUspeakerdiarizationsystemsforRT
thatmethods canbe easilyportedacross differentdomains,as 2009,”inProc.RT’09,NISTRichTranscriptionWorkshop,Melbourne,
shown in Section V, where small differences in meeting data FL,2009.
[14] S. Meignier, J.-F. Bonastre, and S. Igounet, “E-HMM approach for
(recorded at identical sites) lead to large variations in perfor-
learningandadaptingsoundmodelsforspeakerindexing,”inProc.
mance.Inthemeantime,largerdatasetsneedtobecompiledin Odyssey Speaker and Lang. Recognition Workshop, Chania, Creete,
orderforresultstobecomemoremeaningfulandforsystemsto Jun.2001,pp.175–180.
[15] C. Fredouille and N. Evans, “The LIA RT’07 speaker diarization
bemorerobusttounseenvariations.Ofcourse,withincreasing system,” in Proc. Multimodal Technol. for Perception of Humans:
dataset sizes, systems will have to become more efﬁcient in Int. Eval. Workshops CLEAR 2007 and RT 2007, Baltimore, MD,
USA,May8–11,2007,RevisedSelectedPapers,Berlin,Heidelberg:
ordertoprocesssuchdatainreasonabletime.Still,thebiggest
Springer-Verlag,2008,pp.520–532.
single challenge is probably the handling of overlapping [16] C.Fredouille,S.Bozonnet,andN.W.D.Evans,“TheLIA-EURECOM
speech, which needs to be attributed to multiple speakers. As RT’09speakerdiarizationsystem,”inProc.RT’09,NISTRichTran-
scriptionWorkshop,Melbourne,FL,2009.
a relatively embryonic community, at least compared to the
[17] S.Bozonnet,N.W.D.Evans,andC.Fredouille,“TheLIA-EURECOM
moreestablishedﬁeldsofspeechandspeakerrecognition,there RT’09speakerdiarizationsystem:Enhancementsinspeakermodelling
andclusterpuriﬁcation,”inProc.ICASSP,Dallas,TX,Mar.14–19,
are thus outstanding opportunities for signiﬁcant advances
2010,pp.4958–4961.
and important changes to the somewhat ad hoc and heuristic [18] D.Vijayasenan, F.Valente, and H. Bourlard, “Agglomerative infor-
approachesthatcurrentlydominatetheﬁeld. mationbottleneckforspeakerdiarizationofmeetingsdata,”inProc.
ASRU,Dec.2007,pp.250–255.
Overall, the future of the ﬁeld seems even broader and
[19] D.Vijayasenan,F.Valente,andH.Bourlard,“Aninformationtheoretic
brighter than the present, as more and more people acknowl- approachtospeakerdiarizationofmeetingdata,”IEEETrans.Audio,
Speech,Lang.Process.,vol.17,no.7,pp.1382–1393,Sep.2009.
edgetheusefulnessofaudiomethodsformanytasksthathave
[20] S. McEachern, “Estimating normal means with a conjugate style
traditionally been thought to be exclusively solvable in the dirichlet process prior,” in Proc. Commun. Statist.: Simul. Comput.,
visual domain. Speaker diarization is one of the fundamental 1994,vol.23,pp.727–741.
[21] G.E.HintonandD.vanCamp,“Keepingtheneuralnetworkssimpleby
problems underlying virtually any task that involves acoustics
minimizingthedescriptionlengthoftheweights,”inProc.6thAnnu.
andthepresenceofmorethanoneperson. Conf.Comput.Learn.Theory,NewYork,1993,COLT’93,pp.5–13.
[22] M.J.WainwrightandM.I.Jordan,“Variationalinferenceingraphical
models:Theviewfromthemarginalpolytope,”inProc.41stAnnu.
AllertonConf.Commun.,Control,Comput.,Urbana-Champaign,IL,
ACKNOWLEDGMENT 2003.
[23] F.Valente,“VariationalBayesianmethodsforaudioindexing,”Ph.D.
TheauthorswouldliketothankthoseRTparticipatingsites dissertation,EurecomInst.,Sophia-Antipolis,France,2005.
thatlentthemtheirresultsforthisstudy,inparticularJ.Luque [24] D.Reynolds,P.Kenny,andF.Castaldo,“Astudyofnewapproaches
tospeakerdiarization,”inProc.Interspeech,2009.
from UPC, Spain, and T.H. Nguyen and H. Li from I2R and
[25] P.Kenny,“BayesianAnalysisofSpeakerDiarizationwithEigenvoice
NTU,Singapore. Priors,”TechnicalReport. Montreal,QC,Canada:CRIM,2008.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 368 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
[26] X.AngueraandJ.-F.Bonastre,“Anovelspeakerbinarykeyderived [50] J.Ramirez,J.M.Girriz,andJ.C.Segura,M.GrimmandK.Kroschel,
fromanchormodels,”inProc.Interspeech,2010. Eds.,“Voiceactivitydetection.Fundamentalsandspeechrecognition
[27] X.AngueraandJ.-F.Bonastre,“Fastspeakerdiarizationbasedonbi- systemrobustness,”inProc.RobustSpeechRecognit.Understand.,Vi-
narykeys,”inProc.ICASSP,2011. enna,Austria,Jun.2007,p.460.
[28] Y. Huang, O. Vinyals, G. Friedland, C. Muller, N. Mirghafori, and [51] C.FredouilleandG.Senay,“TechnicalimprovementsoftheE-HMM
C.Wooters,“Afast-matchapproachforrobust,fasterthanreal-time basedspeakerdiarizationsystemformeetingrecords,”inProc.MLMI
speakerdiarization,”inProc.IEEEWorkshopAutom.SpeechRecogni- Third Int. Workshop, Bethesda, MD, USA, Revised Selected Paper,
tionUnderstanding,Kyoto,Japan,Dec.2007,pp.693–698. Berlin,Heidelberg:Springer-Verlag,2006,pp.359–370.
[29] G.Friedland,J.Ching,andA.Janin,“Parallelizingspeaker-attributed [52] D.A.V.LeeuwenandM.Konecˇný,“ProgressintheAMIDAspeaker
speechrecognitionformeetingbrowsing,”inProc.IEEEInt.Symp. diarizationsystemformeetingdata,”inProc.MultimodalTechnol.for
Multimedia,Taichung,Taiwan,Dec.2010,pp.121–128. Percept.ofHumans:Int.Eval.WorkshopsCLEAR2007andRT2007,
[30] X.Anguera,C.Wooters,andJ.Hernando,“Friendsandenemies:A Baltimore,MD,May8–11,2007,RevisedSelectedPapers,Berlin,Hei-
novelinitializationforspeakerdiarization,”inProc.ICSLP,Pittsburgh, delberg:Springer-Verlag,2008,pp.475–483.
PA,Sep.2006. [53] A. Rentzeperis, A. Stergious, C. Boukis, A. Pnevmatikakis, and L.
[31] J. Ajmera,“A robust speakerclustering algorithm,”in Proc.ASRU, Polymenakos,“The2006Athensinformationtechnologyspeechac-
2003,pp.411–416. tivitydetectionandspeakerdiarizationsystems,”inProc.Mach.Learn.
[32] X. Anguera, C. Wooters, and J. Hernando, “Purity algorithms for Multimodal Interaction: 3rd Int. Workshop, MLMI 2006, Bethesda,
speaker diarization of meetings data,” in Proc. ICASSP, Toulouse, MD, Revised Selected Paper, Berlin, Heidelberg: Springer-Verlag,
France,May2006,pp.1025–1028. 2006,pp.385–395.
[33] S. S. Chen and P. S. Gopalakrishnan, “Speaker, environment and [54] A.Temko,D.Macho,andC.Nadeu,“EnhancedSVMtrainingforro-
channelchangedetectionandclusteringviathebayesianinformation bustspeechactivitydetection,”inProc.ICASSP,Honolulu,HI,2007,
criterion,”inProc.DARPABroadcastNewsTranscriptionandUnder- pp.1025–1028.
standingWorkshop,Lansdowne,VA,Feb.1998,pp.127–132. [55] X.Anguera,C.Wooters,M.Anguilo,andC.Nadeu,“Hybridspeech/
[34] H.GishandM.Schmidt,“Textindependentspeakeridentiﬁcation,” non-speech detector applied to speaker diarization of meetings,” in
IEEESignalProcess.Mag.,vol.11,no.4,pp.18–32,Oct.1994. Proc.SpeakerOdysseyWorkshop,PuertoRico,Jun.2006.
[35] A.Janin,J.Ang,S.Bhagat,R.Dhillon,J.Edwards,J.Macias-Guarasa, [56] H.Sun,T.L.Nwe,B.Ma,andH.Li,“Speakerdiarizationformeeting
N. Morgan, B. Peskin, E. Shriberg, A. Stolcke, C. Wooters, and B. roomaudio,”inProc.Interspeech’09,Sep.2009.
Wrede,“TheICSImeetingproject:Resourcesandresearch,”inProc. [57] T. L. Nwe, H. Sun, H. Li, and S. Rahardja, “Speaker diarization
ICASSPMeetingRecognitionWorkshop,2004. in meeting audio,” in Proc. ICASSP, Taipei, Taiwan, 2009, pp.
[36] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. 4073–4076.
Flynn,M.Guillemot,T.Hain,J.Kadlec,V.Karaiskos,M.Kronen- [58] E.El-Khoury,C.Senac,andJ.Pinquier,“Improvedspeakerdiariza-
thal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, tionsystemformeetings,”inProc.ICASSP,Taipei,Taiwan,2009,pp.
and P. Wellner, “The AMI meeting corpus,” in Proc. Meas. Be- 4097–4100.
havior, 2005. [59] L.Lu,H.-J.Zhang,andH.Jiang,“Contentanalysisforaudioclassiﬁ-
[37] D.Mostefa,N.Moreau,K.Choukri,G.Potamianos,S.M.Chu,A. cationandsegmentation,”IEEETrans.SpeechAudioProcess.,vol.10,
Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnev- no.7,pp.504–516,Oct.2002.
matikakis,V.Mylonakis,F.Talantzis,S.Burger,R.Stiefelhagen,K. [60] R.Li, Q.Jin,and T.Schultz, “Improving speakersegmentation via
Bernardin,andC.Rochet,“TheCHILaudiovisualcorpusforlecture speaker identiﬁcation and text segmentation,” in Proc. Interspeech,
andmeetinganalysisinsidesmartrooms,”Lang.ResourcesEval.,vol. Sep.2009,pp.3073–3076.
41,Dec.2007. [61] M.Ben,M.Betser,F.Bimbot,andG.Gravier,“Speakerdiarization
[38] C.Fredouille,D.Moraru,S.Meignier,L.Besacier,andJ.-F.Bonastre, usingbottom-upclusteringbasedonaparameter-deriveddistancebe-
“The NIST 2004 spring rich transcription evaluation: Two-axis tweenadaptedgmms,”inProc.ICSLP,JejuIsland,Korea,2004.
mergingstrategyinthecontextofmultipledistantmicrophonebased [62] D. Van Leeuwen and M. Huijbregts, “The AMI speaker diarization
meeting speaker segmentation,” in Proc. NIST 2004 Spring Rich system for NIST RT06s meeting data,” in Machine Learning for
Transcript.Eval.Workshop,Montreal,QC,Canada,2004. Multimodal Interaction. Berlin, Germany: Springer-Verlag, 2007,
[39] Q.Jin,K.Laskowski,T.Schultz,andA.Waibel,“Speakersegmen- vol.4299,LectureNotesinComputerScience,pp.371–384.
tationandclusteringinmeetings,”inProc.ICSLP,Jeju,Korea,Sep. [63] A.Vandecatseye,J.-P.Martens,J.Neto,H.Meinedo,C.Garcia-Mateo,
2004. J. Dieguez, F. Mihelic, J. Zibert, J. Nouza, P. David, M. Pleva, A.
[40] D.Istrate,C.Fredouille,S.Meignier,L.Besacier,andJ.-F.Bonastre, Cizmar,H.Papageorgiou,andC.Alexandris,“Thecost278pan-Eu-
“NISTRT05Sevaluation:Pre-processingtechniquesandspeakerdi- ropeanbroadcastnewsdatabase,”inProc.LREC,Lisbon,Portugal,5,
arizationonmultiplemicrophonemeetings,”inProc.NIST2005Spring 2004,vol.4,pp.873–876.
RichTranscript.Eval.Workshop,Edinburgh,U.K.,Jul.2005. [64] K.MoriandS.Nakagawa,“Speakerchangedetectionandspeakerclus-
[41] X.Anguera,C.Wooters,B.Peskin,andM.Aguilo,“Robustspeaker teringusingVQdistortionforbroadcastnewsspeechrecognition,”in
segmentation for meetings: The ICSI-SRI spring 2005 diarization Proc.ICASSP,2001,pp.413–416.
system,”inProc.NISTMLMIMeetingRecognitionWorkshop,Edin- [65] J.AjmeraandI.McCowan,“Robustspeakerchangedetection,”IEEE
burgh,U.K.,2005. SignalProcess.Lett.,vol.11,pp.649–651,2004.
[42] X.Anguera,C.Wooters,andJ.Hernando,“Acousticbeamformingfor [66] L. Lu and H.-J. Zhang, “Real-time unsupervised speaker change
speakerdiarizationofmeetings,”IEEETrans.Audio,Speech,Lang. detection,” in 16th Int. Conf. Pattern Recognit., 2002, vol. 2, pp.
Process.,vol.15,no.7,pp.2011–2023,Sep.2007. 358–361.
[43] X.Anguera,BeamformIt(TheFastandRobustAcousticBeamformer) [67] X.AngueraandJ.Hernando,“Evolutivespeakersegmentationusinga
[Online].Available:http://www.xavieranguera.com/beamformit/ repositorysystem,”inProc.Interspeech,2004.
[44] N.Wiener,Extrapolation,Interpolation,andSmoothingofStationary [68] X. Anguera, C. Wooters, and J. Hernando, “Speaker diarization for
TimeSeries. NewYork:Wiley,1949. multi-partymeetingsusingacousticfusion,”inProc.ASRU,Nov.2005,
[45] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Her- pp.426–431.
mansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qual- [69] A. Malegaonkar, A. Ariyaeeinia, P. Sivakumaran, and J. Fortuna,
comm-ICSI-OGIfeaturesforASR,”inProc.ICSLP,2002,vol.1,pp. “Unsupervised speaker change detection using probabilistic pattern
4–7. matching,”IEEESignal Process.Lett.,vol.13,no.8,pp.509–512,
[46] M.L.Seltzer,B.Raj,andR.M.Stern,“Likelihoodmaximizingbeam- Aug.2006.
formingforrobusthands-freespeechrecognition,”IEEETrans.Speech [70] M.-H.Siu,G.Yu,andH.Gish,“Segregationofspeakersforspeech
AudioProcess.,vol.12,no.5,pp.489–498,Sep.2004. recognitionandspeakeridentiﬁcation,”inProc.ICASSP’91,1991,pp.
[47] L.J.GrifﬁthsandC.W.Jim,“Analternativeapproachtolinearlycon- 873–876.
strainedadaptivebeamforming,”IEEETrans.AntennasPropagat.,vol. [71] P.DelacourtandC.Wellekens,“DISTBIC:Aspeaker-basedsegmen-
AP-30,no.1,pp.27–34,Jan.1982. tationforaudiodataindexing,”SpeechCommun.,pp.111–126,2000.
[48] M.Woelfel andJ. McDonough, DistantSpeechRecognition. New [72] S.S.HanandK.J.Narayanan,“Agglomerativehierarchicalspeaker
York:Wiley,2009. clusteringusingincrementalGaussianmixtureclustermodeling,”in
[49] C. Wooters, J. Fung, B. Peskin, and X. Anguera, “Towards robust Proc.Interspeech’08,Brisbane,Australia,2008,pp.20–23.
speaker segmentation: The ICSI-SRI fall 2004 diarization system,” [73] R.Gangadharaiah,B.Narayanaswamy,andN.Balakrishnan,“Anovel
inProc.Fall2004RichTranscript.Workshop(RT04),Palisades,NY, methodfortwospeakersegmentation,”inProc.ICSLP,Jeju,Korea,
Nov.2004. Sep.2004.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERAMIROetal.:SPEAKERDIARIZATION:AREVIEWOFRECENTRESEARCH 369
[74] D.LiuandF.Kubala,“Fastspeakerchangedetectionforbroadcast [101] K. Boakye, “Audio segmentation for meetings speech processing,”
newstranscriptionandindexing,”inProc.Eurospeech’99,Sep.1999, Ph.D.dissertation,Univ.ofCalifornia,Berkeley,2008.
pp.1031–1034. [102] S.OttersonandM.Ostendorf,“Efﬁcientuseofoverlapinformationin
[75] M.A.Siegler,U.Jain,B.Raj,andR.M.Stern,“Automaticsegmen- speakerdiarization,”inProc.ASRU,Kyoto,Japan,2007,pp.686–6.
tation,classiﬁcationandclusteringofbroadcastnewsaudio,”inProc. [103] B. E. D. Kingsbury, N.Morgan, and S. Greenberg, “Robust speech
DARPASpeechRecognit.Workshop,1997,pp.97–99. recognitionusingthemodulationspectrogram,”SpeechCommun.,vol.
[76] P.ZochováandV.Radová,“ModiﬁedDISTBICalgorithmforspeaker 25,no.1-3,pp.117–132,1998.
changedetection,”inProc.9thEur.Conf.SpeechCommun.Technol., [104] H.J.Nock,G.Iyengar,andC.Neti,“Speakerlocalizationusingaudio-
Bonn,Germany,2005,pp.3073–3076. visualsynchrony:Anempiricalstudy,”LectureNotesinComput.Sci.,
[77] X.Zhu,C.Barras,L.Lamel,andJ.-L.Gauvain,“Speakerdiarization: vol.2728,pp.565–570,2003.
Frombroadcastnewstolectures,”inProc.MLMI,2006,pp.396–406. [105] C.Zhang, P.Yin,Y.Rui,R.Cutler,and P.Viola,“Boosting-based
[78] K.HanandS.Narayanan,“Novelinter-clusterdistancemeasurecom- multimodalspeakerdetectionfordistributedmeetings,”inProc.IEEE
biningGLRandICRforimprovedagglomerativehierarchicalspeaker Int.WorkshopMultimediaSignalProcess.(MMSP),2006,pp.86–91.
clustering,”inProc.ICASSP,Apr.2008,pp.4373–4376. [106] A.NoulasandB.J.A.Krose,“On-linemulti-modalspeakerdiariza-
[79] D.Moraru,M.Ben,andG.Gravier,“Experimentsonspeakertracking tion,” inProc. 9th Int. Conf. Multimodal InterfacesICMI ’07, New
andsegmentationinradiobroadcastnews,”inProc.ICSLP,2005. York,2007,pp.350–357.
[80] C.Barras,X.Zhu,S.Meignier,andJ.-L.Gauvain,“Improvingspeaker [107] Z.GhahramaniandM.I.Jordan,“FactorialhiddenMarkovmodels,”
diarization,”inProc.DARPART04,2004. Mach.Learn.,vol.29,pp.245–273,Nov.1997.
[81] H.Aronowitz,“Trainablespeakerdiarization,”inProc.Interspeech, [108] A.K.Noulas,G.Englebienne,andB.J.A.Krose,“Mutimodalspeaker
Aug.2007,pp.1861–1864. diarization,”IEEETrans.PatternAnal.Mach.Intell.,2011,preprint,to
[82] H.HungandG.Friedland,“Towardsaudio-visualon-linediarizationof bepublished.
participantsingroupmeetings,”inProc.WorkshopMulti-Cameraand [109] S. Tamura, K. Iwano, and S. Furui, “Multi-modal speech recogni-
Multi-ModalSensorFusionAlgorithmsApplicat.–M2SFA2,Marseille, tion using optical-ﬂow analysis for lip images,” Real World Speech
France,2008. Process.,vol.36,no.2–3,pp.117–124,2004.
[83] G.FriedlandandO.Vinyals,“Livespeakeridentiﬁcationinconversa- [110] T.ChenandR.Rao,“Cross-modalpredictioninaudio-visualcommu-
tions,”inProc.MM’08:Proc.16thACMInt.Conf.Multimedia,New nication,”inProc.ICASSP,1996,vol.4,pp.2056–2059.
York,2008,pp.1017–1018. [111] J.W.Fisher,T.Darrell,W.T.Freeman,andP.A.Viola,“Learningjoint
[84] G.Friedland,O.Vinyals,Y.Huang,andC.Muller,“Prosodicandother statistical models for audio-visual fusion and segregation,” in Proc.
long-termfeaturesforspeakerdiarization,”IEEETrans.Audio,Speech, NIPS,2000,pp.772–778.
Lang.Process.,vol.17,no.5,pp.985–993,Jul.2009. [112] J.W.FisherandT.Darrell,“Speakerassociationwithsignal-levelau-
[85] J.Luque,X.Anguera,A.Temko,andJ.Hernando,“Speakerdiariza- diovisualfusion,”IEEETrans.Multimedia,vol.6,no.3,pp.406–413,
tion for conference room: The UPC RT07s evaluation system,” in Jun.2004.
Proc. Multimodal Technol. Perception of Humans: Int. Eval. Work- [113] R.RaoandT.Chen,“Exploitingaudio-visualcorrelationincodingof
shopsCLEAR2007andRT2007,Baltimore,MD,May8–11,2007, talkingheadsequences,”inProc.Int.PictureCodingSymp.,Mar.1996.
RevisedSelectedPapers,Berlin,Heidelberg:Springer-Verlag, 2008, [114] M.SiracusaandJ.Fisher,“Dynamicdependencytestsforaudio-visual
pp.543–553. speakerassociation,”inProc.ICASSP,Apr.2007,pp.457–460.
[86] J.Pardo,X.Anguera,andC.Wooters,“Speakerdiarizationformul- [115] E.K.Patterson,S.Gurbuz,Z.Tufekci,andJ.N.Gowdy,“CUAVE:A
tipledistantmicrophonemeetings:Mixingacousticfeaturesandinter- newaudio-visualdatabaseformultimodalhuman–computerinterface
channeltimedifferences,”inProc.Interspeech,2006. research,”inProc.ICASSP,2002,pp.2017–2020.
[87] G.LathoudandI.M.Cowan,“Locationbasedspeakersegmentation,” [116] D. McNeill, Language and Gesture. New York: Cambridge Univ.
inProc.ICASSP,2003,vol.1,pp.176–179. Press,2000.
[88] D.EllisandJ.C.Liu,“Speakerturndetectionbasedonbetween-chan- [117] H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi, “Audio
nelsdifferences,”inProc.ICASSP,2004. segmentation and speaker localization in meeting videos,” in Proc.
[89] J.Ajmera,G.Lathoud,andL.McCowan,“Clusteringandsegmenting 18th Int. Conf. Pattern Recognit. (ICPR’06), 2006, vol. 2, pp.
speakersandtheirlocationsinmeetings,”inProc.ICASSP,2004,vol. 1150–1153.
1,pp.605–608. [118] H.Hung,Y.Huang,C.Yeo,andD.Gatica-Perez,“Associatingaudio-
[90] J. M. Pardo, X. Anguera, and C. Wooters, “Speaker diarization for visualactivitycuesinadominanceestimationframework,”inProc.
multipledistantmicrophonemeetings:Mixingacousticfeaturesand IEEEComput.Soc.Conf.Comput.Vis.PatternRecognition(CVPR)
inter-channeltimedifferences,”inProc.Interspeech,2006. Workshop Human Communicative Behavior, Anchorage, AK, 2008,
[91] J.Pardo,X.Anguera,andC.Wooters,“Speakerdiarizationformul- pp.1–6.
tiple-distant-microphonemeetingsusingseveralsourcesofinforma- [119] N.CampbellandN.Suzuki,“Workingwithverysparsedatatodetect
tion,”IEEETrans.Comput.,vol.56,no.9,pp.1212–1224,Sep.2007. speakerandlistenerparticipationinameetingscorpus,”inProc.Work-
[92] N.W.D.Evans,C.Fredouille,andJ.-F.Bonastre,“Speakerdiariza- shopProgramme,May2006,vol.10.
tionusingunsuperviseddiscriminantanalysisofinter-channeldelay [120] G.Friedland,H.Hung,andC.Yeo,“Multimodalspeakerdiarization
features,”inProc.ICASSP,Apr.2009,pp.4061–4064. ofreal-worldmeetingsusingcompressed-domainvideofeatures,”in
[93] M.Wölfel,Q.Yang,Q.Jin,andT.Schultz,“Speakeridentiﬁcation Proc.ICASSP,Apr.2009,pp.4069–4072.
usingwarpedMVDRcepstralfeatures,”inProc.Interspeech,2009. [121] G.Friedland,C.Yeo,andH.Hung,“Visualspeakerlocalizationaided
[94] E.Shriberg,“Higher-levelfeaturesinspeakerrecognition,”inSpeaker byacousticmodels,”inProc.17thACMInt.Conf.MultimediaMM’09:
Classiﬁcation I, C. Müller, Ed. Berlin, Heidelberg, Germany: ,NewYork,2009,pp.195–202.
Springer,2007,vol.4343,LectureNotesinArtiﬁcialIntelligence. [122] S.Meignier,D.Moraru,C.Fredouille,J.-F.Bonastre,andL.Besacier,
[95] D.ImsengandG.Friedland,“Tuning-robustinitializationmethodsfor “Step-by-stepandintegratedapproachesinbroadcastnewsspeakerdi-
speakerdiarization,”IEEETrans.Audio,Speech,Lang.Process.,vol. arization,” in Proc. CSL, Sel. Papers from Speaker Lang. Recognit.
18,no.8,pp.2028–2037,Nov.2010. Workshop(Odyssey’04),2006,pp.303–330.
[96] D. Imseng and G. Friedland, “Robust speaker diarization for short [123] D. Vijayasenan, F. Valente, and H. Bourlard, “Combination of ag-
speechrecordings,”inProc.IEEEWorkshopAutom.SpeechRecognit. glomerativeandsequentialclusteringforspeakerdiarization,”inProc.
Understand.,Dec.2009,pp.432–437. ICASSP,LasVegas,NV,2008,pp.4361–4364.
[97] E. Shriberg, A. Stolcke, and D. Baron, “Observations on overlap: [124] E.El-Khoury,C.Senac,andS.Meignier,“Speakerdiarization:Com-
Findings and implications for automatic processing of multi-party binationoftheLIUMandIRITsystems,”inInternalReport,2008.
conversations,”inProc.Eurospeech’01,Aalborg,Denmark,2001,pp. [125] V. Gupta, P. Kenny, P. Ouellet, G. Boulianne, and P. Dumouchel,
1359–1362. “Combining Gaussianized/non-Gaussianized features to improve
[98] O.ÇetinandE.Shriberg,“SpeakeroverlapsandASRerrorsinmeet- speaker diarization of telephone conversations,” in IEEE Signal
ings:Effectsbefore,during,andaftertheoverlap,”inProc.ICASSP, Process.Lett.,Dec.2007,vol.14,no.12,pp.1040–1043.
Toulouse,France,2006,pp.357–360. [126] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-
[99] K.Boakye,B.Trueba-Hornero,O.Vinyals,andG.Friedland,“Over- lems,”Ann.Statist.,vol.1,no.2,pp.209–230,1973.
lappedspeechdetectionforimprovedspeakerdiarizationinmultiparty [127] F.Valente,“Inﬁnitemodelsforspeakerclustering,”inProc.Int.Conf.
meetings,”inProc.ICASSP,2008,pp.4353–4356. SpokenLang.Process.,2006,iDIAP-RR06–19.
[100] B.Trueba-Hornero,“Handlingoverlappedspeechinspeakerdiariza- [128] Y.W.Teh,M.I.Jordan,M.J.Beal,andD.M.Blei,“Hierarchical
tion,”M.S.thesis,Univ.PolitecnicadeCatalunya,Barcelona,Spain, Dirichletprocesses,” J.Amer. Statist. Assoc., vol. 101, no.476, pp.
2008. 1566–1581,2006.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 370 IEEETRANSACTIONSONAUDIO,SPEECH,ANDLANGUAGEPROCESSING,VOL.20,NO.2,FEBRUARY2012
[129] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “An biometrics,speechenhancement,andacousticechocancellation.Histeamled
HDP-HMMforsystemswithstatepersistence,”inProc.ICML,Jul. LIA-EURECOM’sjointentrytotheNISTRichTranscriptionevaluationsin
2008. 2009.Hehasauthoredorcoauthoredinexcessof50peer-reviewedresearch
[130] M.HuijbregtsandC.Wooters,“Theblamegame:Performanceanalysis articlesandparticipatesinseveralnationalandEuropeanprojects,allinvolving
ofspeakerdiarizationsystemcomponents,”inProc.Interspeech,Aug. speechprocessing.
2007,pp.1857–60. Dr.EvansisamemberoftheIEEESignalProcessingSociety,ISCA,and
EURASIPandheservesasanAssociateEditoroftheEURASIPJournalon
XavierAngueraMiro(M’06)receivedtheTelecom- Audio,Speech,andMusicProcessing.
munications Engineering and European Masters in
LanguageandSpeech(M.S.)degreesfromtheUni-
versitatPolitècnicadeCatalunya(UPC),Barcelona,
Spain,in2001andthePh.D.degreefromUPC,witha CorinneFredouillereceivedthePh.D.degreefrom
thesison“RobustSpeakerDiarizationforMeetings.” theLaboratoireInformatiqued’Avignon(LIA),Uni-
From2001to2003,hewaswithPanasonicSpeech versityofAvignon,Avignon,France,in2000
TechnologyLab,SantaBarbara,CA.From2004to She was appointed as an Assistant Professor at
2006, he was a Visiting Researcher at the Interna- LIAin2003.Herresearchinterestsincludeacoustic
tionalComputerScienceInstitute(ICSI),Berkeley, analysis, voice quality assessment, statistical
CA,wherehepursuedresearchonspeakerdiariza- modeling, automatic speaker recognition, speaker
tionformeetings,contributingtoICSI’sparticipationintheNISTRTevalua- diarization and, more recently, speech and voice
tionsin2004(broadcastnews)and2005–2007(meetings),obtainingstate-of- disorder assessment and acoustic-based characteri-
the-artresults.HebrieﬂyjoinedLIMSI,Paris,France,in2006.Hehasbeen zation.Shehasparticipatedinseveralnationaland
withTelefonicaResearch,Barcelona,Spain,since2007,pursuingresearchin international speaker diarization system evaluation
multimedia.Hiscurrentresearchinterestsincludespeakercharacterization(in- campaignsandhaspublishedover15researchpapersinthisﬁeld.
cludingdiarization,recognition,etc.),languageidentiﬁcation(includingapar- Prof.FredouilleisamemberoftheInternationalSpeechCommunicationAs-
ticipationinNISTLRE’07evaluation)andseveraltopicsinmultimodalmulti- sociation(ISCA)andsecretaryoftheFrenchspeakingcommunicationassoci-
mediaanalysis(e.g.,videocopydetection,involvingtheparticipationinNIST ation(AFCP),SpecialInterestGroup(SIG)ofISCA.
TRECVID2009and2010evaluations).Hehasauthoredorcoauthoredover50
peer-reviewedresearcharticles.HeisthemaindeveloperoftheBeamformIt
toolkit,extensivelyusedbytheRTcommunityforprocessingmultiplemicro-
phonerecordings. GeraldFriedland(M’08)receivedthediplomand
Dr.AngueraMiroisamemberofISCA,ACM,andIEEESignalProcessing doctorate (summa cum laude) degrees in computer
SocietyandhasbeeninvolvedintheorganizationofseveralACMandIEEE science from Freie Universität Berlin, Berlin, Ger-
conferences.Hehasbeenareviewerformanyconferences,aswellasforseveral many,in2002and2006,respectively.
journalsinthemultimediadomain. HeisaSeniorResearchScientistattheInterna-
tionalComputerScienceInstitute(ICSI),Berkeley,
CA,anindependentnonproﬁtresearchlabassociated
withtheUniversityofCalifornia(UC)atBerkeley
Simon Bozonnet (S’08) received the diploma in wherehe,amongotherfunctions,iscurrentlyleading
electricalengineeringfromINSAdeLyon,France, thespeakerdiarizationresearch.Apartfromspeech,
in 2008 with specialization in signal processing hisinterestsalsoincludeimageandvideoprocessing
andtheMasterofResearchinImagesandSystems andmultimodalmachinelearning.HeisaPrincipalInvestigatoronanIARPA
from INSA. He undertook his M.S. thesis at the projectonvideoconceptdetectionandaCo-PrincipalInvestigatoronanNGA
Nuclear Energy Center (CEA), Bruyères-le-Châtel, NURIgrantonmultimodallocationestimation.Until2009hehadbeenasite
France, where he worked on signal fusion and Coordinator for the EU-funded AMIDA and the Swiss-funded IM2 projects
intelligent systems for source localization. He is whichsponsoredtheresearchonmultimodalmeetinganalysisalgorithms.
currently pursuing the Ph.D. degree from Telecom Dr.FriedlandisamemberoftheIEEEComputerSocietyandtheIEEECom-
ParisTech,Paris,France,andjoinedtheMultimedia municationSociety,andheisinvolvedintheorganizationofvariousACMand
Communications Department as a Ph.D. candidate IEEEconferences,includingtheIEEEInternationalConferenceonSemantic
withLIA-EURECOM,Sophia-Antipolis,France. Computing(ICSC),whereheservedascochairandtheIEEEInternationalSym-
As part of his studies, he spent one year at KTH (Royal Institute of posiumonMultimedia(ISM2009),whereheservedasprogramcochair.Heis
Technology),Stockholm, Sweden. His research interests includemultimedia alsocofounderandProgramDirectoroftheIEEEInternationalSummerSchool
indexing, and speciﬁcally speaker diarization. He participated in LIA-EU- forSemanticComputingatUCBerkeley.Heistherecipientofseveralresearch
RECOMrecentsubmissiontotheNISTRT’09evaluationandcontributeshis andindustryrecognitions,amongthemtheMultimediaEntrepreneurAwardby
expertiseinspeakerdiarizationtothenational“ACAV”projectwhichaimsto theGermangovernmentandtheEuropeanAcademicSoftwareAward.Mostre-
improvewebaccessibilityforthevisuallyandhearingimpaired. cently,hewontheﬁrstprizeintheACMMultimediaGrandChallenge2009.
Nicholas Evans (M’06) received the M.Eng. Oriol Vinyals received a double degree in math-
and Ph.D. degrees from the University of Wales ematics and telecommunication engineering from
Swansea(UWS),Swansea,U.K.,in1999and2003, thePolytechnicUniversityofCatalonia,Barcelona,
respectively. Spain,andtheM.S.degreeincomputersciencefrom
From2002and2006,hewasaLectureratUWS the University of California, San Diego, in 2009.
andwasanHonoraryLectureruntil2009.Hebrieﬂy He is currently pursuing the Ph.D. degree at the
joined the Laboratoire Informatique d’Avignon UniversityofCalifornia,Berkeley.
(LIA), at the Université d’Avignon et des Pays de His interests include artiﬁcial intelligence, with
Vaucluse(UAPV),Avignon,France,in2006before particularemphasisonmachinelearning,speech,and
moving to EURECOM, Sophia Antipolis, France, vision.He wasa VisitingScholarat theComputer
in2007whereheisnowanAssistantProfessor.At Science Department, Carnegie Mellon University,
EURECOM,heheadsresearchinspeechandaudioprocessingandiscurrently Pittsburgh,PA,in2006,whereheworkedincomputervisionandrobotics.
active in the ﬁelds of speaker diarization, speaker recognition, multimodal Dr.VinyalsreceivedaMicrosoftResearchPh.D.Fellowshipin2011.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 356 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
Speaker Diarization: A Review of Recent Research
Xavier Anguera Miro, Member, IEEE, Simon Bozonnet, Student Member, IEEE, Nicholas Evans, Member, IEEE,
Corinne Fredouille, Gerald Friedland, Member, IEEE, and Oriol Vinyals
Abstract—Speaker diarization is the task of determining “who Speaker diarization has utility in a majority of applications
spoke when?” in an audio or video recording that contains an related to audio and/or video document processing, such as
unknown amount of speech and also an unknown number of
information retrieval for example. Indeed, it is often the case
speakers. Initially, it was proposed as a research topic related to
that audio and/or video recordings contain more than one
automatic speech recognition, where speaker diarization serves
as an upstream processing step. Over recent years, however, active speaker. This is the case for telephone conversations (for
speaker diarization has become an important key technology for example stemming from call centers), broadcast news, debates,
many tasks, such as navigation, retrieval, or higher level inference shows, movies, meetings, domain-speciﬁc videos (such as
on audio data. Accordingly, many important improvements in
surgery operations for instance), or even lecture or conference
accuracy and robustness have been reported in journals and
recordings including multiple speakers or questions/answers
conferences in the area. The application domains, from broadcast
news, to lectures and meetings, vary greatly and pose different sessions. In all such cases, it can be advantageous to automat-
problems, such as having access to multiple microphones and ically determine the number of speakers involved in addition
multimodal information or overlapping speech. The most recent to the periods when each speaker is active. Clear examples of
review of existing technology dates back to 2006 and focuses on
applications for speaker diarization algorithms include speech
the broadcast news domain. In this paper, we review the cur-
and speaker indexing, document content structuring, speaker
rent state-of-the-art, focusing on research developed since 2006
that relates predominantly to speaker diarization for conference recognition (in the presence of multiple or competing speakers),
meetings. Finally, we present an analysis of speaker diarization to help in speech-to-text transcription (i.e., so-called speaker at-
performance as reported through the NIST Rich Transcription tributed speech-to-text), speech translation and, more generally,
evaluations on meeting data and identify important areas for
Rich Transcription (RT), a community within which the current
future research.
state-of-the-art technology has been developed. The most sig-
Index Terms—Meetings, rich transcription, speaker diarization.
niﬁcant effort in the Rich Transcription domain comes directly
from the internationally competitive RT evaluations, sponsored
by the National Institute of Standards and Technology (NIST)
I. INTRODUCTION
in the Unites States [1]. Initiated originally within the telephony
S PEAKER diarization has emerged as an increasingly im-
domain, and subsequently in that of broadcast news, today it is
portant and dedicated domain of speech research. Whereas
in the domain of conference meetings that speaker diarization
speaker and speech recognition involve, respectively, the recog-
receives the most attention. Speaker diarization is thus an
nition of a person’s identity or the transcription of their speech,
extremely important area of speech processing research.
speaker diarization relates to the problem of determining “who
An excellent review of speaker diarization research is pre-
spoke when?.” More formally this requires the unsupervised
sented in [2], although it predominantly focuses its attention to
identiﬁcation of each speaker within an audio stream and the
speaker diarization for broadcast news. Coupled with the tran-
intervals during which each speaker is active.
sition to conference meetings, however, the state-of-the-art has
advanced signiﬁcantly since then. This paper presents an up-to-
date review of present state-of-the-art systems and reviews the
Manuscript received August 19, 2010; revised December 03, 2010; accepted
progress made in the ﬁeld of speaker diarization since 2005 up
February 13, 2011. Date of current version January 13, 2012. This work was
supported in part by the joint-national “Adaptable ambient living assistant” until now, including the most recent NIST RT evaluation that
(ALIAS) project funded through the European Ambient Assisted Living (AAL) was held in 2009. Ofﬁcial evaluations are an important vehicle
program under Agreement AAL-2009-2-049 and in part by the “Annotation
for pushing the state-of-the-art forward as it is only with stan-
Collaborative pour l’Accessibilité Vidéo” (ACAV) project funded by the French
Ministry of Industry (Innovative Web call) under Contract 09.2.93.0966. The dard experimental protocols and databases that it is possible to
work of X. Anguera Miro was supported in part by the Torres Quevedo Spanish meaningfully compare different approaches. While we also ad-
program. The associate editor coordinating the review of this manuscript and
dress emerging new research in speaker diarization, in this paper
approving it for publication was Prof. Sadaoki Furui.
X. Anguera Miro is with the Multimedia Research Group, Telefonica Re- special emphasis is placed on established technologies within
search, 08021 Barcelona, Spain (e-mail: xanguera@tid.es). the context of the NIST RT benchmark evaluations, which has
S. Bozonnet and N. Evans are with the Multimedia Communications
become a reliable indicator for the current state-of-the-art in
Department, EURECOM, 06904 Sophia Antipolis Cedex, France (e-mail:
bozonnet@eurecom.fr). speaker diarization. This paper aims at giving a concise refer-
C. Fredouille is with the University of Avignon, CERI/LIA, F-84911 Avignon ence overview of established approaches, both for the general
Cedex 9, France (e-mail: corinne.fredouille@univ-avignon.fr).
reader and for those new to the ﬁeld. Despite rapid gains in
G. Friedland and O. Vinyals are with the International Computer Science
Institute (ICSI), Berkeley, CA 94704 USA (e-mail: fractor@icsi.berkeley.edu; popularity over recent years, the ﬁeld is relatively embryonic
evans@eurecom.fr). compared to the mature ﬁelds of speech and speaker recogni-
Color versions of one or more of the ﬁgures in this paper are available online
tion. There are outstanding opportunities for contributions and
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TASL.2011.2125954 we hope that this paper serves to encourage others to participate.
1558-7916/$31.00 © 2012 IEEE
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 357
Section II presents a brief history of speaker diarization linguistic content and other metadata can be added (such as the
research and the transition to the conference meeting domain. dominant speakers, the level of interactions, or emotions).
We describe the main differences between broadcast news Undertaking benchmarking evaluations has proven to be
and conference meetings and present a high-level overview of an extremely productive means for estimating and comparing
current approaches to speaker diarization. In Section III, we algorithm performance and for verifying genuine technolog-
present a more detailed description of the main algorithms that ical advances. Speaker diarization is no exception and, since
are common to many speaker diarization systems, including 2002, the US National Institute for Standards and Technology
those recently introduced to make use of information coming (NIST) has organized ofﬁcial speaker diarization evaluations1
from multiple microphones, namely delay-and-sum beam- involving broadcast news (BN) and, more recently, meeting
forming. Section IV presents some of the most recent work in data. These evaluations have crucially contributed to bringing
the ﬁeld including efforts to handle multimodal information researchers together and to stimulating new ideas to advance the
and overlapping speech. We also discuss the use of features state-of-the-art. While other contrastive sub-domains such as
based on inter-channel delay and prosodics and also attempts lecture meetings and coffee breaks have also been considered,
to combine speaker diarization systems. In Section V, we the conference meeting scenario has been the primary focus
present an overview of the current status in speaker diarization of the NIST RT evaluations since 2004. The meeting scenario
research. We describe the NIST RT evaluations, the different is often referred to as “speech recognition complete,” i.e., a
datasets and the performance achieved by state-of-the-art sys- scenario in which all of the problems that arise in any speech
tems. We also identify the remaining problems and highlight recognition can be encountered in this domain. Conference
potential solutions in the context of current work. Finally, our meetings thus pose a number of new challenges to speaker
conclusions are presented in Section VI. diarization that typically were less relevant in earlier research.
II. SPEAKER DIARIZATION A. Broadcast News Versus Conference Meetings
Over recent years, the scientiﬁc community has developed
With the change of focus of the NIST RT evaluations from BN
research on speaker diarization in a number of different do-
to meetings diarization algorithms had to be adapted according
mains, with the focus usually being dictated by funded research
to the differences in the nature of the data. First, BN speech
projects. From early work with telephony data, broadcast
data is usually acquired using boom or lapel microphones with
news (BN) became the main focus of research towards the
some recordings being made in the studio and others in the
late 1990s and early 2000s and the use of speaker diariza-
ﬁeld. Conversely, meetings are usually recorded using desktop
tion was aimed at automatically annotating TV and radio
or far-ﬁeld microphones (single microphones or microphone ar-
transmissions that are broadcast daily all over the world. An-
rays) which are more convenient for users than head-mounted or
notations included automatic speech transcription and meta
lapel microphones.2 As a result, the signal-to-noise ratio is gen-
data labeling, including speaker diarization. Interest in the
erally better for BN data than it is for meeting recordings. Addi-
meeting domain grew extensively from 2002, with the launch
tionally, differences between meeting room conﬁgurations and
of several related research projects including the European
microphone placement lead to variations in recording quality,
Union (EU) Multimodal Meeting Manager (M4) project, the
including background noise, reverberation and variable speech
Swiss Interactive Multimodal Information Management (IM2)
levels (depending on the distance between speakers and micro-
project, the EU Augmented Multi-party Interaction (AMI)
phones).
project, subsequently continued through the EU Augmented
Second, BN speech is often read or at least prepared in ad-
Multi-party Interaction with Distant Access (AMIDA) project
vance while meeting speech tends to be more spontaneous in
and, and ﬁnally, the EU Computers in the Human Interaction
nature and contains more overlapping speech. Although BN
Loop (CHIL) project. All these projects addressed the research
recordings can contain speech that is overlapped with music,
and development of multimodal technologies dedicated to the
laughter, or applause (far less common for conference meeting
enhancement of human-to-human communications (notably in
data), in general, the detection of acoustic events and speakers
distant access) by automatically extracting meeting content,
tends to be more challenging for conference meeting data than
making the information available to meeting participants, or for
for BN data.
archiving purposes.
Finally, the number of speakers is usually larger in BN but
These technologies have to meet challenging demands such
speaker turns occur less frequently than they do in conference
as content indexing, linking and/or summarization of on-going
meeting data, resulting in BN having a longer average speaker
or archived meetings, the inclusion of both verbal and nonverbal
turn length. An extensive analysis of BN characteristics is re-
human communication (people movements, emotions, interac-
ported in [3] and a comparison of BN and conference meeting
tions with others, etc.). This is achieved by exploiting several
data can be found in [4].
synchronized data streams, such as audio, video and textual in-
formation (agenda, discussion papers, slides, etc.), that are able 1Speaker diarization was evaluated prior to 2002 through NIST Speaker
to capture different kinds of information that are useful for the Recognition (SR) evaluation campaigns (focusing on telephone speech) and
not within the RT evaluation campaigns.
structuring and analysis of meeting content. Speaker diarization
2Meeting databases recorded for research purposes usually contain
plays an important role in the analysis of meeting data since it al-
head-mounted and lapel microphone recordings for ground-truth creation
lows for such content to be structured in speaker turns, to which purposes only.
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 358 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 20, NO. 2, FEBRUARY 2012
assigned to the two individual clusters. Standard distance met-
rics, such as those described in Section III-C, are used to iden-
tify the closest clusters. A reassignment of frames to clusters
is usually performed after each cluster merging, via Viterbi re-
alignment for example, and the whole process is repeated itera-
tively, until some stopping criterion is reached, upon which there
should remain only one cluster for each detected speaker. Pos-
sible stopping criteria include thresholded approaches such as
the Bayesian Information Criterion (BIC) [9], Kullback–Leibler
(KL)-based metrics [10], the generalized likelihood ratio (GLR)
[11] or the recently proposed metric [12]. Bottom-up systems
submitted to the NIST RT evaluations [9], [13] have performed
consistently well.
2) Top-Down Approach: In contrast with the previous ap-
Fig. 1. General Diarization system. (a) Alternative clustering schemas.
proach, the top-down approach ﬁrst models the entire audio
(b) General speaker diarization architecture.
stream with a single speaker model and successively adds new
models to it until the full number of speakers are deemed to be
B. Main Approaches
accounted for. A single GMM model is trained on all the speech
Most of present state-of-the-art speaker diarization systems segments available, all of which are marked as unlabeled. Using
ﬁt into one of two categories: the bottom-up and the top-down some selection procedure to identify suitable training data from
approaches, as illustrated in Fig. 1(a). The top-down approach the non-labeled segments, new speaker models are iteratively
is initialized with very few clusters (usually one) whereas the added to the model one-by-one, with interleaved Viterbi realign-
bottom-up approach is initialized with many clusters (usually ment and adaptation. Segments attributed to any one of these
more clusters than expected speakers). In both cases the aim new models are marked as labeled. Stopping criteria similar to
is to iteratively converge towards an optimum number of clus- those employed in bottom-up systems may be used to terminate
ters. If the ﬁnal number is higher than the optimum then the the process or it can continue until no more relevant unlabeled
system is said to under-cluster. If it is lower it is said to over- segments with which to train new speaker models remain. Top-
cluster. Both bottom-up and top-down approaches are generally down approaches are far less popular than their bottom-up coun-
based on hidden Markov models (HMMs) where each state is a terparts. Some examples include [14]–[16]. While they are gen-
Gaussian mixture model (GMM) and corresponds to a speaker. erally out-performed by the best bottom-up systems, top-down
Transitions between states correspond to speaker turns. In this approaches have performed consistently and respectably well
section, we brieﬂy outline the standard bottom-up and top-down against the broader ﬁeld of other bottom-up entries. Top-down
approaches as well as two recently proposed alternatives: one approaches are also extremely computationally efﬁcient and can
based on information theory; and a second one based on a non be improved through cluster puriﬁcation [17].
parametric Bayesian approach. Although these new approaches 3) Other Approaches: A recent alternative approach, though
have not been reported previously in the context of ofﬁcial NIST also bottom-up in nature, is inspired from rate-distortion theory
RT evaluations they have shown strong potential on NIST RT and is based on an information-theoretic framework [18]. It is
evaluation datasets and are thus included here. Additionally, completely non parametric and its results have been shown to
some other works propose sequential single-pass segmentation be comparable to those of state-of-the-art parametric systems,
and clustering approaches [5]–[7], although their performance with signiﬁcant savings in computation. Clustering is based on
tends to fall short of the state-of-the-art. mutual information, which measures the mutual dependence
1) Bottom-Up Approach: The bottom-up approach is by far of two variables [19]. Only a single global GMM is tuned for
the most common in the literature. Also known as agglomer- the full audio stream, and mutual information is computed in
ative hierarchical clustering (AHC or AGHC), the bottom-up a new space of relevance variables deﬁned by the GMM com-
approach trains a number of clusters or models and aims at ponents. The approach aims at minimizing the loss of mutual
successively merging and reducing the number of clusters until information between successive clusterings while preserving as
only one remains for each speaker. Various initializations have much information as possible from the original dataset. Two
been studied and, whereas some have investigated -means clus- suitable methods have been reported: the agglomerative infor-
tering, many systems use a uniform initialization, where the mation bottleneck (aIB) [18] and the sequential information bot-
audio stream is divided into a number of equal length abutted tleneck (sIB) [19]. Even if this new system does not lead to
segments. This simpler approach generally leads to equivalent better performance than parametric approaches, results com-
performance [8]. In all cases the audio stream is initially over- parable to state-of-the-art GMM systems are reported and are
segmented into a number of segments which exceeds the antic- achieved with great savings in computation.
ipated maximum number of speakers. The bottom-up approach Alternatively, Bayesian machine learning became popular by
then iteratively selects closely matching clusters to merge, hence the end of the 1990s and has recently been used for speaker
reducing the number of clusters by one upon each iteration. diarization. The key component of Bayesian inference is that
Clusters are generally modeled with a GMM and, upon merging, it does not aim at estimating the parameters of a system (i.e.,
a single new GMM is trained on the data that was previously to perform point estimates), but rather the parameters of their
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. ANGUERA MIRO et al.: SPEAKER DIARIZATION: A REVIEW OF RECENT RESEARCH 359
related distribution (hyperparameters). This allows for avoiding clustering [15], [16]. Next, in Fig. 1(b)-iii/iv, a distance between
any premature hard decision in the diarization problem and for clusters and a split/merging mechanism (see Section III-D) is
automatically regulating the system with the observations (e.g., used to iteratively merge clusters [13], [31] or to introduce new
the complexity of the model is data dependent). However, the ones [16]. Optionally, data puriﬁcation algorithms can be used
computation of posterior distributions often requires intractable to make clusters more discriminant [13], [17], [32]. Finally, as
integrals and, as a result, the statistics community has developed illustrated in Fig. 1(b)-v, stopping criteria are used to determine
approximate inference methods. Monte Carlo Markov chains when the optimum number of clusters has been reached [33],
(MCMCs) were ﬁrst used [20] to provide a systematic approach [34].
to the computation of distributions via sampling, enabling the
deployment of Bayesian methods. However, sampling methods A. Acoustic Beamforming
are generally slow and prohibitive when the amount of data is The application of speaker diarization to the meeting domain
large, and they require to be run several times as the chains may triggered the need for dealing with multiple microphones which
get stuck and not converge in a practical number of iterations. are often used to record the same meeting from different lo-
Another alternative approach, known as Variational Bayes, cations in the room [35]–[37]. The microphones can have dif-
has been popular since 1993 [21], [22] and aims at providing a ferent characteristics: wall-mounted microphones (intended for
deterministic approximation of the distributions. It enables an speaker localization), lapel microphones, desktop microphones
inference problem to be converted to an optimization problem positioned on the meeting room table or microphone arrays. The
by approximating the intractable distribution with a tractable use of different microphone combinations as well as differences
approximation obtained by minimizing the Kullback–Leibler in microphone quality called for new approaches to speaker di-
divergence between them. In [23] a Variational Bayes-EM arization with multiple channels.
algorithm is used to learn a GMM speaker model and optimize The multiple distant microphone (MDM) condition was in-
a change detection process and the merging criterion. In [24], troduced in the NIST RT’04 (Spring) evaluation. A variety of
variational Bayes is combined successfully with eigenvoice algorithms have been proposed to extend mono-channel diariza-
modeling, described in [25], for the speaker diarization of tion systems to handle multiple channels. One option, proposed
telephone conversations. However, these systems still con- in [38], is to perform speaker diarization on each channel inde-
sider classical Viterbi decoding for the classiﬁcation and pendently and then to merge the individual outputs. In order to
differ from the nonparametric Bayesian systems introduced in do so, a two axis merging algorithm is used which considers the
Section IV-F. longest detected speaker segments in each channel and iterates
Finally, the recently proposed speaker binary keys [26] have over the segmentation output. In the same year, a late-stage fu-
been successfully applied to speaker diarization in meetings sion approach was also proposed [39]. In it, speaker segmen-
[27] with similar performance to state-of-the-art systems but tation is performed separately in all channels and diarization
also with considerable computational savings (running in is applied only taking into account the channel whose speech
around 0.1 times real-time). Speaker binary keys are small bi- segments have the best signal-to-noise ratio (SNR). Subsequent
nary vectors computed from the acoustic data using a universal approaches investigated preprocessing to combine the acoustic
background model (UBM)-like model. Once they are computed signals to obtain a single channel which could then be processed
all processing tasks take place in the binary domain. Other by a regular mono-channel diarization system. In [40], the mul-
works in speaker diarization concerned with speed include [28], tiple channels are combined with a simple weighted sum ac-
[29] which achieve faster than real-time processing through the cording to their SNR. Though straightforward to implement, it
use of several processing tricks applied to a standard bottom-up does not take into account the time difference of arrival between
approach ([28]) or by parallelizing most of the processing each microphone channel and might easily lead to a decrease in
in a GPU unit ([29]). The need for efﬁcient diarization sys- performance.
tems is emphasized when processing very large databases or Since the NIST RT’05 evaluation, the most common ap-
when using diarization as a preprocessing step to other speech proach to multi-channel speaker diarization involves acoustic
algorithms. beamforming as initially proposed in [41] and described in de-
tail in [42]. Many RT participants use the free and open-source
III. MAIN ALGORITHMS acoustic beamforming toolkit known as BeamformIt [43]
Fig. 1(b) shows a block diagram of the generic modules which which consists of an enhanced delay-and-sum algorithm to
make up most speaker diarization systems. The data prepro- correct misalignments due to the time-delay-of-arrival (TDOA)
cessing step (Fig. 1(b)-i) tends to be somewhat domain spe- of speech to each microphone. Speech data can be optionally
ciﬁc. For meeting data, preprocessing usually involves noise re- preprocessed using Wiener ﬁltering [44] to attenuate noise
duction (such as Wiener ﬁltering for example), multi-channel using, for example, [45]. A reference channel is selected and
acoustic beamforming (see Section III-A), the parameterization the other channels are appropriately aligned and combined with
of speech data into acoustic features (such as MFCC, PLP, etc.) a standard delay-and-sum algorithm. The contribution made by
and the detection of speech segments with a speech activity each signal channel to the output is then dynamically weighted
detection algorithm (see Section III-B). Cluster initialization according to its SNR or by using a cross-correlation-based
(Fig. 1(b)-ii) depends on the approach to diarization, i.e., the metric. Various additional algorithms are available in the
choice of an initial set of clusters in bottom-up clustering [8], BeamformIt toolkit to select the optimum reference channel
[13], [30] (see Section III-C) or a single segment in top-down and to stabilize the TDOA values between channels before the
Authorized licensed use limited to: INRIA. Downloaded on October 05,2020 at 07:30:52 UTC from IEEE Xplore.  Restrictions apply. 