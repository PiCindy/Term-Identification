Deep Neural Network Embeddings for Text-Independent Speaker Veriﬁcation
DavidSnyder,DanielGarcia-Romero,DanielPovey,SanjeevKhudanpur
CenterforLanguageandSpeechProcessing&HumanLanguageTechnologyCenterofExcellence,
TheJohnsHopkinsUniversity,USA
{david.ryan.snyder, dpovey}@gmail.com, {dgromero, khudanpur}@jhu.edu
Abstract model(UBM)thatisusedtocollectsufﬁcientstatistics,alarge
projectionmatrixtoextracti-vectors,andaprobabilisticlinear
Thispaperinvestigatesreplacingi-vectorsfortext-independent
discriminantanalysis(PLDA)backendtocomputeasimilarity
speaker veriﬁcation with embeddings extracted from a feed-
scorebetweeni-vectors[2,3,4,5,6,7].
forward deep neural network. Long-term speaker characteris-
tics are captured in the network by a temporal pooling layer Traditionally, the UBM is a Gaussian mixture model
thataggregatesovertheinputspeech.Thisenablesthenetwork (GMM) trained on acoustic features. Recent work has shown
to be trained to discriminate between speakers from variable- that incorporating an ASR DNN acoustic model can improve
lengthspeechsegments. Aftertraining,utterancesaremapped theUBM’sabilitytomodelphoneticcontent[8,9,10,11,12].
directlytoﬁxed-dimensionalspeakerembeddingsandpairsof However, this comes at the cost of greatly increased compu-
embeddingsarescoredusingaPLDA-basedbackend.Wecom- tational complexity compared to traditional systems [11]. In
pareperformancewithatraditionali-vectorbaselineonNIST addition, theadvantagesofincorporatingASRDNNsintothe
SRE2010and2016. Weﬁndthattheembeddingsoutperform i-vectorpipelinehavebeenlargelyisolatedtoEnglishlanguage
i-vectorsforshortspeechsegmentsandarecompetitiveonlong speech; [13]foundnobeneﬁtinamulti-languagesetting. For
durationtestconditions. Moreover,thetworepresentationsare these reasons, we restrict the scope of study to traditional i-
complementary,andtheirfusionimprovesonthebaselineatall vectorsystemsusingGMMs.
operatingpoints. Similarsystemshaverecentlyshownpromis-
ingresultswhentrainedonverylargeproprietarydatasets,but
1.2. SpeakerveriﬁcationwithDNNs
tothebestofourknowledge,thesearethebestresultsreported
for speaker-discriminative neural networks when trained and
It may be possible to produce more powerful SV systems by
testedonpubliclyavailablecorpora.
trainingthemtodirectlydiscriminatebetweenspeakers. Some
Index Terms: speaker recognition, speaker veriﬁcation, deep
studieshaveinvestigateddiscriminativelytrainingcomponents
neuralnetworks
ofthei-vectorsystem[14,15]. Giventheirsuccessinotherar-
easofspeechtechnology,anaturalalternativeistouseDNNs
1. Introduction
trainedonspeaker-discriminativetasks. Inearlysystems,neu-
ralnetworksaretrainedtoclassifytrainingspeakers[16,17]or
Speaker veriﬁcation (SV) is the task of authenticating the
inSiamesearchitecturestoseparatesame-speakeranddifferent-
claimed identity of a speaker, based on some speech signal
speakerpairs[18,19,20]. Aftertraining,frame-levelfeatures
and enrolled speaker record. Typically, low-dimensional rep-
areextractedfromthenetworksandusedasinputtoGaussian
resentationsrichinspeakerinformationareextractedforboth
speakermodels. However,wearenotawareofanyworksug-
enrollment and test speech, and compared to enable a same-
gestingthatthosemethodsarecompetitivewithmoderni-vector
or-different speaker decision. In modern systems, the repre-
systemsfortext-independentSV.
sentations are usually i-vectors. If the lexical content of the
utterancesisﬁxedtosomephrase,thetaskisconsideredtext- Progresshasbeenprimarilyconcentratedintext-dependent
dependent,otherwiseitistext-independent. Thispaperinvesti- SVonlargeproprietarydatasets. In[21],afeed-forwardDNN
gatesreplacingi-vectorswithembeddingsproducedbyadeep istrainedtoclassifyspeakersattheframe-level,onthephrase
neural network (DNN) for text-independent SV. The relative “OKGoogle.” Aftertraining, thesoftmaxoutputlayerisdis-
strengths and weaknesses of this approach are assessed under carded and speaker representations (called d-vectors) are cre-
a variety of conditions. In some practical applications, ver- ated by averaging hidden layer activations. [22] built on this
iﬁcation must be performed using only a limited amount of approach for the same application, by training an end-to-end
testspeech, eithertoavoidlatencyinanonlineapplicationor system to discriminate between same-speaker and different-
duetolimitedavailability. Tosupplementthecore2010NIST speakerpairs.
speakerrecognitionevaluation(SRE),weconstructamodiﬁed
Recently, [23] showed that an end-to-end system that
versioninwhichtheenrollmentutterancesarefull-length, but
jointlylearnsembeddingsalongwithasimilaritymetriccould
thetestutteranceshavebeentruncatedtotheﬁrstfewseconds
outperformatraditionali-vectorbaselinefortext-independent
of speech. Finally, we assess performance on the Cantonese
SV. However, the approach required a large number of in-
and Tagalog NIST SRE 2016, which combines short-duration
domaintrainingspeakerstobeeffective. Oursystemisbased
testconditionswithlanguage-mismatch.
on [23], but modiﬁed in an effort to improve performance on
smaller,publiclyavailabledatasets.Wesplittheend-to-endap-
1.1. Speakerveriﬁcationwithi-vectors
proachintotwoparts:aDNNtoproduceembeddingsandasep-
Most text-independent SV systems are based on i-vectors [1]. aratebackendtocomparepairsofembeddings.Finally,instead
Thestandardsystemconsistsofapipelineofgenerativemod- oftrainingthesystemtoseparatesame-speakeranddifferent-
els, trained on independent subtasks: a universal background speakerpairs,theDNNlearnstoclassifytrainingspeakers.3.3. Neuralnetworkarchitecture
Thenetwork,illustratedinFigure1,consistsoflayersthatop-
erateonspeechframes,astatisticspoolinglayerthataggregates
overtheframe-levelrepresentations,additionallayersthatoper-
ateatthesegment-level,andﬁnallyasoftmaxoutputlayer.The
nonlinearitiesarerectiﬁedlinearunits(ReLUs).
The ﬁrst 5 layers of the network work at the frame level,
with a time-delay architecture [26]. Suppose t is the current
timestep.Attheinput,wesplicetogetherframesat{t−2,t−
1,t,t+1,t+2}.Thenexttwolayerssplicetogethertheoutput
ofthepreviouslayerattimes{t−2,t,t+2}and{t−3,t,t+3},
respectively.Thenexttwolayersalsooperateattheframe-level,
but without any added temporal context. In total, the frame-
levelportionofthenetworkhasatemporalcontextoft−8to
t+8frames.Layersvaryinsize,from512to1536,depending
onthesplicingcontextused.
Thestatisticspoolinglayerreceivestheoutputoftheﬁnal
frame-level layer as input, aggregates over the input segment,
andcomputesitsmeanandstandarddeviation.Thesesegment-
levelstatisticsareconcatenatedtogetherandpassedtotwoad-
Figure1:DiagramoftheDNN.Segment-levelembeddings(e.g.,
ditional hidden layers with dimension 512 and 300 (either of
aorb)canbeextractedfromanylayerofthenetworkafterthe
whichmaybeusedtocomputeembeddings)andﬁnallythesoft-
statisticspoolinglayer.
maxoutputlayer. Excludingthesoftmaxoutputlayer(because
itisnotneededaftertraining)thereisatotalof4.4millionpa-
rameters.
2. Baselinei-vectorsystem
3.4. Training
Thebaselineisatraditionali-vectorsystemthatisbasedonthe
Thenetworkistrainedtoclassifytrainingspeakersusingamul-
GMM-UBMKaldirecipedescribedin[11]. Thefront-endfea-
ticlasscrossentropyobjectivefunction(Equation1). Thepri-
tures consist of 20 MFCCs with a frame-length of 25ms that
marydifferencebetweenthisandtrainingin[16,17,21]isthat
aremean-normalizedoveraslidingwindowofupto3seconds.
our system is trained to predict speakers from variable-length
Deltaandaccelerationareappendedtocreate60dimensionfea-
segments,ratherthanframes. SupposethereareK speakersin
turevectors.Anenergy-basedVADselectsfeaturescorrespond-
N trainingsegments. ThenP(spkr | x(n))istheprobabil-
ing to speech frames. The UBM is a 2048 component full- k 1:T
covariance GMM. The system uses a 600 dimension i-vector ityofspeakerk givenT inputframesx(1n),x(2n),...x(Tn). The
extractor. Prior to PLDA scoring, i-vectors are centered, di- quantitydnk is1ifthespeakerlabelforsegmentnisk,other-
mensionalityreducedto150usingLDA,andlengthnormalized. wiseit’s0.
PLDAscoresarenormalizedusingadaptives-norm[24].
N K
E =−(cid:88)(cid:88)d ln(P(spkr |x(n))) (1)
nk k 1:T
3. DNNembeddingsystem n=1k=1
TheDNNistrainedonthecombinedSWBDandSREdata
3.1. Overview
describedinSection4.1.Wereﬁnethedatasetbyremovingany
recordings that are less than 10 seconds long, and any speak-
Theproposedsystemisafeed-forwardDNN(depictedinFig-
erswithfewerthan4recordings. Thisleavesatotalof4,733
ure1)thatcomputesspeakerembeddingsfromvariable-length
speakers,whichisthesizeofthesoftmaxoutputlayer.
acousticsegments. Thearchitectureisbasedontheend-to-end
To reduce sensitivity to utterance length, it is desirable to
systemdescribedin[23].However,anend-to-endapproachre-
traintheDNNonspeechchunksthatcapturetherangeofdu-
quires a large amount of in-domain data to be effective. We
rationsweexpecttoencounterattesttime(e.g.,afewseconds
replacetheend-to-endlosswithamulticlasscrossentropyob-
to a few minutes). However, GPU memory limitations force
jective.Inaddition,aseparatelytrainedPLDAbackendisused
a tradeoff between minibatch size and maximum training ex-
to compare pairs of embeddings. This enables the DNN and
ample length. As a comprise, we pick examples that range
similaritymetrictobetrainedonpotentiallydifferentdatasets.
from2to10seconds(200to1000frames)alongwithamini-
Thenetworkisimplementedusingthennet3neuralnetworkli-
batchsizeof32to64.Theexamplespeechchunksaresampled
braryintheKaldiSpeechRecognitionToolkit[25].
denselyfromtherecordings,resultinginabout3,400examples
per speaker. The network is trained for several epochs using
3.2. Features
naturalgradientstochasticgradientdescent[27].
The features are 20 dimensional MFCCs with a frame-length
3.5. Speakerembeddings
of 25ms, mean-normalized over a sliding window of up to 3
seconds. The same energy-based VAD from Section 2 ﬁlters Ultimately, thegoaloftrainingthenetworkistoproduceem-
outnonspeechframes. Insteadofstackingframesattheinput, beddings that generalize well to speakers that have not been
short-term temporal context is handled by a time-delay DNN seeninthetrainingdata.Wewouldlikeembeddingstocapture
architecture. speakercharacteristicsovertheentireutterance, ratherthanattheframe-level.Thus,anylayerafterthestatisticspoolinglayer ingpoints[29].TheprimarymetricsareabbreviatedtoDCF10
is a sensible place to extract the embedding from. We do not andDCF16respectively.
considerthepresoftmaxafﬁnelayerbecauseofitslargesizeand
dependenceonthenumberofspeakers. Inthenetworkusedin 4.3. Results
thiswork,weareleftwithtwoafﬁnelayersfromwhichtoex-
Inthefollowingresults,ivectorreferstothetraditionali-vector
tractembeddings.ThesearedepictedinFigure1asembeddings
baseline described in Section 2. The labels embedding a and
aandb. Embeddingaistheoutputofanafﬁnelayerdirectly
embeddingbdenotethesystemsconsistingofembeddingsex-
ontopofthestatistics.Embeddingbisextractedfromthenext
tractedfromeitherembeddinglayerofthesameDNN(seeSec-
afﬁnelayerafteraReLU,andsoitisanonlinearfunctionofthe
tion3.5)andusedasfeaturestotheirownPLDAbackends.The
statistics.SincetheyarepartofthesameDNN,ifembeddingb
labelembeddingsistheaverageofthePLDAbackendsforthe
iscomputedthenwegetembeddingafor“free.”
individual embeddings. In the following results, we focus on
comparing the i-vector baseline with these combined embed-
3.6. PLDAbackend
dings.Finally,fusionreferstotheequallyweightedsumfusion
Weusethesamebackendfori-vectorsandembeddings. Em- ofthePLDAscoresofivectorandembeddings.
beddings are centered and dimensionality is reduced using
LDA.Asinthei-vectorsystem,wefoundthatanLDAdimen- 4.3.1. NISTSRE10
sion of 25% of the original worked well. After dimensional-
ityreduction,theembeddingsarelengthnormalizedandpairs
  40  
of embeddings are compared using PLDA. PLDA scores are
normalized using adaptive s-norm [24]. As described in Sec-
tion3.5,theDNNarchitecturepresentstheoptionofusingem-
beddings a or b or in combination. Instead of concatenating   20  
embeddingstogether,wecomputeseparatePLDAbackendsfor %)
eachembedding,andaveragethescores. n   10  
y (i
4. Experiments bilit   5   
a
b
o
4.1. Trainingdata pr   2   
s 
s
The training data consists of telephone speech, the bulk of Mi   1   
whichisEnglish. TheSWBDportionconsistsofSwitchboard
 0.5   ivector
2Phases1,2,and3,andSwitchboardCellular. TheSREpor-
embeddings
tion contains NIST SREs from 2004 through 2008. In total, fusion
thereareabout65,000recordingsfrom6,500speakers. Thei-   0.1 
vectorUBMandextractoraswellasthespeakerdiscriminative  0.01    0.1   0.5    1     2      5     10     20     40  
DNNaretrainedonthisdata. BothsystemsusePLDA-based False Alarm probability (in %)
backendstrainedonjusttheSREdata. Finally,the2016NIST
Figure2:DETcurveforthepooled5-60sportionofSRE10.
SREwasdistributedwithanunlabeledsetof2,472utterancesin
CantoneseandTagalog.Forbothsystems,weusethistocenter
thecorrespondingevaluationutterancesandforscorenormal-
ization. Table1:EER(%)onNISTSRE10
4.2. Evaluation 10s-10s 5s 10s 20s 60s full
ivector 11.0 9.1 6.0 3.9 2.3 1.9
WeassessperformanceonNIST2010and2016speakerrecog-
embeddinga 11.0 9.5 5.7 3.9 3.0 2.6
nition evaluations [28, 29]. In the remaining sections, these
embeddingb 9.2 8.8 6.6 5.5 4.4 3.9
willbeabbreviatedasSRE10andSRE16respectively. SRE10
embeddings 7.9 7.6 5.0 3.8 2.9 2.6
consistsofEnglishtelephonespeech. Ourevaluationisbased
on the extended core condition 5 and the 10s-10s condition. fusion 8.1 6.8 4.3 2.9 2.1 1.8
To supplement the core SRE10 condition, we produce addi-
tional conditions in which the enrollment utterances are full-
length, but the test utterances have been truncated to the ﬁrst
T ∈ {5,10,20,60} seconds of speech, as determined by an Table2:DCF10onNISTSRE10
energy-basedVAD.The10s-10sconditionwaspartoftheofﬁ-
cialSRE10andconsistsoftestandenrollmentutterancesthat 10s-10s 5s 10s 20s 60s full
contain about 10 seconds of speech. SRE16 is comprised of ivector 0.962 0.901 0.749 0.613 0.460 0.403
TagalogandCantoneselanguagetelephonespeech.Theenroll-
embeddinga 0.907 0.902 0.790 0.654 0.518 0.468
mentutterancescontainabout60secondsofspeechwhilethe
embeddingb 0.951 0.927 0.866 0.828 0.782 0.768
testutterancesrangefrom10to60secondsofspeech.
embeddings 0.854 0.875 0.738 0.667 0.567 0.539
In addition to equal error-rate (EER), results are reported
fusion 0.859 0.788 0.645 0.556 0.432 0.383
usingtheofﬁcialperformancemetricforeachSRE.ForSRE10,
thismetricwastheminimumofthenormalizeddetectioncost
function(DCF)withP =10−3[28].TheprimarySRE16
Target
metricwasabalanced(equalized)DCFaveragedattwooperat- Inthissection,welookatperformanceontheSRE10con-ditionsdescribedinSection4.2.Tables1and2demonstratethe Table3:EER(%)onNISTSRE16
interplaybetweenutterance-lengthandperformanceonSRE10.
Weseethati-vectorsarestilldominantforthelongestrecord- Cantonese Tagalog pool
ings,andoutperformembeddingsatboththeEERandDCF10 ivector 8.3 17.6 13.6
operating points. However, as the test utterance length de- embeddinga 7.7 17.6 13.1
creases, theperformanceoftheembeddingsimprovesrelative embeddingb 7.8 17.4 13.1
to the baseline. At 20 seconds of test speech, the combined embeddings 6.5 16.3 11.9
embeddingsare3%betterthani-vectorsinEERbut8%worse
fusion 6.3 15.4 11.3
atDCF10. Withjust10and5secondsoftestspeech,theem-
beddingsare17%and16%betterinEERandslightlybetterat
DCF10. The relative advantage of embeddings appears to be
largest when both enrollment and test utterances are short: in Table4:DCF16onNISTSRE2016
thecolumnlabeled10s-10sboththetestandenrollutterances
contain only about 10 seconds of speech, and we see that the Cantonese Tagalog pool
combinedembeddingsare28%betterinEERand11%betterin ivector 0.549 0.842 0.711
DCF10. Figure2illustratesthedetectionerrortradeoff(DET) embeddinga 0.532 0.835 0.689
curves for the systems when pooled across the truncated test embeddingb 0.630 0.851 0.741
conditions.AlthoughembeddingsarebetterattheEERoperat-
embeddings 0.508 0.803 0.658
ingpoint, theyfavorthelowmissrate, andareslightlyworse
fusion 0.442 0.794 0.622
whencomparedataverylowfalsealarmrate.
Sincethei-vectorandDNNsystemsaresodissimilar, we
expectgoodperformancefromtheirfusion.Weobserveanim-
provementoverusingi-vectorsaloneatalloperatingpointsand
conditions.ThelargestimprovementisinEERonthe10scon- spectively. Pooledacrosslanguages,weseethatthecombined
dition,whichis28%betterthanthebaseline. Evenonthefull- embeddingsoutperformsthei-vectorbaselineby13%inEER
length condition, where i-vectors are strongest, there is a 5% and 7% in DCF16. After combining with i-vectors, the im-
improvementoverusingthei-vectorsalone,inbothDCF10and provementincreasesto17%inEERand13%inDCF16. The
EER. DETplotinFigure3showsthattheseimprovementsarecon-
sistentacrossoperatingpoints.
4.3.2. NISTSRE16 AlthoughtheembeddingsalsoperformbetteronTagalog,
improvementislargestfortheCantoneseportion.Comparedto
thei-vectorbaseline,theembeddingsare22%betterintermsof
  80  
EERand7%inDCF16. Thefusedsystemisevenbetter,and
improvesonthei-vectorbaselineby24%inEERand19%in
  60   DCF16.
5. Conclusions
%)  40  
n 
y (i Inthispaper,weinvestigateddeepneuralnetworkembeddings
abilit  20   fdoinrgtesxatp-ipnedaerpteonbdeenctosmppeaektietirvveewriiﬁthcaatiotrna.diOtiovnearallil-,vtehcetoermbbaesde--
b
o line and are complementary when fused. We found that, al-
pr
s   10   though i-vectors are more effective on the full-length SRE10,
s
Mi embeddingsarebetterontheshortdurationconditions.Thisun-
  5    derscorestheﬁndingsof[23,30]thatDNNsmaybecapableof
ivector
producingmorepowerfulrepresentationsofspeakersfromshort
embeddings
  2    fusion speechsegments. SRE16presentedthechallengeoflanguage
  1    mismatchbetweenthepredominantlyEnglishtrainingdataand
 0.01    0.1   0.5    1     2      5     10     20     40   theCantoneseandTagalogevaluation.Wesawthatembeddings
False Alarm probability (in %) outperformedi-vectorsonbothlanguages,suggestingthatthey
maybemorerobusttothisdomainmismatch.Althoughthere-
Figure3: DETcurveforSRE16,pooledacrossCantoneseand
sultsarequitepromising,webelievethatPLDAmaynotbethe
Tagalog.
optimalsimilaritymetricfortheembeddings. Infuturework,
we will use the method described in this paper as pretraining
Inthissection,weevaluatethesamesystemsfromSection forthefullyend-to-endapproachin[23],sothatamoreappro-
4.3.1onSRE16. Usingthesameembeddingandi-vectorsys- priatesimilaritymetricislearnedalongwiththeembeddings.
temsforbothSRE10andSRE16avoidsthecomplexityofde-
velopingvariantsofeachsystemthatareoptimizedfordifferent 6. Acknowledgments
evaluations. However,thisdoescauseamismatchbetweenthe
predominatelyEnglishtrainingdata(usedtooptimizebothsys- This work was partially supported by NSF Grant No CRI-
tems)andtheTagalogandCantoneseevaluationspeech. Asa 1513128, Nanyang Technological University (NTU) Science
result, the performance reported here may lag behind that of of Learning Grant, National Institutes of Standards and Tech-
counterpartsoptimizedspeciﬁcallyforSRE16. nologyGrantNo70NANB16H039. TheauthorsthankPatrick
Tables3and4reportperformanceinEERandDCF16re- KennyandPaolaGarciafortheirhelpfuldiscussions.7. References
[19] ——,“Extractingspeaker-speciﬁcinformationwitharegularized
siamesedeepnetwork,”inAdvancesinNeuralInformationPro-
[1] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,
cessingSystems(NIPS11),2011.
“Front-endfactoranalysisforspeakerveriﬁcation,”IEEETrans-
actions on Audio, Speech, and Language Processing, vol. 19, [20] A.Salman,“Learningspeaker-speciﬁccharacteristicswithdeep
no.4,pp.788–798,2011. neuralarchitecture,”Ph.D.dissertation,UniversityofManchester,
2012.
[2] S.PrinceandJ.Elder,“Probabilisticlineardiscriminantanalysis
forinferencesaboutidentity,”inIEEE11thInternationalConfer- [21] E.Variani, X.Lei, E.McDermott, I.Moreno, andJ.Gonzalez-
enceonComputerVision,2007.ICCV2007.,Oct2007,pp.1–8. Dominguez, “Deep neural networks for small footprint text-
dependentspeakerveriﬁcation,”in2014IEEEInternationalCon-
[3] N.Bru¨mmerandE.DeVilliers,“Thespeakerpartitioningprob-
ference on Acoustics, Speech and Signal Processing (ICASSP).
lem.”inOdyssey,2010,p.34.
IEEE,2014,pp.4052–4056.
[4] J. Villalba and N. Bru¨mmer, “Towards fully bayesian speaker
[22] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, “End-to-
recognition: Integratingoutthebetween-speakercovariance.”in
endtext-dependentspeakerveriﬁcation,”in2016IEEEInterna-
INTERSPEECH,2011,pp.505–508.
tional Conference on Acoustics, Speech and Signal Processing
[5] P. Kenny, “Bayesian speaker veriﬁcation with heavy-tailed pri- (ICASSP). IEEE,2016,pp.5115–5119.
ors.”inOdyssey,2010,p.14.
[23] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
[6] D. Garcia-Romero and C. Espy-Wilson, “Analysis of i-vector Y. Carmiel, and S. Khudanpur, “Deep neural network-based
lengthnormalizationinspeakerrecognitionsystems.”inINTER- speakerembeddingsforend-to-endspeakerveriﬁcation,”inSpo-
SPEECH,2011,pp.249–252. kenLanguageTechnologyWorkshop(SLT),2016IEEE. IEEE,
2016.
[7] D.Garcia-Romero,X.Zhou,andC.Espy-Wilson,“Multicondi-
tiontrainingofGaussianpldamodelsini-vectorspacefornoise [24] S.Cumani,P.D.Batzu,D.Colibro,C.Vair,P.Laface,andV.Vasi-
andreverberationrobustspeakerrecognition,”in2012IEEEIn- lakakis,“Comparisonofspeakerrecognitionapproachesforreal
ternationalConferenceonAcoustics,SpeechandSignalProcess- applications,”inINTERSPEECH. ISCA,2011.
ing(ICASSP). IEEE,2012,pp.4257–4260.
[25] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
[8] P.Kenny,V.Gupta,T.Stafylakis,P.Ouellet,andJ.Alam,“Deep N.Goel,M.Hannemann,P.Motl´ıcˇek,Y.Qian,P.Schwarzetal.,
neuralnetworksforextractingBaum-Welchstatisticsforspeaker “TheKaldispeechrecognitiontoolkit,”inProceedingsoftheAu-
recognition,”inProc.Odyssey,2014. tomaticSpeechRecognition&Understanding(ASRU)Workshop,
2011.
[9] Y.Lei,N.Scheffer,L.Ferrer,andM.McLaren,“Anovelscheme
for speaker recognition using a phonetically-aware deep neural [26] V.Peddinti, D.Povey, andS.Khudanpur, “Atimedelayneural
network,”in2014IEEEInternationalConferenceonAcoustics, networkarchitectureforefﬁcientmodelingoflongtemporalcon-
SpeechandSignalProcessing(ICASSP). IEEE,2014,pp.1695– texts.”inINTERSPEECH,2015,pp.3214–3218.
1699.
[27] D. Povey, X. Zhang, and S. Khudanpur, “Parallel training
[10] D. Garcia-Romero, X. Zhang, A. McCree, and D. Povey, “Im- of deep neural networks with natural gradient and parameter
provingspeakerrecognitionperformanceinthedomainadapta- averaging,”CoRR,vol.abs/1410.7455,2015.[Online].Available:
tionchallengeusingdeepneuralnetworks,”inSpokenLanguage http://arxiv.org/abs/1410.7455
TechnologyWorkshop(SLT),2014IEEE. IEEE,2014,pp.378–
[28] “The NIST year 2010 speaker recognition evaluation plan,”
383.
http://www.itl.nist.gov/iad/mig/tests/sre/2010/,2010.
[11] D.Snyder,D.Garcia-Romero,andD.Povey,“Timedelaydeep
[29] “NIST speaker recognition evaluation 2016,”
neural network-based universal background models for speaker
https://www.nist.gov/itl/iad/mig/speaker-recognition-evaluation-
recognition,” in 2015 IEEE Workshop on Automatic Speech
2016/,2016.
RecognitionandUnderstanding(ASRU). IEEE,2015,pp.92–97.
[30] D.Garcia-Romero,D.Snyder,G.Sell,D.Povey,andA.McCree,
[12] F.Richardson,D.Reynolds,andN.Dehak,“Deepneuralnetwork
“Speakerdiarizationusingdeepneuralnetworkembeddings,”in
approachestospeakerandlanguagerecognition,”SignalProcess-
2017 IEEE International Conference on Acoustics, Speech and
ingLetters,IEEE,vol.22,no.10,pp.1671–1675,2015.
SignalProcessing(ICASSP). IEEE,2017,pp.4930–4934.
[13] O.Novotny´,P.Mateˇjka,O.Glembeck,O.Plchot,F.Gre´zl,L.Bur-
get, and J. Cˇernocky´, “Analysis of the dnn-based sre systems
in multi-language conditions,” in Spoken Language Technology
Workshop(SLT),2016IEEE. IEEE,2016.
[14] O.Glembek,L.Burget,N.Brummer,O.Plchot,andP.Matejka,
“Discriminativelytrainedi-vectorextractorforspeakerveriﬁca-
tion,”inINTERSPEECH,2011.
[15] L.Burget, O.Plchot, S.Cumani, O.Glembek, P.Mateˇjka, and
N. Bru¨mmer, “Discriminatively trained probabilistic linear dis-
criminantanalysisforspeakerveriﬁcation,”in2011IEEEinter-
nationalconferenceonacoustics, speechandsignalprocessing
(ICASSP). IEEE,2011,pp.4832–4835.
[16] Y.Konig,L.Heck,M.Weintraub,andK.Sonmez,“Nonlineardis-
criminantfeatureextractionforrobusttext-independentspeaker
recognition,”inProc.RLA2C,ESCAworkshoponSpeakerRecog-
nitionanditsCommercialandForensicApplications,1998.
[17] L.Heck,Y.Konig,K.Sonmez,andM.Weintraub,“Robustness
totelephonehandsetdistortioninspeakerrecognitionbydiscrim-
inativefeaturedesign,”inSpeechCommunication,vol.31,no.2,
2000,pp.181–192.
[18] K. Chen and A. Salman, “Learning speaker-speciﬁc character-
isticswithadeepneuralarchitecture,”inIEEETransactionson
NeuralNetworks,vol.22,no.11,2011,pp.1744–1756.Deep Neural Network Embeddings for Text-Independent Speaker Veriﬁcation
DavidSnyder,DanielGarcia-Romero,DanielPovey,SanjeevKhudanpur
CenterforLanguageandSpeechProcessing&HumanLanguageTechnologyCenterofExcellence,
TheJohnsHopkinsUniversity,USA
{david.ryan.snyder, dpovey}@gmail.com, {dgromero, khudanpur}@jhu.edu
Abstract model(UBM)thatisusedtocollectsufﬁcientstatistics,alarge
projectionmatrixtoextracti-vectors,andaprobabilisticlinear
Thispaperinvestigatesreplacingi-vectorsfortext-independent
discriminantanalysis(PLDA)backendtocomputeasimilarity
speaker veriﬁcation with embeddings extracted from a feed-
scorebetweeni-vectors[2,3,4,5,6,7].
forward deep neural network. Long-term speaker characteris-
tics are captured in the network by a temporal pooling layer Traditionally, the UBM is a Gaussian mixture model
thataggregatesovertheinputspeech.Thisenablesthenetwork (GMM) trained on acoustic features. Recent work has shown
to be trained to discriminate between speakers from variable- that incorporating an ASR DNN acoustic model can improve
lengthspeechsegments. Aftertraining,utterancesaremapped theUBM’sabilitytomodelphoneticcontent[8,9,10,11,12].
directlytoﬁxed-dimensionalspeakerembeddingsandpairsof However, this comes at the cost of greatly increased compu-
embeddingsarescoredusingaPLDA-basedbackend.Wecom- tational complexity compared to traditional systems [11]. In
pareperformancewithatraditionali-vectorbaselineonNIST addition, theadvantagesofincorporatingASRDNNsintothe
SRE2010and2016. Weﬁndthattheembeddingsoutperform i-vectorpipelinehavebeenlargelyisolatedtoEnglishlanguage
i-vectorsforshortspeechsegmentsandarecompetitiveonlong speech; [13]foundnobeneﬁtinamulti-languagesetting. For
durationtestconditions. Moreover,thetworepresentationsare these reasons, we restrict the scope of study to traditional i-
complementary,andtheirfusionimprovesonthebaselineatall vectorsystemsusingGMMs.
operatingpoints. Similarsystemshaverecentlyshownpromis-
ingresultswhentrainedonverylargeproprietarydatasets,but
1.2. SpeakerveriﬁcationwithDNNs
tothebestofourknowledge,thesearethebestresultsreported
for speaker-discriminative neural networks when trained and
It may be possible to produce more powerful SV systems by
testedonpubliclyavailablecorpora.
trainingthemtodirectlydiscriminatebetweenspeakers. Some
Index Terms: speaker recognition, speaker veriﬁcation, deep
studieshaveinvestigateddiscriminativelytrainingcomponents
neuralnetworks
ofthei-vectorsystem[14,15]. Giventheirsuccessinotherar-
easofspeechtechnology,anaturalalternativeistouseDNNs
1. Introduction
trainedonspeaker-discriminativetasks. Inearlysystems,neu-
ralnetworksaretrainedtoclassifytrainingspeakers[16,17]or
Speaker veriﬁcation (SV) is the task of authenticating the
inSiamesearchitecturestoseparatesame-speakeranddifferent-
claimed identity of a speaker, based on some speech signal
speakerpairs[18,19,20]. Aftertraining,frame-levelfeatures
and enrolled speaker record. Typically, low-dimensional rep-
areextractedfromthenetworksandusedasinputtoGaussian
resentationsrichinspeakerinformationareextractedforboth
speakermodels. However,wearenotawareofanyworksug-
enrollment and test speech, and compared to enable a same-
gestingthatthosemethodsarecompetitivewithmoderni-vector
or-different speaker decision. In modern systems, the repre-
systemsfortext-independentSV.
sentations are usually i-vectors. If the lexical content of the
utterancesisﬁxedtosomephrase,thetaskisconsideredtext- Progresshasbeenprimarilyconcentratedintext-dependent
dependent,otherwiseitistext-independent. Thispaperinvesti- SVonlargeproprietarydatasets. In[21],afeed-forwardDNN
gatesreplacingi-vectorswithembeddingsproducedbyadeep istrainedtoclassifyspeakersattheframe-level,onthephrase
neural network (DNN) for text-independent SV. The relative “OKGoogle.” Aftertraining, thesoftmaxoutputlayerisdis-
strengths and weaknesses of this approach are assessed under carded and speaker representations (called d-vectors) are cre-
a variety of conditions. In some practical applications, ver- ated by averaging hidden layer activations. [22] built on this
iﬁcation must be performed using only a limited amount of approach for the same application, by training an end-to-end
testspeech, eithertoavoidlatencyinanonlineapplicationor system to discriminate between same-speaker and different-
duetolimitedavailability. Tosupplementthecore2010NIST speakerpairs.
speakerrecognitionevaluation(SRE),weconstructamodiﬁed
Recently, [23] showed that an end-to-end system that
versioninwhichtheenrollmentutterancesarefull-length, but
jointlylearnsembeddingsalongwithasimilaritymetriccould
thetestutteranceshavebeentruncatedtotheﬁrstfewseconds
outperformatraditionali-vectorbaselinefortext-independent
of speech. Finally, we assess performance on the Cantonese
SV. However, the approach required a large number of in-
and Tagalog NIST SRE 2016, which combines short-duration
domaintrainingspeakerstobeeffective. Oursystemisbased
testconditionswithlanguage-mismatch.
on [23], but modiﬁed in an effort to improve performance on
smaller,publiclyavailabledatasets.Wesplittheend-to-endap-
1.1. Speakerveriﬁcationwithi-vectors
proachintotwoparts:aDNNtoproduceembeddingsandasep-
Most text-independent SV systems are based on i-vectors [1]. aratebackendtocomparepairsofembeddings.Finally,instead
Thestandardsystemconsistsofapipelineofgenerativemod- oftrainingthesystemtoseparatesame-speakeranddifferent-
els, trained on independent subtasks: a universal background speakerpairs,theDNNlearnstoclassifytrainingspeakers.3.3. Neuralnetworkarchitecture
Thenetwork,illustratedinFigure1,consistsoflayersthatop-
erateonspeechframes,astatisticspoolinglayerthataggregates
overtheframe-levelrepresentations,additionallayersthatoper-
ateatthesegment-level,andﬁnallyasoftmaxoutputlayer.The
nonlinearitiesarerectiﬁedlinearunits(ReLUs).
The ﬁrst 5 layers of the network work at the frame level,
with a time-delay architecture [26]. Suppose t is the current
timestep.Attheinput,wesplicetogetherframesat{t−2,t−
1,t,t+1,t+2}.Thenexttwolayerssplicetogethertheoutput
ofthepreviouslayerattimes{t−2,t,t+2}and{t−3,t,t+3},
respectively.Thenexttwolayersalsooperateattheframe-level,
but without any added temporal context. In total, the frame-
levelportionofthenetworkhasatemporalcontextoft−8to
t+8frames.Layersvaryinsize,from512to1536,depending
onthesplicingcontextused.
Thestatisticspoolinglayerreceivestheoutputoftheﬁnal
frame-level layer as input, aggregates over the input segment,
andcomputesitsmeanandstandarddeviation.Thesesegment-
levelstatisticsareconcatenatedtogetherandpassedtotwoad-
Figure1:DiagramoftheDNN.Segment-levelembeddings(e.g.,
ditional hidden layers with dimension 512 and 300 (either of
aorb)canbeextractedfromanylayerofthenetworkafterthe
whichmaybeusedtocomputeembeddings)andﬁnallythesoft-
statisticspoolinglayer.
maxoutputlayer. Excludingthesoftmaxoutputlayer(because
itisnotneededaftertraining)thereisatotalof4.4millionpa-
rameters.
2. Baselinei-vectorsystem
3.4. Training
Thebaselineisatraditionali-vectorsystemthatisbasedonthe
Thenetworkistrainedtoclassifytrainingspeakersusingamul-
GMM-UBMKaldirecipedescribedin[11]. Thefront-endfea-
ticlasscrossentropyobjectivefunction(Equation1). Thepri-
tures consist of 20 MFCCs with a frame-length of 25ms that
marydifferencebetweenthisandtrainingin[16,17,21]isthat
aremean-normalizedoveraslidingwindowofupto3seconds.
our system is trained to predict speakers from variable-length
Deltaandaccelerationareappendedtocreate60dimensionfea-
segments,ratherthanframes. SupposethereareK speakersin
turevectors.Anenergy-basedVADselectsfeaturescorrespond-
N trainingsegments. ThenP(spkr | x(n))istheprobabil-
ing to speech frames. The UBM is a 2048 component full- k 1:T
covariance GMM. The system uses a 600 dimension i-vector ityofspeakerk givenT inputframesx(1n),x(2n),...x(Tn). The
extractor. Prior to PLDA scoring, i-vectors are centered, di- quantitydnk is1ifthespeakerlabelforsegmentnisk,other-
mensionalityreducedto150usingLDA,andlengthnormalized. wiseit’s0.
PLDAscoresarenormalizedusingadaptives-norm[24].
N K
E =−(cid:88)(cid:88)d ln(P(spkr |x(n))) (1)
nk k 1:T
3. DNNembeddingsystem n=1k=1
TheDNNistrainedonthecombinedSWBDandSREdata
3.1. Overview
describedinSection4.1.Wereﬁnethedatasetbyremovingany
recordings that are less than 10 seconds long, and any speak-
Theproposedsystemisafeed-forwardDNN(depictedinFig-
erswithfewerthan4recordings. Thisleavesatotalof4,733
ure1)thatcomputesspeakerembeddingsfromvariable-length
speakers,whichisthesizeofthesoftmaxoutputlayer.
acousticsegments. Thearchitectureisbasedontheend-to-end
To reduce sensitivity to utterance length, it is desirable to
systemdescribedin[23].However,anend-to-endapproachre-
traintheDNNonspeechchunksthatcapturetherangeofdu-
quires a large amount of in-domain data to be effective. We
rationsweexpecttoencounterattesttime(e.g.,afewseconds
replacetheend-to-endlosswithamulticlasscrossentropyob-
to a few minutes). However, GPU memory limitations force
jective.Inaddition,aseparatelytrainedPLDAbackendisused
a tradeoff between minibatch size and maximum training ex-
to compare pairs of embeddings. This enables the DNN and
ample length. As a comprise, we pick examples that range
similaritymetrictobetrainedonpotentiallydifferentdatasets.
from2to10seconds(200to1000frames)alongwithamini-
Thenetworkisimplementedusingthennet3neuralnetworkli-
batchsizeof32to64.Theexamplespeechchunksaresampled
braryintheKaldiSpeechRecognitionToolkit[25].
denselyfromtherecordings,resultinginabout3,400examples
per speaker. The network is trained for several epochs using
3.2. Features
naturalgradientstochasticgradientdescent[27].
The features are 20 dimensional MFCCs with a frame-length
3.5. Speakerembeddings
of 25ms, mean-normalized over a sliding window of up to 3
seconds. The same energy-based VAD from Section 2 ﬁlters Ultimately, thegoaloftrainingthenetworkistoproduceem-
outnonspeechframes. Insteadofstackingframesattheinput, beddings that generalize well to speakers that have not been
short-term temporal context is handled by a time-delay DNN seeninthetrainingdata.Wewouldlikeembeddingstocapture
architecture. speakercharacteristicsovertheentireutterance, ratherthanattheframe-level.Thus,anylayerafterthestatisticspoolinglayer ingpoints[29].TheprimarymetricsareabbreviatedtoDCF10
is a sensible place to extract the embedding from. We do not andDCF16respectively.
considerthepresoftmaxafﬁnelayerbecauseofitslargesizeand
dependenceonthenumberofspeakers. Inthenetworkusedin 4.3. Results
thiswork,weareleftwithtwoafﬁnelayersfromwhichtoex-
Inthefollowingresults,ivectorreferstothetraditionali-vector
tractembeddings.ThesearedepictedinFigure1asembeddings
baseline described in Section 2. The labels embedding a and
aandb. Embeddingaistheoutputofanafﬁnelayerdirectly
embeddingbdenotethesystemsconsistingofembeddingsex-
ontopofthestatistics.Embeddingbisextractedfromthenext
tractedfromeitherembeddinglayerofthesameDNN(seeSec-
afﬁnelayerafteraReLU,andsoitisanonlinearfunctionofthe
tion3.5)andusedasfeaturestotheirownPLDAbackends.The
statistics.SincetheyarepartofthesameDNN,ifembeddingb
labelembeddingsistheaverageofthePLDAbackendsforthe
iscomputedthenwegetembeddingafor“free.”
individual embeddings. In the following results, we focus on
comparing the i-vector baseline with these combined embed-
3.6. PLDAbackend
dings.Finally,fusionreferstotheequallyweightedsumfusion
Weusethesamebackendfori-vectorsandembeddings. Em- ofthePLDAscoresofivectorandembeddings.
beddings are centered and dimensionality is reduced using
LDA.Asinthei-vectorsystem,wefoundthatanLDAdimen- 4.3.1. NISTSRE10
sion of 25% of the original worked well. After dimensional-
ityreduction,theembeddingsarelengthnormalizedandpairs
  40  
of embeddings are compared using PLDA. PLDA scores are
normalized using adaptive s-norm [24]. As described in Sec-
tion3.5,theDNNarchitecturepresentstheoptionofusingem-
beddings a or b or in combination. Instead of concatenating   20  
embeddingstogether,wecomputeseparatePLDAbackendsfor %)
eachembedding,andaveragethescores. n   10  
y (i
4. Experiments bilit   5   
a
b
o
4.1. Trainingdata pr   2   
s 
s
The training data consists of telephone speech, the bulk of Mi   1   
whichisEnglish. TheSWBDportionconsistsofSwitchboard
 0.5   ivector
2Phases1,2,and3,andSwitchboardCellular. TheSREpor-
embeddings
tion contains NIST SREs from 2004 through 2008. In total, fusion
thereareabout65,000recordingsfrom6,500speakers. Thei-   0.1 
vectorUBMandextractoraswellasthespeakerdiscriminative  0.01    0.1   0.5    1     2      5     10     20     40  
DNNaretrainedonthisdata. BothsystemsusePLDA-based False Alarm probability (in %)
backendstrainedonjusttheSREdata. Finally,the2016NIST
Figure2:DETcurveforthepooled5-60sportionofSRE10.
SREwasdistributedwithanunlabeledsetof2,472utterancesin
CantoneseandTagalog.Forbothsystems,weusethistocenter
thecorrespondingevaluationutterancesandforscorenormal-
ization. Table1:EER(%)onNISTSRE10
4.2. Evaluation 10s-10s 5s 10s 20s 60s full
ivector 11.0 9.1 6.0 3.9 2.3 1.9
WeassessperformanceonNIST2010and2016speakerrecog-
embeddinga 11.0 9.5 5.7 3.9 3.0 2.6
nition evaluations [28, 29]. In the remaining sections, these
embeddingb 9.2 8.8 6.6 5.5 4.4 3.9
willbeabbreviatedasSRE10andSRE16respectively. SRE10
embeddings 7.9 7.6 5.0 3.8 2.9 2.6
consistsofEnglishtelephonespeech. Ourevaluationisbased
on the extended core condition 5 and the 10s-10s condition. fusion 8.1 6.8 4.3 2.9 2.1 1.8
To supplement the core SRE10 condition, we produce addi-
tional conditions in which the enrollment utterances are full-
length, but the test utterances have been truncated to the ﬁrst
T ∈ {5,10,20,60} seconds of speech, as determined by an Table2:DCF10onNISTSRE10
energy-basedVAD.The10s-10sconditionwaspartoftheofﬁ-
cialSRE10andconsistsoftestandenrollmentutterancesthat 10s-10s 5s 10s 20s 60s full
contain about 10 seconds of speech. SRE16 is comprised of ivector 0.962 0.901 0.749 0.613 0.460 0.403
TagalogandCantoneselanguagetelephonespeech.Theenroll-
embeddinga 0.907 0.902 0.790 0.654 0.518 0.468
mentutterancescontainabout60secondsofspeechwhilethe
embeddingb 0.951 0.927 0.866 0.828 0.782 0.768
testutterancesrangefrom10to60secondsofspeech.
embeddings 0.854 0.875 0.738 0.667 0.567 0.539
In addition to equal error-rate (EER), results are reported
fusion 0.859 0.788 0.645 0.556 0.432 0.383
usingtheofﬁcialperformancemetricforeachSRE.ForSRE10,
thismetricwastheminimumofthenormalizeddetectioncost
function(DCF)withP =10−3[28].TheprimarySRE16
Target
metricwasabalanced(equalized)DCFaveragedattwooperat- Inthissection,welookatperformanceontheSRE10con-ditionsdescribedinSection4.2.Tables1and2demonstratethe Table3:EER(%)onNISTSRE16
interplaybetweenutterance-lengthandperformanceonSRE10.
Weseethati-vectorsarestilldominantforthelongestrecord- Cantonese Tagalog pool
ings,andoutperformembeddingsatboththeEERandDCF10 ivector 8.3 17.6 13.6
operating points. However, as the test utterance length de- embeddinga 7.7 17.6 13.1
creases, theperformanceoftheembeddingsimprovesrelative embeddingb 7.8 17.4 13.1
to the baseline. At 20 seconds of test speech, the combined embeddings 6.5 16.3 11.9
embeddingsare3%betterthani-vectorsinEERbut8%worse
fusion 6.3 15.4 11.3
atDCF10. Withjust10and5secondsoftestspeech,theem-
beddingsare17%and16%betterinEERandslightlybetterat
DCF10. The relative advantage of embeddings appears to be
largest when both enrollment and test utterances are short: in Table4:DCF16onNISTSRE2016
thecolumnlabeled10s-10sboththetestandenrollutterances
contain only about 10 seconds of speech, and we see that the Cantonese Tagalog pool
combinedembeddingsare28%betterinEERand11%betterin ivector 0.549 0.842 0.711
DCF10. Figure2illustratesthedetectionerrortradeoff(DET) embeddinga 0.532 0.835 0.689
curves for the systems when pooled across the truncated test embeddingb 0.630 0.851 0.741
conditions.AlthoughembeddingsarebetterattheEERoperat-
embeddings 0.508 0.803 0.658
ingpoint, theyfavorthelowmissrate, andareslightlyworse
fusion 0.442 0.794 0.622
whencomparedataverylowfalsealarmrate.
Sincethei-vectorandDNNsystemsaresodissimilar, we
expectgoodperformancefromtheirfusion.Weobserveanim-
provementoverusingi-vectorsaloneatalloperatingpointsand
conditions.ThelargestimprovementisinEERonthe10scon- spectively. Pooledacrosslanguages,weseethatthecombined
dition,whichis28%betterthanthebaseline. Evenonthefull- embeddingsoutperformsthei-vectorbaselineby13%inEER
length condition, where i-vectors are strongest, there is a 5% and 7% in DCF16. After combining with i-vectors, the im-
improvementoverusingthei-vectorsalone,inbothDCF10and provementincreasesto17%inEERand13%inDCF16. The
EER. DETplotinFigure3showsthattheseimprovementsarecon-
sistentacrossoperatingpoints.
4.3.2. NISTSRE16 AlthoughtheembeddingsalsoperformbetteronTagalog,
improvementislargestfortheCantoneseportion.Comparedto
thei-vectorbaseline,theembeddingsare22%betterintermsof
  80  
EERand7%inDCF16. Thefusedsystemisevenbetter,and
improvesonthei-vectorbaselineby24%inEERand19%in
  60   DCF16.
5. Conclusions
%)  40  
n 
y (i Inthispaper,weinvestigateddeepneuralnetworkembeddings
abilit  20   fdoinrgtesxatp-ipnedaerpteonbdeenctosmppeaektietirvveewriiﬁthcaatiotrna.diOtiovnearallil-,vtehcetoermbbaesde--
b
o line and are complementary when fused. We found that, al-
pr
s   10   though i-vectors are more effective on the full-length SRE10,
s
Mi embeddingsarebetterontheshortdurationconditions.Thisun-
  5    derscorestheﬁndingsof[23,30]thatDNNsmaybecapableof
ivector
producingmorepowerfulrepresentationsofspeakersfromshort
embeddings
  2    fusion speechsegments. SRE16presentedthechallengeoflanguage
  1    mismatchbetweenthepredominantlyEnglishtrainingdataand
 0.01    0.1   0.5    1     2      5     10     20     40   theCantoneseandTagalogevaluation.Wesawthatembeddings
False Alarm probability (in %) outperformedi-vectorsonbothlanguages,suggestingthatthey
maybemorerobusttothisdomainmismatch.Althoughthere-
Figure3: DETcurveforSRE16,pooledacrossCantoneseand
sultsarequitepromising,webelievethatPLDAmaynotbethe
Tagalog.
optimalsimilaritymetricfortheembeddings. Infuturework,
we will use the method described in this paper as pretraining
Inthissection,weevaluatethesamesystemsfromSection forthefullyend-to-endapproachin[23],sothatamoreappro-
4.3.1onSRE16. Usingthesameembeddingandi-vectorsys- priatesimilaritymetricislearnedalongwiththeembeddings.
temsforbothSRE10andSRE16avoidsthecomplexityofde-
velopingvariantsofeachsystemthatareoptimizedfordifferent 6. Acknowledgments
evaluations. However,thisdoescauseamismatchbetweenthe
predominatelyEnglishtrainingdata(usedtooptimizebothsys- This work was partially supported by NSF Grant No CRI-
tems)andtheTagalogandCantoneseevaluationspeech. Asa 1513128, Nanyang Technological University (NTU) Science
result, the performance reported here may lag behind that of of Learning Grant, National Institutes of Standards and Tech-
counterpartsoptimizedspeciﬁcallyforSRE16. nologyGrantNo70NANB16H039. TheauthorsthankPatrick
Tables3and4reportperformanceinEERandDCF16re- KennyandPaolaGarciafortheirhelpfuldiscussions.7. References
[19] ——,“Extractingspeaker-speciﬁcinformationwitharegularized
siamesedeepnetwork,”inAdvancesinNeuralInformationPro-
[1] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,
cessingSystems(NIPS11),2011.
“Front-endfactoranalysisforspeakerveriﬁcation,”IEEETrans-
actions on Audio, Speech, and Language Processing, vol. 19, [20] A.Salman,“Learningspeaker-speciﬁccharacteristicswithdeep
no.4,pp.788–798,2011. neuralarchitecture,”Ph.D.dissertation,UniversityofManchester,
2012.
[2] S.PrinceandJ.Elder,“Probabilisticlineardiscriminantanalysis
forinferencesaboutidentity,”inIEEE11thInternationalConfer- [21] E.Variani, X.Lei, E.McDermott, I.Moreno, andJ.Gonzalez-
enceonComputerVision,2007.ICCV2007.,Oct2007,pp.1–8. Dominguez, “Deep neural networks for small footprint text-
dependentspeakerveriﬁcation,”in2014IEEEInternationalCon-
[3] N.Bru¨mmerandE.DeVilliers,“Thespeakerpartitioningprob-
ference on Acoustics, Speech and Signal Processing (ICASSP).
lem.”inOdyssey,2010,p.34.
IEEE,2014,pp.4052–4056.
[4] J. Villalba and N. Bru¨mmer, “Towards fully bayesian speaker
[22] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, “End-to-
recognition: Integratingoutthebetween-speakercovariance.”in
endtext-dependentspeakerveriﬁcation,”in2016IEEEInterna-
INTERSPEECH,2011,pp.505–508.
tional Conference on Acoustics, Speech and Signal Processing
[5] P. Kenny, “Bayesian speaker veriﬁcation with heavy-tailed pri- (ICASSP). IEEE,2016,pp.5115–5119.
ors.”inOdyssey,2010,p.14.
[23] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
[6] D. Garcia-Romero and C. Espy-Wilson, “Analysis of i-vector Y. Carmiel, and S. Khudanpur, “Deep neural network-based
lengthnormalizationinspeakerrecognitionsystems.”inINTER- speakerembeddingsforend-to-endspeakerveriﬁcation,”inSpo-
SPEECH,2011,pp.249–252. kenLanguageTechnologyWorkshop(SLT),2016IEEE. IEEE,
2016.
[7] D.Garcia-Romero,X.Zhou,andC.Espy-Wilson,“Multicondi-
tiontrainingofGaussianpldamodelsini-vectorspacefornoise [24] S.Cumani,P.D.Batzu,D.Colibro,C.Vair,P.Laface,andV.Vasi-
andreverberationrobustspeakerrecognition,”in2012IEEEIn- lakakis,“Comparisonofspeakerrecognitionapproachesforreal
ternationalConferenceonAcoustics,SpeechandSignalProcess- applications,”inINTERSPEECH. ISCA,2011.
ing(ICASSP). IEEE,2012,pp.4257–4260.
[25] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
[8] P.Kenny,V.Gupta,T.Stafylakis,P.Ouellet,andJ.Alam,“Deep N.Goel,M.Hannemann,P.Motl´ıcˇek,Y.Qian,P.Schwarzetal.,
neuralnetworksforextractingBaum-Welchstatisticsforspeaker “TheKaldispeechrecognitiontoolkit,”inProceedingsoftheAu-
recognition,”inProc.Odyssey,2014. tomaticSpeechRecognition&Understanding(ASRU)Workshop,
2011.
[9] Y.Lei,N.Scheffer,L.Ferrer,andM.McLaren,“Anovelscheme
for speaker recognition using a phonetically-aware deep neural [26] V.Peddinti, D.Povey, andS.Khudanpur, “Atimedelayneural
network,”in2014IEEEInternationalConferenceonAcoustics, networkarchitectureforefﬁcientmodelingoflongtemporalcon-
SpeechandSignalProcessing(ICASSP). IEEE,2014,pp.1695– texts.”inINTERSPEECH,2015,pp.3214–3218.
1699.
[27] D. Povey, X. Zhang, and S. Khudanpur, “Parallel training
[10] D. Garcia-Romero, X. Zhang, A. McCree, and D. Povey, “Im- of deep neural networks with natural gradient and parameter
provingspeakerrecognitionperformanceinthedomainadapta- averaging,”CoRR,vol.abs/1410.7455,2015.[Online].Available:
tionchallengeusingdeepneuralnetworks,”inSpokenLanguage http://arxiv.org/abs/1410.7455
TechnologyWorkshop(SLT),2014IEEE. IEEE,2014,pp.378–
[28] “The NIST year 2010 speaker recognition evaluation plan,”
383.
http://www.itl.nist.gov/iad/mig/tests/sre/2010/,2010.
[11] D.Snyder,D.Garcia-Romero,andD.Povey,“Timedelaydeep
[29] “NIST speaker recognition evaluation 2016,”
neural network-based universal background models for speaker
https://www.nist.gov/itl/iad/mig/speaker-recognition-evaluation-
recognition,” in 2015 IEEE Workshop on Automatic Speech
2016/,2016.
RecognitionandUnderstanding(ASRU). IEEE,2015,pp.92–97.
[30] D.Garcia-Romero,D.Snyder,G.Sell,D.Povey,andA.McCree,
[12] F.Richardson,D.Reynolds,andN.Dehak,“Deepneuralnetwork
“Speakerdiarizationusingdeepneuralnetworkembeddings,”in
approachestospeakerandlanguagerecognition,”SignalProcess-
2017 IEEE International Conference on Acoustics, Speech and
ingLetters,IEEE,vol.22,no.10,pp.1671–1675,2015.
SignalProcessing(ICASSP). IEEE,2017,pp.4930–4934.
[13] O.Novotny´,P.Mateˇjka,O.Glembeck,O.Plchot,F.Gre´zl,L.Bur-
get, and J. Cˇernocky´, “Analysis of the dnn-based sre systems
in multi-language conditions,” in Spoken Language Technology
Workshop(SLT),2016IEEE. IEEE,2016.
[14] O.Glembek,L.Burget,N.Brummer,O.Plchot,andP.Matejka,
“Discriminativelytrainedi-vectorextractorforspeakerveriﬁca-
tion,”inINTERSPEECH,2011.
[15] L.Burget, O.Plchot, S.Cumani, O.Glembek, P.Mateˇjka, and
N. Bru¨mmer, “Discriminatively trained probabilistic linear dis-
criminantanalysisforspeakerveriﬁcation,”in2011IEEEinter-
nationalconferenceonacoustics, speechandsignalprocessing
(ICASSP). IEEE,2011,pp.4832–4835.
[16] Y.Konig,L.Heck,M.Weintraub,andK.Sonmez,“Nonlineardis-
criminantfeatureextractionforrobusttext-independentspeaker
recognition,”inProc.RLA2C,ESCAworkshoponSpeakerRecog-
nitionanditsCommercialandForensicApplications,1998.
[17] L.Heck,Y.Konig,K.Sonmez,andM.Weintraub,“Robustness
totelephonehandsetdistortioninspeakerrecognitionbydiscrim-
inativefeaturedesign,”inSpeechCommunication,vol.31,no.2,
2000,pp.181–192.
[18] K. Chen and A. Salman, “Learning speaker-speciﬁc character-
isticswithadeepneuralarchitecture,”inIEEETransactionson
NeuralNetworks,vol.22,no.11,2011,pp.1744–1756.Deep Neural Network Embeddings for Text-Independent Speaker Veriﬁcation
David Snyder, Daniel Garcia-Romero, Daniel Povey, Sanjeev Khudanpur
Center for Language and Speech Processing & Human Language Technology Center of Excellence,
The Johns Hopkins University, USA
{david.ryan.snyder, dpovey}@gmail.com, {dgromero, khudanpur}@jhu.edu
Abstract model (UBM) that is used to collect sufﬁcient statistics, a large
projection matrix to extract i-vectors, and a probabilistic linear
This paper investigates replacing i-vectors for text-independent
discriminant analysis (PLDA) backend to compute a similarity
speaker veriﬁcation with embeddings extracted from a feed-
score between i-vectors [2, 3, 4, 5, 6, 7].
forward deep neural network. Long-term speaker characteris-
tics are captured in the network by a temporal pooling layer Traditionally, the UBM is a Gaussian mixture model
that aggregates over the input speech. This enables the network (GMM) trained on acoustic features. Recent work has shown
to be trained to discriminate between speakers from variable- that incorporating an ASR DNN acoustic model can improve
length speech segments. After training, utterances are mapped the UBM’s ability to model phonetic content [8, 9, 10, 11, 12].
directly to ﬁxed-dimensional speaker embeddings and pairs of However, this comes at the cost of greatly increased compu-
embeddings are scored using a PLDA-based backend. We com- tational complexity compared to traditional systems [11]. In
pare performance with a traditional i-vector baseline on NIST addition, the advantages of incorporating ASR DNNs into the
SRE 2010 and 2016. We ﬁnd that the embeddings outperform i-vector pipeline have been largely isolated to English language
i-vectors for short speech segments and are competitive on long speech; [13] found no beneﬁt in a multi-language setting. For
duration test conditions. Moreover, the two representations are these reasons, we restrict the scope of study to traditional i-
complementary, and their fusion improves on the baseline at all vector systems using GMMs.
operating points. Similar systems have recently shown promis-
ing results when trained on very large proprietary datasets, but
1.2. Speaker veriﬁcation with DNNs
to the best of our knowledge, these are the best results reported
for speaker-discriminative neural networks when trained and
It may be possible to produce more powerful SV systems by
tested on publicly available corpora.
training them to directly discriminate between speakers. Some
Index Terms: speaker recognition, speaker veriﬁcation, deep
studies have investigated discriminatively training components
neural networks
of the i-vector system [14, 15]. Given their success in other ar-
eas of speech technology, a natural alternative is to use DNNs
1. Introduction
trained on speaker-discriminative tasks. In early systems, neu-
ral networks are trained to classify training speakers [16, 17] or
Speaker veriﬁcation (SV) is the task of authenticating the
in Siamese architectures to separate same-speaker and different-
claimed identity of a speaker, based on some speech signal
speaker pairs [18, 19, 20]. After training, frame-level features
and enrolled speaker record. Typically, low-dimensional rep-
are extracted from the networks and used as input to Gaussian
resentations rich in speaker information are extracted for both
speaker models. However, we are not aware of any work sug-
enrollment and test speech, and compared to enable a same-
gesting that those methods are competitive with modern i-vector
or-different speaker decision. In modern systems, the repre-
systems for text-independent SV.
sentations are usually i-vectors. If the lexical content of the
utterances is ﬁxed to some phrase, the task is considered text- Progress has been primarily concentrated in text-dependent
dependent, otherwise it is text-independent. This paper investi- SV on large proprietary datasets. In [21], a feed-forward DNN
gates replacing i-vectors with embeddings produced by a deep is trained to classify speakers at the frame-level, on the phrase
neural network (DNN) for text-independent SV. The relative “OK Google.” After training, the softmax output layer is dis-
strengths and weaknesses of this approach are assessed under carded and speaker representations (called d-vectors) are cre-
a variety of conditions. In some practical applications, ver- ated by averaging hidden layer activations. [22] built on this
iﬁcation must be performed using only a limited amount of approach for the same application, by training an end-to-end
test speech, either to avoid latency in an online application or system to discriminate between same-speaker and different-
due to limited availability. To supplement the core 2010 NIST speaker pairs.
speaker recognition evaluation (SRE), we construct a modiﬁed
Recently, [23] showed that an end-to-end system that
version in which the enrollment utterances are full-length, but
jointly learns embeddings along with a similarity metric could
the test utterances have been truncated to the ﬁrst few seconds
outperform a traditional i-vector baseline for text-independent
of speech. Finally, we assess performance on the Cantonese
SV. However, the approach required a large number of in-
and Tagalog NIST SRE 2016, which combines short-duration
domain training speakers to be effective. Our system is based
test conditions with language-mismatch.
on [23], but modiﬁed in an effort to improve performance on
smaller, publicly available datasets. We split the end-to-end ap-
1.1. Speaker veriﬁcation with i-vectors
proach into two parts: a DNN to produce embeddings and a sep-
Most text-independent SV systems are based on i-vectors [1]. arate backend to compare pairs of embeddings. Finally, instead
The standard system consists of a pipeline of generative mod- of training the system to separate same-speaker and different-
els, trained on independent subtasks: a universal background speaker pairs, the DNN learns to classify training speakers.3.3. Neural network architecture
The network, illustrated in Figure 1, consists of layers that op-
erate on speech frames, a statistics pooling layer that aggregates
over the frame-level representations, additional layers that oper-
ate at the segment-level, and ﬁnally a softmax output layer. The
nonlinearities are rectiﬁed linear units (ReLUs).
The ﬁrst 5 layers of the network work at the frame level,
with a time-delay architecture [26]. Suppose t is the current
time step. At the input, we splice together frames at {t − 2, t −
1, t, t + 1, t + 2}. The next two layers splice together the output
of the previous layer at times {t−2, t, t+2} and {t−3, t, t+3},
respectively. The next two layers also operate at the frame-level,
but without any added temporal context. In total, the frame-
level portion of the network has a temporal context of t − 8 to
t + 8 frames. Layers vary in size, from 512 to 1536, depending
on the splicing context used.
The statistics pooling layer receives the output of the ﬁnal
frame-level layer as input, aggregates over the input segment,
and computes its mean and standard deviation. These segment-
level statistics are concatenated together and passed to two ad-
Figure 1: Diagram of the DNN. Segment-level embeddings (e.g.,
ditional hidden layers with dimension 512 and 300 (either of
a or b) can be extracted from any layer of the network after the
which may be used to compute embeddings) and ﬁnally the soft-
statistics pooling layer.
max output layer. Excluding the softmax output layer (because
it is not needed after training) there is a total of 4.4 million pa-
rameters.
2. Baseline i-vector system
3.4. Training
The baseline is a traditional i-vector system that is based on the
The network is trained to classify training speakers using a mul-
GMM-UBM Kaldi recipe described in [11]. The front-end fea-
ticlass cross entropy objective function (Equation 1). The pri-
tures consist of 20 MFCCs with a frame-length of 25ms that
mary difference between this and training in [16, 17, 21] is that
are mean-normalized over a sliding window of up to 3 seconds.
our system is trained to predict speakers from variable-length
Delta and acceleration are appended to create 60 dimension fea-
segments, rather than frames. Suppose there are K speakers in
ture vectors. An energy-based VAD selects features correspond-
N training segments. Then P (spkr | x(n) ) is the probabil-
ing to speech frames. The UBM is a 2048 component full- k 1:T
covariance GMM. The system uses a 600 dimension i-vector ity of speaker k given T input frames x(1n), x(2n), ...x(Tn). The
extractor. Prior to PLDA scoring, i-vectors are centered, di- quantity dnk is 1 if the speaker label for segment n is k, other-
mensionality reduced to 150 using LDA, and length normalized. wise it’s 0.
PLDA scores are normalized using adaptive s-norm [24].
N K
E = − (cid:88) (cid:88) d ln(P (spkr | x(n) )) (1)
nk k 1:T
3. DNN embedding system n=1 k=1
The DNN is trained on the combined SWBD and SRE data
3.1. Overview
described in Section 4.1. We reﬁne the dataset by removing any
recordings that are less than 10 seconds long, and any speak-
The proposed system is a feed-forward DNN (depicted in Fig-
ers with fewer than 4 recordings. This leaves a total of 4,733
ure 1) that computes speaker embeddings from variable-length
speakers, which is the size of the softmax output layer.
acoustic segments. The architecture is based on the end-to-end
To reduce sensitivity to utterance length, it is desirable to
system described in [23]. However, an end-to-end approach re-
train the DNN on speech chunks that capture the range of du-
quires a large amount of in-domain data to be effective. We
rations we expect to encounter at test time (e.g., a few seconds
replace the end-to-end loss with a multiclass cross entropy ob-
to a few minutes). However, GPU memory limitations force
jective. In addition, a separately trained PLDA backend is used
a tradeoff between minibatch size and maximum training ex-
to compare pairs of embeddings. This enables the DNN and
ample length. As a comprise, we pick examples that range
similarity metric to be trained on potentially different datasets.
from 2 to 10 seconds (200 to 1000 frames) along with a mini-
The network is implemented using the nnet3 neural network li-
batch size of 32 to 64. The example speech chunks are sampled
brary in the Kaldi Speech Recognition Toolkit [25].
densely from the recordings, resulting in about 3,400 examples
per speaker. The network is trained for several epochs using
3.2. Features
natural gradient stochastic gradient descent [27].
The features are 20 dimensional MFCCs with a frame-length
3.5. Speaker embeddings
of 25ms, mean-normalized over a sliding window of up to 3
seconds. The same energy-based VAD from Section 2 ﬁlters Ultimately, the goal of training the network is to produce em-
out nonspeech frames. Instead of stacking frames at the input, beddings that generalize well to speakers that have not been
short-term temporal context is handled by a time-delay DNN seen in the training data. We would like embeddings to capture
architecture. speaker characteristics over the entire utterance, rather than atthe frame-level. Thus, any layer after the statistics pooling layer ing points [29]. The primary metrics are abbreviated to DCF10
is a sensible place to extract the embedding from. We do not and DCF16 respectively.
consider the presoftmax afﬁne layer because of its large size and
dependence on the number of speakers. In the network used in 4.3. Results
this work, we are left with two afﬁne layers from which to ex-
In the following results, ivector refers to the traditional i-vector
tract embeddings. These are depicted in Figure 1 as embeddings
baseline described in Section 2. The labels embedding a and
a and b. Embedding a is the output of an afﬁne layer directly
embedding b denote the systems consisting of embeddings ex-
on top of the statistics. Embedding b is extracted from the next
tracted from either embedding layer of the same DNN (see Sec-
afﬁne layer after a ReLU, and so it is a nonlinear function of the
tion 3.5) and used as features to their own PLDA backends. The
statistics. Since they are part of the same DNN, if embedding b
label embeddings is the average of the PLDA backends for the
is computed then we get embedding a for “free.”
individual embeddings. In the following results, we focus on
comparing the i-vector baseline with these combined embed-
3.6. PLDA backend
dings. Finally, fusion refers to the equally weighted sum fusion
We use the same backend for i-vectors and embeddings. Em- of the PLDA scores of ivector and embeddings.
beddings are centered and dimensionality is reduced using
LDA. As in the i-vector system, we found that an LDA dimen- 4.3.1. NIST SRE10
sion of 25% of the original worked well. After dimensional-
ity reduction, the embeddings are length normalized and pairs
  40  
of embeddings are compared using PLDA. PLDA scores are
normalized using adaptive s-norm [24]. As described in Sec-
tion 3.5, the DNN architecture presents the option of using em-
beddings a or b or in combination. Instead of concatenating   20  
embeddings together, we compute separate PLDA backends for %)
each embedding, and average the scores. n    10  
y (i
4. Experiments bilit   5   
a
b
o
4.1. Training data pr   2   
s 
s
The training data consists of telephone speech, the bulk of Mi   1   
which is English. The SWBD portion consists of Switchboard
 0.5   ivector
2 Phases 1, 2, and 3, and Switchboard Cellular. The SRE por-
embeddings
tion contains NIST SREs from 2004 through 2008. In total, fusion
there are about 65,000 recordings from 6,500 speakers. The i-   0.1 
vector UBM and extractor as well as the speaker discriminative  0.01    0.1   0.5    1     2      5     10     20     40  
DNN are trained on this data. Both systems use PLDA-based False Alarm probability (in %)
backends trained on just the SRE data. Finally, the 2016 NIST
Figure 2: DET curve for the pooled 5-60s portion of SRE10.
SRE was distributed with an unlabeled set of 2,472 utterances in
Cantonese and Tagalog. For both systems, we use this to center
the corresponding evaluation utterances and for score normal-
ization. Table 1: EER(%) on NIST SRE10
4.2. Evaluation 10s-10s 5s 10s 20s 60s full
ivector 11.0 9.1 6.0 3.9 2.3 1.9
We assess performance on NIST 2010 and 2016 speaker recog-
embedding a 11.0 9.5 5.7 3.9 3.0 2.6
nition evaluations [28, 29]. In the remaining sections, these
embedding b 9.2 8.8 6.6 5.5 4.4 3.9
will be abbreviated as SRE10 and SRE16 respectively. SRE10
embeddings 7.9 7.6 5.0 3.8 2.9 2.6
consists of English telephone speech. Our evaluation is based
on the extended core condition 5 and the 10s-10s condition. fusion 8.1 6.8 4.3 2.9 2.1 1.8
To supplement the core SRE10 condition, we produce addi-
tional conditions in which the enrollment utterances are full-
length, but the test utterances have been truncated to the ﬁrst
T ∈ {5, 10, 20, 60} seconds of speech, as determined by an Table 2: DCF10 on NIST SRE10
energy-based VAD. The 10s-10s condition was part of the ofﬁ-
cial SRE10 and consists of test and enrollment utterances that 10s-10s 5s 10s 20s 60s full
contain about 10 seconds of speech. SRE16 is comprised of ivector 0.962 0.901 0.749 0.613 0.460 0.403
Tagalog and Cantonese language telephone speech. The enroll-
embedding a 0.907 0.902 0.790 0.654 0.518 0.468
ment utterances contain about 60 seconds of speech while the
embedding b 0.951 0.927 0.866 0.828 0.782 0.768
test utterances range from 10 to 60 seconds of speech.
embeddings 0.854 0.875 0.738 0.667 0.567 0.539
In addition to equal error-rate (EER), results are reported
fusion 0.859 0.788 0.645 0.556 0.432 0.383
using the ofﬁcial performance metric for each SRE. For SRE10,
this metric was the minimum of the normalized detection cost
function (DCF) with P = 10−3 [28]. The primary SRE16
Target
metric was a balanced (equalized) DCF averaged at two operat- In this section, we look at performance on the SRE10 con-ditions described in Section 4.2. Tables 1 and 2 demonstrate the Table 3: EER(%) on NIST SRE16
interplay between utterance-length and performance on SRE10.
We see that i-vectors are still dominant for the longest record- Cantonese Tagalog pool
ings, and outperform embeddings at both the EER and DCF10 ivector 8.3 17.6 13.6
operating points. However, as the test utterance length de- embedding a 7.7 17.6 13.1
creases, the performance of the embeddings improves relative embedding b 7.8 17.4 13.1
to the baseline. At 20 seconds of test speech, the combined embeddings 6.5 16.3 11.9
embeddings are 3% better than i-vectors in EER but 8% worse
fusion 6.3 15.4 11.3
at DCF10. With just 10 and 5 seconds of test speech, the em-
beddings are 17% and 16% better in EER and slightly better at
DCF10. The relative advantage of embeddings appears to be
largest when both enrollment and test utterances are short: in Table 4: DCF16 on NIST SRE 2016
the column labeled 10s-10s both the test and enroll utterances
contain only about 10 seconds of speech, and we see that the Cantonese Tagalog pool
combined embeddings are 28% better in EER and 11% better in ivector 0.549 0.842 0.711
DCF10. Figure 2 illustrates the detection error tradeoff (DET) embedding a 0.532 0.835 0.689
curves for the systems when pooled across the truncated test embedding b 0.630 0.851 0.741
conditions. Although embeddings are better at the EER operat-
embeddings 0.508 0.803 0.658
ing point, they favor the low miss rate, and are slightly worse
fusion 0.442 0.794 0.622
when compared at a very low false alarm rate.
Since the i-vector and DNN systems are so dissimilar, we
expect good performance from their fusion. We observe an im-
provement over using i-vectors alone at all operating points and
conditions. The largest improvement is in EER on the 10s con- spectively. Pooled across languages, we see that the combined
dition, which is 28% better than the baseline. Even on the full- embeddings outperforms the i-vector baseline by 13% in EER
length condition, where i-vectors are strongest, there is a 5% and 7% in DCF16. After combining with i-vectors, the im-
improvement over using the i-vectors alone, in both DCF10 and provement increases to 17% in EER and 13% in DCF16. The
EER. DET plot in Figure 3 shows that these improvements are con-
sistent across operating points.
4.3.2. NIST SRE16 Although the embeddings also perform better on Tagalog,
improvement is largest for the Cantonese portion. Compared to
the i-vector baseline, the embeddings are 22% better in terms of
  80  
EER and 7% in DCF16. The fused system is even better, and
improves on the i-vector baseline by 24% in EER and 19% in
  60   DCF16.
5. Conclusions
%)   40  
n 
y (i In this paper, we investigated deep neural network embeddings
abilit   20   fdoinr gtesxatp-ipnedaerpteonbdeenctosmppeaektietirvveewriiﬁthcaatiotrna.diOtiovnearallil-,vtehcetoermbbaesde--
b
o line and are complementary when fused. We found that, al-
pr
s    10   though i-vectors are more effective on the full-length SRE10,
s
Mi embeddings are better on the short duration conditions. This un-
  5    derscores the ﬁndings of [23, 30] that DNNs may be capable of
ivector
producing more powerful representations of speakers from short
embeddings
  2    fusion speech segments. SRE16 presented the challenge of language
  1    mismatch between the predominantly English training data and
 0.01    0.1   0.5    1     2      5     10     20     40   the Cantonese and Tagalog evaluation. We saw that embeddings
False Alarm probability (in %) outperformed i-vectors on both languages, suggesting that they
may be more robust to this domain mismatch. Although the re-
Figure 3: DET curve for SRE16, pooled across Cantonese and
sults are quite promising, we believe that PLDA may not be the
Tagalog.
optimal similarity metric for the embeddings. In future work,
we will use the method described in this paper as pretraining
In this section, we evaluate the same systems from Section for the fully end-to-end approach in [23], so that a more appro-
4.3.1 on SRE16. Using the same embedding and i-vector sys- priate similarity metric is learned along with the embeddings.
tems for both SRE10 and SRE16 avoids the complexity of de-
veloping variants of each system that are optimized for different 6. Acknowledgments
evaluations. However, this does cause a mismatch between the
predominately English training data (used to optimize both sys- This work was partially supported by NSF Grant No CRI-
tems) and the Tagalog and Cantonese evaluation speech. As a 1513128, Nanyang Technological University (NTU) Science
result, the performance reported here may lag behind that of of Learning Grant, National Institutes of Standards and Tech-
counterparts optimized speciﬁcally for SRE16. nology Grant No 70NANB16H039. The authors thank Patrick
Tables 3 and 4 report performance in EER and DCF16 re- Kenny and Paola Garcia for their helpful discussions.7. References
[19] ——, “Extracting speaker-speciﬁc information with a regularized
siamese deep network,” in Advances in Neural Information Pro-
[1] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,
cessing Systems (NIPS11), 2011.
“Front-end factor analysis for speaker veriﬁcation,” IEEE Trans-
actions on Audio, Speech, and Language Processing, vol. 19, [20] A. Salman, “Learning speaker-speciﬁc characteristics with deep
no. 4, pp. 788–798, 2011. neural architecture,” Ph.D. dissertation, University of Manchester,
2012.
[2] S. Prince and J. Elder, “Probabilistic linear discriminant analysis
for inferences about identity,” in IEEE 11th International Confer- [21] E. Variani, X. Lei, E. McDermott, I. Moreno, and J. Gonzalez-
ence on Computer Vision, 2007. ICCV 2007., Oct 2007, pp. 1–8. Dominguez, “Deep neural networks for small footprint text-
dependent speaker veriﬁcation,” in 2014 IEEE International Con-
[3] N. Bru¨mmer and E. De Villiers, “The speaker partitioning prob-
ference on Acoustics, Speech and Signal Processing (ICASSP).
lem.” in Odyssey, 2010, p. 34.
IEEE, 2014, pp. 4052–4056.
[4] J. Villalba and N. Bru¨mmer, “Towards fully bayesian speaker
[22] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, “End-to-
recognition: Integrating out the between-speaker covariance.” in
end text-dependent speaker veriﬁcation,” in 2016 IEEE Interna-
INTERSPEECH, 2011, pp. 505–508.
tional Conference on Acoustics, Speech and Signal Processing
[5] P. Kenny, “Bayesian speaker veriﬁcation with heavy-tailed pri- (ICASSP). IEEE, 2016, pp. 5115–5119.
ors.” in Odyssey, 2010, p. 14.
[23] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
[6] D. Garcia-Romero and C. Espy-Wilson, “Analysis of i-vector Y. Carmiel, and S. Khudanpur, “Deep neural network-based
length normalization in speaker recognition systems.” in INTER- speaker embeddings for end-to-end speaker veriﬁcation,” in Spo-
SPEECH, 2011, pp. 249–252. ken Language Technology Workshop (SLT), 2016 IEEE. IEEE,
2016.
[7] D. Garcia-Romero, X. Zhou, and C. Espy-Wilson, “Multicondi-
tion training of Gaussian plda models in i-vector space for noise [24] S. Cumani, P. D. Batzu, D. Colibro, C. Vair, P. Laface, and V. Vasi-
and reverberation robust speaker recognition,” in 2012 IEEE In- lakakis, “Comparison of speaker recognition approaches for real
ternational Conference on Acoustics, Speech and Signal Process- applications,” in INTERSPEECH. ISCA, 2011.
ing (ICASSP). IEEE, 2012, pp. 4257–4260.
[25] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
[8] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam, “Deep N. Goel, M. Hannemann, P. Motl´ıcˇek, Y. Qian, P. Schwarz et al.,
neural networks for extracting Baum-Welch statistics for speaker “The Kaldi speech recognition toolkit,” in Proceedings of the Au-
recognition,” in Proc. Odyssey, 2014. tomatic Speech Recognition & Understanding (ASRU) Workshop,
2011.
[9] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, “A novel scheme
for speaker recognition using a phonetically-aware deep neural [26] V. Peddinti, D. Povey, and S. Khudanpur, “A time delay neural
network,” in 2014 IEEE International Conference on Acoustics, network architecture for efﬁcient modeling of long temporal con-
Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 1695– texts.” in INTERSPEECH, 2015, pp. 3214–3218.
1699.
[27] D. Povey, X. Zhang, and S. Khudanpur, “Parallel training
[10] D. Garcia-Romero, X. Zhang, A. McCree, and D. Povey, “Im- of deep neural networks with natural gradient and parameter
proving speaker recognition performance in the domain adapta- averaging,” CoRR, vol. abs/1410.7455, 2015. [Online]. Available:
tion challenge using deep neural networks,” in Spoken Language http://arxiv.org/abs/1410.7455
Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 378–
[28] “The NIST year 2010 speaker recognition evaluation plan,”
383.
http://www.itl.nist.gov/iad/mig/tests/sre/2010/, 2010.
[11] D. Snyder, D. Garcia-Romero, and D. Povey, “Time delay deep
[29] “NIST speaker recognition evaluation 2016,”
neural network-based universal background models for speaker
https://www.nist.gov/itl/iad/mig/speaker-recognition-evaluation-
recognition,” in 2015 IEEE Workshop on Automatic Speech
2016/, 2016.
Recognition and Understanding (ASRU). IEEE, 2015, pp. 92–97.
[30] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree,
[12] F. Richardson, D. Reynolds, and N. Dehak, “Deep neural network
“Speaker diarization using deep neural network embeddings,” in
approaches to speaker and language recognition,” Signal Process-
2017 IEEE International Conference on Acoustics, Speech and
ing Letters, IEEE, vol. 22, no. 10, pp. 1671–1675, 2015.
Signal Processing (ICASSP). IEEE, 2017, pp. 4930–4934.
[13] O. Novotny´, P. Mateˇjka, O. Glembeck, O. Plchot, F. Gre´zl, L. Bur-
get, and J. Cˇ ernocky´, “Analysis of the dnn-based sre systems
in multi-language conditions,” in Spoken Language Technology
Workshop (SLT), 2016 IEEE. IEEE, 2016.
[14] O. Glembek, L. Burget, N. Brummer, O. Plchot, and P. Matejka,
“Discriminatively trained i-vector extractor for speaker veriﬁca-
tion,” in INTERSPEECH, 2011.
[15] L. Burget, O. Plchot, S. Cumani, O. Glembek, P. Mateˇjka, and
N. Bru¨mmer, “Discriminatively trained probabilistic linear dis-
criminant analysis for speaker veriﬁcation,” in 2011 IEEE inter-
national conference on acoustics, speech and signal processing
(ICASSP). IEEE, 2011, pp. 4832–4835.
[16] Y. Konig, L. Heck, M. Weintraub, and K. Sonmez, “Nonlinear dis-
criminant feature extraction for robust text-independent speaker
recognition,” in Proc. RLA2C, ESCA workshop on Speaker Recog-
nition and its Commercial and Forensic Applications, 1998.
[17] L. Heck, Y. Konig, K. Sonmez, and M. Weintraub, “Robustness
to telephone handset distortion in speaker recognition by discrim-
inative feature design,” in Speech Communication, vol. 31, no. 2,
2000, pp. 181–192.
[18] K. Chen and A. Salman, “Learning speaker-speciﬁc character-
istics with a deep neural architecture,” in IEEE Transactions on
Neural Networks, vol. 22, no. 11, 2011, pp. 1744–1756.