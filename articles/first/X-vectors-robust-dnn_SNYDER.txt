X-VECTORS:ROBUSTDNNEMBEDDINGSFORSPEAKERRECOGNITION
DavidSnyder,DanielGarcia-Romero,GregorySell,DanielPovey,SanjeevKhudanpur
CenterforLanguageandSpeechProcessing&HumanLanguageTechnologyCenterofExcellence
TheJohnsHopkinsUniversity,Baltimore,MD21218,USA
ABSTRACT Alternatively,neuralnetworkscanbedirectlyoptimizedtodis-
criminate between speakers. This has potential to produce power-
Inthispaper,weusedataaugmentationtoimproveperformanceof ful,compactsystems[13],thatonlyrequirespeakerlabelstotrain.
deep neural network (DNN) embeddings for speaker recognition. In early systems, neural networks are trained to separate speakers,
TheDNN,whichistrainedtodiscriminatebetweenspeakers,maps andframe-levelrepresentationsareextractedfromthenetworkand
variable-lengthutterancestoﬁxed-dimensionalembeddingsthatwe usedasfeaturesforGaussianspeakermodels[14,15,16]. Heigold
call x-vectors. Prior studies have found that embeddings leverage etal.,introducedanend-to-endsystem,trainedonthephrase“OK
large-scaletrainingdatasetsbetterthani-vectors.However,itcanbe Google,” that jointly learns an embedding along with a similarity
challengingtocollectsubstantialquantitiesoflabeleddatafortrain- metrictocomparepairsofembeddings[13]. Snyderetal.,adapted
ing.Weusedataaugmentation,consistingofaddednoiseandrever- thisapproachtoatext-independentapplicationandinsertedatem-
beration,asaninexpensivemethodtomultiplytheamountoftrain- poralpoolinglayerintothenetworktohandlevariable-lengthseg-
ingdataandimproverobustness. Thex-vectorsarecomparedwith ments[17]. Theworkin[1]splittheend-to-endapproachintotwo
i-vectorbaselinesonSpeakersintheWildandNISTSRE2016Can- parts:aDNNtoproduceembeddingsandaseparatelytrainedclassi-
tonese. WeﬁndthatwhileaugmentationisbeneﬁcialinthePLDA ﬁertocomparethem. Thisfacilitatestheuseofalltheaccumulated
classiﬁer, it is not helpful in the i-vector extractor. However, the backendtechnologydevelopedovertheyearsfori-vectors,suchas
x-vectorDNNeffectivelyexploitsdataaugmentation,duetoitssu- length-normalization, PLDA scoring, and domain adaptation tech-
pervisedtraining. Asaresult,thex-vectorsachievesuperiorperfor- niques.
manceontheevaluationdatasets.
DNNembeddingperformanceappearstobehighlyscalablewith
IndexTerms— speakerrecognition,deepneuralnetworks,data theamountoftrainingdata. Asaresult,thesesystemshavefound
augmentation,x-vectors successleveraginglargeproprietarydatasets[13,17,18]. However,
recentsystemshaveshownpromisingperformancetrainedononly
publiclyavailablespeakerrecognitioncorpora[1,19,20]. Thispa-
1. INTRODUCTION perisbasedontheworkin[1]andappliesdataaugmentationtothe
DNNtrainingprocedure.Thisincreasestheamountanddiversityof
Usingdeepneuralnetworks(DNN)tocapturespeakercharacteris- theexistingtrainingdata,andachievesasigniﬁcantimprovementfor
ticsiscurrentlyaveryactiveresearcharea. Inourapproach,repre- thex-vectorsystem.Incomparingwithx-vectors,wealsocontribute
sentationscalledx-vectorsareextractedfromaDNNandusedlike astudyofaugmentationini-vectorsystems.
i-vectors.ThispaperbuildsonourrecentDNNembeddingarchitec-
ture[1]. Weshowthatartiﬁciallyaugmentingthetrainingdatawith
noisesandreverberationisahighlyeffectivestrategyforimproving
performanceinDNNembeddingsystems. 2. SPEAKERRECOGNITIONSYSTEMS
Most speaker recognition systems are based on i-vectors [2].
The standard approach consists of a universal background model This section describes the speaker recognition systems developed
(UBM),andalargeprojectionmatrixTthatarelearnedinanunsu- forthisstudy,whichconsistoftwoi-vectorbaselinesandtheDNN
pervisedwaytomaximizethedatalikelihood. Theprojectionmaps x-vectorsystem.AllsystemsarebuiltusingtheKaldispeechrecog-
high-dimensional statistics from the UBM into a low-dimensional nitiontoolkit[21].
representation,knownasani-vector.Aprobabilisticlineardiscrimi-
nantanalysis(PLDA)[3]classiﬁerisusedtocomparei-vectors,and
enablesame-or-differentspeakerdecisions[4,5,6].
TheDNNsmostoftenfoundinspeakerrecognitionaretrained 2.1. Acoustici-vector
asacousticmodelsforautomaticspeechrecognition(ASR),andare
thenusedtoenhancephoneticmodelinginthei-vectorUBM:either A traditional i-vector system based on the GMM-UBM recipe de-
posteriorsfromtheASRDNNreplacethosefromaGaussianmix- scribedin[11]servesasouracoustic-featurebaselinesystem. The
turemodel(GMM)[7,8],orbottleneckfeaturesareextractedfrom featuresare20MFCCswithaframe-lengthof25msthataremean-
theDNNandcombinedwithacousticfeatures[9]. Ineithercase,if normalized over a sliding window of up to 3 seconds. Delta and
theASRDNNistrainedonin-domaindata,theimprovementover acceleration are appended to create 60 dimension feature vectors.
traditional acoustic i-vectors is substantial [10, 11, 12]. However, Anenergy-basedspeechactivitydetection(SAD)systemselectsfea-
thisapproachintroducestheneedfortranscribedtrainingdataand tures corresponding to speech frames. The UBM is a 2048 com-
greatlyincreasescomputationalcomplexitycomparedtotraditional ponent full-covariance GMM. The system uses a 600 dimensional
i-vectors. i-vectorextractorandPLDAforscoring(seeSection2.4).2.2. Phoneticbottlenecki-vector getherandpropagatedthroughsegment-levellayersandﬁnallythe
softmaxoutputlayer. Thenonlinearitiesareallrectiﬁedlinearunits
This i-vector system incorporates phonetic bottleneck features
(ReLUs).
(BNF) from an ASR DNN acoustic model and is similar to [9].
The DNN is trained to classify the N speakers in the training
The DNN is a time-delay acoustic model with p-norm nonlineari-
data. A training example consists of a chunk of speech features
ties.TheASRDNNistrainedontheFisherEnglishcorpusanduses
(about3secondsaverage),andthecorrespondingspeakerlabel.Af-
thesamerecipeandarchitectureasthesystemdescribedinSection
tertraining,embeddingsareextractedfromtheafﬁnecomponentof
2.2of[11], exceptthatthepenultimatelayerisreplacedwitha60
layer segment6. Excluding the softmax output layer and segment7
dimensionallinearbottlenecklayer. Excludingthesoftmaxoutput
(because they are not needed after training) there is a total of 4.2
layer, which is not needed to compute BNFs, the DNN has 9.2
millionparameters.
millionparameters.
The BNFs are concatenated with the same 20 dimensional
MFCCs described in Section 2.1 plus deltas to create 100 dimen- 2.4. PLDAclassiﬁer
sional features. The remaining components of the system (feature
The same type of PLDA [3] classiﬁer is used for the x-vector and
processing,UBM,i-vectorextractor,andPLDAclassiﬁer)areiden-
i-vector systems. The representations (x-vectors or i-vectors) are
ticaltotheacousticsysteminSection2.1.
centered,andprojectedusingLDA.TheLDAdimensionwastuned
on the SITW development set to 200 for i-vectors and 150 for
2.3. Thex-vectorsystem x-vectors. After dimensionality reduction, the representations are
length-normalized and modeled by PLDA. The scores are normal-
Thissectiondescribesthex-vectorsystem. ItisbasedontheDNN
izedusingadaptives-norm[22].
embeddingsin[1]anddescribedingreaterdetailthere.
Oursoftwareframework hasbeenmadeavailableinthe Kaldi
toolkit.AnexamplerecipeisinthemainbranchofKaldiathttps: 3. EXPERIMENTALSETUP
//github.com/kaldi-asr/kaldi/tree/master/egs/
sre16/v2 and a pretrained x-vector system can be downloaded 3.1. Trainingdata
from http://kaldi-asr.org/models.html. The recipe
Thetrainingdataconsistsofbothtelephoneandmicrophonespeech,
and model are similar to the x-vector system described in Section
thebulkofwhichisinEnglish.Allwidebandaudioisdownsampled
4.4.
to8kHz.
TheSWBDportionconsistsofSwitchboard2Phases1,2,and3
Layer Layercontext Totalcontext Inputxoutput
aswellasSwitchboardCellular.Intotal,theSWBDdatasetcontains
frame1 [t−2,t+2] 5 120x512
about 28k recordings from 2.6k speakers. The SRE portion con-
frame2 {t−2,t,t+2} 9 1536x512
sistsofNISTSREsfrom2004to2010alongwithMixer6andcon-
frame3 {t−3,t,t+3} 15 1536x512
tainsabout63krecordingsfrom4.4kspeakers. Intheexperiments
frame4 {t} 15 512x512
inSections4.1–4.4theextractors(UBM/TorembeddingDNN)are
frame5 {t} 15 512x1500
trainedonSWBDandSREandthePLDAclassiﬁersaretrainedon
statspooling [0,T) T 1500Tx3000
justSRE.DataaugmentationisdescribedinSection3.3andisap-
segment6 {0} T 3000x512
pliedtothesedatasetsasexplainedthroughoutSection4.
segment7 {0} T 512x512
InthelastexperimentinSection4.5weincorporateaudiofrom
softmax {0} T 512xN thenewVoxCelebdataset[19]intobothextractorandPLDAtrain-
inglists. Thedatasetconsistsofvideosfrom1,251celebrityspeak-
ers. Although SITW and VoxCeleb were collected independently,
Table1. TheembeddingDNNarchitecture. x-vectorsareextracted
wediscoveredanoverlapof60speakersbetweenthetwodatasets.
at layer segment6, before the nonlinearity. The N in the softmax
WeremovedtheoverlappingspeakersfromVoxCelebpriortousing
layercorrespondstothenumberoftrainingspeakers.
itfortraining. Thisreducesthesizeofthedatasetto1,191speakers
andabout20krecordings.
Thefeaturesare24dimensionalﬁlterbankswithaframe-length TheASRDNNusedinthei-vector(BNF)systemwastrained
of25ms,mean-normalizedoveraslidingwindowofupto3seconds. ontheFisherEnglishcorpus. Toachievealimitedformofdomain
The same energy SAD as used in the baseline systems ﬁlters out adaptation,thedevelopmentdatafromSITWandSRE16ispooled
nonspeechframes. andusedforcenteringandscorenormalization.Noaugmentationis
TheDNNconﬁgurationisoutlinedinTable1.Supposeaninput appliedtotheselists.
segmenthasT frames.Theﬁrstﬁvelayersoperateonspeechframes,
withasmalltemporalcontextcenteredatthecurrentframet. For
3.2. Evaluation
example,theinputtolayerframe3isthesplicedoutputofframe2,at
framest−3,tandt+3.Thisbuildsonthetemporalcontextofthe Ourevaluationconsistsoftwodistinctdatasets:SpeakersintheWild
earlierlayers,sothatframe3seesatotalcontextof15frames. (SITW)Core[23]andtheCantoneseportionoftheNISTSRE2016
ThestatisticspoolinglayeraggregatesallT frame-leveloutputs evaluation(SRE16)[24].SITWconsistsofunconstrainedvideoau-
from layer frame5 and computes its mean and standard deviation. dio of English speakers, with naturally occurring noises, reverber-
Thestatisticsare1500dimensionalvectors,computedonceforeach ation, as well as device and codec variability. The SRE16 portion
inputsegment. Thisprocessaggregatesinformationacrossthetime consists of Cantonese conversational telephone speech. Both en-
dimensionsothatsubsequentlayersoperateontheentiresegment. roll and test SITW utterances vary in length form 6–240 seconds.
InTable1,thisisdenotedbyalayercontextof{0}andatotalcon- ForSRE16, theenrollmentutterancescontainabout60secondsof
text of T. The mean and standard deviation are concatenated to- speechwhilethetestutterancesvaryfrom10–60seconds.SITWCore SRE16Cantonese
EER(%) DCF10−2 DCF10−3 EER(%) DCF10−2 DCF10−3
i-vector(acoustic) 9.29 0.621 0.785 9.23 0.568 0.741
4.1 Originalsystems i-vector(BNF) 9.10 0.558 0.719 9.68 0.574 0.765
x-vector 9.40 0.632 0.790 8.00 0.491 0.697
i-vector(acoustic) 8.64 0.588 0.755 8.92 0.544 0.717
4.2 PLDAaug. i-vector(BNF) 8.00 0.514 0.689 8.82 0.532 0.726
x-vector 7.56 0.586 0.746 7.45 0.463 0.669
i-vector(acoustic) 8.89 0.626 0.790 9.20 0.575 0.748
4.3 Extractoraug. i-vector(BNF) 7.27 0.533 0.730 8.89 0.569 0.777
x-vector 7.19 0.535 0.719 6.29 0.428 0.626
i-vector(acoustic) 8.04 0.578 0.752 8.95 0.555 0.720
PLDAand
4.4 i-vector(BNF) 6.49 0.492 0.690 8.29 0.534 0.749
extractoraug.
x-vector 6.00 0.488 0.677 5.86 0.410 0.593
i-vector(acoustic) 7.45 0.552 0.723 9.23 0.557 0.742
4.5 Incl.VoxCeleb i-vector(BNF) 6.09 0.472 0.660 8.12 0.523 0.751
x-vector 4.16 0.393 0.606 5.71 0.399 0.569
Table 2. Results using data augmentation in various systems. “Extractor” refers to either the UBM/T or the embedding DNN. For each
experiment,thebestresultsareboldface.
Wereportresultsintermsofequalerror-rate(EER)andthemin- 4.1. Originalsystems
imumofthenormalizeddetectioncostfunction(DCF)atP =
Target
10−2 and P = 10−3. Note that the SRE16 results have not Inthissection,weevaluatesystemswithoutdataaugmentation.The
Target
extractors are trained on the SWBD and SRE datasets described
been“equalized[24].”
in Section 3.1. The PLDA classiﬁers are trained on just the SRE
dataset. Withoutusingaugmentation,thebestresultsonSITWare
3.3. Dataaugmentation
obtainedbyi-vector(BNF),whichis12%betterthanthex-vector
Augmentation increases the amount and diversity of the existing system at DCF10−2. The acoustic i-vector system also achieves
training data. Our strategy employs additive noises and reverber- slightlylowererror-ratesthanthex-vectorsystemonSITW.How-
ation. Reverberation involves convolving room impulse responses ever, even without augmentation, the best results for SRE16 Can-
(RIR) with audio. We use the simulated RIRs described by Ko et toneseareobtainedbythex-vectors. IntermsofDCF10−2, these
al. in[25],andthereverberationitselfisperformedwiththemulti- embeddings are about 14% better than either i-vector system. We
conditiontrainingtoolsintheKaldiASpIRErecipe[21]. Foraddi- observethati-vector(BNF)hasnoadvantageoveri-vector(acous-
tivenoise,weusetheMUSANdataset,whichconsistsofover900 tic)forthisCantonesespeech. Thisechoesrecentstudiesthathave
noises,42hoursofmusicfromvariousgenresand60hoursofspeech foundthatthelargegainsachievedbyBNFsinEnglishspeechare
fromtwelvelanguages[26].BothMUSANandtheRIRdatasetsare notnecessarilytransferabletonon-Englishdata[27].
freelyavailablefromhttp://www.openslr.org.
Weusea3-foldaugmentationthatcombinestheoriginal“clean”
4.2. PLDAaugmentation
traininglistwithtwoaugmentedcopies.Toaugmentarecording,we
choosebetweenoneofthefollowingrandomly: In this experiment, the augmentation strategy described in Section
• babble: Threetosevenspeakers arerandomlypickedfrom 3.3isappliedtoonlythePLDAtraininglist. Weusethesameex-
MUSANspeech,summedtogether,thenaddedtotheoriginal tractorsastheprevioussection,whichweretrainedontheoriginal
signal(13-20dBSNR). datasets. PLDAaugmentationresultsinaclearimprovementforall
three systems relative to Section 4.1. However, it appears that the
• music: A single music ﬁle is randomly selected from MU-
x-vectorsmaybeneﬁtfromthePLDAaugmentationmorethanthe
SAN, trimmed or repeated as necessary to match duration,
baseline systems. On SITW, the x-vector system achieves slightly
andaddedtotheoriginalsignal(5-15dBSNR).
lowererror-ratesthani-vector(acoustic),butcontinuestolagbehind
• noise: MUSAN noises are added at one second intervals i-vector(BNF)atmostoperatingpoints. OnSRE16, thex-vectors
throughouttherecording(0-15dBSNR). maintainanadvantageoverthei-vectorsbyabout14%inDCF10−2.
• reverb:Thetrainingrecordingisartiﬁciallyreverberatedvia
convolutionwithsimulatedRIRs.
4.3. Extractoraugmentation
4. RESULTS Wenowapplydataaugmentationtotheextractor(UBM/Torem-
bedding DNN) training lists but not the PLDA list. The effect of
ThemainresultsarepresentedinTable2andarereferencedthrough- augmentingtheUBM/Tisinconsistentinthei-vectorsystem. This
outSections4.1–4.5. Wecompareperformanceoftwoi-vectorsys- observation is supported by prior studies on i-vectors, which have
tems, labeled i-vector (acoustic) and i-vector (BNF), with the x- found that augmentation is only effective in the PLDA classiﬁer
vector system. ThesystemsaredescribedinSections2.1, 2.2and [28,29].Ontheotherhand,augmentingtheembeddingDNNtrain-
2.3,respectively.Throughoutthefollowingsections,weusetheterm inglistresultsinalargeimprovement.Incontrasttothei-vectorsys-
extractortorefertoeithertheUBM/TortheembeddingDNN. tems,thisisconsiderablymoreeffectivethanaugmentingthePLDAtraining list. On SITW, the x-vector system achieves lower error-   60  
i-vector (acoustic)
ratesthani-vector(acoustic)andhasnowcaughtuptothei-vector
i-vector (BNF)
(BNF)system.OnSRE16,thex-vectorsarenow25%betterthanthe   40   x-vector
i-vectorsinDCF10−2,whichisalmostdoubletheimprovementthe
DNNembeddingshadwithPLDAaugmentationalone.Theﬁndings
inthissectionindicatethatdataaugmentationisonlybeneﬁcialfor %)  20  
extractorstrainedwithsupervision. y (in 
bilit  10  
a
4.4. PLDAandextractoraugmentation b   5   
o
pr
Intheprevioussections,wesawthatPLDAaugmentationwashelp- ss    2   
fulinbothi-vectorandDNNembeddingsystems,althoughextractor Mi
  1   
augmentationwasonlyclearlybeneﬁcialintheembeddingsystem.
Inthisexperiment,weapplydataaugmentationtoboththeextractor  0.5  
and PLDA training lists. We continue to use SWBD and SRE for
extractortrainingandonlySREforPLDA.OnSITWthex-vectors   0.1 
arenow10-25%betterthani-vector(acoustic)andareslightlybetter  0.01    0.1   0.5    1     2     5     10    20     40     60  
False Alarm probability (in %)
thani-vector(BNF)atalloperatingpoints. OnSRE16Cantonese,
the x-vectors continue to maintain the large lead over the i-vector
systemsestablishedinSection4.3. Fig.2.DETcurvefortheSITWCoreusingSection4.5systems.
4.5. IncludingVoxCeleb Theseresultsareillustratedbydetectionerrortradeoff(DET)plots
inFigures1and2.
  60  
i-vector (acoustic) 5. CONCLUSIONS
i-vector (BNF)
  40   x-vector This paper studied DNN embeddings for speaker recognition. We
foundthatdataaugmentationisaneasilyimplementedandeffective
%)  20   strategyforimprovingtheirperformance.Wealsomadethex-vector
n  system–ourimplementationofDNNembeddings–availableinthe
y (i Kalditoolkit.Wefoundthatthex-vectorsystemsigniﬁcantlyoutper-
bilit  10   formedtwostandardi-vectorbaselinesonSRE16Cantonese. After
ba   5    includingalargeamountofaugmentedmicrophonespeech, thex-
o
pr vectors achieved much lower error-rates than our best baseline on
ss    2    Speakers in the Wild. Bottleneck features from an ASR DNN are
Mi usedinourbesti-vectorsystem,andsoitrequirestranscribeddata
  1   
during training. On the other hand, the x-vector DNN needs only
 0.5  
speakerlabelstotrain,makingitpotentiallyidealfordomainswith
little transcribed speech. More generally, it appears that x-vectors
  0.1  are now a strong contender for next-generation representations for
 0.01    0.1   0.5    1     2     5     10    20     40     60  
speakerrecognition.
False Alarm probability (in %)
Fig.1. DETcurvefortheCantoneseportionofNISTSRE16using 6. ACKNOWLEDGMENTS
Section4.5systems.
This material is based upon work supported by the National Sci-
ence Foundation Graduate Research Fellowship under Grant No.
ThetrainingdatainSections4.1–4.4isdominatedbytelephone
1232825.ThisworkwaspartiallysupportedbyNSFGrantNoCRI-
speech. Inthisexperiment,weexploretheeffectofaddingalarge
1513128. Anyopinion, ﬁndings,andconclusionsorrecommenda-
amount of microphone speech to the systems in Section 4.4. The
tionsexpressedinthismaterialarethoseoftheauthors(s)anddonot
VoxCelebdataset[19]isaugmented,andaddedtoboththeextractor
necessarilyreﬂecttheviewsoftheNationalScienceFoundation.
and PLDA lists. As noted in Section 3.1, we found 60 speakers
whichoverlapwithSITW;allspeechforthesespeakerswasremoved
fromthetraininglists. 7. REFERENCES
On SITW, both i-vector and x-vector systems improve signif-
[1] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudan-
icantly. However, the x-vector exploits the large increase in the
pur, “Deep neural network embeddings for text-independent
amount of in-domain data better than the i-vector systems. Com-
speakerveriﬁcation,” Proc.Interspeech,pp.999–1003,2017.
paredtoi-vector(acoustic),thex-vectorsarebetterby44%inEER
and29%inDCF10−2.Comparedtothei-vector(BNF)system,itis [2] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
nowbetterby32%inEERand17%inDCF10−2. OnSRE16,the let, “Front-endfactoranalysisforspeakerveriﬁcation,” IEEE
i-vectorsystemsremainroughlythesamecomparedtoSection4.4, TransactionsonAudio,Speech,andLanguageProcessing,vol.
butthex-vectorsimproveonalloperatingpointsbyasmallamount. 19,no.4,pp.788–798,2011.[3] S.Ioffe,“Probabilisticlineardiscriminantanalysis,”Computer [19] A.Nagrani,J.S.Chung,andA.Zisserman,“Voxceleb:alarge-
Vision–ECCV2006,pp.531–542,2006. scalespeakeridentiﬁcationdataset,” inInterspeech,2017.
[4] P.Kenny,“Bayesianspeakerveriﬁcationwithheavy-tailedpri- [20] C. Zhang and K. Koishida, “End-to-end text-independent
ors.,” inOdyssey,2010,p.14. speakerveriﬁcationwithtripletlossonshortutterances,”Proc.
[5] N. Bru¨mmer and E. De Villiers, “The speaker partitioning Interspeech,pp.1487–1491,2017.
problem.,” inOdyssey,2010,p.34. [21] D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
[6] D.Garcia-RomeroandC.Espy-Wilson, “Analysisofi-vector N. Goel, M. Hannemann, P. Motl´ıcˇek, Y. Qian, P. Schwarz,
lengthnormalizationinspeakerrecognitionsystems.,” inIn- etal., “TheKaldispeechrecognitiontoolkit,” inProceedings
terspeech,2011,pp.249–252. oftheAutomaticSpeechRecognition&Understanding(ASRU)
Workshop,2011.
[7] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, “A novel
scheme for speaker recognition using a phonetically-aware [22] D. Sturim and D. Reynolds, “Speaker adaptive cohort se-
deepneuralnetwork,” in2014IEEEInternationalConference lection for tnorm in text-independent speaker veriﬁcation,”
onAcoustics,SpeechandSignalProcessing(ICASSP).IEEE, in Acoustics, Speech, and Signal Processing, 2005. Proceed-
2014,pp.1695–1699. ings.(ICASSP’05). IEEE International Conference on. IEEE,
2005,vol.1,pp.I–741.
[8] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam,
“Deep neural networks for extracting Baum-Welch statistics [23] M.McLaren,L.Ferrer,D.Castan,andA.Lawson, “The2016
forspeakerrecognition,” inProc.Odyssey,2014. speakersinthewildspeakerrecognitionevaluation.,” inInter-
[9] M. McLaren, Y. Lei, and L. Ferrer, “Advances in deep neu- speech,2016,pp.823–827.
ralnetworkapproachestospeakerrecognition,” inAcoustics, [24] “NIST speaker recognition evaluation 2016,”
SpeechandSignalProcessing(ICASSP),2015IEEEInterna- https://www.nist.gov/itl/iad/mig/
tionalConferenceon.IEEE,2015,pp.4814–4818. speaker-recognition-evaluation-2016/,2016.
[10] D.Garcia-Romero,X.Zhang,A.McCree,andD.Povey, “Im- [25] T.Ko,V.Peddinti,D.Povey,M.Seltzer,andS.Khudanpur,“A
provingspeakerrecognitionperformanceinthedomainadap- study on data augmentation of reverberant speech for robust
tationchallengeusingdeepneuralnetworks,” inSpokenLan- speechrecognition,” inAcoustics,SpeechandSignalProcess-
guage Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, ing(ICASSP),2017IEEEInternationalConferenceon.IEEE,
pp.378–383. 2017,pp.5220–5224.
[11] D.Snyder,D.Garcia-Romero,andD.Povey,“Timedelaydeep
[26] D. Snyder, G Chen, and D. Povey, “MUSAN: A Music,
neuralnetwork-baseduniversalbackgroundmodelsforspeaker
Speech,andNoiseCorpus,”2015, arXiv:1510.08484v1.
recognition,” in 2015 IEEE Workshop on Automatic Speech
RecognitionandUnderstanding(ASRU).IEEE,2015,pp.92– [27] O. Novotny´, P. Mateˇjka, O. Glembeck, O Plchot, F. Gre´zl,
97. L. Burget, and J. Cˇernocky´, “Analysis of the dnn-based sre
systems in multi-language conditions,” in Spoken Language
[12] S. O. Sadjadi, J. Pelecanos, and S. Ganapathy, “The ibm
TechnologyWorkshop(SLT).IEEE,2016.
speaker recognition system: Recent advances and error anal-
ysis,” Interspeech,pp.3633–3637,2016. [28] Y. Lei, L. Burget, L. Ferrer, M. Graciarena, and N. Scheffer,
“Towardsnoise-robustspeakerrecognitionusingprobabilistic
[13] G.Heigold,I.Moreno,S.Bengio,andN.Shazeer,“End-to-end
lineardiscriminantanalysis,” inAcoustics,SpeechandSignal
text-dependent speaker veriﬁcation,” in 2016 IEEE Interna-
Processing(ICASSP),2012IEEEInternationalConferenceon.
tionalConferenceonAcoustics,SpeechandSignalProcessing
IEEE,2012,pp.4253–4256.
(ICASSP).IEEE,2016,pp.5115–5119.
[29] D.Garcia-Romero,X.Zhou,andC.Espy-Wilson, “Multicon-
[14] Y.Konig, L.Heck, M.Weintraub, andK.Sonmez, “Nonlin-
dition training of Gaussian plda models in i-vector space for
eardiscriminantfeatureextractionforrobusttext-independent
noiseandreverberationrobustspeakerrecognition,” in2012
speaker recognition,” in Proc. RLA2C, ESCA workshop on
IEEEInternationalConferenceonAcoustics,SpeechandSig-
SpeakerRecognitionanditsCommercialandForensicAppli-
nalProcessing(ICASSP).IEEE,2012,pp.4257–4260.
cations,1998.
[15] L. Heck, Y. Konig, K. Sonmez, and M. Weintraub, “Ro-
bustnesstotelephonehandsetdistortioninspeakerrecognition
bydiscriminativefeaturedesign,” inSpeechCommunication,
2000,vol.31,pp.181–192.
[16] A. Salman, Learning speaker-speciﬁc characteristics with
deepneuralarchitecture, Ph.D.thesis,UniversityofManch-
ester,2012.
[17] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
Y. Carmiel, and S. Khudanpur, “Deep neural network-based
speaker embeddings for end-to-end speaker veriﬁcation,” in
SpokenLanguageTechnologyWorkshop(SLT).IEEE,2016.
[18] S.Zhang,Z.Chen,Y.Zhao,J.Li,andY.Gong, “End-to-end
attention based text-dependent speaker veriﬁcation,” in Spo-
kenLanguageTechnologyWorkshop(SLT),2016IEEE.IEEE,
2016,pp.171–178.X-VECTORS:ROBUSTDNNEMBEDDINGSFORSPEAKERRECOGNITION
DavidSnyder,DanielGarcia-Romero,GregorySell,DanielPovey,SanjeevKhudanpur
CenterforLanguageandSpeechProcessing&HumanLanguageTechnologyCenterofExcellence
TheJohnsHopkinsUniversity,Baltimore,MD21218,USA
ABSTRACT Alternatively,neuralnetworkscanbedirectlyoptimizedtodis-
criminate between speakers. This has potential to produce power-
Inthispaper,weusedataaugmentationtoimproveperformanceof ful,compactsystems[13],thatonlyrequirespeakerlabelstotrain.
deep neural network (DNN) embeddings for speaker recognition. In early systems, neural networks are trained to separate speakers,
TheDNN,whichistrainedtodiscriminatebetweenspeakers,maps andframe-levelrepresentationsareextractedfromthenetworkand
variable-lengthutterancestoﬁxed-dimensionalembeddingsthatwe usedasfeaturesforGaussianspeakermodels[14,15,16]. Heigold
call x-vectors. Prior studies have found that embeddings leverage etal.,introducedanend-to-endsystem,trainedonthephrase“OK
large-scaletrainingdatasetsbetterthani-vectors.However,itcanbe Google,” that jointly learns an embedding along with a similarity
challengingtocollectsubstantialquantitiesoflabeleddatafortrain- metrictocomparepairsofembeddings[13]. Snyderetal.,adapted
ing.Weusedataaugmentation,consistingofaddednoiseandrever- thisapproachtoatext-independentapplicationandinsertedatem-
beration,asaninexpensivemethodtomultiplytheamountoftrain- poralpoolinglayerintothenetworktohandlevariable-lengthseg-
ingdataandimproverobustness. Thex-vectorsarecomparedwith ments[17]. Theworkin[1]splittheend-to-endapproachintotwo
i-vectorbaselinesonSpeakersintheWildandNISTSRE2016Can- parts:aDNNtoproduceembeddingsandaseparatelytrainedclassi-
tonese. WeﬁndthatwhileaugmentationisbeneﬁcialinthePLDA ﬁertocomparethem. Thisfacilitatestheuseofalltheaccumulated
classiﬁer, it is not helpful in the i-vector extractor. However, the backendtechnologydevelopedovertheyearsfori-vectors,suchas
x-vectorDNNeffectivelyexploitsdataaugmentation,duetoitssu- length-normalization, PLDA scoring, and domain adaptation tech-
pervisedtraining. Asaresult,thex-vectorsachievesuperiorperfor- niques.
manceontheevaluationdatasets.
DNNembeddingperformanceappearstobehighlyscalablewith
IndexTerms— speakerrecognition,deepneuralnetworks,data theamountoftrainingdata. Asaresult,thesesystemshavefound
augmentation,x-vectors successleveraginglargeproprietarydatasets[13,17,18]. However,
recentsystemshaveshownpromisingperformancetrainedononly
publiclyavailablespeakerrecognitioncorpora[1,19,20]. Thispa-
1. INTRODUCTION perisbasedontheworkin[1]andappliesdataaugmentationtothe
DNNtrainingprocedure.Thisincreasestheamountanddiversityof
Usingdeepneuralnetworks(DNN)tocapturespeakercharacteris- theexistingtrainingdata,andachievesasigniﬁcantimprovementfor
ticsiscurrentlyaveryactiveresearcharea. Inourapproach,repre- thex-vectorsystem.Incomparingwithx-vectors,wealsocontribute
sentationscalledx-vectorsareextractedfromaDNNandusedlike astudyofaugmentationini-vectorsystems.
i-vectors.ThispaperbuildsonourrecentDNNembeddingarchitec-
ture[1]. Weshowthatartiﬁciallyaugmentingthetrainingdatawith
noisesandreverberationisahighlyeffectivestrategyforimproving
performanceinDNNembeddingsystems. 2. SPEAKERRECOGNITIONSYSTEMS
Most speaker recognition systems are based on i-vectors [2].
The standard approach consists of a universal background model This section describes the speaker recognition systems developed
(UBM),andalargeprojectionmatrixTthatarelearnedinanunsu- forthisstudy,whichconsistoftwoi-vectorbaselinesandtheDNN
pervisedwaytomaximizethedatalikelihood. Theprojectionmaps x-vectorsystem.AllsystemsarebuiltusingtheKaldispeechrecog-
high-dimensional statistics from the UBM into a low-dimensional nitiontoolkit[21].
representation,knownasani-vector.Aprobabilisticlineardiscrimi-
nantanalysis(PLDA)[3]classiﬁerisusedtocomparei-vectors,and
enablesame-or-differentspeakerdecisions[4,5,6].
TheDNNsmostoftenfoundinspeakerrecognitionaretrained 2.1. Acoustici-vector
asacousticmodelsforautomaticspeechrecognition(ASR),andare
thenusedtoenhancephoneticmodelinginthei-vectorUBM:either A traditional i-vector system based on the GMM-UBM recipe de-
posteriorsfromtheASRDNNreplacethosefromaGaussianmix- scribedin[11]servesasouracoustic-featurebaselinesystem. The
turemodel(GMM)[7,8],orbottleneckfeaturesareextractedfrom featuresare20MFCCswithaframe-lengthof25msthataremean-
theDNNandcombinedwithacousticfeatures[9]. Ineithercase,if normalized over a sliding window of up to 3 seconds. Delta and
theASRDNNistrainedonin-domaindata,theimprovementover acceleration are appended to create 60 dimension feature vectors.
traditional acoustic i-vectors is substantial [10, 11, 12]. However, Anenergy-basedspeechactivitydetection(SAD)systemselectsfea-
thisapproachintroducestheneedfortranscribedtrainingdataand tures corresponding to speech frames. The UBM is a 2048 com-
greatlyincreasescomputationalcomplexitycomparedtotraditional ponent full-covariance GMM. The system uses a 600 dimensional
i-vectors. i-vectorextractorandPLDAforscoring(seeSection2.4).2.2. Phoneticbottlenecki-vector getherandpropagatedthroughsegment-levellayersandﬁnallythe
softmaxoutputlayer. Thenonlinearitiesareallrectiﬁedlinearunits
This i-vector system incorporates phonetic bottleneck features
(ReLUs).
(BNF) from an ASR DNN acoustic model and is similar to [9].
The DNN is trained to classify the N speakers in the training
The DNN is a time-delay acoustic model with p-norm nonlineari-
data. A training example consists of a chunk of speech features
ties.TheASRDNNistrainedontheFisherEnglishcorpusanduses
(about3secondsaverage),andthecorrespondingspeakerlabel.Af-
thesamerecipeandarchitectureasthesystemdescribedinSection
tertraining,embeddingsareextractedfromtheafﬁnecomponentof
2.2of[11], exceptthatthepenultimatelayerisreplacedwitha60
layer segment6. Excluding the softmax output layer and segment7
dimensionallinearbottlenecklayer. Excludingthesoftmaxoutput
(because they are not needed after training) there is a total of 4.2
layer, which is not needed to compute BNFs, the DNN has 9.2
millionparameters.
millionparameters.
The BNFs are concatenated with the same 20 dimensional
MFCCs described in Section 2.1 plus deltas to create 100 dimen- 2.4. PLDAclassiﬁer
sional features. The remaining components of the system (feature
The same type of PLDA [3] classiﬁer is used for the x-vector and
processing,UBM,i-vectorextractor,andPLDAclassiﬁer)areiden-
i-vector systems. The representations (x-vectors or i-vectors) are
ticaltotheacousticsysteminSection2.1.
centered,andprojectedusingLDA.TheLDAdimensionwastuned
on the SITW development set to 200 for i-vectors and 150 for
2.3. Thex-vectorsystem x-vectors. After dimensionality reduction, the representations are
length-normalized and modeled by PLDA. The scores are normal-
Thissectiondescribesthex-vectorsystem. ItisbasedontheDNN
izedusingadaptives-norm[22].
embeddingsin[1]anddescribedingreaterdetailthere.
Oursoftwareframework hasbeenmadeavailableinthe Kaldi
toolkit.AnexamplerecipeisinthemainbranchofKaldiathttps: 3. EXPERIMENTALSETUP
//github.com/kaldi-asr/kaldi/tree/master/egs/
sre16/v2 and a pretrained x-vector system can be downloaded 3.1. Trainingdata
from http://kaldi-asr.org/models.html. The recipe
Thetrainingdataconsistsofbothtelephoneandmicrophonespeech,
and model are similar to the x-vector system described in Section
thebulkofwhichisinEnglish.Allwidebandaudioisdownsampled
4.4.
to8kHz.
TheSWBDportionconsistsofSwitchboard2Phases1,2,and3
Layer Layercontext Totalcontext Inputxoutput
aswellasSwitchboardCellular.Intotal,theSWBDdatasetcontains
frame1 [t−2,t+2] 5 120x512
about 28k recordings from 2.6k speakers. The SRE portion con-
frame2 {t−2,t,t+2} 9 1536x512
sistsofNISTSREsfrom2004to2010alongwithMixer6andcon-
frame3 {t−3,t,t+3} 15 1536x512
tainsabout63krecordingsfrom4.4kspeakers. Intheexperiments
frame4 {t} 15 512x512
inSections4.1–4.4theextractors(UBM/TorembeddingDNN)are
frame5 {t} 15 512x1500
trainedonSWBDandSREandthePLDAclassiﬁersaretrainedon
statspooling [0,T) T 1500Tx3000
justSRE.DataaugmentationisdescribedinSection3.3andisap-
segment6 {0} T 3000x512
pliedtothesedatasetsasexplainedthroughoutSection4.
segment7 {0} T 512x512
InthelastexperimentinSection4.5weincorporateaudiofrom
softmax {0} T 512xN thenewVoxCelebdataset[19]intobothextractorandPLDAtrain-
inglists. Thedatasetconsistsofvideosfrom1,251celebrityspeak-
ers. Although SITW and VoxCeleb were collected independently,
Table1. TheembeddingDNNarchitecture. x-vectorsareextracted
wediscoveredanoverlapof60speakersbetweenthetwodatasets.
at layer segment6, before the nonlinearity. The N in the softmax
WeremovedtheoverlappingspeakersfromVoxCelebpriortousing
layercorrespondstothenumberoftrainingspeakers.
itfortraining. Thisreducesthesizeofthedatasetto1,191speakers
andabout20krecordings.
Thefeaturesare24dimensionalﬁlterbankswithaframe-length TheASRDNNusedinthei-vector(BNF)systemwastrained
of25ms,mean-normalizedoveraslidingwindowofupto3seconds. ontheFisherEnglishcorpus. Toachievealimitedformofdomain
The same energy SAD as used in the baseline systems ﬁlters out adaptation,thedevelopmentdatafromSITWandSRE16ispooled
nonspeechframes. andusedforcenteringandscorenormalization.Noaugmentationis
TheDNNconﬁgurationisoutlinedinTable1.Supposeaninput appliedtotheselists.
segmenthasT frames.Theﬁrstﬁvelayersoperateonspeechframes,
withasmalltemporalcontextcenteredatthecurrentframet. For
3.2. Evaluation
example,theinputtolayerframe3isthesplicedoutputofframe2,at
framest−3,tandt+3.Thisbuildsonthetemporalcontextofthe Ourevaluationconsistsoftwodistinctdatasets:SpeakersintheWild
earlierlayers,sothatframe3seesatotalcontextof15frames. (SITW)Core[23]andtheCantoneseportionoftheNISTSRE2016
ThestatisticspoolinglayeraggregatesallT frame-leveloutputs evaluation(SRE16)[24].SITWconsistsofunconstrainedvideoau-
from layer frame5 and computes its mean and standard deviation. dio of English speakers, with naturally occurring noises, reverber-
Thestatisticsare1500dimensionalvectors,computedonceforeach ation, as well as device and codec variability. The SRE16 portion
inputsegment. Thisprocessaggregatesinformationacrossthetime consists of Cantonese conversational telephone speech. Both en-
dimensionsothatsubsequentlayersoperateontheentiresegment. roll and test SITW utterances vary in length form 6–240 seconds.
InTable1,thisisdenotedbyalayercontextof{0}andatotalcon- ForSRE16, theenrollmentutterancescontainabout60secondsof
text of T. The mean and standard deviation are concatenated to- speechwhilethetestutterancesvaryfrom10–60seconds.SITWCore SRE16Cantonese
EER(%) DCF10−2 DCF10−3 EER(%) DCF10−2 DCF10−3
i-vector(acoustic) 9.29 0.621 0.785 9.23 0.568 0.741
4.1 Originalsystems i-vector(BNF) 9.10 0.558 0.719 9.68 0.574 0.765
x-vector 9.40 0.632 0.790 8.00 0.491 0.697
i-vector(acoustic) 8.64 0.588 0.755 8.92 0.544 0.717
4.2 PLDAaug. i-vector(BNF) 8.00 0.514 0.689 8.82 0.532 0.726
x-vector 7.56 0.586 0.746 7.45 0.463 0.669
i-vector(acoustic) 8.89 0.626 0.790 9.20 0.575 0.748
4.3 Extractoraug. i-vector(BNF) 7.27 0.533 0.730 8.89 0.569 0.777
x-vector 7.19 0.535 0.719 6.29 0.428 0.626
i-vector(acoustic) 8.04 0.578 0.752 8.95 0.555 0.720
PLDAand
4.4 i-vector(BNF) 6.49 0.492 0.690 8.29 0.534 0.749
extractoraug.
x-vector 6.00 0.488 0.677 5.86 0.410 0.593
i-vector(acoustic) 7.45 0.552 0.723 9.23 0.557 0.742
4.5 Incl.VoxCeleb i-vector(BNF) 6.09 0.472 0.660 8.12 0.523 0.751
x-vector 4.16 0.393 0.606 5.71 0.399 0.569
Table 2. Results using data augmentation in various systems. “Extractor” refers to either the UBM/T or the embedding DNN. For each
experiment,thebestresultsareboldface.
Wereportresultsintermsofequalerror-rate(EER)andthemin- 4.1. Originalsystems
imumofthenormalizeddetectioncostfunction(DCF)atP =
Target
10−2 and P = 10−3. Note that the SRE16 results have not Inthissection,weevaluatesystemswithoutdataaugmentation.The
Target
extractors are trained on the SWBD and SRE datasets described
been“equalized[24].”
in Section 3.1. The PLDA classiﬁers are trained on just the SRE
dataset. Withoutusingaugmentation,thebestresultsonSITWare
3.3. Dataaugmentation
obtainedbyi-vector(BNF),whichis12%betterthanthex-vector
Augmentation increases the amount and diversity of the existing system at DCF10−2. The acoustic i-vector system also achieves
training data. Our strategy employs additive noises and reverber- slightlylowererror-ratesthanthex-vectorsystemonSITW.How-
ation. Reverberation involves convolving room impulse responses ever, even without augmentation, the best results for SRE16 Can-
(RIR) with audio. We use the simulated RIRs described by Ko et toneseareobtainedbythex-vectors. IntermsofDCF10−2, these
al. in[25],andthereverberationitselfisperformedwiththemulti- embeddings are about 14% better than either i-vector system. We
conditiontrainingtoolsintheKaldiASpIRErecipe[21]. Foraddi- observethati-vector(BNF)hasnoadvantageoveri-vector(acous-
tivenoise,weusetheMUSANdataset,whichconsistsofover900 tic)forthisCantonesespeech. Thisechoesrecentstudiesthathave
noises,42hoursofmusicfromvariousgenresand60hoursofspeech foundthatthelargegainsachievedbyBNFsinEnglishspeechare
fromtwelvelanguages[26].BothMUSANandtheRIRdatasetsare notnecessarilytransferabletonon-Englishdata[27].
freelyavailablefromhttp://www.openslr.org.
Weusea3-foldaugmentationthatcombinestheoriginal“clean”
4.2. PLDAaugmentation
traininglistwithtwoaugmentedcopies.Toaugmentarecording,we
choosebetweenoneofthefollowingrandomly: In this experiment, the augmentation strategy described in Section
• babble: Threetosevenspeakers arerandomlypickedfrom 3.3isappliedtoonlythePLDAtraininglist. Weusethesameex-
MUSANspeech,summedtogether,thenaddedtotheoriginal tractorsastheprevioussection,whichweretrainedontheoriginal
signal(13-20dBSNR). datasets. PLDAaugmentationresultsinaclearimprovementforall
three systems relative to Section 4.1. However, it appears that the
• music: A single music ﬁle is randomly selected from MU-
x-vectorsmaybeneﬁtfromthePLDAaugmentationmorethanthe
SAN, trimmed or repeated as necessary to match duration,
baseline systems. On SITW, the x-vector system achieves slightly
andaddedtotheoriginalsignal(5-15dBSNR).
lowererror-ratesthani-vector(acoustic),butcontinuestolagbehind
• noise: MUSAN noises are added at one second intervals i-vector(BNF)atmostoperatingpoints. OnSRE16, thex-vectors
throughouttherecording(0-15dBSNR). maintainanadvantageoverthei-vectorsbyabout14%inDCF10−2.
• reverb:Thetrainingrecordingisartiﬁciallyreverberatedvia
convolutionwithsimulatedRIRs.
4.3. Extractoraugmentation
4. RESULTS Wenowapplydataaugmentationtotheextractor(UBM/Torem-
bedding DNN) training lists but not the PLDA list. The effect of
ThemainresultsarepresentedinTable2andarereferencedthrough- augmentingtheUBM/Tisinconsistentinthei-vectorsystem. This
outSections4.1–4.5. Wecompareperformanceoftwoi-vectorsys- observation is supported by prior studies on i-vectors, which have
tems, labeled i-vector (acoustic) and i-vector (BNF), with the x- found that augmentation is only effective in the PLDA classiﬁer
vector system. ThesystemsaredescribedinSections2.1, 2.2and [28,29].Ontheotherhand,augmentingtheembeddingDNNtrain-
2.3,respectively.Throughoutthefollowingsections,weusetheterm inglistresultsinalargeimprovement.Incontrasttothei-vectorsys-
extractortorefertoeithertheUBM/TortheembeddingDNN. tems,thisisconsiderablymoreeffectivethanaugmentingthePLDAtraining list. On SITW, the x-vector system achieves lower error-   60  
i-vector (acoustic)
ratesthani-vector(acoustic)andhasnowcaughtuptothei-vector
i-vector (BNF)
(BNF)system.OnSRE16,thex-vectorsarenow25%betterthanthe   40   x-vector
i-vectorsinDCF10−2,whichisalmostdoubletheimprovementthe
DNNembeddingshadwithPLDAaugmentationalone.Theﬁndings
inthissectionindicatethatdataaugmentationisonlybeneﬁcialfor %)  20  
extractorstrainedwithsupervision. y (in 
bilit  10  
a
4.4. PLDAandextractoraugmentation b   5   
o
pr
Intheprevioussections,wesawthatPLDAaugmentationwashelp- ss    2   
fulinbothi-vectorandDNNembeddingsystems,althoughextractor Mi
  1   
augmentationwasonlyclearlybeneﬁcialintheembeddingsystem.
Inthisexperiment,weapplydataaugmentationtoboththeextractor  0.5  
and PLDA training lists. We continue to use SWBD and SRE for
extractortrainingandonlySREforPLDA.OnSITWthex-vectors   0.1 
arenow10-25%betterthani-vector(acoustic)andareslightlybetter  0.01    0.1   0.5    1     2     5     10    20     40     60  
False Alarm probability (in %)
thani-vector(BNF)atalloperatingpoints. OnSRE16Cantonese,
the x-vectors continue to maintain the large lead over the i-vector
systemsestablishedinSection4.3. Fig.2.DETcurvefortheSITWCoreusingSection4.5systems.
4.5. IncludingVoxCeleb Theseresultsareillustratedbydetectionerrortradeoff(DET)plots
inFigures1and2.
  60  
i-vector (acoustic) 5. CONCLUSIONS
i-vector (BNF)
  40   x-vector This paper studied DNN embeddings for speaker recognition. We
foundthatdataaugmentationisaneasilyimplementedandeffective
%)  20   strategyforimprovingtheirperformance.Wealsomadethex-vector
n  system–ourimplementationofDNNembeddings–availableinthe
y (i Kalditoolkit.Wefoundthatthex-vectorsystemsigniﬁcantlyoutper-
bilit  10   formedtwostandardi-vectorbaselinesonSRE16Cantonese. After
ba   5    includingalargeamountofaugmentedmicrophonespeech, thex-
o
pr vectors achieved much lower error-rates than our best baseline on
ss    2    Speakers in the Wild. Bottleneck features from an ASR DNN are
Mi usedinourbesti-vectorsystem,andsoitrequirestranscribeddata
  1   
during training. On the other hand, the x-vector DNN needs only
 0.5  
speakerlabelstotrain,makingitpotentiallyidealfordomainswith
little transcribed speech. More generally, it appears that x-vectors
  0.1  are now a strong contender for next-generation representations for
 0.01    0.1   0.5    1     2     5     10    20     40     60  
speakerrecognition.
False Alarm probability (in %)
Fig.1. DETcurvefortheCantoneseportionofNISTSRE16using 6. ACKNOWLEDGMENTS
Section4.5systems.
This material is based upon work supported by the National Sci-
ence Foundation Graduate Research Fellowship under Grant No.
ThetrainingdatainSections4.1–4.4isdominatedbytelephone
1232825.ThisworkwaspartiallysupportedbyNSFGrantNoCRI-
speech. Inthisexperiment,weexploretheeffectofaddingalarge
1513128. Anyopinion, ﬁndings,andconclusionsorrecommenda-
amount of microphone speech to the systems in Section 4.4. The
tionsexpressedinthismaterialarethoseoftheauthors(s)anddonot
VoxCelebdataset[19]isaugmented,andaddedtoboththeextractor
necessarilyreﬂecttheviewsoftheNationalScienceFoundation.
and PLDA lists. As noted in Section 3.1, we found 60 speakers
whichoverlapwithSITW;allspeechforthesespeakerswasremoved
fromthetraininglists. 7. REFERENCES
On SITW, both i-vector and x-vector systems improve signif-
[1] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudan-
icantly. However, the x-vector exploits the large increase in the
pur, “Deep neural network embeddings for text-independent
amount of in-domain data better than the i-vector systems. Com-
speakerveriﬁcation,” Proc.Interspeech,pp.999–1003,2017.
paredtoi-vector(acoustic),thex-vectorsarebetterby44%inEER
and29%inDCF10−2.Comparedtothei-vector(BNF)system,itis [2] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouel-
nowbetterby32%inEERand17%inDCF10−2. OnSRE16,the let, “Front-endfactoranalysisforspeakerveriﬁcation,” IEEE
i-vectorsystemsremainroughlythesamecomparedtoSection4.4, TransactionsonAudio,Speech,andLanguageProcessing,vol.
butthex-vectorsimproveonalloperatingpointsbyasmallamount. 19,no.4,pp.788–798,2011.[3] S.Ioffe,“Probabilisticlineardiscriminantanalysis,”Computer [19] A.Nagrani,J.S.Chung,andA.Zisserman,“Voxceleb:alarge-
Vision–ECCV2006,pp.531–542,2006. scalespeakeridentiﬁcationdataset,” inInterspeech,2017.
[4] P.Kenny,“Bayesianspeakerveriﬁcationwithheavy-tailedpri- [20] C. Zhang and K. Koishida, “End-to-end text-independent
ors.,” inOdyssey,2010,p.14. speakerveriﬁcationwithtripletlossonshortutterances,”Proc.
[5] N. Bru¨mmer and E. De Villiers, “The speaker partitioning Interspeech,pp.1487–1491,2017.
problem.,” inOdyssey,2010,p.34. [21] D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
[6] D.Garcia-RomeroandC.Espy-Wilson, “Analysisofi-vector N. Goel, M. Hannemann, P. Motl´ıcˇek, Y. Qian, P. Schwarz,
lengthnormalizationinspeakerrecognitionsystems.,” inIn- etal., “TheKaldispeechrecognitiontoolkit,” inProceedings
terspeech,2011,pp.249–252. oftheAutomaticSpeechRecognition&Understanding(ASRU)
Workshop,2011.
[7] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, “A novel
scheme for speaker recognition using a phonetically-aware [22] D. Sturim and D. Reynolds, “Speaker adaptive cohort se-
deepneuralnetwork,” in2014IEEEInternationalConference lection for tnorm in text-independent speaker veriﬁcation,”
onAcoustics,SpeechandSignalProcessing(ICASSP).IEEE, in Acoustics, Speech, and Signal Processing, 2005. Proceed-
2014,pp.1695–1699. ings.(ICASSP’05). IEEE International Conference on. IEEE,
2005,vol.1,pp.I–741.
[8] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam,
“Deep neural networks for extracting Baum-Welch statistics [23] M.McLaren,L.Ferrer,D.Castan,andA.Lawson, “The2016
forspeakerrecognition,” inProc.Odyssey,2014. speakersinthewildspeakerrecognitionevaluation.,” inInter-
[9] M. McLaren, Y. Lei, and L. Ferrer, “Advances in deep neu- speech,2016,pp.823–827.
ralnetworkapproachestospeakerrecognition,” inAcoustics, [24] “NIST speaker recognition evaluation 2016,”
SpeechandSignalProcessing(ICASSP),2015IEEEInterna- https://www.nist.gov/itl/iad/mig/
tionalConferenceon.IEEE,2015,pp.4814–4818. speaker-recognition-evaluation-2016/,2016.
[10] D.Garcia-Romero,X.Zhang,A.McCree,andD.Povey, “Im- [25] T.Ko,V.Peddinti,D.Povey,M.Seltzer,andS.Khudanpur,“A
provingspeakerrecognitionperformanceinthedomainadap- study on data augmentation of reverberant speech for robust
tationchallengeusingdeepneuralnetworks,” inSpokenLan- speechrecognition,” inAcoustics,SpeechandSignalProcess-
guage Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, ing(ICASSP),2017IEEEInternationalConferenceon.IEEE,
pp.378–383. 2017,pp.5220–5224.
[11] D.Snyder,D.Garcia-Romero,andD.Povey,“Timedelaydeep
[26] D. Snyder, G Chen, and D. Povey, “MUSAN: A Music,
neuralnetwork-baseduniversalbackgroundmodelsforspeaker
Speech,andNoiseCorpus,”2015, arXiv:1510.08484v1.
recognition,” in 2015 IEEE Workshop on Automatic Speech
RecognitionandUnderstanding(ASRU).IEEE,2015,pp.92– [27] O. Novotny´, P. Mateˇjka, O. Glembeck, O Plchot, F. Gre´zl,
97. L. Burget, and J. Cˇernocky´, “Analysis of the dnn-based sre
systems in multi-language conditions,” in Spoken Language
[12] S. O. Sadjadi, J. Pelecanos, and S. Ganapathy, “The ibm
TechnologyWorkshop(SLT).IEEE,2016.
speaker recognition system: Recent advances and error anal-
ysis,” Interspeech,pp.3633–3637,2016. [28] Y. Lei, L. Burget, L. Ferrer, M. Graciarena, and N. Scheffer,
“Towardsnoise-robustspeakerrecognitionusingprobabilistic
[13] G.Heigold,I.Moreno,S.Bengio,andN.Shazeer,“End-to-end
lineardiscriminantanalysis,” inAcoustics,SpeechandSignal
text-dependent speaker veriﬁcation,” in 2016 IEEE Interna-
Processing(ICASSP),2012IEEEInternationalConferenceon.
tionalConferenceonAcoustics,SpeechandSignalProcessing
IEEE,2012,pp.4253–4256.
(ICASSP).IEEE,2016,pp.5115–5119.
[29] D.Garcia-Romero,X.Zhou,andC.Espy-Wilson, “Multicon-
[14] Y.Konig, L.Heck, M.Weintraub, andK.Sonmez, “Nonlin-
dition training of Gaussian plda models in i-vector space for
eardiscriminantfeatureextractionforrobusttext-independent
noiseandreverberationrobustspeakerrecognition,” in2012
speaker recognition,” in Proc. RLA2C, ESCA workshop on
IEEEInternationalConferenceonAcoustics,SpeechandSig-
SpeakerRecognitionanditsCommercialandForensicAppli-
nalProcessing(ICASSP).IEEE,2012,pp.4257–4260.
cations,1998.
[15] L. Heck, Y. Konig, K. Sonmez, and M. Weintraub, “Ro-
bustnesstotelephonehandsetdistortioninspeakerrecognition
bydiscriminativefeaturedesign,” inSpeechCommunication,
2000,vol.31,pp.181–192.
[16] A. Salman, Learning speaker-speciﬁc characteristics with
deepneuralarchitecture, Ph.D.thesis,UniversityofManch-
ester,2012.
[17] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
Y. Carmiel, and S. Khudanpur, “Deep neural network-based
speaker embeddings for end-to-end speaker veriﬁcation,” in
SpokenLanguageTechnologyWorkshop(SLT).IEEE,2016.
[18] S.Zhang,Z.Chen,Y.Zhao,J.Li,andY.Gong, “End-to-end
attention based text-dependent speaker veriﬁcation,” in Spo-
kenLanguageTechnologyWorkshop(SLT),2016IEEE.IEEE,
2016,pp.171–178.